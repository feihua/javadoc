# spark-2.4.5-bin-hadoop2.6安装和使用

本次使用3台主机进行集群部署

## 1.版本

|**组件**|**版本**|**备注**|
|:----:|:----|:----:|:----|:----:|:----|
| centos |           64位            |                           7.x以上                            |
|  jdk   |       jdk1.8.0_201        |                      oracal官方网站下载                      |
| hadoop |  hadoop-2.6.0-cdh5.16.2   |              http://archive.cloudera.com上下载               |
| spark  | spark-2.4.5-bin-hadoop2.6 | https://mirror.bit.edu.cn/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.6.tgz |


## 2.主机规划

|**ip**|**host**|**安装软件**|
|:----:|:----|:----:|:----|:----:|:----|
| 10.168.14.49 | yrz-elk-redis01 | jdk，hadoop,spark |
| 10.168.14.50 | yrz-elk-redis02 | jdk，hadoop,spark |
| 10.168.14.51 | yrz-elk-redis03 | jdk，hadoop,spark |


## 3.由于之前已经安装过jdk和hadoop了，在这里不再进行描述

## 4.安装到/usr/local/nlp/

```powershell
tar -zxvf spark-2.4.6-bin-hadoop2.6.tgz && mv spark-2.4.6-bin-hadoop2.6 spark && cd spark
```
## 5.修改配置文件

### 5.1修改 spark-env.sh

```powershell
echo 'export SPARK_SSH_OPTS="-p 42214"' >>conf/spark-env.sh
echo 'export JAVA_HOME=/usr/local/software/jdk1.8.0_73' >>conf/spark-env.sh
echo 'export SPARK_MASTER_IP=hadoop101' >>conf/spark-env.sh
echo 'export HADOOP_CONF_DIR=/usr/local/software/hadoop/etc/hadoop' >>conf/spark-env.sh
echo 'export SPARK_MASTER_PORT=8108' >>conf/spark-env.sh
echo 'export SPARK_MASTER_WEBUI_PORT=8109' >>conf/spark-env.sh
echo 'export SPARK_WORKER_WEBUI_PORT=8110' >>conf/spark-env.sh
```

### 5.2修改slaves

```powershell
echo 'hadoop101' >>conf/slaves
echo 'hadoop102' >>conf/slaves
echo 'hadoop103' >>conf/slaves
cat >a.log<<EOF
yrz-elk-redis05
yrz-elk-redis06
yrz-elk-redis07
EOF
```
## 6.拷贝到其他节点

```powershell
scp -P 42214 -r spark uaren@yrz-elk-redis02:/usr/local/nlp/
scp -P 42214 -r spark uaren@yrz-elk-redis03:/usr/local/nlp/
scp -r spark hadoop@hadoop102:/usr/local/software/
scp -r spark hadoop@hadoop103:/usr/local/software/
```
## 7.配置环境变量

echo 'export SPARK_HOME=/usr/local/software/spark' >>/home/hadoop/.bash_profile

echo 'export PATH=$PATH:$SPARK_HOME/bin' >>/home/hadoop/.bash_profile

source /home/hadoop/.bash_profile

## 8.启动和停止

```powershell
启动
/usr/local/software/spark/sbin/start-all.sh
停止
/usr/local/software/spark/sbin/stop-all.sh
```
## 9.验证

```powershell
/bin/run-example SparkPi 2>&1 | grep "Pi is roughly"
```
## 10.停止

```powershell
/usr/local/software/spark/sbin/stop-all.sh
```
## 常用连接

spark-web-ui: http://192.168.62.129:8109

## 11.spark shell的使用

### 11.1启动

```
bin/spark-shell --conf spark.ui.port=8112
```
### 11.2上传文件

```
hadoop fs -put README.md /
```
### 11.3执行代码

sc.textFile("hdfs://hadoop101:8100/user/README.txt").flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey( _+_).collect().foreach(println)

```

```

## 12.hive和spark整合

### 12.1把hive-site.xml拷贝到spark中

```
cp hive/conf/hive-site.xml spark/conf/
```
### 12.2启动spark-shell

```
/usr/local/software/spark/bin/spark-shell --conf spark.ui.port=8112
或者
/usr/local/software/spark/bin/spark-sql --conf spark.ui.port=8112
```
### 12.3查看

```
spark.sql("use acappr")
spark.sql("show tables").show(false)
```
### 12.4代码实现hive-sql

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
<modelVersion>4.0.0</modelVersion>
<groupId>com.spark.demo</groupId>
<artifactId>sparkdemo</artifactId>
<version>1.0-SNAPSHOT</version>
<properties>
<spark.version>2.4.5</spark.version>
<scala.version>2.11.12</scala.version>
<log4j.version>1.2.17</log4j.version>
<slf4j.version>1.7.22</slf4j.version>
<fastjson.version>1.2.47</fastjson.version>
<httpclient.version>4.5.5</httpclient.version>
<httpmime.version>4.3.6</httpmime.version>
<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
<project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
<java.version>1.8</java.version>
</properties>
<dependencies>
<!-- 此处放日志包，所有项目都要引用 -->
<!-- 所有子项目的日志框架 -->
<dependency>
<groupId>org.slf4j</groupId>
<artifactId>jcl-over-slf4j</artifactId>
<version>${slf4j.version}</version>
</dependency>
<dependency>
<groupId>org.slf4j</groupId>
<artifactId>slf4j-api</artifactId>
<version>${slf4j.version}</version>
</dependency>
<dependency>
<groupId>org.slf4j</groupId>
<artifactId>slf4j-log4j12</artifactId>
<version>${slf4j.version}</version>
</dependency>
<!-- 具体的日志实现 -->
<dependency>
<groupId>log4j</groupId>
<artifactId>log4j</artifactId>
<version>${log4j.version}</version>
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-core_2.11</artifactId>
<version>${spark.version}</version>
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-hive_2.11</artifactId>
<version>${spark.version}</version>
<!--<scope>provided</scope>-->
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-sql_2.11</artifactId>
<version>${spark.version}</version>
</dependency>
<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>5.1.47</version>
</dependency>
<dependency>
<groupId>org.codehaus.janino</groupId>
<artifactId>commons-compiler</artifactId>
<version>3.0.9</version>
</dependency>
<dependency>
<groupId>org.codehaus.janino</groupId>
<artifactId>janino</artifactId>
<version>3.0.9</version>
</dependency>
</dependencies>
<build>
<plugins>
<!-- 该插件用于将Scala代码编译成class文件 -->
<plugin>
<groupId>net.alchim31.maven</groupId>
<artifactId>scala-maven-plugin</artifactId>
<version>3.4.6</version>
<executions>
<execution>
<!-- 声明绑定到maven的compile阶段 -->
<goals>
<goal>compile</goal>
</goals>
</execution>
</executions>
</plugin>
</plugins>
</build>
</project>
```
```scala
package com.liufeihua.spark.demo
import org.apache.spark.sql.SparkSession
/**
* 描述: hive-sql
* 作者: 刘飞华
* 日期: 2020/4/17 14:26
*/
object SparkDemoApp {
def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.master("local[*]")
.appName("SparkDemoApp")
.enableHiveSupport()
.getOrCreate()
spark.sql("use test")
spark.sql("select * from student").show(false)
spark.stop()
}
}
```

## 13.提交

### 13.1本地提交

```shell
 ./spark-submit --class org.apache.spark.examples.SparkPi --master local /usr/local/software/spark/examples/jars/spark-examples_2.11-2.4.6.jar
```
### 13.2独立前端模式

```shell
 ./spark-submit --class org.apache.spark.examples.SparkPi --master spark://hadoop101:8108 /usr/local/software/spark/examples/jars/spark-examples_2.11-2.4.6.jar
```
### 13.3独立后端模式

```shell
./spark-submit --master spark://hadoop101:8108 --deploy-mode cluster --class com.liufeihua.StatStreamingApp spark-example.jar
```
### 13.4yarn前端模式

```shell
 ./spark-submit --class org.apache.spark.examples.SparkPi --master yarn /usr/local/software/spark/examples/jars/spark-examples_2.11-2.4.6.jar
./spark-submit --class com.liufeihua.StatStreamingApp --master yarn --deploy-mode client spark-example.jar
```
### 13.5yarn后端模式

```plain
 ./spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster/usr/local/software/spark/examples/jars/spark-examples_2.11-2.4.6.jar
./spark-submit --class com.liufeihua.StatStreamingApp --master yarn --deploy-mode cluster  spark-example.jar
```
