{"./":{"url":"./","title":"前言","keywords":"","body":"声明:内容来源于网上收集，不能用于商业用途，如需删除请告知 联系方式wx:ABCzzzxx 一、运维专题 二、源码专题 三、并发专题 四、调优专题 五、分布式专题 六、微服务专题 七、项目实战专题 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 14:15:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/":{"url":"automation/","title":"一、运维专题","keywords":"","body":"运维专题 本专题主要概括了在开发过程中的一些基本知识，例如git的版本控制代码，Maven依赖管理，Jenkins自动化部署，K8s服务编排和ServiceMesh服务网格这个5个方面 Git版本控制 1.Git基本概念与核心命令掌握 Maven依赖管理 1.Maven基本使用 Jenkins自动化 1.jenkins启动spring boot的脚本 K8s服务编排 K8s安装 1.部署一个K8s集群 2.搭建高可用的K8s集群 1.K8s相关命令 2.K8s部署应用I 3.Ingress暴露应用 ServiceMesh服务网格 1.ServiceMesh介绍 2.Istio进阶 3.Istio实战 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 14:50:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/git/":{"url":"automation/git/","title":"Git版本控制","keywords":"","body":"Git版本控制 1.Git基本概念与核心命令掌握 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:45:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/git/git-base-use.html":{"url":"automation/git/git-base-use.html","title":"1.Git基本概念与核心命令掌握","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.GIT体系概述 1.1GIT 与 svn 主要区别： 1.1.1存储方式区别 1.1.2使用方式区别 1.1.3版本管理模式区别 2.GIT 核心命令使用 2.1安装git客户端安装 2.2认识git的基本使用 2.3分支管理 2.4远程仓库管理 2.5tag 管理 2.6日志管理 概要： GIT体系概述 GIT 核心命令使用1.GIT体系概述 提问： 大家公司是用什么工具来管理代码版本？SVN、CVS、GIT GIT 和 SVN 有什么区别呢？ 1.1GIT 与 svn 主要区别： 存储方式不一样 使用方式不一样 管理模式不一样 1.1.1存储方式区别 GIT 把内容按元数据方式存储类似k/v 数据库，而 SVN 是按文件(新版 svn 已改成元数据存储) 1.1.2使用方式区别 从本地把文件推送远程服务，SVN 只需要commint而 GIT 需要add、commint、push 三个步骤 SVN 基本使用过程 Git 基本使用过程 1.1.3版本管理模式区别 git 是一个分布式的版本管理系统，而要 SVN 是一个远程集中式的管理系统 集中式 分布式 2.GIT 核心命令使用 主要内容: git 客户端安装配置 整体认识 GIT 的基本使用 分支管理 标签管理 远程仓库配置 2.1安装git客户端安装 官方客户端： httpsd://git-scm.com/downloads 其它客户端： https://tortoisegit.org/download/ 2.2认识git的基本使用 基于远程仓库克隆至本地 git clone 当前目录初始化为 git 本地仓库 git init 基于 mvn 模板创建项目 mvn archetype:generate 本地添加 添加指定文件至暂存区 git add 添加指定目录至暂存区 git add 添加所有 git add -A 将指定目录及子目录移除出暂存区 git rm --cached target -r 本地提交 提交至本地仓库 git commit file -m '提交评论' 快捷提交至本地仓库 git commit -am '快添加与提交' 2.3分支管理 查看当前分支 git branch [-avv] 基于当前分支新建分支 git branch 基于提交新建分支 git branch git branch -d {dev} 切换分支 git checkout 合并分支 git merge 解决冲突，如果因冲突导致自动合并失败，此时 status 为 mergeing 状态. 需要手动修改后重新提交（commit） 2.4远程仓库管理 查看远程配置 git remote [-v] 添加远程地址 git remote add origin http:xxx.xxx 删除远程地址 git remote remove origin 上传新分支至远程 git push --set-upstream origin master 将本地分支与远程建立关联 git branch --track --set-upstream-to=origin/test test 2.5tag 管理 查看当前 git tag 创建分支 git tag 删除分支 git tag -d 2.6日志管理 查看当前分支下所有提交日志 git log 查看当前分支下所有提交日志 git log {branch} 单行显示日志 git log --oneline 比较两个版本的区别 git log master..experiment 以图表的方式显示提交合并网络 git log --pretty=format:'%h %s' --graph Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 11:25:51 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/maven/":{"url":"automation/maven/","title":"Maven依赖管理","keywords":"","body":"Maven依赖管理 1.Maven基本使用 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:46:17 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/maven/maven-base-use.html":{"url":"automation/maven/maven-base-use.html","title":"1.Maven基本使用 ","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.maven 安装与核心概念 1.1安装 1.2maven 编译 1.3Maven打包 1.4maven 单元测试演示 1.5maven 依赖管理 2.maven核心配置 2.1依赖传播特性: 2.2依赖优先原则 2.3可选依赖 2.4排除依赖 2.5依赖范围 手动加入本地仓库 1、聚合 2、继承 3、依赖管理 4、项目属性： 3.maven 生命周期 3.1生命周期的概念与意义 3.2maven 三大生命周期与其对应的 phase(阶段) 3.3生命周期与插件的关系 3.4生命周期与插件的默认绑定 4.maven 自定义插件开发 4.1maven 插件相关概念 4.2常用插件的使用 4.3开发一个自定义插件 概要： maven 基本概念 maven 核心配置 maven 生命周期 Maven 自定义插件开发1.maven 安装与核心概念 概要： maven 安装 maven 编译(compile) 执行测试用例(test) maven 打包 maven 依懒管理 1.1安装 官网下载 Maven （http://maven.apache.org/download.cgi） 解压指定目录 配置环境变量 检查安装是否成功 （mvn -version） maven 是什么？它的基本功能是什么？编译、打包、测试、依赖管理直观感受一下 maven 编译打包的过程。 1.2maven 编译 mvn compile maven 采用了约定的方式从指项目结构中获取源码与资源文件进行编译打包。 1. 主源码文件：${project}/src/main/java 2. 主资源文件：${project}/src/main/resources 3. 测试源码文件：${project}/src/test/java 4. 测试资源文件：${project}/src/test/resources 1.3Maven打包 mvn package 1.4maven 单元测试演示 mvn test 1.5maven 依赖管理 在 pom 文件中添加 junit 依赖 junit junit 4.0 test maven 核心功能总结： maven 核心作用是编译、测试、打包。 根目录下的 pom.xml 文件设置分组 ID 与 artifactId。 maven 基于约定的方式从项目中获取源码与资源文件进行编译打包。 对于项目所依懒的组件与会本地仓库引用，如果本地仓库不存在则会从中央仓库下载。2.maven核心配置 概要： 项目依懒(内部、外部) 项目聚合与继承 项目构建配置 项目依赖是指 maven 通过依赖传播、依赖优先原则、可选依赖、排除依赖、依赖范围等特性来管理项目ClassPath。 2.1依赖传播特性: 我们的项目通常需要依赖第三方组件，而第三方组件又会依赖其它组件遇到这种情况 Maven 会将依赖网络中的所有节点都会加入 ClassPath 当中，这就是 Maven 的依赖传播特性。 * 举例演示 Spring MVC 的依赖网络 org.springframework spring-webmvc 4.0.4.RELEASE 2.2依赖优先原则 基于依赖传播特性，导致整个依赖网络会很复杂，难免会出现相同组件不同版本的情况。Maven 此时会基于依赖优先原则选择其中一个版本。 第一原则：最短路径优先。 第二原则：相同路径下配置在前的优先。 2.3可选依赖 可选依赖表示这个依赖不是必须的。通过在 添 true 表示，默认是不可选的。可选依赖不会被传递。 2.4排除依赖 即排除指定的间接依赖。通过配置 配置排除指定组件。 org.springframework spring-web 2.5依赖范围 像 junit 这个组件 我们只有在运行测试用例的时候去要用到，这就没有必要在打包的时候把 junit.jar 包过构建进去，可以通过 Mave 的依赖范围配置来达到这种目的。maven 总共支持以下四种依赖范围： compile(默认): 编译范围，编译和打包都会依赖。 provided：提供范围，编译时依赖，但不会打包进去。如：servlet-api.jar runtime：运行时范围，打包时依赖，编译不会。如：mysql-connector-java.jar test：测试范围，编译运行测试用例依赖，不会打包进去。如：junit.jar system：表示由系统中 CLASSPATH 指定。编译时依赖，不会打包进去。配合 一起使用。示例：java.home 下的 tool.jar system 除了可以用于引入系统 classpath 中包，也可以用于引入系统非 maven 收录的第三方 Jar，做法是将第三方 Jar 放置在 项目的 lib 目录下，然后配置 相对路径，但因 system 不会打包进去所以需要配合 maven-dependency-plugin 插件配合使用。当然推荐大家还是通过 将第三方 Jar 手动 install 到仓库。 手动加入本地仓库 mvn install:install-file -Dfile=abc_client_v1.20.jar -DgroupId=tuling -DartifactId=tuling-client -Dversion=1.20 -Dpackaging=jar 项目聚合与继承 1、聚合 是指将多个模块整合在一起，统一构建，避免一个一个的构建。聚合需要个父工程，然后使用 进行配置其中对应的是子工程的相对路径 xx-client xx-server 2、继承 继承是指子工程直接继承父工程 当中的属性、依赖、插件等配置，避免重复配置。 属性继承： 依赖继承： 插件继承： 上面的三个配置子工程都可以进行重写，重写之后以子工程的为准。 3、依赖管理 通过继承的特性，子工程是可以间接依赖父工程的依赖，但多个子工程依赖有时并不一至，这时就可以在父工程中加入 声明该功程需要的 JAR 包，然后在子工程中引入。 junit junit 4.12 junit junit 4、项目属性： 通过 配置 属性参数，可以简化配置。 ddd ${proName} maven 默认的属性 ${basedir} 项目根目录 ${version}表示项目版本; ${project.basedir}同${basedir}; ${project.version}表示项目版本,与${version}相同; ${project.build.directory} 构建目录，缺省为 target ${project.build.sourceEncoding}表示主源码的编码格式; ${project.build.sourceDirectory}表示主源码路径; ${project.build.finalName}表示输出文件名称; ${project.build.outputDirectory} 构建过程输出目录，缺省为 target/classes 3.maven 生命周期 知识点概要： 生命周期的概念与意义 maven 三大生命周期与其对应的 phase(阶段) 生命周期与插件的关系 生命周期与默认插件的绑定3.1生命周期的概念与意义 在项目构建时通常会包含清理、编译、测试、打包、验证、部署，文档生成等步骤，maven 统一对其进行了整理抽像成三个生命周期 (lifecycle)及各自对应的多个阶段(phase)。这么做的意义是： 每个阶段都成为了一个扩展点，可以采用不同的方式来实现，提高了扩展性与灵活性。 规范统一了 maven 的执行路径。 3.2maven 三大生命周期与其对应的 phase(阶段) maven 总共包含三大生生命周期 clean Lifecycle：清理生命周期，用于于清理项目 default Lifecycle：默认生命周期，用于编译、打包、测试、部署等 site Lifecycle站点文档生成，用于构建站点文档 |生命周期(lifecycle)|阶段(phase)|描述(describe)| |:----|:----|:----|:----|:----|:----| |clean Lifecycle|pre-clean|预清理| | |clean|清理| | |post-clean|清理之后| |default Lifecycle|validate|验证| | |initialize|初始化| | |generate-sources| | | |process-sources| | | |generate-resources| | | |process-resources| | | |compile|编译| | |process-classes| | | |generate-test-sources| | | |process-test-sources| | | |generate-test-resources| | | |process-test-resources| | | |test-compile|编译测试类| | |process-test-classes| | | |test|执行测试| | |prepare-package|构建前准备| | |package|打包构建| | |pre-integration-test| | | |integration-test| | | |post-integration-test| | | |verify|验证| | |install|上传到本地仓库| | |deploy|上传到远程仓库| |site Lifecycle|pre-site|准备构建站点| | |site|构建站点| | |post-site|构建站点之后| | |site-deploy|站点部署| 三大生命周期其相互独立执行，也可以合在一起执行。但 lifecycle 中的 phase 是有严格执行的顺序的，比如必须是先执行完 compile 才能执行 pakcage 动作，此外 phase 还有包含逻辑存在，即当你执行一个 phase 时 其前面的 phase 会自动执行。 3.3生命周期与插件的关系 生命周期的 phase 组成了项目过建的完整过程，但这些过程具体由谁来实现呢？这就是插件，maven 的核心部分代码量其实很少，其大部分实现都是由插件来完成的。比如：test 阶段就是由maven-surefire-plugin 实现。在 pom.xml 中我们可以设置指定插件目标(gogal)与 phase 绑定，当项目构建到达指定 phase 时就会触发些插件 gogal 的执行。 一个插件有时会实现多个 phas 比如：maven-compiler-plugin插件分别实现了 compile 和 testCompile。 总结： 生命周期的 阶段 可以绑定具体的插件及目标 不同配置下同一个阶段可以对应多个插件和目标 phase==>plugin==>goal(功能) 3.4生命周期与插件的默认绑定 在我们的项目当中并没有配置 maven-compiler-plugin 插件,但当我们执行 compile 阶段时一样能够执行编译操作，原因是 maven 默认为指定阶段绑定了插件实现。列如下以下两个操作在一定程度上是等价的。 mvn compile #直接执行 compile 插件目标 mvn org.apache.maven.plugins:maven-compiler-plugin:3.1:compile 4.maven 自定义插件开发 场景：mybatis自动生成 知识点： 插件的相关概念 常用插件的使用 开发一个自定义插件4.1maven 插件相关概念 插件坐标定位： 插件与普通 jar 包一样包含 一组件坐标定位属性即： groupId、artifactId、version，当使用该插件时会从本地仓库中搜索，如果没有即从远程仓库下载 org.apache.maven.plugins maven-dependency-plugin 2.10 插件执行 execution： execution 配置包含一组指示插件如何执行的属性： id： 执行器命名 phase：在什么阶段执行？ goals：执行一组什么目标或功能？ configuration：执行目标所需的配置文件？ 4.2常用插件的使用 除了通过配置的方式使用插件以外，Maven 也提供了通过命令直接调用插件目标其命令格式如下： mvn groupId:artifactId:version:goal -D{参数名} 展示 pom 的依赖关系树 mvn org.apache.maven.plugins:maven-dependency-plugin:2.10:tree 也可以直接简化版的命令，但前提必须是 maven 官方插件 mvn dependency:tree 其它常用插件： 查看 pom 文件的最终配置 mvn help:effective-pom 原型项目生成 archetype:generate #快速创建一个 WEB 程序 mvn archetype:generate -DgroupId=tuling -DartifactId=simple-webbapp -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=false #快速创建一个 java 项目 mvn archetype:generate -DgroupId=tuling -DartifactId=simple-java -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false 4.3开发一个自定义插件 实现步骤： * 创建 maven 插件项目 * 设定 packaging 为 maven-plugin * 添加插件依赖 * 编写插件实现逻辑 * 打包构建插件 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 14:25:03 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/jenkins/":{"url":"automation/jenkins/","title":"Jenkins自动化","keywords":"","body":"jenkins自动化部署 1.jenkins启动spring boot的脚本 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 15:00:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/jenkins/springboot.html":{"url":"automation/jenkins/springboot.html","title":"1.jenkins启动springboot的脚本","keywords":"","body":"jenkins启动spring boot的脚本 for i in authentication timer_task msg_center area eagle_eyes system_service jpush short_message register_center user_center project_center machine_center order_center clock_center employment_center; do pid=`ps -ef | grep $i.jar | grep -v grep | awk '{print $2}'` echo “旧应用进程id：$pid” if [ -n \"$pid\" ] then kill -9 $pid fi echo \"=== stop $i successful\" BUILD_ID=dontKillMe nohup java -jar $i/target/$i.jar --spring.profiles.active=test >/root/logs/$i-log.txt & echo \"=== start $i successful\" done Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 14:48:52 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/":{"url":"automation/k8s/","title":"K8s服务编排","keywords":"","body":"K8s服务编排 K8s安装 1.部署一个K8s集群 2.搭建高可用的K8s集群 1.K8s相关命令 2.K8s部署应用I 3.Ingress暴露应用 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:46:09 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/install.html":{"url":"automation/k8s/install.html","title":"K8s安装","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-21 17:41:19 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/kubeadm-install-one.html":{"url":"automation/k8s/kubeadm-install-one.html","title":"1.部署一个K8s集群","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 安装要求 2. 准备环境 3. 所有节点安装 Docker/kubeadm/kubelet 3.1 安装 Docker 3.2 添加阿里云 YUM 软件源 3.3 安装 kubeadm，kubelet 和 kubectl 4. 部署 Kubernetes Master 5. 加入 Kubernetes Node 6. 部署 CNI 网络插件 7. 测试 kubernetes 集群 kubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具。 这个工具能通过两条指令完成一个 kubernetes 集群的部署： # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join 1. 安装要求 在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点 禁止 swap 分区2. 准备环境 角色 IP master 192.168.62.132 node1 192.168.62.133 node2 192.168.62.134 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭selinux sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭swap swapoff -a # 临时 sed -ri 's/.*swap.*/#&/' /etc/fstab # 永久 # 根据规划设置主机名 hostnamectl set-hostname # 在master添加hosts cat >> /etc/hosts /etc/sysctl.d/k8s.conf 3. 所有节点安装 Docker/kubeadm/kubelet Kubernetes 默认 CRI（容器运行时）为 Docker，因此先安装 Docker。 3.1 安装 Docker $ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ yum -y install docker-ce-18.06.1.ce-3.el7 $ systemctl enable docker && systemctl start docker $ docker --version Docker version 18.06.1-ce, build e68fc7a $ cat > /etc/docker/daemon.json 3.2 添加阿里云 YUM 软件源 $ cat > /etc/yum.repos.d/kubernetes.repo 3.3 安装 kubeadm，kubelet 和 kubectl 由于版本更新频繁，这里指定版本号部署： $ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 $ systemctl enable kubelet 4. 部署 Kubernetes Master 在 192.168.62.132（Master）执行。 $ kubeadm init \\ --apiserver-advertise-address=192.168.62.132 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.18.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 由于默认拉取镜像地址 k8s.gcr.io 国内无法访问，这里指定阿里云镜像仓库地址。 使用 kubectl 工具： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes 5. 加入 Kubernetes Node 在 192.168.62.133/134（Node）执行。 向集群添加新节点，执行在 kubeadm init 输出的 kubeadm join 命令： $ kubeadm join 192.168.62.132:6443 --token dcxghk.5zgiiw6yk7qf5wol \\ --discovery-token-ca-cert-hash sha256:aad826e486e6728e176b14e803199a42805572ed8b266269d7581f1e244df33c 默认 token 有效期为 24 小时，当过期之后，该 token 就不可用了。这时就需要重新创建 token，操作如下： kubeadm token create --print-join-command 6. 部署 CNI 网络插件 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 默认镜像地址无法访问，sed 命令修改为 docker hub 镜像仓库。 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 或者用啊里的源 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-aliyun.yml kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE kube-flannel-ds-amd64-2pc95 1/1 Running 0 72s 7. 测试 kubernetes 集群 在 Kubernetes 集群中创建一个 pod，验证是否正常运行： $ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 --type=NodePort $ kubectl get pod,svc 访问地址：http://NodeIP:Port Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:40:07 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/kubeadm-install-all.html":{"url":"automation/k8s/kubeadm-install-all.html","title":"2.搭建高可用的K8s集群","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 安装要求 2. 准备环境 3. 所有 master 节点部署 keepalived 3.1 安装相关包和 keepalived 3.2 配置 master 节点 3.3 启动和检查 4. 部署 haproxy 4.1 安装 4.2 配置 4.3 启动和检查 5. 所有节点安装 Docker/kubeadm/kubelet 5.1 安装 Docker 5.2 添加阿里云 YUM 软件源 5.3 安装 kubeadm，kubelet 和 kubectl 6. 部署 Kubernetes Master 6.1 创建 kubeadm 配置文件 6.2 在 master1 节点执行 7.安装集群网络 8、master2 节点加入集群 8.1 复制密钥及相关文件 8.2 master2 加入集群 5. 加入 Kubernetes Node 7. 测试 kubernetes 集群 kubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具。 这个工具能通过两条指令完成一个 kubernetes 集群的部署： # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join 1. 安装要求 在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点 禁止 swap 分区2. 准备环境 角色 IP master1 192.168.44.155 master2 192.168.44.156 node1 192.168.44.157 VIP（虚拟 ip） 192.168.44.158 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭 selinux sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭 swap swapoff -a # 临时 sed -ri 's/.*swap.*/#&/' /etc/fstab # 永久 # 根据规划设置主机名 hostnamectl set-hostname # 在 master 添加 hosts cat >> /etc/hosts /etc/sysctl.d/k8s.conf 3. 所有 master 节点部署 keepalived 3.1 安装相关包和 keepalived yum install -y conntrack-tools libseccomp libtool-ltdl yum install -y keepalived 3.2 配置 master 节点 master1 节点配置 cat > /etc/keepalived/keepalived.conf master2 节点配置 cat > /etc/keepalived/keepalived.conf 3.3 启动和检查 在两台 master 节点都执行 # 启动 keepalived $ systemctl start keepalived.service 设置开机启动 $ systemctl enable keepalived.service # 查看启动状态 $ systemctl status keepalived.service 启动后查看 master1 的网卡信息 ip a s ens33 4. 部署 haproxy 4.1 安装 yum install -y haproxy 4.2 配置 两台 master 节点的配置均相同，配置中声明了后端代理的两个 master 节点服务器，指定了 haproxy 运行的端口为 16443 等，因此 16443 端口为集群的入口 cat > /etc/haproxy/haproxy.cfg 4.3 启动和检查 两台 master 都启动 # 设置开机启动 $ systemctl enable haproxy # 开启 haproxy $ systemctl start haproxy # 查看启动状态 $ systemctl status haproxy 检查端口 netstat -lntup|grep haproxy 5. 所有节点安装 Docker/kubeadm/kubelet Kubernetes 默认 CRI（容器运行时）为 Docker，因此先安装 Docker。 5.1 安装 Docker $ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ yum -y install docker-ce-18.06.1.ce-3.el7 $ systemctl enable docker && systemctl start docker $ docker --version Docker version 18.06.1-ce, build e68fc7a $ cat > /etc/docker/daemon.json 5.2 添加阿里云 YUM 软件源 $ cat > /etc/yum.repos.d/kubernetes.repo 5.3 安装 kubeadm，kubelet 和 kubectl 由于版本更新频繁，这里指定版本号部署： $ yum install -y kubelet-1.16.3 kubeadm-1.16.3 kubectl-1.16.3 $ systemctl enable kubelet 6. 部署 Kubernetes Master 6.1 创建 kubeadm 配置文件 在具有 vip 的 master 上操作，这里为 master1 $ mkdir /usr/local/kubernetes/manifests -p $ cd /usr/local/kubernetes/manifests/ $ vi kubeadm-config.yaml apiServer: certSANs: - master1 - master2 - master.k8s.io - 192.168.44.158 - 192.168.44.155 - 192.168.44.156 - 127.0.0.1 extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta1 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: \"master.k8s.io:16443\" controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.16.3 networking: dnsDomain: cluster.local podSubnet: 10.244.0.0/16 serviceSubnet: 10.1.0.0/16 scheduler: {} 6.2 在 master1 节点执行 $ kubeadm init --config kubeadm-config.yaml 按照提示配置环境变量，使用 kubectl 工具： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get nodes $ kubectl get pods -n kube-system 按照提示保存以下内容，一会要使用： kubeadm join master.k8s.io:16443 --token jv5z7n.3y1zi95p952y9p65 \\ --discovery-token-ca-cert-hash sha256:403bca185c2f3a4791685013499e7ce58f9848e2213e27194b75a2e3293d8812 \\ --control-plane 查看集群状态 kubectl get cs kubectl get pods -n kube-system 7.安装集群网络 从官方地址获取到 flannel 的 yaml，在 master1 上执行 mkdir flannel cd flannel wget -c https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装 flannel 网络 kubectl apply -f kube-flannel.yml 检查 kubectl get pods -n kube-system 8、master2 节点加入集群 8.1 复制密钥及相关文件 从 master1 复制密钥及相关文件到 master2 # ssh root@192.168.44.156 mkdir -p /etc/kubernetes/pki/etcd # scp /etc/kubernetes/admin.conf root@192.168.44.156:/etc/kubernetes # scp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*} root@192.168.44.156:/etc/kubernetes/pki # scp /etc/kubernetes/pki/etcd/ca.* root@192.168.44.156:/etc/kubernetes/pki/etcd 8.2 master2 加入集群 执行在 master1 上 init 后输出的 join 命令,需要带上参数--control-plane表示把 master 控制节点加入集群 kubeadm join master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba --control-plane 检查状态 kubectl get node kubectl get pods --all-namespaces 5. 加入 Kubernetes Node 在 node1 上执行 向集群添加新节点，执行在 kubeadm init 输出的 kubeadm join 命令： kubeadm join master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba 集群网络重新安装，因为添加了新的 node 节点 检查状态 kubectl get node kubectl get pods --all-namespaces 7. 测试 kubernetes 集群 在 Kubernetes 集群中创建一个 pod，验证是否正常运行： $ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 --type=NodePort $ kubectl get pod,svc 访问地址：http://NodeIP:Port Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:39:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/cmd.html":{"url":"automation/k8s/cmd.html","title":"1.K8s相关命令","keywords":"","body":"查看一下，相关命令： kubectl get node(s) kubectl get service(s) kubectl get deployment (deploy) kubectl get pod(s) //删除service kubectl delete service nginx //删除nginx的控制器 kubectl delete deployment nginx //删除pod kubectl delete pod nginx-6799fc88d8-zc48m（pod名字） //kubectl命令帮助 kubectl --help Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-21 17:41:19 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/Kubernetes-deployment.html":{"url":"automation/k8s/Kubernetes-deployment.html","title":"2.K8s部署应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.在Kubernetes集群中部署一个Nginx： 2.在Kubernetes集群中部署一个Tomcat： 3.K8s部署微服务（springboot程序） Kubernetes部署-容器化应用 Docker应用-->在docker里面部署一个java程序（springboot） 1.在Kubernetes集群中部署一个Nginx： kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --port=80 --type=NodePort kubectl get pod,svc 访问地址：http://NodeIP:Port 2.在Kubernetes集群中部署一个Tomcat： kubectl create deployment tomcat --image=tomcat kubectl expose deployment tomcat --port=8080 --type=NodePort 访问地址：http://NodeIP:Port 3.K8s部署微服务（springboot程序） 项目打包（jar、war）-->可以采用一些工具git、maven、jenkins 制作Dockerfile文件，生成镜像； FROM java:8 VOLUME /tmp ADD appApi.jar app.jar RUN bash -c 'touch /app.jar' RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ && echo 'Asia/Shanghai' >/etc/timezone EXPOSE 8080 ENTRYPOINT [\"java\",\"-Xmx3000m\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/app.jar\"] kubectl create deployment nginx --image= 你的镜像 你的springboot就部署好了，是以docker容器的方式运行在pod里面的； apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: springboot-k8s name: springboot-k8s spec: replicas: 1 selector: matchLabels: app: springboot-k8s strategy: {} template: metadata: creationTimestamp: null labels: app: springboot-k8s spec: containers: - image: 38-springboot-k8s-1.0.0-jar name: 38-springboot-k8s-1-0-0-jar-8ntrx imagePullPolicy: Never resources: {} args: status: {} --- apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: springboot-k8s name: springboot-k8s spec: ports: - port: 8080 protocol: TCP targetPort: 8080 nodePort: 30008 selector: app: springboot-k8s type: NodePort status: loadBalancer: {} Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-21 17:41:19 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/Ingress.html":{"url":"automation/k8s/Ingress.html","title":"3.Ingress暴露应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.NodePort 2.LoadBalancer 3.Ingress 3.1使用Ingress Nginx的步骤： 3.2采用Ingress暴露容器化应用（Nginx） kubernetes部署Spring Cloud微服务 Ingress暴露应用 1.NodePort NodePort服务是让外部请求直接访问服务的最原始方式，NodePort是在所有的节点（虚拟机）上开放指定的端口，所有发送到这个端口的请求都会直接转发到服务中的pod里； NodePort服务的YAML文件如下： apiVersion: v1 kind: Service metadata: name: my-nodeport-service selector: app: my-appspec: type: NodePort ports: - name: http port: 80 targetPort: 80 nodePort: 30008 protocol: TCP 这种方式有一个“nodePort\"的端口，能在节点上指定开放哪个端口，如果没有指定端口，它会选择一个随机端口，大多数时候应该让Kubernetes随机选择端口； 这种方式的不足： 一个端口只能供一个服务使用； 只能使用30000–32767之间的端口； 如果节点/虚拟机的IP地址发生变化，需要人工进行处理； 因此，在生产环境不推荐使用这种方式来直接发布服务，如果不要求运行的服务实时可用，或者用于演示或者临时运行一个应用可以用这种方式； 三种端口说明 ports: - name: http port: 80 targetPort: 80 nodePort: 30008 protocol: TCP nodePort 外部机器（在windows浏览器）可以访问的端口； 比如一个Web应用需要被其他用户访问，那么需要配置type=NodePort，而且配置nodePort=30001，那么其他机器就可以通过浏览器访问scheme://node:30001访问到该服务； targetPort 容器的端口，与制作容器时暴露的端口一致（Dockerfile中EXPOSE），例如docker.io官方的nginx暴露的是80端口； port Kubernetes集群中的各个服务之间访问的端口，虽然mysql容器暴露了3306端口，但外部机器不能访问到mysql服务，因为他没有配置NodePort类型，该3306端口是集群内其他容器需要通过3306端口访问该服务； kubectl expose deployment springboot-k8s --port=8080 --target-port=8080 --type=NodePort 2.LoadBalancer LoadBlancer可以暴露服务，这种方式需要向云平台申请负载均衡器，目前很多云平台都支持，但是这种方式深度耦合了云平台；（相当于是购买服务） 从外部的访问通过负载均衡器LoadBlancer转发到后端的Pod，具体如何实现要看云提供商； 3.Ingress Ingress 英文翻译为：入口、进入、进入权、进食，也就是入口，即外部请求进入k8s集群必经之口，我们看一张图： 虽然k8s集群内部署的pod、service都有自己的IP，但是却无法提供外网访问，以前我们可以通过监听NodePort的方式暴露服务，但是这种方式并不灵活，生产环境也不建议使用； Ingresss是k8s集群中的一个API资源对象，相当于一个集群网关，我们可以自定义路由规则来转发、管理、暴露服务(一组pod)，比较灵活，生产环境建议使用这种方式； Ingress不是kubernetes内置的（安装好k8s之后，并没有安装ingress），ingress需要单独安装，而且有多种类型Google Cloud Load Balancer，Nginx，Contour，Istio等等，我们这里选择官方维护的Ingress Nginx； 3.1使用Ingress Nginx的步骤： 部署Ingress Nginx； 配置Ingress Nginx规则； 3.2采用Ingress暴露容器化应用（Nginx） 部署一个容器化应用（pod），比如Nginx、SpringBoot程序； kubectl create deployment nginx --image=nginx 暴露该服务； kubectl expose deployment nginx --port=80--target-port=80--type=NodePort 部署Ingress Nginx https://github.com/kubernetes/ingress-nginx ingress-nginx是使用NGINX作为反向代理和负载均衡器的Kubernetes的Ingress控制器； kubectl apply -f[https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.2/deploy/static/provider/baremetal/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.2/deploy/static/provider/baremetal/deploy.yaml?fileGuid=i2JiSs1tqMMuCw2F) 332行修改成阿里云镜像： 阿里云镜像首页：http://dev.aliyun.com/ 修改镜像地址为： registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.33.0 添加一个配置项： 应用： kubectl apply -f deploy.yaml 查看Ingress的状态 kubectl get service -n ingress-nginx kubectl get deploy -n ingress-nginx kubectl get pods -n ingress-nginx 创建Ingress规则 kubectl apply -f ingress-nginx-rule.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-ingress spec: rules: - host: www.abc.com http: paths: - pathType: Prefix path: / backend: service: name: nginx port: number: 80 报如下错误： 解决： kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission 然后再次执行： kubectl apply -f ingress-nginx-rule.yaml 检查一下： kubectl get ing(ress) --查规则 kubernetes部署Spring Cloud微服务 项目本身打成jar包或者war包； 制作项目镜像（写Dockerfile文件）； 用k8s部署镜像（命令方式、yaml方式）； 对外暴露服务； 生成镜像： docker build -t spring-cloud-alibaba-consumer -f Dockerfile-consumer . docker build -t spring-cloud-alibaba-provider -f Dockerfile-provider . docker build -t spring-cloud-alibaba-gateway -f Dockerfile-gateway . 部署提供者： kubectl create deployment spring-cloud-alibaba-provider --image=spring-cloud-alibaba-provider --dry-run -o yaml > provider.yaml kubectl apply -f provider.yaml 部署消费者： kubectl create deployment spring-cloud-alibaba-consumer --image=spring-cloud-alibaba-consumer --dry-run -o yaml > consumer.yaml kubectl apply -f consumer.yaml kubectl expose deployment spring-cloud-alibaba-consumer --port=9090 --target-port=9090 --type=NodePort 部署网关： kubectl create deployment spring-cloud-alibaba-gateway --image=spring-cloud-alibaba-gateway --dry-run -o yaml > gateway.yaml kubectl apply -f gateway.yaml kubectl expose deployment spring-cloud-alibaba-gateway --port=80 --target-port=80 --type=NodePort 在网关上面部署ingress，统一入口； 按照上面讲解ingress暴露nginx的步骤进行就可以； kubectl get service -n ingress-nginx kubectl get deploy -n ingress-nginx kubectl get pods -n ingress-nginx 注意：deploy.yaml文件里面镜像从本地拉取； containers: image: 38-springboot-k8s-1.0.0-jar name: 38-springboot-k8s-1-0-0-jar-8ntrx imagePullPolicy: Never 把镜像拉取策略改为Never； 看pod详情： kubectl describe pods spring-cloud-alibaba-consumer-5d557f4765-d27d9 看pod运行日志： kubectl logs -f spring-cloud-alibaba-consumer-8697896544-g6rsf Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-21 17:41:19 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/ServiceMesh/":{"url":"automation/ServiceMesh/","title":"ServiceMesh服务网格","keywords":"","body":"ServiceMesh服务网格 1.ServiceMesh介绍 2.Istio进阶 3.Istio实战 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:46:29 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/ServiceMesh/base.html":{"url":"automation/ServiceMesh/base.html","title":"1.ServiceMesh介绍","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.1 单机小型机时代 1.2 垂直拆分 1.3 集群化负载均衡架构 1.4 服务化改造架构 1.5 服务治理 1.6 微服务时代 1.7 服务网格新时期 （service mesh） 1.7.1 背景 1.7.2 SideCar 1.7.3 Linkerd 1.7.4 istio 1.7.5 什么是服务网格 1.7.6 什么是Service Mesh 1.7.7 CNCF云原生组织发展和介绍 1.7.8 国内兴起的服务网格 02 Istio 基本介绍 2.1 什么是Istio 2.2 Istio特征 2.2.1 连接 2.2.2 安全 2.2.3 策略 2.2.4 观察 2.3 Istio与服务治理 2.3.1服务治理的三种形态 2.4 Istio与Kubernetes 2.4.1 Kubernetes介绍 2.4.2 Istio是Kubernetes的好帮手 2.4.3 Kubernetes是Istio的好基座 2.5 Istio与服务网格 2.5.1 时代选择服务网格 2.5.2 服务网格选择Istio 总结 01 架构的发展历史 发展历史时间轴 1.1 单机小型机时代 第一个计算机网络诞生于1969年，也就是美军的阿帕网，阿帕网能够实现与其它计算机进行联机操作，但是早期仅仅是为了军事目的而服务 2000年初，中国的网民大约890万，很多人都不知道互联网为何物，因此大多数服务业务单一且简单，采用典型的单机+数据库模式，所有的功能都写在一个应用里并进行集中部署 论坛业务、聊天室业务、邮箱业务全部都耦合在一台小型机上面，所有的业务数据也都存储在一台数据库上。 1.2 垂直拆分 随着应用的日益复杂与多样化，开发者对系统的容灾，伸缩以及业务响应能力有了更高的要求，如果小型机和数据库中任何一个出现故障，整个系统都会崩溃，若某个板块的功能需要更新，那么整个系统都需要重新发布，显然，对于业务迅速发展的万物互联网时代是不允许的。 如何保障可用性的同时快速响应业务的变化，需要将系统进行拆分，将上面的应用拆分出多个子应用。 优点：应用跟应用解耦，系统容错提高了，也解决了独立应用发布的问题 应用垂直拆分解决了应用发布的问题，但是随着用户数量的增加，单机的计算能力依旧是杯水车薪 1.3 集群化负载均衡架构 用户量越来越大，就意味着需要更多的小型机，但是小型机价格昂贵，操作维护成本高。 此时更优的选择是采用多台PC机部署同一个应用的方案，但是此时就需要对这些应用做负载均衡，因为客户端不知道请求会落到哪一个后端PC应用上的。 负载均衡可以分为硬件层面和软件层面。 硬件层面：F5 软件负载层面：LVS、Nginx、Haproxy 负载均衡的思想：对外暴露一个统一的接口，根据用户的请求进行对应规则转发，同时负载均衡还可以做限流等等 有了负载均衡之后，后端的应用可以根据流量的大小进行动态扩容，我们称为\"水平扩展\" 阿里巴巴在2008提出去“IOE”，也就是IBM小型机、Oracle数据库，EMC存储，全部改成集群化负载均衡架构，在2013年支付宝最后一台IBM小型机下线 优点：应用跟应用解耦，系统容错提高了，也解决了独立应用发布的问题，同时可以水平扩展来提供应用的并发量 1.4 服务化改造架构 虽然系统经过了垂直拆分，但是拆分之后发现在论坛和聊天室中有重复的功能，比如，用户注册、发邮件等等，一旦项目大了，集群部署多了，这些重复的功能无疑会造成资源浪费，所以会把重复功能抽取出来，名字叫\"XX服务（Service）\" 为了解决服务跟服务如何相互调用，需要一个程序之间的通信协议，所以就有了远程过程调用（RPC），作用就是让服务之间的程序调用变得像本地调用一样的简单 优点：在前面的架构之上解决了业务重用的问题 1.5 服务治理 随着业务的增大，基础服务越来越多，调用网的关系由最初的几个增加到几十上百，造成了调用链路错综复杂,需要对服务进行治理。 服务治理要求： 1、当我们服务节点数几十上百的时候，需要对服务有动态的感知，引入了注册中心 2、当服务链路调用很长的时候如何实现链路的监控 3、单个服务的异常，如何能避免整条链路的异常（雪崩），需要考虑熔断、降级、限流 4、服务高可用：负载均衡 典型框架比如有：Dubbo,默认采用的是Zookeeper作为注册中心。 1.6 微服务时代 分布式微服务时代 微服务是在2012年提出的概念，微服务的希望的重点是一个服务只负责一个独立的功能。 拆分原则，任何一个需求不会因为发布或者维护而影响到不相关的服务，一切可以做到独立部署运维。 比如传统的“用户中心”服务，对于微服务来说，需要根据业务再次拆分，可能需要拆分成“买家服务”、“卖家服务”、“商家服务”等。 典型代表：Spring Cloud，相对于传统分布式架构，SpringCloud使用的是HTTP作为RPC远程调用，配合上注册中心Eureka和API网关Zuul，可以做到细分内部服务的同时又可以对外暴露统一的接口，让外部对系统内部架构无感，此外Spring Cloud的config组件还可以把配置统一管理。 马丁大师对微服务的定义：https://martinfowler.com/articles/microservices.html 微服务真正定义的时间是在2014年 The term \"Microservice Architecture\" has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services. While there is no precise definition of this architectural style, there are certain common characteristics around organization around business capability, automated deployment, intelligence in the endpoints, and decentralized control of languages and data. 大概意思：可独立部署服务，服务会越来越细 spring cloud地址：https://spring.io/projects/spring-cloud 1.7 服务网格新时期 （service mesh） 1.7.1 背景 早期 我们最开始用Spring+SpringMVC+Mybatis写业务代码 微服务时代 微服务时代有了Spring Cloud就完美了吗？不妨想一想会有哪些问题? （1）最初是为了业务而写代码，比如登录功能、支付功能等，到后面会发现要解决网络通信的问题，虽然有 Spring Cloud里面的组件帮我们解决了，但是细想一下它是怎么解决的？在业务代码里面加上spring cloud maven依赖，加上spring cloud组件注解，写配置，打成jar的时候还必须要把非业务的代码也要融合在一起，称为“侵入式框架”； （2）微服务中的服务支持不同语言开发，也需要维护不同语言和非业务代码的成本； （3）业务代码开发者应该把更多的精力投入到业务熟悉度上，而不应该是非业务上，Spring Cloud虽然能解决微服务领域的很多问题，但是学习成本还是较大的； （4）互联网公司产品的版本升级是非常频繁的，为了维护各个版本的兼容性、权限、流量等，因为Spring Cloud是“代码侵入式的框架”，这时候版本的升级就注定要让非业务代码一起，一旦出现问题，再加上多语言之间的调用，工程师会非常痛苦； （5）其实我们到目前为止应该感觉到了，服务拆分的越细，只是感觉上轻量级解耦了，但是维护成本却越高了，那么怎么 办呢？ 我们不是说spring cloud不好，只是为了引出service mesh, 目前spring cloud微服务还是比较主流的， 我们指出spring cloud的不好也只是为了突出service mesh的优点 问题解决思路 本质上是要解决服务之间通信的问题，不应该将非业务的代码融合到业务代码中 也就是从客户端发出的请求，要能够顺利到达对应的服务，这中间的网络通信的过程要和业务代码尽量无关 服务通信无非就是服务发现、负载均衡、版本控制等等 在很早之前的单体架构中，其实通信问题也是需要写在业务代码中的，那时候怎么解决的呢？ 解决方案：把网络通信，流量转发等问题放到了计算机网络模型中的TCP/UDP层，也就是非业务功能代码下沉，把这些网络的问题下沉到计算机网络模型当中，也就是网络七层模型 网络七层模型：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层 思考： 我们是否也可以把每个服务配置一个代理，所有通信的问题都交给这个代理去做，就好比大家熟悉的nginx,haproxy其实它们做反向代理把请求转发给其他服务器,也就为 Service Mesh的诞生和功能实现提供了一个解决思路 1.7.2 SideCar 它降低了与微服务架构相关的复杂性，并且提供了负载平衡、服务发现、流量管理、电路中断、遥测、故障注入等功能特性。 Sidecar模式是一种将应用功能从应用本身剥离出来作为单独进程的方式。该模式允许我们向应用无侵入添加多种功能，避免了为满足第三方组件需求而向应用添加额外的配置代码。 很多公司借鉴了Proxy模式，推出了Sidecar的产品，比如像Netflix的Prana，蚂蚁金服的SofaMesh 服务业务代码与Sidecar绑定在一起，每个服务都配置了一个Sidecar代理，每个服务所有的流量都经过sidecar,sidecar帮我们屏蔽了通信的细节，我的业务开发人员只需要关注业务就行了，而通信的事情交给sidecar处理 总结：可以理解成是代理，控制了服务的流量的进出，sidecar是为了通用基础设施而设计，可以做到与公司框架技术无侵入性 SideCar的探索之路还在继续 很多公司借鉴了Proxy模式，推出了Sidecar的产品，比如像Netflix的Prana，蚂蚁金服的SofaMesh 2014年 Netflix发布的Prana 2015年 唯品会发布local proxy 2016年 Twitter的基础设施工程师发布了第一款Service Mesh项目：Linkerd (所以下面介绍Linkerd) 1.7.3 Linkerd 2016年1月，离开Twitter的基础设施工程师打造的一个服务网格项目,第一个Service Mesh项目由此诞生，解决通用性。 Linkerd很好地结合了Kubernetes所提供的功能，以此为基础，在每个Kubernetes Node上都部署运行一个Linkerd实例，用代理的方式将加入Mesh的Pod通信转接给Linkerd，这样Linkerd就能在通信链路中完成对通信的控制和监控。 Linkerd设计思想 Linderd的思想跟sidecar很类似，目标也是屏蔽网络通信细节 Linkerd除了完成对Service Mesh的命名，以及Service Mesh各主要功能的落地，还有以下重要创举： 无须侵入工作负载的代码，直接进行通信监视和管理； 提供了统一的配置方式，用于管理服务之间的通信和边缘通信； 除了支持Kubernetes，还支持多种底层平台。 总结： 跟我们前面sidecar很类似，以前的调用方式都是服务来调用服务，在Linkerd思想要求所有的流量都走sidecar，Linkerd帮业务人员屏蔽了通信细节，通信不需要侵入到业务代码内部了，这样业务开发者就专注于业务开发的本身 Linkerd在面世之后，迅速获得用户的关注，并在多个用户的生产环境上成功部署、运行。2017年，Linkerd加入CNCF，随后宣布完成对千亿次生产环境请求的处理，紧接着发布了1.0版本，并且具备一定数量的商业用户，一时间风光无限，一直持续到Istio横空出世。 问题: 在早期的时候又要部署服务，又要部署sidecar，对于运维人员来说比较困难的，所以没有得到很好的发展，其实主要的 问题是Linkerd只是实现了数据层面的问题，但没有对其进行很好的管理。 数据层面:通过sidecar解决了数据处理的问题 打开github:搜索linkerd，是由scala语言编写的 1.7.4 istio 由Google、IBM和Lyft共同发起的开源项目 打开github:搜索istio，是由go语言编写的 什么是istio? 地址：https://istio.io/docs/concepts/what-is-istio/#why-use-istio Istio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code. You add Istio support to services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, then configure and manage Istio using its control plane functionality 翻译： 通过Istio，可以轻松创建带有负载平衡，服务到服务的身份验证，监视等功能的已部署服务网络，使得服务中的代码更改很少或没有更改。 通过在整个环境中部署一个特殊的sidecar代理来拦截微服务之间的所有网络通信，然后使用其控制平面功能配置和管理，可以为服务添加Istio支持。 注意这句话： 使得服务中的代码更改很少或没有更改 这句描述非常重要，如果我们用的是spring cloud通信功能，是不是要加依赖、加注解、改配置 什么是控制平面？ 控制平面就是来管理数据平面，也就是管理sideCar 所以istio既有数据平面也有控制平面 istio能干什么? Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic. Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection. A pluggable policy layer and configuration API supporting access controls, rate limits and quotas. Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress. Secure service-to-service communication in a cluster with strong identity-based authentication and authorization. 翻译： 1.HTTP、gRPC、WebSocket和TCP流量的自动负载平衡。 2.路由、重试、故障转移和故障注入对流量行为进行细粒度控制。 3.支持访问控制、速率限制、配置API。 4.集群内所有流量的自动衡量、日志和跟踪，包括集群入口和出口。 5.使用基于身份验证和授权来保护集群中服务跟服务之间的通信。 总结：很明显Istio不仅拥有“数据平面（Data Plane）”，而且还拥有“控制平面（Control Plane），也就是拥有了数据 接管与集中控制能力。 1.7.5 什么是服务网格 服务网格：指的是微服务网络应用之间的交互，随着规模和复杂性增加，服务跟服务调用错综复杂 如下图所示 如果每一个格子都是一个sidecar数据平面，然后sidecar进行彼此通信，那么servicemech就是来管理每个格子的控制平面,这就是服务网格，从架构层面看起来跟网格很像 特点： 基础设施：服务网格是一种处理服务之间通信的基础设施层。 支撑云原生：服务网格尤其适用于在云原生场景下帮助应用程序在复杂的服务间可靠地传递请求。 网络代理：在实际使用中，服务网格一般是通过一组轻量级网络代理来执行治理逻辑的。 对应用透明：轻量网络代理与应用程序部署在一起，但应用感知不到代理的存在，还是使用原来的方式工作。 1.7.6 什么是Service Mesh istio官网 也对什么是service mesh给出了定义 地址：https://istio.io/docs/concepts/what-is-istio/#what-is-a-service-mesh Istio addresses the challenges developers and operators face as monolithic applications transition towards a distributed microservice architecture. To see how, it helps to take a more detailed look at Istio’s service mesh. 翻译： 解决开发与运维部署分布式微服务面临的问题 The term service mesh is used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring. A service mesh also often has more complex operational requirements, like A/B testing, canary rollouts, rate limiting, access control, and end-to-end authentication. 翻译： 也是解决微服务之间服务跟服务之间通信的问题，可以包括服务发现、负载平衡、故障恢复、度量和监视，服务网格通常还具有更复杂的操作需求，如A/B测试、速率限制、访问控制和端到端身份验证 1.7.7 CNCF云原生组织发展和介绍 云原生发展历史时间轴 微服务 马丁大师在2014年定义了微服务 Kubernetes 从2014年6月由Google宣布开源，到2015年7月发布1.0这个正式版本并进入CNCF基金会，再到2018年3月从CNCF基金会正式毕业，迅速成为容器编排领域的标准，是开源历史上发展最快的项目之一 Linkerd Scala语言编写，运行在JVM中，Service Mesh名词的创造者 2016年01月15号，0.0.7发布 2017年01月23号，加入CNCF组织 2017年04月25号，1.0版本发布 Envoy envoy是一个开源的服务代理，为云原生设计的程序，由C++语言编程[Lyft] 2016年09月13号，1.0发布 2017年09月14号，加入CNCF组织 Istio Google、IBM、Lyft发布0.1版本 Istio是开源的微服务管理、保护和监控框架。Istio为希腊语，意思是”起航“。 CNCF介绍 CNCF 是一个开源软件基金会，致力于使云原生计算具有普遍性和可持续性。 云原生计算使用开源软件技术栈将应用程序部署为微服务，将每个部分打包到自己的容器中，并动态编排这些容器以优化资源利用率。 云原生技术使软件开发人员能够更快地构建出色的产品。 CNCF解决了什么问题 统一基础平台：kubernetes 如果我们需要日志监控：Prometheus 需要代理：Envoy 需要分布式链路跟踪：Jaeger ...... 地址：https://www.cncf.io/ 介绍几个常用的已经毕业的云原生项目 Kubernetes Kubernetes 是世界上最受欢迎的容器编排平台也是第一个 CNCF 项目。 Kubernetes 帮助用户构建、扩展和管理应用程序及其动态生命周期。 Prometheus Prometheus 为云原生应用程序提供实时监控、警报包括强大的查询和可视化能力，并与许多流行的开源数据导入、导出工具集成。 Jaeger Jaeger 是由 Uber 开发的分布式追踪系统，用于监控其大型微服务环境。 Jaeger 被设计为具有高度可扩展性和可用性，它具有现代 UI，旨在与云原生系统（如 OpenTracing、Kubernetes 和 Prometheus）集成。 Containerd Containerd 是由 Docker 开发并基于 Docker Engine 运行时的行业标准容器运行时组件。 作为容器生态系统的选择，Containerd 通过提供运行时，可以将 Docker 和 OCI 容器镜像作为新平台或产品的一部分进行管理。 Envoy Envoy 是最初在 Lyft 创建的 Service Mesh（服务网格），现在用于Google、Apple、Netflix等公司内部。 Envoy 是用 C++ 编写的，旨在最大限度地减少内存和 CPU 占用空间，同时提供诸如负载均衡、网络深度可观察性、微服务环境中的跟踪和数据库活动等功能。 Fluentd Fluentd 是一个统一的日志记录工具，可收集来自任何数据源（包括数据库、应用程序服务器、最终用户设备）的数据，并与众多警报、分析和存储工具配合使用。 Fluentd 通过提供一个统一的层来帮助用户更好地了解他们的环境中发生的事情，以便收集、过滤日志数据并将其路由到许多流行的源和目的地。 孵化中的项目， Open Tracing OpenTracing：为不同的平台，供应中立的API，使开发人员可以轻松地应用分布式跟踪。 GRPC gRPC 是一个高性能、开源和通用的 RPC 框架,语言中立，支持多种语言。 CNI CNI 就是这样一个标准，它旨在为容器平台提供网络的标准化。不同的容器平台能够通过相同的接口调用不同的网络组件。 Helm Helm 是 Kubernetes 的包管理器。包管理器类似于我们在Centos中使用的yum一样，能快速查找、下载和安装软件包。 Etcd 一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。一般用的最多的就是作为一个注册中心来使用 1.7.8 国内兴起的服务网格 前面提到，在Service Mesh这个概念得到具体定义之前，实际上已经有很多厂商开始了微服务新的尝试，这一动作势必引发对微服务治理的强劲需求。在Service Mesh概念普及之后，有的厂商意识到自身产品也具备Service Mesh的特点，也有厂商受其启发，将自有的服务治理平台进行完善和改造，推出自己的Service Mesh产品。例如，蚂蚁金服、腾讯和华为都推出自己的网格产品，华为的产品甚至已被投入公有云进行商业应用。 蚂蚁金服 sofa Mesh 代理架构 前身是SOFA RPC ，2018年07月正式开源 腾讯 Tencent Service Mesh 代理架构 华为 CSE Mesher 代理架构 总结: 基本上都借鉴了Sidecar、Envoy和Istio等设计思想 02 Istio 基本介绍 2.1 什么是Istio 地址：https://istio.io/ 服务网格是一个独立的基础设施层，用来处理服务之间的通信。现代的云原生应用是由各种复杂技术构建的服务体系，服务网格负责在这些组成部分之间进行可靠的请求传递。目前典型的服务网格通常提供了一组轻量级的网络代理，这些代理会在应用无感知的情况下，同应用并行部署、运行。 前面提到，Istio出自名门，由Google、IBM和Lyft在2017年5月合作推出，它的初始设计目标是在Kubernetes的基础上，以非侵入的方式为运行在集群中的微服务提供流量管理、安全加固、服务监控和策略管理等功能。 Istio有助于降低部署的复杂性，并减轻开发团队的压力。它是一个完全开放源代码的服务网格，透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志平台、遥测或策略系统中的api。Istio的多种功能集使我们能够成功、高效地运行分布式微服务体系结构，并提供一种统一的方式来保护、连接和监视微服务。 传统的spring cloud微服务项目 基于Istio架构微服务项目 Istio是基于Sidecar模式、数据平面和控制平台、是主流Service Mesh解决方案。 2.2 Istio特征 地址：https://istio.io/zh/ 连接：对网格内部的服务之间的调用所产生的流量进行智能管理，并以此为基础，为微服务的部署、测试和升级等操作提供有力保障。 安全：为网格内部的服务之间的调用提供认证、加密和鉴权支持，在不侵入代码的情况下，加固现有服务，提高其安全性。 策略：在控制平面定制策略，并在服务中实施。 观察：对服务之间的调用进行跟踪和测量，获取服务的状态信息。 下面对这些特性展开详细描述。 2.2.1 连接 微服务错综复杂，要完成其业务目标，连接问题是首要问题。连接存在于所有服务的整个生命周期中，用于维持服务的运行，算得上重中之重。 相对于传统的单体应用，微服务的端点数量会急剧增加，现代的应用系统在部分或者全部生命周期中，都存在同一服务的不同版本，为不同的客户、场景或者业务提供不同的服务。同时，同一服务的不同版本也可能有不同的访问要求，甚至产生了在生产环境中进行测试的新方法论。错综复杂的服务关系对开发者来说都是很严峻的考验。 针对目前的常见业务形态，这里画一个简单的示意图来描述Service Mesh的连接功能 从不同的外部用户的角度来看，他们访问的都是同一服务端口，但实际上会因为不同的用户识别，分别访问服务A的不同版本；在网格内部，服务A的版本1可能会访问服务B的两个版本，服务A的版本2则只会访问服务B的版本1；服务B的版本1需要访问外部的云服务，版本2则无此需求。 在这个简化的模型中，包含了以下诉求： ◎ 网格内部的调用（服务A→服务B）； ◎ 出站连接（服务B→外部云服务）； ◎ 入站连接（用户→服务A）； ◎ 流量分割（A服务跟B服务只负责与自己相关流量请求）； ◎ 按调用方的服务版本进行路由（服务A的版本1分别调用了服务B的版本1和版本2）； ◎ 按用户身份进行路由。 这里除了这些问题，还存在一些潜在需求，如下所述。 （1）在网格内部的服务之间如何根据实际需要对服务间调用进行路由，条件可能包括： ◎ 调用的源和目的服务； ◎ 调用内容； ◎ 认证身份。 （2）如何应对网络故障或者服务故障。 （3）如何处理不同服务不同版本之间的关系。 （4）怎样对出站连接进行控制。 （5）怎样接收入站连接来启动后续的整个服务链条。 这些当然不是问题的全部，其中，与流量相关的问题还引发了几个关键的功能需求，如下所述。 （1）服务注册和发现：要求能够对网格中不同的服务和不同的版本进行准确标识，不同的服务可以经由同一注册机构使用公认的方式互相查找。 （2）负载均衡策略：不同类型的服务应该由不同的策略来满足不同的需要。 （3）服务流量特征：在服务注册发现的基础之上，根据调用双方的服务身份，以及服务流量特征来对调用过程进行甄别。 （4）动态流量分配：根据对流量特征的识别，在不同的服务和版本之间对流量进行引导。 连接是服务网格应用过程中从无到有的最重要的一个环节。 2.2.2 安全 安全也是一个常谈常新的话题，在过去私有基础设施结合单体应用的环境下，这一问题并不突出，然而进入容器云时代之后，以下问题出现了。 （1）有大量容器漂浮在容器云中，采用传统的网络策略应对这种浮动的应用是比较吃力的。 （2）在由不同的语言、平台所实现的微服务之间，实施一致的访问控制也经常会因为实现的不一致而困难重重。 （3）如果是共享集群，则服务的认证和加密变得尤为重要，例如： ◎ 服务之间的通信要防止被其他服务监听； ◎ 只有提供有效身份的客户端才可以访问指定的服务； ◎ 服务之间的相互访问应该提供更细粒度的控制功能。 总之，要提供网格内部的安全保障，就应具备服务通信加密、服务身份认证和服务访问控制（授权和鉴权）功能。 上述功能通常需要数字证书的支持，这就隐藏了对CA的需求，即需要完成证书的签发、传播和更新业务。 除了上述核心要求，还存在对认证失败的处理、外部证书（统一 CA）的接入等相关支撑内容。 2.2.3 策略 Istio 通过可动态插拔、可扩展的策略实现访问控制、速率限制、配额管理等功能使得资源在消费者之间公平分配 在Istio中使用Mixer作为策略的执行者，Envoy的每次调用，在逻辑上都会通过Mixer进行事先预检和事后报告，这样Mixer就拥有了对流量的部分控制能力；在Istio中还有为数众多的内部适配器及进程外适配器，可以和外部软件设施一起完成策略的制定和执行。 组件简单介绍，后面会详细讲解 Mixer: Mixer 在整个服务网格中执行访问控制和策略执行，并从 Envoy 代理和其他服务收集遥测数据。 Envoy: 在istio框架中使用Envoy作为代理，使用的是C++开发的软件，用于为服务网格中的所有服务提供所有的入站和出站流量,唯一一个与数据平面打交道的 2.2.4 观察 随着服务数量的增加，监控和跟踪需求自然水涨船高。在很多情况下，可观察的保障都是系统功能的重要组成部分，是系统运维功能的重要保障。 随着廉价服务器（相对于传统小型机）的数量越来越多，服务器发生故障的频率也越来越高，人们开始产生争论：我们应该将服务器视为家畜还是宠物？家畜的特点：是有功能、无个性、可替换；而宠物的特点：是有功能、有个性、难替换。 我们越来越倾向于将服务器视为无个性、可替换的基础设施，如果主机发生故障，那么直接将其替换即可；并且，我们更加关注的是服务的总体质量。因此，微服务系统监控，除了有传统的主机监控，还更加重视高层次的服务健康监控。 服务的健康情况往往不是非黑即白的离散值，而是一系列连续状态，例如我们经常需要关注服务的调用成功率、响应时间、调用量、传输量等表现。 而且，面对数量众多的服务，我们应该能对各种级别和层次的指标进行采样、采集及汇总，获取较为精密、翔实的运行数据，最终通过一定的方法进行归纳总结和展示。 与此同时，服务网格还应提供分布式跟踪功能，对服务的调用链路进行跟踪。 观察性：动态获取服务运行数据和输出，提供强大的调用链、监控和调用日志收集输出的能力。配合可视化工具，可方便运维人员了解服务的运行状况，发现并解决问题。 2.3 Istio与服务治理 Istio是一个服务治理平台，治理的是服务间的访问，只要有访问就可以治理，不在乎这个服务是不是是所谓的微服务，也不要求跑的代码是微服务化的。单体应用不满足微服务用Istio治理也是完全可以的。提起“服务治理”，大家最先想到的一定是“微服务的服务治理”，就让我们从微服务的服务治理说起。 2.3.1服务治理的三种形态 服务治理的演变至少经过了以下三种形态。 第1种形态：在应用程序中包含治理逻辑 在微服务化的过程中，将服务拆分后会发现一堆麻烦事儿，连基本的业务连通都成了问题。在处理一些治理逻辑，比如怎么找到对端的服务实例，怎么选择一个对端实例发出请求，都需要自己写代码来实现。这种方式简单，对外部依赖少，但会导致存在大量的重复代码。所以，微服务越多，重复的代码越多，维护越难；而且，业务代码和治理逻辑耦合，不管是对治理逻辑的全局升级，还是对业务的升级，都要改同一段代码。 如下图所示 第2种形态：治理逻辑独立的代码 在解决第1种形态的问题时，我们很容易想到把治理的公共逻辑抽象成一个公共库，让所有微服务都使用这个公共库。在将这些治理能力包含在开发框架中后，只要是用这种开发框架开发的代码，就包含这种能力，非常典型的这种服务治理框架就是Spring Cloud。这种形态的治理工具在过去一段时间里得到了非常广泛的应用。 SDK模式虽然在代码上解耦了业务和治理逻辑，但业务代码和 SDK还是要一起编译的，业务代码和治理逻辑还在一个进程内。这就导致几个问题：业务代码必须和 SDK 基于同一种语言，即语言绑定。例如，Spring Cloud等大部分治理框架都基于Java，因此也只适用于 Java 语言开发的服务。经常有客户抱怨自己基于其他语言编写的服务没有对应的治理框架；在治理逻辑升级时，还需要用户的整个服务升级，即使业务逻辑没有改变，这对用户来说是非常不方便的。 如下图所示 第3种形态：治理逻辑独立的进程 SDK模式仍旧侵入了用户的代码，那就再解耦一层，把治理逻辑彻底从用户的业务代码中剥离出来，这就是前面提过的Sidecar模式。显然，在这种形态下，用户的业务代码和治理逻辑都以独立的进程存在，两者的代码和运行都无耦合，这样可以做到与开发语言无关，升级也相互独立。在对已存在的系统进行微服务治理时，只需搭配 Sidecar 即可，对原服务无须做任何修改，并且可以对老系统渐进式升级改造，先对部分服务进行微服务化。 如下图所示 总结 比较以上三种服务治理形态，我们可以看到服务治理组件的位置在持续下沉，对应用的侵入逐渐减少。 微服务作为一种架构风格，更是一种敏捷的软件工程实践，说到底是一套方法论；与之对应的 Istio 等服务网格则是一种完整的实践，Istio 更是一款设计良好的具有较好集成及可扩展能力的可落地的服务治理工具和平台。 所以，微服务是一套理论，Istio是一种实践。 2.4 Istio与Kubernetes 2.4.1 Kubernetes介绍 Kubernetes是一款用于管理容器化工作的负载和服务的可移植、可扩展的开源平台，拥有庞大、快速发展的生态系统，它面向基础设施，将计算、网络、存储等资源进行紧密整合，为容器提供最佳运行环境，并面向应用提供封装好的、易用的工作负载与服务编排接口，以及运维所需的资源规格、弹性、运行参数、调度等配置管理接口，是新一代的云原生基础设施平台。 从平台架构而言，Kubernetes的设计围绕平台化理念，强调插件化设计与易扩展性，这是它与其他同类系统的最大区别之一，保障了对各种不同客户应用场景的普遍适应性。另外，Kubernetes与其他容器编排系统的显著区别是Kubernetes并不把无状态化、微服务化等条件作为可运行的工作负载的约束。 如今，容器技术已经进入产业落地期，而Kubernetes作为容器平台的标准已经得到了广泛应用。 2.4.2 Istio是Kubernetes的好帮手 从场景来看，Kubernetes已经提供了非常强大的应用负载的部署、升级、扩容等运行管理能力。Kubernetes 中的 Service 机制也已经可以做服务注册、服务发现和负载均衡，支持通过服务名访问到服务实例。 从微服务的工具集观点来看，Kubernetes本身是支持微服务的架构，在Pod中部署微服务很合适，也已经解决了微服务的互访互通问题，但对服务间访问的管理如服务的熔断、限流、动态路由、调用链追踪等都不在Kubernetes的能力范围内。那么，如何提供一套从底层的负载部署运行到上层的服务访问治理端到端的解决方案？ 目前，最完美的答案就是在Kubernetes上叠加Istio这个好帮手 2.4.3 Kubernetes是Istio的好基座 Istio最大化地利用了Kubernetes这个基础设施，与之叠加在一起形成了一个更强大的用于进行服务运行和治理的基础设施，充分利用了Kubernetes的优点实现Istio的功能，例如： 1.数据面 数据面Sidecar运行在Kubernetes的Pod里，作为一个Proxy和业务容器部署在一起。在服务网格的定义中要求应用程序在运行的时感知不到Sidecar的存在。而基于Kubernetes的一个 Pod 多个容器的优秀设计使得部署运维 对用户透明，用户甚至感知不到部署 Sidecar的过程。用户还是用原有的方式创建负载，通过 Istio 的自动注入服务，可以自动给指定的负载注入Proxy。如果在另一种环境下部署和使用Proxy，则不会有这样的便利。 2.统一服务发现 Istio的服务发现机制非常完美地基于Kubernetes的域名访问机制构建而成，省去了再搭一个类似 Eureka 的注册中心的麻烦，更避免了在 Kubernetes 上运行时服务发现数据不一致的问题。 3.基于Kubernetes CRD描述规则 Istio的所有路由规则和控制策略都是通过 Kubernetes CRD实现的，因此各种规则策略对应的数据也被存储在 Kube-apiserver 中，不需要另外一个单独的 APIServer 和后端的配置管理。所以，可以说Istio的APIServer就是Kubernetes的APIServer，数据也自然地被存在了对应Kubernetes的etcd中。 Istio非常巧妙地应用了Kubernetes这个好基座，基于Kubernetes的已有能力来构建自身功能。Kubernetes里已经有的，绝不再自己搞一套，避免了数据不一致和用户使用体验的问题。 Istio和Kubernetes架构的关系，可以看出，Istio不仅数据面Envoy跑在Kubernetes的Pod里，其控制面也运行在Kubernetes集群中，其控制面组件本身存在的形式也是以Kubernetes Deployment和Service，基于Kubernetes扩展和构建。 回顾一下上面提到的K8S组件 APIServer API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。 kubernetes API Server的功能： -提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； -提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; -是资源配额控制的入口； -拥有完备的集群安全机制. Deployment 一旦运行了 Kubernetes 集群，就可以在其上部署容器化应用程序。 为此，需要创建 Kubernetes Deployment 配置。 Deployment 负责 Kubernetes 如何创建和更新应用程序的实例。 Service Service可以看作是一组提供相同服务的Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡 Ingress ingress是Kubernetes资源的一种，可以让外部请求访问到k8s内部的资源上 总结 Kubernetes在容器编排领域已经成为无可争辩的事实标准；微服务化的服务与容器在轻量、敏捷、快速部署运维等特征上匹配，这类服务在容器中的运行也正日益流行；随着Istio 的成熟和服务网格技术的流行，使用 Istio 进行服务治理的实践也越来越多，正成为服务治理的趋势；而 Istio 与 Kubernetes 的天然融合且基于 Kubernetes 构建，也补齐了Kubernetes的治理能力，提供了端到端的服务运行治理平台。这都使得Istio、微服务、容器及Kubernetes形成一个完美的闭环。 云原生应用采用 Kubernetes 构建应用编排能力，采用 Istio 构建服务治理能力，将逐渐成为企业技术转型的标准配置。 2.5 Istio与服务网格 2.5.1 时代选择服务网格 在云原生时代，随着采用各种语言开发的服务剧增，应用间的访问拓扑更加复杂，治理需求也越来越多。原来的那种嵌入在应用中的治理功能无论是从形态、动态性还是可扩展性来说都不能满足需求，迫切需要一种具备云原生动态、弹性特点的应用治理基础设施。 采用Sidecar代理与应用进程的解耦带来的是应用完全无侵入、也屏蔽了开发语言无关等特点解除了开发语言的约束，从而极大降低了应用开发者的开发成本。 这种方式也经常被称为一种应用的基础设施层，类比TCP/IP网络协议栈，应用程序像使用TCP/IP一样使用这个通用代理：TCP/IP 负责将字节码可靠地在网络节点之间传递，Sidecar 则负责将请求可靠地在服务间进行传递。TCP/IP 面向的是底层的数据流，Sidecar 则可以支持多种高级协议（HTTP、gRPC、HTTPS 等），以及对服务运行时进行高级控制，使服务变得可监控、可管理。 然后，从全局来看，在多个服务间有复杂的互相访问时才有服务治理的需求。即我们关注的是这些 Sidecar 组成的网格，对网格内的服务间访问进行管理，应用还是按照本来的方式进行互相访问，每个应用程序的入口流量和出口流量都要经过Sidecar代理，并在Sidecar上执行治理动作。 最后，Sidecar是网格动作的执行体，全局的管理规则和网格内的元数据维护需要通过一个统一的控制面实现。 Sidecar拦截入口流量，执行治理动作。这就引入两个问题： ◎ 增加了两处延迟和可能的故障点； ◎ 多出来的这两跳对于访问性能、整体可靠性及整个系统的复杂度都带来了新的挑战。 所以，对于考虑使用服务网格的用户来说，事情就会变成一个更简单的选择题：是否愿意花费额外的资源在这些基础设施上来换取开发、运维的灵活性、业务的非侵入性和扩展性等便利。 目前，华为、谷歌、亚马逊等云服务厂商将这种服务以云服务形态提供了出来，并和底层的基础设施相结合，提供了完整的服务治理解决方案。这对于广大应用开发者来说，更加方便和友好。 2.5.2 服务网格选择Istio > > 在多种服务网格项目和产品中，最引人注目的是后来居上的 Istio，它有希望成为继Kubernetes之后的又一款 重量级产品。 Istio 解决了生产大规模集群的性能、资源利用率和可靠性问题，提供了众多生产中实际应用的新特性，已经达到企业级可用的标准。 首先，在控制面上，Istio作为一种全新的设计，在功能、形态、架构和扩展性上提供了远超服务网格的能力范围。它提供了一套标准的控制面规范，向数据面传递服务信息和治理规则。 Istio使用Envoy V2版本的API，即gRPC协议。标准的控制面API解耦了控制面和数据面的绑定。 最后，在大厂的支持上，Istio 由谷歌和 IBM 共同推出，从应用场景的分析规划到本身的定位，从自身架构的设计到与周边生态的结合，都有着比较严密的论证。Istio项目在发起时已经确认了将云原生生态系统中的容器作为核心，将Kubernetes作为管理容器的编排系统，需要一个系统管理在容器平台上运行的服务之间的交互，包括控制访问、安全、运行数据收集等，而 Istio 正是为此而生的；另外，Istio 成为架构的默认部分，就像容器和Kubernetes已经成为云原生架构的默认部分一样。 云原生社区的定位与多个云厂商的规划也不谋而合。华为云已经在 2018 年 8 月率先在其容器服务CCE（Cloud Container Engine）中内置Istio；Google的GKE也在2018年12月宣布内置 Istio；越来越多的云厂商也已经选择将 Istio作为其容器平台的一部分提供给用户，即提供一套开箱即用的容器应用运行治理的全栈服务。正因为看到了 Istio 在技术和产品上的巨大潜力，各大厂商在社区的投入也在不断加大，其中包括Google、IBM、华为、思科、红帽等主流厂商。 istio原理图 总结 时代选择服务网格是因为架构的发展 服务网格选择istio是因为提供一套开箱即用的容器应用运行治理的全栈服务 总结 本节课主要是围绕Service Mesh来介绍，包括了架构发展历史（包括每一种架构的优缺点）、服务网格发展历史、云原生的介绍、Sidecar代理模式、istio与服务治理/kubernetes/服务网格之间的关系，为后续深入学习Istio框架做了理论铺垫，没有涉及到istio安装、istio组件、istio实战，这些都会在后面的课程会进行介绍和演示。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-22 09:20:55 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/ServiceMesh/use.html":{"url":"automation/ServiceMesh/use.html","title":"2.Istio进阶","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 02 Istio组件介绍 2.1 Pilot http请求 2.2 Mixer 2.3 Citadel 2.4 Galley 2.5 Sidecar-injector 2.6 Proxy(Envoy) 2.7 Ingressgateway 2.8 其他组件 03 Istio安装 3.1 在本地搭建Istio环境 3.1.1 Kubernetes集群环境 docker查看版本命令 3.1.2 安装Istio 3.1.2.1 快速部署Istio crds.yaml路径： 执行 统计个数 3.1.2.2 回顾K8S组件以及使用 Deployment Labels and Selectors Namespace Service 集群内部访问方式（ClusterIP） 外部服务访问集群中的Pod(NodePort) Ingress 3.1.2.3 初步感受istio 3.1.2.4 手动注入 3.1.2.5 自动注入sidecar 查询 istio-demo命名空间下面是否存在资源 在istio-demo命名空间创建资源 总结 01 Istio 架构 Istio的架构，分为控制平面和数据面平两部分。 - 数据平面：由一组智能代理（[Envoy]）组成，被部署为 sidecar。这些代理通过一个通用的策略和遥测中心传递和控制微服务之间的所有网络通信。 - 控制平面：管理并配置代理来进行流量路由。此外，控制平面配置 Mixer 来执行策略和收集遥测数据。 下图展示了组成每个平面的不同组件： 架构图可以看到，主要分为两个平面，控制面主要包括Istio的一些组件，例如：Pilot、Mixer、Citadel等服务组件；数据面由伴随每个应用程序部署的代理程序Envoy组成，执行针对应用程序的治理逻辑。为了避免静态、刻板地描述组件，在介绍组件的功能前，我们先通过一个动态场景来了解图架构图中对象的工作机制，即观察前端服务对后台服务进行一次访问时，在 Istio 内部都发生了什么，以及 Istio 的各个组件是怎样参与其中的，分别做了哪些事情。 先简单理解 Pilot：提供服务发现功能和路由规则 Mixer：策略控制，比如：服务调用速率限制 Citadel：起到安全作用，比如：服务跟服务通信的加密 Sidecar/Envoy: 代理，处理服务的流量 架构图上带圆圈的数字代表在数据面上执行的若干重要动作。虽然从时序上来讲，控制面的配置在前，数据面执行在后，但为了便于理解，在下面介绍这些动作时以数据面上的数据流为入口，介绍数据面的功能，然后讲解涉及的控制面如何提供对应的支持，进而理解控制面上组件的对应功能。 （1）自动注入：（由架构图得知前端服务跟后端服务都有envoy,我们这里以前端服务envoy为例说明）指在创建应用程序时自动注入 Sidecar代理。那什么情况下会自动注入你？在 Kubernetes场景下创建 Pod时，Kube-apiserver调用管理平面组件的 Sidecar-Injector服务，然后会自动修改应用程序的描述信息并注入Sidecar。在真正创建Pod时，在创建业务容器的同时在Pod中创建Sidecar容器。 # 原始的yaml文件 apiVersion: apps/v1 kind: Deployment spec: containers: - name: nginx image: nginx ...省略 调用Sidecar-Injector服务之后，yaml文件会发生改变 # 原始的yaml文件 apiVersion: apps/v1 kind: Deployment spec: containers: - name: nginx image: nginx ...省略 # 增加一个容器image地址 containers: - name: sidecar image: sidecar ...省略 总结：会在pod里面自动生产一个代理，业务服务无感知 （2）流量拦截：在 Pod 初始化时设置 iptables 规则，当有流量到来时，基于配置的iptables规则拦截业务容器的入口流量和出口流量到Sidecar上。但是我们的应用程序感知不到Sidecar的存在，还以原本的方式进行互相访问。在架构图中，流出前端服务的流量会被 前端服务侧的 Envoy拦截，而当流量到达后台服务时，入口流量被后台服务V1/V2版本的Envoy拦截。 总结：每个pod中都会有一个代理来来拦截所有的服务流量（不管是入口流量还是出口流量） （3）服务发现：前端服务怎么知道后端服务的服务信息呢？这个时候就需要服务发现了，所以服务发起方的 Envoy 调用控制面组件 Pilot 的服务发现接口获取目标服务的实例列表。在架构图中，前端服务内的 Envoy 通过 控制平面Pilot 的服务发现接口得到后台服务各个实例的地址，为访问做准备。 总结：Pilot提供了服务发现功能，调用方需要到Pilot组件获取提供者服务信息 （4）负载均衡：数据面的各个Envoy从Pilot中获取后台服务的负载均衡衡配置，并执行负载均衡动作，服务发起方的Envoy（前端服务envoy）根据配置的负载均衡策略选择服务实例，并连接对应的实例地址。 总结：Pilot也提供了负载均衡功能，调用方根据配置的负载均衡策略选择服务实例 （5）流量治理：Envoy 从 Pilot 中获取配置的流量规则，在拦截到 入口 流量和出口 流量时执行治理逻辑。比如说，在架构图中，前端服务的 Envoy 从 Pilot 中获取流量治理规则，并根据该流量治理规则将不同特征的流量分发到后台服务的v1或v2版本。当然，这只是Istio流量治理的一个场景，Istio支持更丰富的流量治理能力。 总结：Pilot也提供了路由转发规则 （6）访问安全：在服务间访问时通过双方的Envoy进行双向认证和通道加密，并基于服务的身份进行授权管理。在架构图中，Pilot下发安全相关配置，在前端模块服务和后端服务的Envoy上自动加载证书和密钥来实现双向认证，其中的证书和密钥由另一个控制平面组件Citadel维护。 总结：Citadel维护了服务代理通信需要的证书和密钥 （7）服务遥测：在服务间通信时，通信双方的Envoy都会连接控制平面组件Mixer上报访问数据，并通过Mixer将数据转发给对应的监控后端。比如说，在架构图中，前端模块服务对后端服务的访问监控指标、日志和调用链都可以通过Mixer收集到对应的监控后端。 总结：Mixer组件可以收集各个服务上的日志，从而可以进行监控 （8）策略执行：在进行服务访问时，通过Mixer连接后端服务来控制服务间的访问，判断对访问是放行还是拒绝。在架构图中，数据面在转发服务的请求前调用Mixer接口检查是否允许访问，Mixer 会做对应检查，给代理（Envoy）返回允许访问还是拒绝, 比如：可以对前端模块服务到后台服务的访问进行速率控制。 总结：Mixer组件可以对服务速率进行控制（也就是限流） （9）外部访问：在架构图中，外部服务通过Gateway访问入口将流量转发到服务前端服务内的Envoy组件，对前端服务的负载均衡和一些流量治理策略都在这个Gateway上执行。 总结：这里总结在以上过程中涉及的动作和动作主体，可以将其中的每个过程都抽象成一句话：服务调用双方的Envoy代理拦截流量，并根据控制平面的相关配置执行相应的服务治理动作，这也是Istio的数据平面和控制平面的配合方式。 02 Istio组件介绍 2.1 Pilot Pilot在Istio架构中必须要有 思考: 为什么Envoy能够服务发现？并且Envoy为什么可以流量控制？ 就是因为Pilot存在 什么是Pilot Pilot类似传统C/S架构中的服务端Master，下发指令控制客户端完成业务功能。和传统的微服务架构对比，Pilot 至少涵盖服务注册中心和向数据平面下发规则 等管理组件的功能。 服务注册中心 如图下图所示，Pilot 为 Envoy sidecar 提供服务发现、用于智能路由的流量管理功能（例如，A/B 测试、金丝雀发布等）以及弹性功能（超时、重试、熔断器等）。 Pilot本身不做服务注册，它会提供一个API接口，对接已有的服务注册系统，比如Eureka，Etcd等。 说白了，Pilot可以看成它是Sidecar的一个领导 (1)Platform Adapter是Pilot抽象模型的实现版本，用于对接外部的不同平台 (2)Polit定了一个抽象模型(Abstract model)，处理Platform Adapter对接外部不同的平台， 从特定平台细节中解耦 (3)Envoy API负责和Envoy的通讯，主要是发送服务发现信息和流量控制规则给Envoy 流程总结： service服务C会注册到Pilot注册中心平台适配器(Platform Adapter)模块上（假如对接的是Eureka, 那么service服务C会注册到Eureka里面），然后抽象模型(Abstract model)进行平台细节的解耦并且用于处理Platform Adapter对接外部的不同平台，最后通过Envoy API负责和Envoy的通讯，主要是发送服务发现信息和流量控制规则给Envoy 数据平面下发规则 Pilot 更重要的一个功能是向数据平面下发规则，Pilot 负责将各种规则转换换成 Envoy 可识别的格式，通过标准的 协议发送给 Envoy，指导Envoy完成动作。在通信上，Envoy通过gRPC流式订阅Pilot的配置资源。 Pilot将表达的路由规则分发到 Evnoy上，Envoy根据该路由规则进行流量转发，配置规则和流程图如下所示。 规则如下： # http请求 http: -match: # 匹配 -header: # 头部 cookie: # 以下cookie中包含group=dev则流量转发到v2版本中 exact: \"group=dev\" route: # 路由 -destination: name: v2 -route: -destination: name: v1 2.2 Mixer Mixer在Istio架构中不是必须的 Mixer分为Policy和Telemetry两个子模块，Policy用于向Envoy提供准入策略控制，黑白名单控制，速率限制等相关策略；Telemetry为Envoy提供了数据上报和日志搜集服务，以用于监控告警和日志查询。 Telemetry介绍 Mixer是一个平台无关的组件。Mixer的Telemetry 在整个服务网格中执行访问控制和策略使用，并从 Envoy 代理和其他服务收集遥测数据，流程如下图所示。 遥测报告上报，比如从Envoy中收集数据[请求数据、使用时间、使用的协议等]，通过Adapater上 报给Promethues、Heapster等 说白了，就是数据收集，然后通过adapter上传到监控容器里面 policy介绍 policy是另外一个Mixer服务，和istio-telemetry基本上是完全相同的机制和流程。数据面在转发服务的请求前调用istio-policy的Check接口是否允许访问，Mixer 根据配置将请求转发到对应的 Adapter 做对应检查，给代理返回允许访问还是拒绝。可以对接如配额、授权、黑白名单等不同的控制后端，对服务间的访问进行可扩展的控制。 策略控制：检查请求释放可以运行访问 2.3 Citadel Citadel在Istio架构中不是必须的 Istio的认证授权机制主要是由Citadel完成，同时需要和其它组件一起配合，参与到其中的组件还有Pilot、Envoy、Mixer，它们四者在整个流程中的作用分别为： Citadel：用于负责密钥和证书的管理，在创建服务时会将密钥及证书下发至对应的Envoy代理中； Pilot: 用于接收用户定义的安全策略并将其整理下发至服务旁的Envoy代理中； Envoy：用于存储Citadel下发的密钥和证书，保障服务间的数据传输安全； Mixer: 负责核心功能为前置条件检查和遥测报告上报; 流程如下 具体工作流程可描述如下： Kubernetes某集群节点新部署了服务Service，此时集群中有两个Pod被启动，每个Pod由Envoy代理容器和Service容器构成，在启动过程中Istio的Citadel组件会将密钥及证书依次下发至每个Pod中的Envoy代理容器中，以保证后续服务A，B之间的安全通信。 用户通过Rules API下发安全策略至Pilot组件，Pilot组件通过Pilot-discovery进程整理安全策略中Kubernetes服务注册和配置信息并以Envoy API方式暴露给Envoy。 Pod 中的Envoy代理会通过Envoy API方式定时去Pilot拉取安全策略配置信息，并将信息保存至Envoy代理容器中。 当pod内的服务相互调用时，会调用各自Envoy容器中的证书及密钥实现服务间的通信，同时Envoy容器还会根据用户下发的安全策略进行更细粒度的访问控制。 Mixer在整个工作流中核心功能为前置条件检查和遥测报告上报，在每次请求进出服务时，服务中的Envoy代理会向Mixer发送check请求，检查是否满足一些前提条件，比如ACL检查，白名单检查，日志检查等，如果前置条件检查通过，处理完后再通过Envoy向Mixer上报日志，监控等数据，从而完成审计工作。 使用场景： 在有一些场景中，对于安全要求是非常高的，比如支付，所以Citadel就是用来保证安全的。 回顾kubernetes API Server的功能： * 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； * 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; * 资源配额控制的入口； * 拥有完备的集群安全机制. 总结： 用于负责密钥和证书的管理，在创建服务时会将密钥及证书下发至对应的Envoy代理中 2.4 Galley Galley在istio架构中不是必须的 Galley在控制面上向其他组件提供支持。Galley作为负责配置管理的组件，并将这些配置信息提供给管理面的 Pilot和 Mixer服务使用，这样其他管理面组件只用和 Galley打交道，从而与底层平台解耦。 galley优点 配置统一管理，配置问题统一由galley负责 如果是相关的配置，可以增加复用 配置跟配置是相互隔离而且，而且配置也是权限控制，比如组件只能访问自己的私有配置 MCP协议 Galley负责控制平面的配置分发主要依托于一一种协议，这个协议叫（MCP） MCP提供了一套配置订阅和分发的API，里面会包含这个几个角色： source: 配置的提供端，在istio中Galley即是source 说白了就是Galley组件，它提供yaml配置 sink:配置的消费端，istio组件中Pilot和Mixer都属于sink resource: source和sink关注的资源体，也就是yaml配置 Galley 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。Galley 接管 Istio 获取配置、 处理和分配组件的顶级责任。它将负责将其他的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。 说白了：这样其他控制平面（Pilot和 Mixer）面组件只用和 Galley打交道，从而与底层平台解耦。 2.5 Sidecar-injector Sidecar-injector 是负责自动注入的组件，只要开启了自动注入，那么在创建pod的时候就会自动调用Sidecar-injector 服务 配置参数：istio-injection=enabled，我们后面会有案例演示 在istio中sidecar注入有两种方式 需要使用istioctl命令手动注入 （不需要配置参数：istio-injection=enabled） 基于kubernetes自动注入（配置参数：istio-injection=enabled） 手动注入和自动注入会在istio安装之后案例演示 两种区别： 手动注入需要每次在执行配置都需要加上istioctl命令 自动注入只需要做一下开启参数即可 sidecar模式具有以下优势 把业务逻辑无关的功能抽取出来（比如通信），可以降低业务代码的复杂度 sidecar可以独立升级、部署，与业务代码解耦 注入流程 在 Kubernetes环境下，根据自动注入配置，Kube-apiserver在拦截到 Pod创建的请求时，会调用自动注入服务 istio-sidecar-injector 生成 Sidecar 容器的描述并将其插入原 Pod的定义中，这样，在创建的 Pod 内, 除了包括业务容器，还包括 Sidecar容器。这个注入过程对用户透明，用户使用原方式创建工作负载。 总结：sidecar模式具有以下优势 把业务逻辑无关的功能抽取出来（比如通信），可以降低业务代码的复杂度 sidecar可以独立升级、部署，与业务代码解耦 2.6 Proxy(Envoy) Proxy是Istio数据平面的轻量代理。 Envoy是用C++开发的非常有影响力的轻量级高性能开源服务代理。作为服务网格的数据面，Envoy提供了动态服务发现、负载均衡、TLS、HTTP/2 及 gRPC代理、熔断器、健康检查、流量拆分、灰度发布、故障注入等功能。 Envoy 代理是唯一与数据平面流量交互的 Istio 组件。 Envoy组件解析 为了便于理解Istio中Envoy与服务的关系，如图所示： 一个pod里面运行了一个Envoy容器和service A容器，而Envoy容器内部包含了两个进程，分别是Pilot-agent和Envoy两个进程 pilot-agent pilot-agent跟Envoy打包在同一个docker镜像里面 pilot-agent作用 * 生成envoy配置 * 启动envoy * 监控envoy的运行状态，比如envoy出错是pilot-agent负责重启envoy,huozhe envoy配置变更之后reload envoy Envoy 负责拦截pod流量，负责从控制平面pilot组件获取配置和服务发现，上报数据给mixer组件 2.7 Ingressgateway ingressgateway 就是入口处的 Gateway，从网格外访问网格内的服务就是通过这个Gateway进行的。ingressgateway比较特别，是一个Loadbalancer类型的Service，不同于其他服务组件只有一两个端口，ingressgateway 开放了一组端口，这些就是网格内服务的外部访问端口。 网格入口网关ingressgateway和网格内的 Sidecar是同样的执行体，也和网格内的其他 Sidecar一样从 Pilot处接收流量规则并执行。因为入口处的流量都走这个服务。 流程图如下 由于gateway暴露了一个端口，外部的请求就可以根据这个端口把请求发给gateway了然后由gateway把请求分发给网格内部的pod上 2.8 其他组件 在Istio集群中一般还安装grafana、Prometheus、Tracing组件，这些组件提供了Istio的调用链、监控等功能，可以选择安装来完成完整的服务监控管理功能。 总结 主要介绍了一些常见的istio组件，其中有一些组件是istio默认就已经使用了，有一些组件我们后面也会来演示。 03 Istio安装 Istio支持在不同的平台下安装其控制平面，例如Kubernetes、Mesos和虚拟机等。 课程上以 Kubernetes 为基础讲解如何在集群中安装 Istio （Istio 1.0.6 要求Kubernetes的版本在1.11及以上）。 可以在本地或公有云上搭建Istio环境，也可以直接使用公有云平台上已经集成了Istio的托管服务。 3.1 在本地搭建Istio环境 3.1.1 Kubernetes集群环境 目前有许多软件提供了在本地搭建Kubernetes集群的能力，例如Minikube/kubeadm都可以搭建kubernetes集群，我这边所选用的是kubeadm来安装Kubernetes集群。 Kubeadm 是一个工具，它提供了 kubeadm init 以及 kubeadm join 这两个命令作为快速创建 kubernetes 集群的最佳实践。 准备机器 两台centos7的虚拟机，地址分别为 192.168.187.137 192.168.187.138 大家根据自己的情况来准备centos7的虚拟机。 虚拟机要保证彼此之间能够ping通，也就是处于同一个网络中。 Kubernets官网推荐虚拟机的配置最低要求：2核2G（这边建议最低2核3G配置） Docker环境 在每一台机器上都安装好Docker，我这边使用的版本为18.09.0 # docker查看版本命令 docker --version 修改hosts文件 (1)设置master角色,在192.168.187.138打开hosts文件 # 打开hosts文件 vi /etc/hosts # 设置192.168.187.138为master的hostname,用m来表示 192.168.187.138 m # 设置192.168.187.137为worker的hostname,用w1来表示 192.168.187.137 w1 (2)设置worker角色,在192.168.187.137打开hosts文件 # 打开hosts文件 vi /etc/hosts # 设置192.168.187.138为master的hostname,用m来表示 192.168.187.138 m # 设置192.168.187.137为worker的hostname,用w1来表示 192.168.187.137 w1 (3)使用ping测试一下 ping m ping w1 kubeadm安装版本 安装的版本是1.14.0 kubernetes集群网络插件-calico calico网络插件：https://docs.projectcalico.org/v3.9/getting-started/kubernetes/ calico，同样在master节点上操作 Calico为容器和虚拟机工作负载提供一个安全的网络连接。 验证 Kubernetes安装 1)在master节点上检查集群信息 命令：kubectl get nodes 2)监控 w1节点的状态 ：kubectl get nodes -w 监控成 ready状态 3)查询pod 命令：kubectl get pods -n kube-system 注意：Kubernetes集群安装方式有很多，大家可以安装自己熟悉的方式搭建Kubernetes, 这里只是介绍本次课程上使用的kubernets集群环境 3.1.2 安装Istio 在Istio的版本发布页面https://github.com/istio/istio/releases/tag/1.0.6下载安装包并解压（我用的是一个比较稳定的版本1.0.6版本，放到master上面，以Linux平台的istio-1.0.6-linux.tar.gz为例） 1.解压tar -xzf istio-1.0.6-linux.tar.gz 2.进入istio目录cd istio-1.0.6/ Istio的安装目录及其说明 文件/文件夹 说明 bin 包含客户端工具,用于和Istio APIS交互 install 包含了Consul和Kubernetes平台的Istio安装脚本和文件，在Kubernetes平台上分为YAML资源文件和Helm安装文件 istio.VERSION 配置文件包含版本信息的环境变量 samples 包含了官方文档中用到的各种应用实例如bookinfo/helloworld等等，这些示例可以帮助读者理解Istio的功能以及如何与Istio的各个组件进行交互 tools 包含用于性能测试和在本地机器上进行测试的脚本文件和工具 有以下几种方式安装Istio： 使用install/kubernetes文件夹中的istio-demo.yaml进行安装； 使用Helm template渲染出Istio的YAML安装文件进行安装； 使用Helm和Tiller方式进行安装。 课程中使用的是使用install/kubernetes文件夹中的istio-demo.yaml进行安装 3.1.2.1 快速部署Istio Kubernetes CRD介绍 比如Deployment/Service/etc等资源是kubernetes本身就支持的类型，除了这些类型之外kubernetes还支持资源的扩展，说白了就是可以自定义资源类型，如果没有CRD的支持的话，istio一些资源类型是创建不成功的 #crds.yaml路径： istio-1.0.6/install/kubernetes/helm/istio/templates/crds.yaml # 执行 kubectl apply -f crds.yaml # 统计个数 kubectl get crd -n istio-system | wc -l Kubernetes平台对于分布式服务部署的很多重要的模块都有系统性的支持，借助如下一些平台资源可以满足大多数 分布式系统部署和管理的需求,但是在不同应用业务环境下，对于平台可能有一些特殊的需求，这些需求可以抽象为Kubernetes的扩展资源，而 Kubernetes的CRD(CustomResourceDefifinition)为这样的需求提供了轻量级的机制 执行安装命令 (1)根据istio-1.0.6/install/kubernetes/istio-demo.yaml创建资源 kubectl apply -f istio-demo.yaml # 会发现有这么多的资源被创建了，很多很多 ，里面的命名空间用的是istio-system 2)查看核心组件资源 kubectl get pods -n istio-system kubectl get svc -n istio-system 可以看到有3个是completed，其它的组件都必须是running, completed表示的是用的是k8s里面JOB资源，表示这个任务已经执行结束了 可以看到比如citadel有了，pilot有了，sidecar也有了，其它的比如ingress网关都有了，监控也有了 3.1.2.2 回顾K8S组件以及使用 回顾课程涉及到的kubernetes组件 Deployment 一旦运行了 Kubernetes 集群，就可以在其上部署容器化应用程序。 为此，需要创建 Kubernetes Deployment 配置。 Deployment 负责 Kubernetes 如何创建和更新应用程序的实例。 创建 Deployment 后，Kubernetes master 将应用程序实例调度到集群中的各个节点上。 创建nginx_deployment.yaml文件 apiVersion: apps/v1 ## 定义了一个版本 kind: Deployment ##k8s资源类型是Deployment metadata: ## metadata这个KEY对应的值为一个Maps name: nginx-deployment ##资源名字 nginx-deployment labels: ##将新建的Pod附加Label app: nginx ##一个键值对为key=app,valuen=ginx的Label。 spec: #以下其实就是replicaSet的配置 replicas: 3 ##副本数为3个，也就是有3个pod selector: ##匹配具有同一个label属性的pod标签 matchLabels: ##寻找合适的label，一个键值对为key=app,value=nginx的Labe app: nginx template: #模板 metadata: labels: ##将新建的Pod附加Label app: nginx spec: containers: ##定义容器 - name: nginx ##容器名称 image: nginx:1.7.9 ##镜像地址 ports: - containerPort: 80 ##容器端口 (1)执行资源文件命令 kubectl apply -f nginx_deployment.yaml (2)查看pod kubectl get pods 查看pod详情 kubectl get pods -o wide (3)查看deployment命令 kubectl get deployment (4)查看deployment详情命令 kubectl get deployment -o wide Labels and Selectors 顾名思义，就是给一些资源打上标签的 labels 当资源很多的时候可以用可以用labels标签来对资源分类 apiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx # 表示名称为nginx-pod的pod，有一个label，key为app，value为nginx。 #我们可以将具有同一个label的pod，交给selector管理 selectors 如果我想使用这个标签里面的k8s资源，那么需要用到k8s里面selector组件，用selector来匹配到特定的的label apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: # 定义了一个labels,key=app,value=nginx app: nginx spec: replicas: 3 selector: # 用selector匹配具有同一个label属性的pod标签 matchLabels: app: nginx 查看pod的label标签命令： kubectl get pods --show-labels Namespace 命名空间就是为了隔离不同的资源。比如：Pod、Service、Deployment等。可以在输入命令的时候指定命名空间`-n`，如果不指定，则使用默认的命名空间：default。 查看一下当前的所用命名空间：kubectl get namespaces/ns 查看一下kube-system命名空间：kubectl get pods -n kube-system (1)创建自己的namespace my-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: myns (2)执行命令：kubectl apply -f my-namespace.yaml (3)查看命令 kubectl get ns 删除命名空间 kubectl delete namespaces 空间的名字 注意： 删除一个namespace会自动删除所有属于该namespace的资源。 default和kube-system命名空间不可删除。 Service 集群内部访问方式（ClusterIP） Pod虽然实现了集群内部互相通信，但是Pod是不稳定的，比如通过Deployment管理Pod，随时可能对Pod进行扩缩容，这时候Pod的IP地址是变化的。能够有一个固定的IP，使得集群内能够访问。也就是之前在架构描述的时候所提到的，能够把相同或者具有关联的Pod，打上Label，组成Service。而Service有固定的IP，不管Pod怎么创建和销毁，都可以通过Service的IP进行访问 k8s用service来解决这个问题，因为service会对应一个不会的ip，然后内部通过负载均衡到相同label上的不同pod机器上 (1)创建whoami-deployment.yaml文件 apiVersion: apps/v1 ## 定义了一个版本 kind: Deployment ##资源类型是Deployment metadata: ## metadata这个KEY对应的值为一个Maps name: whoami-deployment ##资源名字 labels: ##将新建的Pod附加Label app: whoami ##key=app:value=whoami spec: ##资源它描述了对象的 replicas: 3 ##副本数为1个，只会有一个pod selector: ##匹配具有同一个label属性的pod标签 matchLabels: ##匹配合适的label app: whoami template: ##template其实就是对Pod对象的定义 (模板) metadata: labels: app: whoami spec: containers: - name: whoami ##容器名字 下面容器的镜像 image: jwilder/whoami ports: - containerPort: 8000 ##容器的端口 jwilder/whoami这是一个可以在docker仓库里面拉取到的镜像，是官方提供的一个演示的镜像 (1)执行命令 kubectl apply -f whoami-deployment.yaml (2)查看详情 kubectl get pods -o wide (3)在集群内正常访问 curl 192.168.221.80:8000/192.168.14.6:8000/192.168.14.7:8000 (5)测试：删除其中一个pod，查看重新生成的ip有没有变化 kubectl delete pod whoami-deployment-678b64444d-jdv49 新加的pod地址发生了变化 (6) Service 登场 查询svc命名空间下的资源 kubectl get svc (7)创建自己的service空间 创建：kubectl expose deployment deployment名字 例如：kubectl expose deployment whoami-deployment (8)重新查询service空间，会发现有一个whoami-deployment的service，ip为10.107.4.74 [root@m k8s]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 2d12h whoami-deployment ClusterIP 10.107.4.74 8000/TCP 3s (9)访问service：curl 10.107.4.74:8000 多试几次会发现service会负载到其中的一个pod上 (10)查看service kubectl describe svc service名字 例如：kubectl describe svc whoami-deployment [root@m k8s]# kubectl describe svc whoami-deployment Name: whoami-deployment Namespace: default Labels: app=whoami Annotations: Selector: app=whoami Type: ClusterIP IP: 10.107.4.74 Port: 8000/TCP TargetPort: 8000/TCP Endpoints: 192.168.190.86:8000,192.168.190.87:8000,192.168.190.89:8000 Session Affinity: None Events: # 说白了 service下面挂在了Endpoints节点 (11)将原来的节点扩容到5个 kubectl scale deployment whoami-deployment --replicas=5 (12)删除service命令 kubectl delete service service名字 kubectl delete service whoami-deployment 总结：其实Service存在的意义就是为了Pod的不稳定性，而上述探讨的就是关于Service的一种类型Cluster IP 外部服务访问集群中的Pod(NodePort) 也是Service的一种类型，可以通过NodePort的方式 说白了，因为外部能够访问到集群的物理机器IP，所以就是在集群中每台物理机器上暴露一个相同的端口锁，比如32008 操作 （1）先删除之前的service kubectl delete svc whoami-deployment （2）再次查看命令 kubectl get svc 发现whoami-deployment已被删除 （3）查看pod命令 kubectl get pods （4）创建type为NodePort的service kubectl expose deployment whoami-deployment --type=NodePort 查看：kubectl get svc 并且生成了一个port端口，会有一个8000端口映射成宿主机的31504端口 注意上述的端口31504，实际上就是暴露在集群中物理机器上的端口 lsof -i tcp:31504 netstat -ntlp|grep 31504 浏览器通过物理机器的IP访问 http://192.168.187.137:31504/ curl 192.168.187.137:31504/ NodePort虽然能够实现外部访问Pod的需求，但是需要占用了各个物理主机上的端口 删除资源 kubectl delete -f whoami-deployment.yaml kubectl delete svc whoami-deployment Ingress 前面我们也学习可以通过service nodeport方式实现外部访问Pod的需求，但是会占用了各个物理主机上的端口，所以 这种方式不好 删除资源 # 删除pod kubectl delete -f whoami-deployment.yaml # 删除service kubectl delete svc whoami-deployment 那接下来还是基于外部访问内部集群的需求，使用Ingress实现访问whoami需求。 （1）创建whoami-service.yaml文件 创建pod和service apiVersion: apps/v1 ## 定义了一个版本 kind: Deployment ##资源类型是Deployment metadata: ## metadata这个KEY对应的值为一个Maps name: whoami-deployment ##资源名字 labels: ##将新建的Pod附加Label app: whoami ##key=app:value=whoami spec: ##资源它描述了对象的 replicas: 3 ##副本数为1个，只会有一个pod selector: ##匹配具有同一个label属性的pod标签 matchLabels: ##匹配合适的label app: whoami template: ##template其实就是对Pod对象的定义 (模板) metadata: labels: app: whoami spec: containers: - name: whoami ##容器名字 下面容器的镜像 image: jwilder/whoami ports: - containerPort: 8000 ##容器的端口 --- apiVersion: v1 kind: Service metadata: name: whoami-service spec: ports: - port: 80 protocol: TCP targetPort: 8000 selector: app: whoami （2）执行资源命令 kubectl apply -f whoami-service.yaml （3）创建whoami-ingress.yaml文件 apiVersion: extensions/v1beta1 kind: Ingress # 资源类型 metadata: name: whoami-ingress # 资源名称 spec: rules: # 定义规则 - host: whoami.qy.com # 定义访问域名 http: paths: - path: / # 定义路径规则，/ 表示能够命中任何路径规则 backend: serviceName: whoami-service # 把请求转发给service资源，这个service就是我们前面运行的service servicePort: 80 # service的端口 （4）执行命令： kubectl apply -f whoami-ingress.yaml （5）、查看ingress资源： kubectl get ingress （6）查看ingress资源详细： kubectl describe ingress whoami-ingress （7）、修改win的hosts文件，添加dns解析 192.168.187.137 whoami.qy.com （8）、打开浏览器，访问whoami.qy.com 流程总结 浏览器发送请求给ingress,ingress根据规则配置把请求转发给对应的service，由于service配置了pod，所以请求最终发给了pod内对应的服务 总结 ingress转发请求更加灵活，而且不需要占用物理机的端口，所以建议使用这种方式转发外部请求到集群内部 3.1.2.3 初步感受istio 在docker中是通过container来部署业务的，在k8s里面是通过pod来部署业务的，那么在istio里面如何体现sidecar呢？ 猜想：会不会在pod中除了业务需要的container之外还会有一个sidecar的container存在呢？ 验证猜想 （1）准备一个资源 first-istio.yaml apiVersion: apps/v1 ## 定义了一个版本 kind: Deployment ##资源类型是Deployment metadata: name: first-istio spec: selector: matchLabels: app: first-istio replicas: 1 template: metadata: labels: app: first-istio spec: containers: - name: first-istio ##容器名字 下面容器的镜像 image: registry.cn-hangzhou.aliyuncs.com/sixupiaofei/spring-docker-demo:1.0 ports: - containerPort: 8080 ##容器的端口 --- apiVersion: v1 kind: Service ##资源类型是Service metadata: name: first-istio ##资源名字first-istio spec: ports: - port: 80 ##对外暴露80 protocol: TCP ##tcp协议 targetPort: 8080 ##重定向到8080端口 selector: app: first-istio ##匹配合适的label，也就是找到合适pod type: ClusterIP ## Service类型ClusterIP 创建文件夹istio,然后把first-istio放进去，按照正常的创建流程里面只会有自己私有的containers,不会有sidecar #执行，会发现 只会有一个containers在运行 kubectl apply -f first-istio.yaml #查看first-isitio service kubectl get svc # 查看pod的具体的日志信息命令 kubectl describe pod first-istio-8655f4dcc6-dpkzh #删除 kubectl delete -f first-istio.yaml 查看pod命令 kubectl get pods 思考：怎么让pod里面自动增加一个Sidecar呢？ 有两种方式：手动注入和自动注入 3.1.2.4 手动注入 （1）删除上述资源，重新创建，使用手动注入sidecar的方式 istioctl kube-inject -f first-istio.yaml | kubectl apply -f - 注意：istioctl 命令需要先在/etc/profile配置PATH vim /etc/profile 增加isito安装目录配置 export ISTIO_HOME=/home/tools/istio-1.0.6 export PATH=$PATH:$ISTIO_HOME/bin 加载profile文件 source profile （2）查看pod数量 kubectl get pods # 注意该pod中容器的数量 ，会发现容器的数量不同了，变成了2个 (3) 查看service kubectl get svc 思考： 我的yaml文件里面只有一个container,执行完之后为什么会是两个呢？ 我的猜想另外一个会不会是Sidecar,那么我描述一下这个pod,看看这两个容器到底是什么 # 查看pod执行明细 kubectl describe pod first-istio-75d4dfcbff-qhmxj 发现竟然除了我的容器之外还多了一个代理容器，此时我们大胆猜想这个代理会不会就是sidecar呢 接着往上翻 此时已经看到了我们需要的答案了 查看yaml文件内容 kubectl get pod first-istio-75d4dfcbff-qhmxj -o yaml 总结 这个yaml文件已经不是我们原来的yaml文件了，会发现这个yaml文件还定义了一个proxy的image,这个image是我们提前就已经准备好了的,所以istio是通过改变yaml文件来实现注入一个代理 (4)删除资源 istioctl kube-inject -f first-istio.yaml | kubectl delete -f - 思考：难道我以后每次都要写那么一大串命令创建sidecar吗，有没有正常的命令来直接创建sidecar呢？ 3.1.2.5 自动注入sidecar 首先自动注入是需要跟命名空间挂钩，所以需要创建一个命名空间，只需要给命名空间开启自动注入，后面创建的资源只要挂载到这个命名空间下，那么这个命名空间下的所有的资源都会自动注入sidecar了 (1)创建命名空间 kubectl create namespace my-istio-ns (2)给命名空间开启自动注入 kubectl label namespace my-istio-ns istio-injection=enabled (3)创建资源,指定命名空间即可 # 查询 istio-demo命名空间下面是否存在资源 kubectl get pods -n my-istio-ns # 在istio-demo命名空间创建资源 kubectl apply -f first-istio.yaml -n my-istio-ns (4)查看资源 kubectl get pods -n my-istio-ns (5)查看资源明细 kubectl describe pod pod-name -n my-istio-ns 发现除了我的容器之外依旧多了一个代理容器 (6)查看service kubectl get svc -n my-istio-ns (7)删除资源 kubectl delete -f first-istio.yaml -n my-istio-ns 大家应该都已经明白了istio怎么注入sidecar的了吧 sidecar注入总结： 不管是自动注入还是手动注入原理都是在yaml文件里面追加一个代理容器，这个代理容器就是sidecar,这里更推荐自动注入的方式来实现 sidecar 的注入 总结 这节课我们主要是讲解了istio的架构、组件，以及istio的安装，对istio有了进一步的了解，下节课我们会继续深入地学习istio，并开始istio案例实战 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-22 09:21:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/ServiceMesh/high.html":{"url":"automation/ServiceMesh/high.html","title":"3.Istio实战","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 prometheus和grafana 访问prometheus 访问grafana 项目案例：bookinfo 理解什么是bookinfo sidecar自动注入到微服务 启动bookinfo 通过ingress方式访问 通过istio的ingressgateway访问 确定 Ingress 的 IP 和端口 流量管理 放开bookinfo自定义路由权限 基于版本方式控制 基于权重的流量版本控制 基于用户来控制流量版本 故障注入 流量的迁移 体验Istio的Observe(观察) Istio监控功能 prometheus和grafana Prometheus存储服务的监控数据，数据来自于istio组件mixer上报 Grafana开源数据可视化工具，展示Prometheus收集到的监控数据 istio已经默认帮我们把grafana和prometheus已经默认部署好了 （1）执行命令查看istio自带的组件 kubectl get pods -n istio-ns 我们打开istio-demo.yaml文件找到找到prometheus和grafana 其实istio已经默认帮我们安装好了grafana和prometheus,只是对应的Service类型是clusterIP类型,表示集群内部可以访问，如果我们需要能够通过浏览器访问，我们只需要ingress访问规则即可，ingress之前已经介绍过了，这边就不在重复了 配置prometheus-ingress.yaml和grafana-ingress.yaml配置文件 prometheus-ingress.yaml #ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: prometheus-ingress namespace: istio-system spec: rules: - host: prometheus.istio.qy.com http: paths: - path: / backend: serviceName: prometheus servicePort: 9090 执行命令 kubectl apply -f prometheus-ingress.yaml和grafana-ingress.yaml grafana-ingress.yaml #ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: grafana-ingress namespace: istio-system spec: rules: - host: grafana.istio.qy.com http: paths: - path: / backend: serviceName: grafana servicePort: 3000 ~ 执行命令 kubectl get ingress -n istio-system 配置prometheus访问域名 在hosts文件里面增加ip域名的映射关系 192.168.187.137 prometheus.istio.qy.com 配置grafana访问域名 在hosts文件里面增加ip域名的映射关系 192.168.187.137 grafana.istio.qy.com 访问prometheus 浏览器输入地址：prometheus.istio.qy.com 访问grafana 设置prometheus地址 找到prometheus在k8s里面服务地址 命令 kubectl get svc -o wide -n istio-system 选择DataSources 选择settings ，把url改成prometheus即可 项目案例：bookinfo 理解什么是bookinfo 这是istio官方给我们提供的案例，Bookinfo 应用中的几个微服务是由不同的语言编写的。 这些服务对 Istio 并无依赖，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 `reviews` 服务具有多个版本。 下图展示了这个应用的端到端架构。 这个案例部署了一个用于演示Istio 特性的应用，该应用由四个单独的微服务构成。 这个应用模仿在线书店的一个分类，显示一本书的信息。 页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。 Bookinfo 应用分为四个单独的微服务： productpage. 这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details. 这个微服务中包含了书籍的信息。 reviews. 这个微服务中包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings. 这个微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 4个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 4个红色星形图标来显示评分信息。 大家一定要从spring cloud思维模式里面跳出来，站着服务网格的立场上思考问题，我们是不需要了解服务的业务代码是什么样的，业务的服务只需要交给istio管理即可 所以大家一定要有一颗拥抱变化的心 sidecar自动注入到微服务 所以第一步我们需要给每一个服务配置一个Sidecar,但是配置sidecar我们前面也说过，可以有两种方式实现，一种是手动注入，一种是自动注入，如果自动注入需要与命名空间相关，需要准备一个命名空间 查看命名空间： kubectl get ns 这个时候我们需要创建一个命名空间，需要打上一个lable表示只要是在这个lable的命名空间里面的都自动注入 创建命名空间命令： kubectl create namespace bookinfo-ns 给命名空间加上label命令 kubectl label namespace bookinfo-ns istio-injection=enabled 查看命名空间下有哪些label命 kubectl get ns bookinfo-ns --show-labels 启动bookinfo 进入istio安装目录：/home/tools/istio-1.0.6/samples/bookinfo/platform/kube 找到bookinfo.yaml文件 查看需要的image个数： cat bookinfo.yaml | grep image： 里面就是bookinfo案例所需要依赖的镜像地址 执行命令 kubectl apply -f bookinfo.yaml -n bookinfo-ns 查看pod情况 kubectl get pods -n bookinfo-ns # 会发现有两个container,有两个container的原因是因为我们有自动注入，这边有六个服务，其实只要四个服务，有一个服务有三个版本仅此而已 查看pod明细 kubectl describe pods pod名字 -n bookinfo-ns # 例如：kubectl describe pods reviews-v1-fd6c96c74-cmqcx -n bookinfo-ns 会发现有两个container,一个是我们自己的container,另外一个是自动注入的代理container 检查一下service kubectl get svc -n bookinfo-ns 可以看到service的类似clusterip类型 验证Bookinfo 应用是否正在运行 请在某个 Pod 中用 curl 命令对应用发送请求，例如 ratings 执行命令 kubectl exec -it $(kubectl get pod -l app=ratings -n bookinfo-ns -o jsonpath='{.items[0].metadata.name}') -c ratings -n bookinfo-ns -- curl productpage:9080/productpage | grep -o \".*\" 看到如图所示表示bookinfo启动成功 命令分析一下 kubectl get pod -l app=ratings -n bookinfo-ns -o jsonpath='{.items[0].metadata.name}';表示的是输出ratings 这个运行时pod的名字 kubectl exec -it $(kubectl get pod -l app=ratings -n bookinfo-ns -o jsonpath='{.items[0].metadata.name}') -c ratings -n bookinfo-ns -- curl productpage:9080/productpage | grep -o \".*\"：进入到ratings内部，然后发送一个http测试，根据响应结果找到title标签 思考：能否通过页面的方法访问bookinfo项目呢 所以下面我们用ingress方式访问bookinfo 通过ingress方式访问 找到productpage-service服务的端口，打开bookInfo.yaml文件 需要给productpage暴露的9080端口进行ingress域名绑定 新建productpageIngress.yaml #ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: productpage-ingress spec: rules: - host: productpage.istio.qy.com http: paths: - path: / backend: serviceName: productpage servicePort: 9080 查询productpage这个pod分布在那台服务上，执行命令 kubectl get pods -o wide -n bookinfo-ns 服务发现在w1机器上 配置hosts文件 192.168.187.137 productpage.istio.qy.com 执行命令 kubectl apply -f productpageIngress.yaml -n bookinfo-ns 访问地址：productpage.istio.qy.com 点击Normal user,查看书籍相关的评论 不停的刷新会有三个Reviews版本一个是不带星的一个是带黑色星的一个是带红星的，跟架构图一样的 通过istio的ingressgateway访问 确定 Ingress 的 IP 和端口 现在 Bookinfo 服务启动并运行中，需要使应用程序可以从外部访问 Kubernetes 集群，例如使用浏览器。可以用Istio Gateway来实现这个目标。 为应用程序定义 Ingress 网关 地址：/home/tools/istio-1.0.6/samples/bookinfo/networking有一个bookinfo-gateway.yaml kubectl apply -f bookinfo-gateway.yaml -n bookinfo-ns 查看gateway kubectl get gateway -n bookinfo-ns 有了gateway之后我们需要配置一些环境变量 配置gateway ip环境 export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}') # 把ingressgateway的ip设置成环境变量 分析命令意思 kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}' # 表示获取istio组件ingressgateway组件的ip 也就是说192.168.187.137就是ingressgateway组件的ip 配置gateway端口 export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}') 把ingressgateway的端口设置成环境变量 分析命令意思 kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}' # 表示获取istio组件ingressgateway组件的端口 设置gateway地址 把前面的host跟端口组成gateway地址 export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT 查看INGRESS_PORT环境端口 env | grep INGRESS_PORT 测试 http://192.168.187.137:31380/productpage 不停的刷新会有三个Reviews版本一个是不带星的一个是带黑色星的一个是带红星的，跟架构图一样的 流量管理 放开bookinfo自定义路由权限 这个文件也是起到了一个路由的功能，必须先执行这个文件之后gateway路由规则才可以自定义 执行destination-rule-all.yaml kubectl apply -f destination-rule-all.yaml -n bookinfo-ns 查看 kubectl get DestinationRule -n bookinfo-ns 打开destination-rule-all.yaml 分析一下 里面定义了各个微服务的路由资源 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule # 声明了一个资源，这个资源也是需要依赖于crd metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 # 版本 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule # 声明了一个资源，这个资源也是需要依赖于crd metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 # 版本 - name: v2 labels: version: v2 # 版本 - name: v3 labels: version: v3 # 版本 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 基于版本方式控制 只需要在/home/tools/istio-1.0.6/samples/bookinfo/networking下执行virtual-service-reviews-v3.yaml即可 打开virtual-service-reviews-v3.yaml文件 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v3 此时会把所有的路由的流量全部都切换到v3版本也就是全部都是红星的版本 执行命令 kubectl apply -f virtual-service-reviews-v3.yaml -n bookinfo-ns 再次刷新页面 http://192.168.187.137:31380/productpage 删除版本控制命令 kubectl delete -f virtual-service-reviews-v3.yaml -n bookinfo-ns 再次刷新页面有其它版本了，这是基于版本的方式来控制流量 基于权重的流量版本控制 只需要在/home/tools/istio-1.0.6/samples/bookinfo/networking下执行virtual-service-reviews-50-v3.yaml即可 打开virtual-service-reviews-50-v3.yaml文件 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 # 50%的流量到v1 - destination: host: reviews subset: v3 weight: 50 # 50%的流量到v3 此时会把所有的路由的流量会在v1和v3之间进行切换，也就是无星和红星页面 执行命令 kubectl apply -f virtual-service-reviews-50-v3.yaml -n bookinfo-ns 再次刷新页面 http://192.168.187.137:31380/productpage 无星页面和红星页面之间切换 删除命令 kubectl delete -f virtual-service-reviews-50-v3.yaml -n bookinfo-ns 基于用户来控制流量版本 只需要在/home/tools/istio-1.0.6/samples/bookinfo/networking下执行virtual-service-reviews-jason-v2-v3.yaml即可 打开virtual-service-reviews-jason-v2-v3.yaml文件 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v3 在登录的时候会在header头部增加一个jason，如果是jason登录那么会访问v2版本，其它的人访问的是v3 执行命令 kubectl apply -f virtual-service-reviews-jason-v2-v3.yaml -n bookinfo-ns 再次刷新页面 http://192.168.187.137:31380/productpage 全是红星页面,因为我不是jason用户所以流量全都在v3版本 删除命令 kubectl delete -f virtual-service-reviews-jason-v2-v3.yaml -n bookinfo-ns 故障注入 为了测试微服务应用程序 Bookinfo 的弹性，在访问的的时候会在header头部增加一个jason，如果是jason访问那么会访问v2版本，其它的人访问的是v3。 访问v3版本的人会注入一个50%几率的延迟2S请求访问。 故障注入：可以故意引发Bookinfo 应用程序中的 bug。尽管引入了 2 秒的延迟，我们仍然期望端到端的流程是没有任何错误的。 创建故障注入规则-执行:test.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - fault: delay: percent: 50 fixedDelay: 2s route: - destination: host: reviews subset: v3 执行:test.yaml kubectl apply -f test.yaml -n bookinfo-ns 测试 1.通过浏览器打开 Bookinfo 应用。 2.使用headers头部不包含jason关机键， 访问到 /productpage 页面。 3.你期望 Bookinfo 主页在有50%几率大约 2 秒钟加载完成并且没有错误，有50%的几率正常加载 4.查看页面的响应时间： 打开浏览器的 开发工具 菜单 打开 网络 标签 重新加载 productpage 页面。你会看到页面加载实际上用了大约 6s。 流量的迁移 一个常见的用例是将流量从一个版本的微服务逐渐迁移到另一个版本。在 Istio 中，您可以通过配置一系列规则来实现此目标， 这些规则将一定百分比的流量路由到一个或另一个服务。在本任务中，您将会把 50％ 的流量发送到 reviews:v1，另外 50％ 的流量发送到 reviews:v3。然后，再把 100％ 的流量发送到 reviews:v3 来完成迁移。 (1)让所有的流量都到v1 kubectl apply -f virtual-service-all-v1.yaml (2)将v1的50%流量转移到v3 kubectl apply -f virtual-service-reviews-50-v3.yaml (3)确保v3版本没问题之后，可以将流量都转移到v3 kubectl apply -f virtual-service-reviews-v3.yaml (4)访问测试，看是否都访问的v3版本 体验Istio的Observe(观察) 观察mixer组件上报的服务数组 采集指标：自动为Istio生成和收集的应用信息，可以配置的YAML文件 进入bookinfo/telemetry目录下面 如果需要metrics收集日志，需要先执行 kubectl apply -f metrics-crd.yaml 检查一下 kubectl get instance -n istio-system 多次属性页面让metrics收集数据：http://192.168.187.137:31380/productpage 现在需要访问普罗米修斯看看有没有拿到metrics收集到的数据，我们可以通过ingress来访问 检查普罗米修斯ingress存不存在 kubectl get ingress -n istio-system 不存在则启动ingress kubectl apply -f prometheus-ingress.yaml 访问普罗米修斯域名 prometheus.istio.qy.com 检查一下有没有数据筛选选择： istio_requests_total 收集原理： 启动grafana来可视化查看，检查grafana的ingress存不存在 kubectl get ingress -n istio-system 启动 kubectl apply -f grafana-ingress.yaml 访问grafana域名 grafana.istio.qy.com 配置grafana对应的普罗米修斯ip 查找普罗米修斯ip kubectl get svc -o wide -n istio-system 访问istio mixer 这边就可以看到内存和CPU使用情况了 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-22 09:22:11 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/":{"url":"source/","title":"二、源码专题","keywords":"","body":"源码专题 本章主要说常用框架的源码，mybatis持久层框架和spring应用程序框架 mybatis持久层框架 1.基本使用 2源码分析 3.mybatis源码剖析-黑马 spring应用程序框架 1.IOC容器与源码 2.AOP事物原理 3.SpringMvc原理解析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:05 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/mybatis/":{"url":"source/mybatis/","title":"mybatis持久层框架","keywords":"","body":"mybatis持久层框架 1.基本使用 2源码分析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:05 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/mybatis/base-use.html":{"url":"source/mybatis/base-use.html","title":"1.基本使用","keywords":"","body":"Mybatis基本使用 熟悉mybatis 源码分析 带徒手mybatis 传统JDBC的弊端： 1、jdbc底层没有用连接池、操作数据库需要频繁的创建和关联链接。消耗很大的资源 2、写原生的jdbc代码在java中，一旦我们要修改sql的话，java需要整体编译，不利于系统维护 3、使用PreparedStatement预编译的话对变量进行设置123数字，这样的序号不利于维护 4、返回result结果集也需要硬编码。 mybatis介绍： Mybatyis:Object relation mapping对象关系映射 快速开始mybatis（xml方式）： 1、maven org.mybatis mybatis x.x.x 2、mybatis-config.xml 3、Mapper.xml Mybatis全局配置详解： Mybatis之annotation： public interface**UserMapper { @Select(\"select * from user where id=#{id}\") public User selectUser(Integer id); } Mybatis之注解和xml优缺点: Xml：增加xml文件、麻烦、条件不确定、容易出错，特殊字符转义 注释：不适合复杂sql，收集sql不方便，重新编译 Mybatis之#与$区别： 参数标记符号 '#'预编译，防止sql注入(推荐) $可以sql注入，代替作用 Mybatis之parameterType与parameterMap区别： 通过parameterType指定输入参数的类型，类型可以是简单类型、hashmap、pojo的包装类型 Mybatis之resultType与resultMap区别： 使用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。 mybatis中使用resultMap完成高级输出结果映射。 Mybatis逆向工程： 什么是逆向工程: MyBatis的一个主要的特点就是需要程序员自己编写sql，那么如果表太多的话，难免会很麻烦，所以mybatis官方提供了一个逆向工程，可以针对单表自动生成mybatis执行所需要的代码（包括mapper.xml、mapper.java、po..）。一般在开发中，常用的逆向工程方式是通过数据库的表生成代码 1、引入jar org.mybatis.generator mybatis-generator-maven-plugin 1.3.7 mysql mysql-connector-java ${mysql-connector-java.version} 2、配置mybatis-genrtator.xml 3、mybatis-generator:generate Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:06 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/mybatis/analysis.html":{"url":"source/mybatis/analysis.html","title":"2源码分析","keywords":"","body":"mybatis核心概念 宏观 微观 画图 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:05 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/mybatis/mybatis-source.html":{"url":"source/mybatis/mybatis-source.html","title":"3.mybatis源码剖析-黑马","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1 MyBatis 框架概述 2 JDBC缺陷总结 3 MyBatis快速入门 3.1 搭建 Mybatis 开发环境 2.2.1 创建Maven工程 2.2.2 导入依赖 2.2.2 创建User 2.2.3 创建UserMapper接口 2.2.4 创建UserMapper.xml 2.2.5 创建SqlMapConfig.xml 2.2.6 创建log4j.properties 2.3 编写测试类 4 自定义 Mybatis 框架 4.1 MyBatis框架设计模式分析 4.2 执行查询所有用户的SQL语句必须的步骤 4.3 基于JDBC实现封装流程分析 4.4 准备工作 4.4.1 需要使用到的jar包:log4j,mysql驱动,dom4j以及其XPath,c3p0连接池 4.4.2 拷贝文件 4.4.3 工程错误改造 4.4.3.1 创建Resources类 4.4.3.2 创建SqlSessionFactoryBuilder类 4.4.3.3 创建SqlSessionFactory接口及其实现类 4.4.3.4 创建SqlSession接口及其实现类 4.5 获取Connection实现 4.5.3 改造Configuration对象，使它具备获取数据库链接Connection的功能 4.5.4 解析UserMapper.xml，提取SQL语句和返回参数类型 4.5.4.1 创建Mapper对象，存储SQL语句和返回类型的全限定名 4.5.4.2 封装一个loadMapper方法将解析映射配置文件的数据封装到Mapper对象中 4.5.4.3 将Mapper存入到Configuration中 4.5.4.4 改造Configuration，让它能够存储Mapper信息 4.6 增删改查流程串联 4.6.1 增删改查思路分析 4.6.2 改造工程 4.6.2.1 改造DefaultSqlSession 4.6.2.2 改造DefaultSqlSessionFactory 4.6.2.3 改造SqlSessionFactoryBuilder 4.6.3 使用动态代理创建代理对象 4.6.3.1 修改SqlSession 4.6.3.2 修改DefaultSqlSession添加代理实现 4.6.3.3 动态代理实现增删改查 4.6.3.4 在SqlSession中编写增删改查，在DefaultSqlSession中实现增删改查 4.6.3.5 Converter转换器编写 4.6.3.6 在getMapper的代理实现中引入SqlSession，通过调用对应增删改查实现数据库操作 4.6.3.7 执行SQL代码封装Executor 总结 MyBatis源码剖析 1 MyBatis 框架概述 mybatis 是一个优秀的基于 java 的持久层框架，它内部封装了 jdbc，使开发者只需要关注 sql 语句本身，而不需要花费精力去处理加载驱动、创建连接、创建 statement 等繁杂的过程。 mybatis 通过 xml或注解的方式将要执行的各种 statement 配置起来，并通过 java 对象和 statement 中 sql的动态参数进行映射生成最终执行的 sql 语句，最后由 mybatis 框架执行 sql 并将结果映射为 java 对象并返回。采用 ORM 思想解决了实体和数据库映射的问题，对 jdbc 进行了封装，屏蔽了 jdbc api 底层访问细节，使我们不用与 jdbc api 打交道，就可以完成对数据库的持久化操作。 2 JDBC缺陷总结 数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 Sql 语句在代码中硬编码，造成代码不易维护，实际应用 sql 变化的可能较大， sql 变动需要改变java 代码。 使用 preparedStatement 向占有位符号传参数存在硬编码，因为 sql 语句的 where 条件不一定，可能多也可能少，修改 sql 还要修改代码，系统不易维护。 对结果集解析存在硬编码（查询列名）， sql 变化导致解析代码变化，系统不易维护，如果能将数据库记录封装成 pojo 对象解析比较方便。 3 MyBatis快速入门 3.1 搭建 Mybatis 开发环境 创建工程之前，我们先新建数据库mybatis,并在数据库中新建一张User表，并加一些数据。表包含:id,username,birthday,sex,address字段 2.2.1 创建Maven工程 2.2.2 导入依赖 mybatis mysql驱动 log4j junit单元测试 4.0.0 com.itheima mybatis-day01-demo1 1.0-SNAPSHOT jar UTF-8 UTF-8 org.mybatis mybatis 3.4.5 mysql mysql-connector-java 5.1.6 runtime log4j log4j 1.2.12 junit junit 4.12 test src/main/java **/*.xml 2.2.2 创建User 创建com.itheima.domain包，在该包下创建User对象，并添加对应的属性。 public class User implements Serializable { private Integer id; private String username; private Date birthday; private String sex; private String address; //略 get...set...toString... } 2.2.3 创建UserMapper接口 创建com.itheima.mapper包，并在该包下创建接口，代码如下： 它其实就是dao层的接口 public interface UserMapper { List findAll(); } 2.2.4 创建UserMapper.xml 这个xml配置文件的位置，必须和对应的那个Mapper接口的位置一样。而且其文件名也要和接口名一样 在com.itheima.mapper包下创建UserMapper.xml,并在UserMapper.xml中添加一个select查询结点，代码如下： SELECT * FROM user 2.2.5 创建SqlMapConfig.xml 在main/resources下创建SqlMapConfig.xml,在文件中配置数据源信息和加载映射文件，代码如下： 这个配置文件的名字不是固定的，你可以随便命名 2.2.6 创建log4j.properties 为了方便查看日志，在main/resources下创建log4j.properties文件，代码如下： log4j.rootLogger=DEBUG,Console log4j.appender.Console=org.apache.log4j.ConsoleAppender log4j.appender.Console.layout=org.apache.log4j.PatternLayout log4j.appender.Console.layout.ConversionPattern=%d [%t] %-5p [%c] - %m%n log4j.logger.org.apache=DEBUG 2.3 编写测试类 在test包下创建com.itheima.test,再在该包下创建MyBatisTest类，代码如下： public class MyBatisTest { @Test public void testFindAll() throws IOException { //读取配置文件 InputStream is = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //创建SqlSessionFactoryBuilder对象 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); //通过SqlSessionBuilder对象构建一个SqlSessionFactory SqlSessionFactory sqlSessionFactory = builder.build(is); //通过SqlSessionFactory构建一个SqlSession SqlSession session = sqlSessionFactory.openSession(); //通过SqlSession实现增删改查 UserMapper userMapper = session.getMapper(UserMapper.class); List users = userMapper.findAll(); //打印输出 for (User user : users) { System.out.println(user); } //关闭资源 session.close(); is.close(); } } 4 自定义 Mybatis 框架 本章我们将使用前面所学的基础知识来构建一个属于自己的持久层框架，将会涉及到的一些知识点：工厂模式（Factory 工厂模式）、构造者模式（Builder 模式）、动态代理模式，反射， xml 解析，数据库元数据，数据库元数据等。 4.1 MyBatis框架设计模式分析 我们来看看MyBatis框架使用过程中用到的一些设计模式。 使用SqlSessionFactoryBuilder创建SqlSessionFactory工厂对象的时候使用的是构建者模式 使用SqlSessionFactory创建SqlSession时候使用的是工厂模式 使用SqlSession创建UserMapper接口代理对象的时候使用的是动态代理模式 4.2 执行查询所有用户的SQL语句必须的步骤 需要获取连接 需要SQL语句 需要将结果集中的数据封装到JavaBean中 4.3 基于JDBC实现封装流程分析 基于上面我们说到的JDBC流程再结合MyBatis流程，我们封装一个持久城框架，达到MyBatis中的增删改查效果。来分析一波： 通过上图分析，我们可以发现: 通过解析主配置文件SqlMapConfig.xml可以获取username，password，driver，url等信息，通过这些信息能够获取Connection对象 通过解析映射配置文件UserMapper.xml可以获取要执行的SQL语句以及结果集要封装到的JavaBean类的全限定名resultType 通过反射机制和数据库元数据可以将结果集中的数据遍历出来并封装到对应的JavaBean对象中 4.4 准备工作 4.4.1 需要使用到的jar包:log4j,mysql驱动,dom4j以及其XPath,c3p0连接池 4.4.2 拷贝文件 分别将上一个入门工程中的User.java、UserMapper.java、MyBatisTest.java、UserMapper.xml、log4j.properties、SqlMapConfig.xml都拷贝到该工程中，将XML中引用的DTD文件约束去掉，不然每次解析都会去网上下载。 4.0.0 com.itheima mybatis-day01-demo2-custom 1.0-SNAPSHOT jar log4j log4j 1.2.12 mysql mysql-connector-java 5.1.36 dom4j dom4j 1.6.1 jaxen jaxen 1.1.3 c3p0 c3p0 0.9.1.2 junit junit 4.12 src/main/java **/*.xml 4.4.3 工程错误改造 我们看到上面的工程存在错误，我们对他进行改造一下，首先创建对应的文件来去掉错误。 4.4.3.1 创建Resources类 该类的主要作用是读取类路径下的资源文件，所以要求被它读取的文件务必放到classes下，我们创建它的目的主要是模拟加载读取UserMapper.xml和SqlMapConfig.xml文件。 public class Resources { public static InputStream getResourceAsStream(String path){ InputStream is = Resources.class.getClassLoader().getResourceAsStream(path); return is; } } 此时MyBatisTest中的Resources类就引用上面创建的类就可以去掉一个错误了。 4.4.3.2 创建SqlSessionFactoryBuilder类 创建该类，并创建一个build方法返回一个SqlSessionFactory对象，但SqlSessionFactory还没创建，所以接着我们需要创建它。 public class SqlSessionFactoryBuilder { public SqlSessionFactory build(InputStream is) { return null; } } 4.4.3.3 创建SqlSessionFactory接口及其实现类 创建SqlSessionFactory接口及其实现类，并且创建一个openSession方法。 public interface SqlSessionFactory { SqlSession openSession(); } public class DefaultSqlSessionFactory implements SqlSessionFactory { @Override public SqlSession openSession() { return null; } } 4.4.3.4 创建SqlSession接口及其实现类 创建SqlSession接口，并在接口里面创建对应方法，然后创建一个DefaultSqlSession实现类 public interface SqlSession { UserMapper getMapper(Class userMapperClass); void close(); } public class DefaultSqlSession implements SqlSession { @Override public UserMapper getMapper(Class userMapperClass) { return null; } @Override public void close() { } } 把MyBatisTest类重新导包后，错误就全部消失了。接着我们就要开始对每个模块展开分析和代码实现了。 4.5 获取Connection实现 我们回到刚才我们的分析，首先我们要解析SqlMapConfig.xml,并把信息存储到Configuration对象中，然后通过Configuration对象获取数据库连接对象Connection。我们可以分这么几个步骤完成： 创建XMLConfigBuilder解析SqlMapConfig.xml 创建Configuration对象，存储解析的数据库连接信息 改造Configuration对象，使它具备获取数据库链接Connection的功能 #####4.5.1 创建Configuration对象，存储解析的数据库连接信息 public class Configuration { private String username; private String password; private String url; private String driver; //get..set..toString.. } #####4.5.2 创建XMLConfigBuilder解析SqlMapConfig.xml public class XMLConfigBuilder{ public static Configuration loadConfiguration(InputStream is){ try { //1）数据库配置信息存储 Configuration cfg = new Configuration(); //创建SAXReader对象读取XML文件字节输入流 SAXReader reader = new SAXReader(); Document document = reader.read(is); //解析配置文件,获取根节点信息,//property表示获取根节点下所有的property结点对象 List rootList = document.selectNodes(\"//property\"); //循环迭代所有结点对象 for (Element element : rootList) { //name属性的值 String name = element.attributeValue(\"name\"); //vallue属性的值 String value = element.attributeValue(\"value\"); //2）将解析的数据库连接信息存储到Configuration中 //数据库驱动 if(name.equals(\"driver\")){ cfg.setDriver(value); }else if(name.equals(\"url\")){ //数据库连接地址 cfg.setUrl(value); }else if(name.equals(\"username\")){ //数据库账号 cfg.setUsername(value); }else if(name.equals(\"password\")){ //数据库密码 cfg.setPassword(value); } } //获取需要解析的XML路径 String resource = element.attributeValue(\"resource\"); //拿到映射配置文件的路径 ...接下来看下面的分析准备解析映射配置文件 } catch (Exception e) { e.printStackTrace(); } } } 4.5.3 改造Configuration对象，使它具备获取数据库链接Connection的功能 在Configuration对象中创建ComboPooledDataSource对象，并创建获得数据源的方法getDataSource，再创建一个获得Connection的方法getConnection，getConnection通过调用getDataSource获得数据源，然后获得Connection对象。 public class Configuration { //数据库用户名 private String username; //数据库用户密码 private String password; //数据库连接地址 private String url; //数据库驱动 private String driver; //创建数据源 private ComboPooledDataSource dataSource = new ComboPooledDataSource(); //get..set..toString.. //获取数据源 private DataSource getDataSource(){ //设置数据源配置 try { dataSource.setUser(username); dataSource.setPassword(password); dataSource.setJdbcUrl(url); dataSource.setDriverClass(driver); } catch (PropertyVetoException e) { e.printStackTrace(); } return dataSource; } public Connection getConnection(){ try { return getDataSource().getConnection(); } catch (SQLException e) { e.printStackTrace(); } return null; } } 4.5.4 解析UserMapper.xml，提取SQL语句和返回参数类型 UserMapper.xml内容： SELECT * FROM user 接着上面流程，我们需要解析UserMapper.xml获取SQL语句，并获取返回值的类型，这个时候我们可以考虑封装一个Mapper对象，存储对应的SQL语句和返回值类型。针对这个操作实现，我们可以分为下面几个步骤实现： 解析UserMapper.xml获得SQL语句和返回类型的全限定名 创建Mapper对象，存储SQL语句和返回类型的全限定名 将解析的结果封装到Mapper中 4.5.4.1 创建Mapper对象，存储SQL语句和返回类型的全限定名 Mapper对象用于存储SQL语句和返回的JavaBean全限定名，这时候我们可以考虑定义2个属性来接收存储。 public class Mapper { //执行的SQL语句 private String sql; //执行SQL语句后要返回的JavaBean全限定名 private String resultType; //带参构造函数 public Mapper(String sql, String resultType) { this.sql = sql; this.resultType = resultType; } //get..set..toString } 4.5.4.2 封装一个loadMapper方法将解析映射配置文件的数据封装到Mapper对象中 我们接着将刚才解析的XML信息存储到Mapper中。我们分析下，目前我们只存在一个select结点，如果以后存在多个怎么操作？如下代码： SELECT * FROM user SELECT * FROM user WHERE id=1 我们可以把多个select结点信息封装成多个Mapper，并将多个Mapper存储到Map中，key可以用id来表示，但这种情况只适合单个UserMapper.xml文件，如果我们再创建一个TeacherMapper.xml解析，同样也有select id=\"findAll\"的话，多个文件解析，key会冲突。解决这种冲突问题，我们可以把namespace也作为key的一部分，只要namespace不冲突就可以保证key不冲突，key=namespace+.+id 即可。代码如下： /** * 解析UserMapper.xml，提取SQL语句和返回JavaBean全限定名 * path为UserMapper.xml的路径 */ public static Map loadMapper(String path){ /**** * 1）定义一个Map mappers * 用于存储解析的XML封装的Mapper信息 */ Map mappers = new HashMap(); try { //获得文件字节输入流 InputStream is = Resources.getResourceAsStream(path); //创建SAXReader对象,加载文件字节输入流 SAXReader reader = new SAXReader(); Document document = reader.read(is); //获得根节点 Element rootElement = document.getRootElement(); //获取命名空间的值 String namespace = rootElement.attributeValue(\"namespace\"); //获取所有select结点 List selectList = document.selectNodes(\"//select\"); //循环所有select结点 for (Element element : selectList) { //获取ID属性值 String id = element.attributeValue(\"id\"); //获取resultType属性值 String resultType = element.attributeValue(\"resultType\"); //获取SQL语句 String sql = element.getText(); //2）构建Mapper对象 Mapper mapper = new Mapper(sql,resultType); //key = namespace+.+id; String key = namespace+\".\"+id; //存储到Map中 mappers.put(key,mapper); } return mappers; } catch (DocumentException e) { e.printStackTrace(); } return mappers; } 4.5.4.3 将Mapper存入到Configuration中 按照JDBC操作流程，最终调用SQL语句的是PreparedStatment对象，而PreparedStatment对象由Connection对象构建，所以我们可以把SQL语句给Configuration对象管理。按照这个思路，可以在获取Configuration对象中创建一个Map来存储Mapper，可以直接在解析完SqlMapConfig.xml后接着解析UserMapper.xml并将解析的Map返回并填充到Configuration中。另外loadMapper(path)中path其实是SqlMapConfig.xml中的中指定的XML，可以直接解析它，把它的值传递给loadMapper(path)方法。因此我们这里可以分为3个步骤： 改造Configuration，让它能够存储Mapper信息 解析mapper，获取UserMapper.xml路径 调用loadMapper解析所有XML结点获取Mapper对象并填充给Configuration 4.5.4.4 改造Configuration，让它能够存储Mapper信息 这里主要添加了一个Map mappers属性和对应的get、set方法。这里注意为了避免每次set方法调用的时候把之前解析存储的信息替换，所以直接new 了一个Map，set的时候只是往Map中塞数据。 public class Configuration { //数据库用户名 private String username; //数据库用户密码 private String password; //数据库连接地址 private String url; //数据库驱动 private String driver; //创建数据源 private ComboPooledDataSource dataSource = new ComboPooledDataSource(); //存储所有SQL语句和返回值全限定名 private Map mappers = new HashMap(); public Map getMappers() { return mappers; } //这里的set方法为了保证每次填充进来的数据不被覆盖，直接调用putAll塞进Map中 public void setMappers(Map mappers) { this.mappers.putAll(mappers); } //...略 } 4.6 增删改查流程串联 4.6.1 增删改查思路分析 我们再来看看MyBatis操作的流程，从代码中我们可以看到，通过SqlSession的getMapper方法创建的代理对象是具备查询数据库功能的，也就是说它拥有操作数据库的能力，而操作数据库的能力的前提是能获得连接数据库Connection对象，我们可以基于这个想法，把上面获得的Configuration对象给DefaultSqlSession对象，这样就能通过SqlSession构建一个代理对象，对赋予这个代理对象操作数据库的能力。 @Test public void testFindAll() throws IOException { //读取配置文件 InputStream is = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //创建SqlSessionFactoryBuilder对象 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); //通过SqlSessionBuilder对象构建一个SqlSessionFactory SqlSessionFactory sqlSessionFactory = builder.build(is); //通过SqlSessionFactory构建一个SqlSession SqlSession session = sqlSessionFactory.openSession(); //通过SqlSession实现增删改查 UserMapper userMapper = session.getMapper(UserMapper.class); List users = userMapper.findAll(); //打印输出 for (User user : users) { System.out.println(user); } //关闭资源 session.close(); is.close(); } 那么我们会有下面这些疑问： 1）什么时候加载解析配置文件呢？ 答：我们可以在SqlSessionFactory.openSqlSession()的时候，初始化加载上面配置文件。 2）加载配置文件会初始化数据库连接信息，这时候需要加载读取SqlMapConfig.xml配置文件，如何通知程序读取这个配置文件？ 答：早在我们第一步的时候就加载读取了SqlMapConfig.xml文件获取了文件字节输入流，我们可以在构建SqlSessionFactory对象的时候把它传给build方法，如果这时候DefaultSqlSessionFactory中可以接受这个文件字节输入流，那么在openSqlSession()的时候，就可以把这个字节输入流传给XMLConfigBuilder来解析，并获取对应的配置。 3）上面说到让DefaultSqlSession对象具备操作数据库的能力，需要把Configuration对象给DefaultSqlSession对象，如果做到呢？ 答：可以在解析XML对象的时候，直接把DefaultSqlSession的实例传给XMLConfigBuilder.loadConfiguration(DefaultSqlSession session,InputStream is) 4.6.2 改造工程 4.6.2.1 改造DefaultSqlSession 在DefaultSqlSession中加上Configuration对象，让他具备操作数据库的能力，创建set方法给Configuration赋值，代码如下： public class DefaultSqlSession implements SqlSession { //把Configuration对象给DefaultSqlSession private Configuration cfg; //创建一个set方法，给Configuration赋值 public void setCfg(Configuration cfg) { this.cfg = cfg; } @Override public UserMapper getMapper(Class userMapperClass) { return null; } @Override public void close() { } } 4.6.2.2 改造DefaultSqlSessionFactory 改造DefaultSqlSessionFactory，加入SqlMapConfig.xml配置文件的字节输入流，并创建DefaultSqlSession对象，加入加载解析配置文件的方法loadConfiguration(sqlSession,is) public class DefaultSqlSessionFactory implements SqlSessionFactory { //SqlMapConfig.xml的字节输入流 private InputStream is; public void setIs(InputStream is) { this.is = is; } @Override public SqlSession openSession() { //创建一个DefaultSqlSession DefaultSqlSession sqlSession = new DefaultSqlSession(); //加载解析配置文件 Configuration cfg = XMLConfigBuilder.loadConfiguration(is); sqlSession.setCfg(cfg); return sqlSession; } } 4.6.2.3 改造SqlSessionFactoryBuilder 改造SqlSessionFactoryBuilder的build方法，创建DefaultSqlSessionFactory对象，并将SqlMapConfig.xml的字节输入流传给DefaultSqlSessionFactory。 public class SqlSessionFactoryBuilder { /*** * 读取并解析配置文件，构建一个SqlSessionFactory对象 * @param is * @return */ public SqlSessionFactory build(InputStream is) { //创建一个SqlSessionFactory的实例 DefaultSqlSessionFactory sqlSessionFactory =new DefaultSqlSessionFactory(); //给SqlSessionFactory的is属性赋值 sqlSessionFactory.setIs(is); return sqlSessionFactory; } } 4.6.3 使用动态代理创建代理对象 4.6.3.1 修改SqlSession 把其中getMapper改成通用的方法 /*** * 改造成通用的方法 * @param clazz * @param * @return */ T getMapper(Class clazz); 4.6.3.2 修改DefaultSqlSession添加代理实现 @Override public T getMapper(Class clazz) { /***** * 参数： * 1)被代理对象的类加载器 * 2)字节数组，让代理对象和被代理对象有相同的行为[行为也就是有相同的方法] * 3)InvocationHandler:增强代码,需要使用提供者增强的代码，改代码是以接口的实现类方式提供的，通常用匿名内部内，但不绝对。 */ return (T) Proxy.newProxyInstance(clazz.getClassLoader(), new Class[]{clazz}, new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return null; } }); } 4.6.3.3 动态代理实现增删改查 按照JDBC操作流程，我们得先拿到Connection对象，再拿到SQL语句，再执行获取返回结果集，再将返回结果集封装成要的对象即可。因此我们需要DefaultSqlSession，通过它来实现增删改查，实现增删改查就需要获取当前所需的Mapper，Mapper里面包含需要执行的SQL语句和执行SQL语句后返回的结果集需要转换的JavaBean对象。 我们需要用DefaultSqlSession来实现增删改查，可以直接考虑在SqlSession接口中编写增删改查，让DefaultSqlSession完成增删改查的实现，因此我们这里只需要引入SqlSession即可。 按照上面这个分析，我们可以总结为如下几个步骤实现： 在SqlSession中编写增删改查，在DefaultSqlSession中实现增删改查 在getMapper的代理实现中确定当前操作所需要的Mapper 在getMapper的代理实现中引入SqlSession，通过调用对应增删改查实现数据库操作 4.6.3.4 在SqlSession中编写增删改查，在DefaultSqlSession中实现增删改查 修改SqlSession，在SqlSession中增加selectList方法 /*** * 集合查询 * @param * @return */ List selectList(String statement); 修改DefaultSqlSession，在DefaultSqlSession中实现selectList方法,其中集合查询我们用到了一个Converter转换器，转换器的写法紧接着在后面会列出。 @Override public List selectList(String statement) { //获取对应的Mapper Mapper mapper = cfg.getMappers().get(statement); //JDBC操作流程实现 if(mapper!=null){ //执行查询 Connection conn = null; PreparedStatement stm = null; ResultSet resultSet = null; try { //获取Connection对象 conn = cfg.getConnection(); //获取PreparedStatment stm = conn.prepareStatement(mapper.getSql()); //执行查询 resultSet = stm.executeQuery(); //调用Converter实现转换 List list = Converter.list(resultSet,Class.forName(mapper.getResultType())); return list; } catch (Exception e) { e.printStackTrace(); }finally { try { if(resultSet!=null){ resultSet.close(); } if(stm!=null){ stm.close(); } //关闭Connection this.close(); } catch (SQLException e) { e.printStackTrace(); } } } return null; } 4.6.3.5 Converter转换器编写 Converter转换器主要利用反射机制实现ResultSet转成JavaBean,代码实现如下： public class Converter { /** * 这个方法，是将结果集中的每一条数据封装到一个JavaBean中，多条数据就对应多个JavaBean，再将多个 JavaBean放到一个List集合中 * @param set * @param clazz * @param * @return */ public static List converList(ResultSet set, Class clazz){ List beans = new ArrayList(); //1.遍历结果集 try { //根据结果集元数据，获取结果集中的每一列的列名 ResultSetMetaData metaData = set.getMetaData(); int columnCount = metaData.getColumnCount();//获取总列数 while (set.next()){ //每次遍历，遍历出一条数据，每条数据就对应一个JavaBean对象 E o = (E) clazz.newInstance(); //获取每一列数据，根据列名获取 //for循环遍历出每一列 for(int i=1;i 4.6.3.6 在getMapper的代理实现中引入SqlSession，通过调用对应增删改查实现数据库操作 我们定义一个代理实现类，在代理实现类中通过被调用的方法来确定Mapper的key。然后直接调用SqlSession中定义的selectList方法。 public class MapperProxyFactory implements InvocationHandler { //1） private SqlSession sqlSession; public MapperProxyFactory(SqlSession sqlSession) { this.sqlSession = sqlSession; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //1、获取当前操作锁对应的Mapper信息 String className = method.getDeclaringClass().getName(); //类的名字，和UserMapper.xml中mapper的namespace一致 String methodName = method.getName(); //方法名字，和UserMapper.xml中的id值一致 String key = className+\".\"+methodName; //确定当前操作是否是查询所有 Class returnType = method.getReturnType(); if(returnType== List.class){ //2）执行集合查询操作 return sqlSession.selectList(key); }else{ return null; } } } DefaultSqlSession中的getMapper方法 @Override public T getMapper(Class clazz) { /***** * 参数： 1)被代理对象的类加载器 * 2)字节数组，让代理对象和被代理对象有相同的行为[行为也就是有相同的方法] * 3)InvocationHandler:增强代码,需要使用提供者增强的代码，改代码是以接口的实现类方式提供的，通常用匿名内部内，但不绝对。 */ return (T) Proxy.newProxyInstance(clazz.getClassLoader(), new Class[]{clazz}, new MapperProxyFactory(this)); } 4.6.3.7 执行SQL代码封装Executor 将DefaultSqlSession中selectList的代码封装到一个Executor的工具类中，方便使用。我们新建一个Executor的工具类。 public class Executor { /*** * 集合查询 * @param conn * @param mapper * @param * @return */ public static List list(Connection conn, Mapper mapper) { //执行查询 PreparedStatement stm = null; ResultSet resultSet = null; try { //获取PreparedStatment stm = conn.prepareStatement(mapper.getSql()); //执行查询 resultSet = stm.executeQuery(); //调用Converter实现转换 List list = Converter.list(resultSet, Class.forName(mapper.getResultType())); return list; } catch (Exception e) { throw new RuntimeException(e); } finally { try { if (resultSet != null) { resultSet.close(); } if (stm != null) { stm.close(); } if(conn!=null){ conn.close(); } } catch (SQLException e) { e.printStackTrace(); } } } } 修改DefaultSqlSession中的selectList方法 @Override public List selectList(String statement) { //获取对应的Mapper Mapper mapper = cfg.getMappers().get(statement); //JDBC操作流程实现 if(mapper!=null){ return Executor.list(cfg.getConnection(), mapper); } return null; } 总结 通过本文，我们对mybatis源码做了深入的剖析，使用了工厂模式、构建者模式、动态代理模式、DOM4J解析xml，反射、数据库元数据等等知识实现了对mybatis的自定义。相信假以时日，我们都能对框架做到知其然知其所以然。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:06 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/spring/":{"url":"source/spring/","title":"spring应用程序框架","keywords":"","body":"spring应用程序框架 1.IOC容器与源码 2.AOP事物原理 3.SpringMvc原理解析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:47:04 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/spring/ioc.html":{"url":"source/spring/ioc.html","title":"1.IOC容器与源码","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 知识点： 1、Ioc 理论概要 2、实体 Bean 的构建 3、bean 的基本特性 4、依赖注入 二、IOC 设计原理与实现 1、源码学习目标： 2、Bean 的构建过程 3、BeanFactory 与 ApplicationContext 区别 课程概要： IOC 核心知识点回顾 IOC 设计原理 一、IOC 核心理论回顾 知识点： Ioc 理念概要 实体 Bean 的创建 Bean 的基本特性 依赖注入 set 方法注入 构造方法注入 自动注入(byName、byType） 依赖检测1、Ioc 理论概要 在 JAVA 的世界中，一个对象 A 怎么才能调用对象 B？通常有以下几种方法。 类别 描述 时间点 外部传入 构造方法传入 属性设置传入 设置对象状态时 运行时做为参数传入 调用时 内部创建 属性中直接创建 创建引用对象时 初始化方法创建 创建引用对象时 运行时动态创建 调用时 上表可以看到， 引用一个对象可以在不同地点（其它引用者）、不同时间由不同的方法完成。如果 B 只是一个非常简单的对象 如直接 new B()，怎样都不会觉得复杂，比如你从来不会觉得创建一个 String 是一个件复杂的事情。但如果 B 是一个有着复杂依赖的 Service 对象，这时在不同时机引用 B 将会变得很复杂。 无时无刻都要维护 B 的复杂依赖关系，试想 B 对象如果项目中有上百过，系统复杂度将会成陪数增加。 IOC 容器 的出现正是为解决这一问题，其可以将对象的构建方式统一，并且自动维护对象的依赖关系，从而降低系统的实现成本。前提是需要提前对目标对象基于 XML 进行声明。 2、实体 Bean 的构建 基于 Class 构建 构造方法构建 静态工厂方法创建 FactoryBean 创建 1、基于 ClassName 构建 这是最常规的方法，其原理是在 spring 底层会基于 class 属性 通过反射进行构建。 2、构造方法构建 如果需要基于参数进行构建，就采用构造方法构建，其对应属性如下： name:构造方法参数变量名称 type:参数类型 index:参数索引，从 0 开始 value:参数值，spring 会自动转换成参数实际类型值 ref:引用容串的其它对象 3、静态工厂方法创建 如果你正在对一个对象进行 A/B 测试 ，就可以采用静态工厂方法的方式创建，其于策略创建不同的对像或填充不同的属性。 该模式下必须创建一个静态工厂方法，并且方法返回该实例，spring 会调用该静态方法创建对象。 public static HelloSpring build(String type) { if (type.equals(\"A\")) { return new HelloSpring(\"luban\", \"man\"); } else if (type.equals(\"B\")) { return new HelloSpring(\"diaocan\", \"woman\"); } else { throw new IllegalArgumentException(\"type must A or B\"); } } 4、FactoryBean 创建 指定一个 Bean 工厂来创建对象，对象构建初始化 完全交给该工厂来实现。配置 Bean 时指定该工厂类的类名。 public class LubanFactoryBean implements FactoryBean { @Override public Object getObject() throws Exception { return new HelloSpring(); } @Override public Class getObjectType() { return HelloSpring.class; } @Override public boolean isSingleton() { return false; } } 3、bean 的基本特性 作用范围 生命周期 装载机制 a、作用范围 很多时候 Bean 对象是无状态的 ，而有些又是有状态的 无状态的对象我们采用单例即可，而有状态则必须是多例的模式，通过 scope 即可创建 scope=“prototype” scope=“singleton” scope=“prototype 如果一个 Bean 设置成 prototype 我们可以 通过 BeanFactoryAware 获取 BeanFactory 对象即可每次获取的都是新对像。 b、生命周期 Bean 对象的创建、初始化、销毁即是 Bean 的生命周期。通过 init-method、destroy-method 属性可以分别指定期构建方法与初始方法。 如果觉得麻烦，可以让 Bean 去实现 InitializingBean.afterPropertiesSet()、DisposableBean.destroy()方法。分别对应 初始和销毁方法 c、加载机制 指示 Bean 在何时进行加载。设置 lazy-init 即可，其值如下： true: 懒加载，即延迟加载 false:非懒加载，容器启动时即创建对象 default:默认，采用 default-lazy-init 中指定值，如果 default-lazy-init 没指定就是 false 什么时候使用懒加载？ 懒加载会容器启动的更快，而非懒加载可以容器启动时更快的发现程序当中的错误 ，选择哪一个就看追求的是启动速度，还是希望更早的发现错误，一般我们会选 择后者。 4、依赖注入 试想 IOC 中如果没有依赖注入，那这个框架就只能帮助我们构建一些简单的 Bean，而之前所说的复杂 Bean 的构建问题将无法解决，spring 这个框架不可能会像现在这样成功。spring 中 ioc 如何依赖注入呢。有以下几种方式： set 方法注入 构造方法注入 自动注入(byName、byType） 方法注入(lookup-method) 2、set 方法注入 3、构造方法注入 4、自动注入（byName\\byType\\constructor) byName：基于变量名与 bean 名称相同作为依据插入 byType：基于变量类别与 bean 名称作 constructor：基于 IOC 中 bean 与构造方法进行匹配（语义模糊，不推荐） 5、依赖方法注入(lookup-method) 当一个单例的 Bean，依赖于一个多例的 Bean，用常规方法只会被注入一次，如果每次都想要获取一个全新实例就可以采用 lookup-method 方法来实现。 #编写一个抽像类 public abstract class MethodInject { public void handlerRequest() { // 通过对该抽像方法的调用获取最新实例 getFine(); } # 编写一个抽像方法 public abstract FineSpring getFine(); } // 设定抽像方法实现 该操作的原理是基于动态代理技术，重新生成一个继承至目标类，然后重写抽像方法到达注入目的。 前面说所单例 Bean 依赖多例 Bean 这种情况也可以通过实现 ApplicationContextAware、BeanFactoryAware 接口来获取 BeanFactory 实例，从而可以直接调用 getBean 方法获取新实例，推荐使用该方法，相比 lookup-method 语义逻辑更清楚一些。 二、IOC 设计原理与实现 1、源码学习的目标 2、Bean 的构建过程 3、BeanFactory 与 ApplicationContext 区别 1、源码学习目标： 不要为了读书而读书，同样不要为了阅读源码而读源码。没有目的一头扎进源码的黑森林当中很快就迷路了。到时就不是我们读源码了，而是源码‘毒’我们。毕竟一个框架是由专业团队，历经 N 次版本迭代的产物，我们不能指望像读一本书的方式去阅读它。 所以必须在读源码之前找到目标。是什么呢？ 大家会想，读源码的目标不就是为了学习吗？这种目标太过抽像，目标无法验证。通常我们会设定两类型目标：一种是对源码进行改造，比如添加修改某些功能，在实现这种目标的过程当中自然就会慢慢熟悉了解该项目。但然这个难度较大，耗费的成本也大。另一个做法是 自己提出一些问题，阅读源码就是为这些问题寻找答案。以下就是我们要一起在源码中寻找答案的问题： Bean 工厂是如何生产 Bean 的？ Bean 的依赖关系是由谁解来决的？ Bean 工厂和应用上文的区别？2、Bean 的构建过程 spring.xml 文件中保存了我们对Bean 的描述配置，BeanFactory 会读取这些配置然后生成对应的 Bean。这是我们对 ioc 原理的一般理解。但在深入一些我们会有更多的问题？ 配置信息最后是谁 JAVA 中哪个对象承载的？ 这些承载对象是谁业读取 XML 文件并装载的？ 这些承载对象又是保存在哪里？ BeanDefinition （Bean 定义） ioc 实现中 我们在 xml 中描述的 Bean 信息最后 都将保存至 BeanDefinition （定义）对象中，其中 xml bean 与 BeanDefinition 程一对一的关系。 由此可见，xml bean 中设置的属性最后都会体现在BeanDefinition中。如: XML-bean BeanDefinition class beanClassName scope scope lazy-init lazyInit constructor-arg ConstructorArgument property MutablePropertyValues factory-method factoryMethodName destroy-method AbstractBeanDefinition.destroyMethodName init-method AbstractBeanDefinition.initMethodName autowire AbstractBeanDefinition.autowireMode id name [ ] 演示查看 BeanDefinition 属性结构 BeanDefinitionRegistry（Bean 注册器） 在上表中我们并没有看到 xml bean 中的 id 和 name 属性没有体现在定义中，原因是 ID 其作为当前 Bean 的存储 key 注册到了 BeanDefinitionRegistry 注册器中。name 作为别名 key 注册到了 AliasRegistry 注册中心。其最后都是指向其对应的 BeanDefinition。 [ ] 演示查看 BeanDefinitionRegistry 属性结构 BeanDefinitionReader（Bean 定义读取） 至此我们学习了 BeanDefinition 中存储了 Xml Bean 信息，而 BeanDefinitionRegister 基于 ID 和 name 保存了 Bean 的定义。接下要学习的是从 xml Bean 到 BeanDefinition 然后在注册至 BeanDefinitionRegister 整个过程。 上图中可以看出 Bean 的定义是由 BeanDefinitionReader 从 xml 中读取配置并构建出 BeanDefinitionReader,然后在基于别名注册到 BeanDefinitionRegister 中。 [ ] 查看 BeanDefinitionReader 结构 方法说明： loadBeanDefinitions(Resource resource) 基于资源装载 Bean 定义并注册至注册器 int loadBeanDefinitions(String location) 基于资源路径装载 Bean 定义并注册至注册器 BeanDefinitionRegistry getRegistry() 获取注册器 ResourceLoader getResourceLoader() 获取资源装载器 [ ] 基于示例演示 BeanDefinitionReader 装载过程//创建一个简单注册器 BeanDefinitionRegistry register = new SimpleBeanDefinitionRegistry(); //创建 bean 定义读取器 BeanDefinitionReader reader = new XmlBeanDefinitionReader(register); // 创建资源读取器 DefaultResourceLoader resourceLoader = new DefaultResourceLoader(); // 获取资源 Resource xmlResource = resourceLoader.getResource(\"spring.xml\"); // 装载 Bean 的定义 reader.loadBeanDefinitions(xmlResource); // 打印构建的 Bean 名称 System.out.println(Arrays.toString(register.getBeanDefinitionNames()); Beanfactory(bean 工厂) 有了 Bean 的定义就相当于有了产品的配方，接下来就是要把这个配方送到工厂进行生产了。在 ioc 当中 Bean 的构建是由 BeanFactory 负责的。其结构如下： 方法说明： getBean(String) 基于 ID 或 name 获取一个 Bean T getBean(Class requiredType) 基于 Bean 的类别获取一个 Bean（如果出现多个该类的实例，将会报错。但可以指定 primary=“true” 调整优先级来解决该错误 ） Object getBean(String name, Object... args) 基于名称获取一个 Bean，并覆盖默认的构造参数 boolean isTypeMatch(String name, Class typeToMatch) 指定 Bean 与指定 Class 是否匹配 以上方法中重点要关注 getBean，当用户调用 getBean 的时候就会触发 Bean 的创建动作，其是如何创建的呢？ [ ] 演示基本 BeanFactory 获取一个 Bean#创建 Bean 堆栈 // 其反射实例化 Bean java.lang.reflect.Constructor.newInstance(Unknown Source:-1) BeanUtils.instantiateClass() //基于实例化策略 实例化 Bean SimpleInstantiationStrategy.instantiate() AbstractAutowireCapableBeanFactory.instantiateBean() // 执行 Bean 的实例化方法 AbstractAutowireCapableBeanFactory.createBeanInstance() AbstractAutowireCapableBeanFactory.doCreateBean() // 执行 Bean 的创建 AbstractAutowireCapableBeanFactory.createBean() // 缓存中没有，调用指定 Bean 工厂创建 Bean AbstractBeanFactory$1.getObject() // 从单例注册中心获取 Bean 缓存 DefaultSingletonBeanRegistry.getSingleton() AbstractBeanFactory.doGetBean() // 获取 Bean AbstractBeanFactory.getBean() // 调用的客户类 com.tuling.spring.BeanFactoryExample.main() Bean 创建时序图： 从调用过程可以总结出以下几点： 调用 BeanFactory.getBean() 会触发 Bean 的实例化。 DefaultSingletonBeanRegistry 中缓存了单例 Bean Bean 的创建与初始化是由AbstractAutowireCapableBeanFactory完成的。3、BeanFactory 与 ApplicationContext 区别 BeanFactory 看下去可以去做 IOC 当中的大部分事情，为什么还要去定义一个 ApplicationContext 呢？ ApplicationContext 结构图 从图中可以看到 ApplicationContext 它由 BeanFactory 接口派生而来，因而提供了 BeanFactory 所有的功能。除此之外 context 包还提供了以下的功能： MessageSource, 提供国际化的消息访问 资源访问，如 URL 和文件 事件传播，实现了 ApplicationListener 接口的 bean 载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的 web 层 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:13:00 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/spring/aop.html":{"url":"source/spring/aop.html","title":"2.AOP事物原理","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、数据库的事物的基本特性 查看mysql 的默认隔离级别 二、Spring 对事物的支持与使用 1、spring 事物相关API说明 2、声明示事物 3、事物传播机制 三、aop 事物底层实现原理 概要： 数据库的事物的基本特性 Sring 对事物的支持与使用 aop 事物底层实现原理一、数据库的事物的基本特性 事物是区分文件存储系统与Nosql数据库重要特性之一，其存在的意义是为了保证即使在并发情况下也能正确的执行crud操作。怎样才算是正确的呢？这时提出了事物需要保证的四个特性即ACID： A: 原子性(atomicity) 事物中各项操作，要么全做要么全不做，任何一项操作的失败都会导致整个事物的失败； C: 一致性(consistency) 事物结束后系统状态是一致的； I: 隔离性(isolation) 并发执行的事物彼此无法看到对方的中间状态； D: 持久性(durability) 事物完成后所做的改动都会被持久化，即使发生灾难性的失败。 在高并发的情况下，要完全保证其ACID特性是非常困难的，除非把所有的事物串行化执行，但带来的负面的影响将是性能大打折扣。很多时候我们有些业务对事物的要求是不一样的，所以数据库中设计了四种隔离级别，供用户基于业务进行选择。 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（SERIALIZABLE） 不可能 不可能 不可能 脏读 : 一个事物读取到另一事物未提交的更新数据 不可重复读 : 在同一事物中,多次读取同一数据返回的结果有所不同, 换句话说, 后续读取可以读到另一事物已提交的更新数据. 相反, “可重复读”在同一事物中多次读取数据时, 能够保证所读数据一样, 也就是后续读取不能读到另一事物已提交的更新数据。 幻读 : 查询表中一条数据如果不存在就插入一条，并发的时候却发现，里面居然有两条相同的数据。这就幻读的问题。 代码演示脏读、不可重复读、幻读的情况。 演示源码：http://git.jiagouedu.com/java-vip/tuling-spring/src/master/tuling-spring-transaction 数据库默认隔离级别： Oracle中默认级别是 Read committed mysql 中默认级别 Repeatable read。另外要注意的是mysql 执行一条查询语句默认是一个独立的事物，所以看上去效果跟Read committed一样。 查看mysql 的默认隔离级别 SELECT @@tx_isolation 二、Spring 对事物的支持与使用 要点： spring 事物相关API说明 声明式事物的使用 事物传播机制1、spring 事物相关API说明 spring 事物是在数据库事物的基础上进行封装扩展 其主要特性如下： 1. 支持原有的数据事物的隔离级别 2. 加入了事物传播的概念 提供多个事物的和并或隔离的功能 3. 提供声明式事物，让业务代码与事物分离，事物变得更易用。 怎么样去使用Spring事物呢？spring 提供了三个接口供使用事物。分别是： TransactionDefinition 事物定义 PlatformTransactionManager 事物管理 TransactionStatus 事物运行时状态 接口结构图： API说明： 基于API实现事物 public class SpringTransactionExample { private static String url = \"jdbc:mysql://192.168.0.147:3306/luban2\"; private static String user = \"root\"; private static String password = \"123456\"; public static Connection openConnection() throws ClassNotFoundException, SQLException { Class.forName(\"com.mysql.jdbc.Driver\"); Connection conn = DriverManager.getConnection(\"jdbc:mysql://192.168.0.147:3306/luban2\", \"root\", \"123456\"); return conn; } public static void main(String[] args) { final DataSource ds = new DriverManagerDataSource(url, user, password); final TransactionTemplate template = new TransactionTemplate(); template.setTransactionManager(new DataSourceTransactionManager(ds)); template.execute(new TransactionCallback() { @Override public Object doInTransaction(TransactionStatus status) { Connection conn = DataSourceUtils.getConnection(ds); Object savePoint = null; try { { // 插入 PreparedStatement prepare = conn. prepareStatement(\"insert INTO account (accountName,user,money) VALUES (?,?,?)\"); prepare.setString(1, \"111\"); prepare.setString(2, \"aaaa\"); prepare.setInt(3, 10000); prepare.executeUpdate(); } // 设置保存点 savePoint = status.createSavepoint(); { // 插入 PreparedStatement prepare = conn. prepareStatement(\"insert INTO account (accountName,user,money) VALUES (?,?,?)\"); prepare.setString(1, \"222\"); prepare.setString(2, \"bbb\"); prepare.setInt(3, 10000); prepare.executeUpdate(); } { // 更新 PreparedStatement prepare = conn. prepareStatement(\"UPDATE account SET money= money+1 where user=?\"); prepare.setString(1, \"asdflkjaf\"); Assert.isTrue(prepare.executeUpdate() > 0, \"\"); } } catch (SQLException e) { e.printStackTrace(); } catch (Exception e) { System.out.println(\"更新失败\"); if (savePoint != null) { status.rollbackToSavepoint(savePoint); } else { status.setRollbackOnly(); } } return null; } }); } } 2、声明示事物 我们前面是通过调用API来实现对事物的控制，这非常的繁琐，与直接操作JDBC事物并没有太多的改善，所以Spring提出了声明示事物，使我们对事物的操作变得非常简单，甚至不需要关心它。 演示声明示事物使用 spring-tx.xml 配置spring.xml 编写服务类 @Transactional public void addAccount(String name, int initMenoy) { String accountid = new SimpleDateFormat(\"yyyyMMddhhmmss\").format(new Date()); jdbcTemplate.update(\"insert INTO account (accountName,user,money) VALUES (?,?,?)\", accountid, name, initMenoy); // 人为报错 int i = 1 / 0; } 演示添加 @Transactional 注解和不添加注解的情况。3、事物传播机制 类别 事物传播类型 说明 支持当前事物 PROPAGATION_REQUIRED（必须的） 如果当前没有事物，就新建一个事物，如果已经存在一个事物中，加入到这个事物中。这是最常见的选择。 PROPAGATION_SUPPORTS（支持） 支持当前事物，如果当前没有事物，就以非事物方式执行。 PROPAGATION_MANDATORY（强制） 使用当前的事物，如果当前没有事物，就抛出异常。 不支持当前事物 PROPAGATION_REQUIRES_NEW(隔离) 新建事物，如果当前存在事物，把当前事物挂起。 PROPAGATION_NOT_SUPPORTED(不支持) 以非事物方式执行操作，如果当前存在事物，就把当前事物挂起。 PROPAGATION_NEVER(强制非事物) 以非事物方式执行，如果当前存在事物，则抛出异常。 套事物 PROPAGATION_NESTED（嵌套事物） 如果当前存在事物，则在嵌套事物内执行。如果当前没有事物，则执行与PROPAGATION_REQUIRED类似的操作。 常用事物传播机制： PROPAGATION_REQUIRED， 这个也是默认的传播机制； PROPAGATION_NOT_SUPPORTED 可以用于发送提示消息，站内信、短信、邮件提示等。不属于并且不应当影响主体业务逻辑，即使发送失败也不应该对主体业务逻辑回滚。 PROPAGATION_REQUIRES_NEW 总是新启一个事物，这个传播机制适用于不受父方法事物影响的操作，比如某些业务场景下需要记录业务日志，用于异步反查，那么不管主体业务逻辑是否完成，日志都需要记录下来，不能因为主体业务逻辑报错而丢失日志； 演示常用事物的传播机制 用例1: 创建用户时初始化一个帐户，表结构和服务类如下。 表结构 服务类 功能描述 user UserSerivce 创建用户，并添加帐户 account AccountService 添加帐户 UserSerivce.createUser(name) 实现代码 @Transactional public void createUser(String name) { // 新增用户基本信息 jdbcTemplate.update(\"INSERT INTO `user` (name) VALUES(?)\", name); //调用accountService添加帐户 accountService.addAccount(name, 10000); ｝ AccountService.addAccount(name,initMoney) 实现代码（方法的最后有一个异常） @Transactional(propagation = Propagation.REQUIRED) public void addAccount(String name, int initMoney) { String accountid = new SimpleDateFormat(\"yyyyMMddhhmmss\").format(new Date()); jdbcTemplate.update(\"insert INTO account (accountName,user,money) VALUES (?,?,?)\", accountid, name, initMenoy); // 出现分母为零的异常 int i = 1 / 0; } 实验预测一： createUser addAccount(异常) 预测结果 场景一 无事物 required user （成功） Account（不成功）正确 场景二 required 无事物 user （不成功） Account（不成功）正确 场景三 required not_supported user （不成功） Account（成功）正确 场景四 required required_new user （不成功） Account（不成功）正确 场景五 required(异常移至createUser方法未尾) required_new user （不成功） Account（成功）正确 场景六 required(异常移至createUser方法未尾)（addAccount 方法称至当前类） required_new user （不成功） Account（不成功） 三、aop 事物底层实现原理 讲事物原理之前我们先来做一个实验，当场景五的环境改变，把addAccount 方法移至UserService 类下，其它配置和代码不变： @Override @Transactional public void createUser(String name) { jdbcTemplate.update(\"INSERT INTO `user` (name) VALUES(?)\", name); addAccount(name, 10000); // 人为报错 int i = 1 / 0; } @Transactional(propagation = Propagation.REQUIRES_NEW) public void addAccount(String name, int initMoney) { String accountid = new SimpleDateFormat(\"yyyyMMddhhmmss\").format(new Date()); jdbcTemplate.update(\"insert INTO account (accountName,user,money) VALUES (?,?,?)\", accountid, name, initMoney); } 演示新场景 经过演示我们发现得出的结果与场景五并不 一至，required_new 没有起到其对应的作用。原因在于spring 声明示事物使用动态代理实现，而当调用同一个类的方法时，是会不会走代理逻辑的，自然事物的配置也会失效。 通过一个动态代理的实现来模拟这种场景 UserSerivce proxyUserSerivce = (UserSerivce) Proxy.newProxyInstance(LubanTransaction.class.getClassLoader(), new Class[]{UserSerivce.class}, new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { System.out.println(\"开启事物:\"+method.getName()); return method.invoke(userSerivce, args); } finally { System.out.println(\"关闭事物:\"+method.getName()); } } }); proxyUserSerivce.createUser(\"luban\"); 当我们调用createUser 方法时 仅打印了 createUser 的事物开启、关闭，并没有打印addAccount 方法的事物开启、关闭，由此可见addAccount 的事物配置是失效的。 如果业务当中上真有这种场景该如何实现呢？在spring xml中配置 暴露proxy 对象，然后在代码中用AopContext.currentProxy() 就可以获当前代理对象 // 基于代理对象调用创建帐户，事物的配置又生效了 ((UserSerivce) AopContext.currentProxy()).addAccount(name, 10000); Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:14:27 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"source/spring/mvc.html":{"url":"source/spring/mvc.html","title":"3.SpringMvc原理解析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、spring mvc 设计思想与体系结构组成 1、回顾servlet 与jsp 执行过程 2、spring mvc 执行流程： 3、spring mvc 体系结构 二、mvc 执行流程解析 要点： 2、HandlerMapping 详解 3、HandlerAdapter详解 4、ViewResolver 与View 详解 5、HandlerExceptionResolver详解 6、HandlerInterceptor 详解 三、注解配置 课程概要 spring mvc 设计思想与体系结构组成 mvc 执行流程解析 注解配置 一、spring mvc 设计思想与体系结构组成 jsp 执行过程回顾 spring mvc执行流程解析 mvc 体系结构1、回顾servlet 与jsp 执行过程 流程说明： 请求Servlet 处理业务逻辑 设置业务Model forward jsp Servlet jsp Servlet 解析封装html 返回 提问：这个是一个MVC应用场景吗？ spring mvc本质上还是在使用Servlet处理，并在其基础上进行了封装简化了开发流程，提高易用性、并使用程序逻辑结构变得更清晰 基于注解的URL映谢 http表单参数转换 全局统一异常处理 拦截器的实现2、spring mvc 执行流程： 整个过程是如何实现的？ dispatchServlet 如何找到对应的Control？ 如何执行调用Control 当中的业务方法？ 回答这些问题之前我们先来认识一下spring mvc 体系结构 3、spring mvc 体系结构 HandlerMapping'hændlə 'mæpɪŋ url与控制器的映谢 HandlerAdapter'hændlə ə'dæptə 控制器执行适配器 ViewResolvervjuː riː'zɒlvə 视图仓库 view 具体解析视图 HandlerExceptionResolver'hændlə ɪk'sepʃ(ə)n riː'zɒlvə 异常捕捕捉器 HandlerInterceptor'hændlə ɪntə'septə 拦截器 配置一个spring mvc 示例演示 验证上述流程 - [ ] 创建一个Controller 类 - [ ] 配置DispatchServlet - [ ] 创建spring-mvc.xml 文件 - [ ] 配置SimpleUrlHandlerMapping - [ ] 配置InternalResourceViewResolver 体系结构UML 二、mvc 执行流程解析 要点： mvc 具体执行流程 HandlerMapping详解 HandlerAdapter 详解 ViewResolver与View详解 HandlerExceptionResolver详解 HandlerInterceptor 详解 mvc 各组件执行流程 2、HandlerMapping 详解 其为mvc 中url路径与Control对像的映射，DispatcherServlet 就是基于此组件来寻找对应的Control，如果找不到就会报Not Found mapping 的异常。 HandlerMapping 接口方法 HandlerMapping 接口结构 目前主流的三种mapping 如下： BeanNameUrlHandlerMapping: 基于ioc name 中已 \"/\" 开头的Bean时行 注册至映谢. SimpleUrlHandlerMapping：基于手动配置 url 与control 映谢 RequestMappingHandlerMapping：基于@RequestMapping注解配置对应映谢 [ ] 演示基于 BeanNameUrlHandlerMapping 配置映谢。 编写mvc 文件 // beanname control 控制器 public class BeanNameControl implements HttpRequestHandler { @Override public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException { request.getRequestDispatcher(\"/WEB-INF/page/userView.jsp\").forward(request, response); } } 当IOC 中实例化这些类之后 DispatcherServlet 就会通过org.springframework.web.servlet.DispatcherServlet#getHandler() 方法基于request查找对应Handler。 但找到对应的Handler之后我们发现他是一个Object类型，并没有实现特定接口。如何调用Handler呢？ 3、HandlerAdapter详解 这里spring mvc 采用适配器模式来适配调用指定Handler，根据Handler的不同种类采用不同的Adapter,其Handler与 HandlerAdapter 对应关系如下: Handler类别 对应适配器 描述 Controller SimpleControllerHandlerAdapter 标准控制器，返回ModelAndView HttpRequestHandler HttpRequestHandlerAdapter 业务自行处理 请求，不需要通过modelAndView 转到视图 Servlet SimpleServletHandlerAdapter 基于标准的servlet 处理 HandlerMethod RequestMappingHandlerAdapter 基于@requestMapping对应方法处理 HandlerAdapter 接口方法 HandlerAdapter 接口结构图 [ ] 演示基于Servlet 处理 SimpleServletHandlerAdapter // 标准Servlet public class HelloServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().println(\"hello luban \"); } } 上述例子中当IOC 中实例化这些类之后 DispatcherServlet 就会通过 org.springframework.web.servlet.DispatcherServlet#getHandlerAdapter() 方法查找对应handler的适配器 ，如果找不到就会报 No adapter for handler 。 4、ViewResolver 与View 详解 找到应的Adapter 之后就会基于适配器调用业务处理，处理完之后业务方会返回一个ModelAndView ，在去查找对应的视图进行处理。其在org.springframework.web.servlet.DispatcherServlet#resolveViewName() 中遍历 viewResolvers 列表查找，如果找不到就会报一个 Could not resolve view with name 异常。 在下一步就是基于ViewResolver.resolveViewName() 获取对应View来解析生成Html并返回 。对应VIEW结构如下： 至此整个正向流程就已经走完了，如果此时程序处理异常 MVC 该如何处理呢？ 5、HandlerExceptionResolver详解 该组件用于指示 当出现异常时 mvc 该如何处理。 dispatcherServlet 会调用org.springframework.web.servlet.DispatcherServlet#processHandlerException() 方法，遍历 handlerExceptionResolvers 处理异常，处理完成之后返回errorView 跳转到异常视图。 - [ ] 演示自定义异常捕捉 public class SimpleExceptionHandle implements HandlerExceptionResolver { @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) { return new ModelAndView(\"error\"); } } HandlerExceptionResolver 结构 除了上述组件之外 spring 中还引入了 我Interceptor 拦截器 机制，类似于Filter。 6、HandlerInterceptor 详解 [ ] 演示HandlerInterceptorpublic class SimpleHandlerInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\"preHandle\"); return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\"postHandle\"); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\"afterCompletion\"); } } 其实现机制是基于 HandlerExecutionChain 分别在 doDispatch 方法中执行以下方法： preHandle ：业务处理前执行 postHandle：业务处理后（异常则不执行） afterCompletion：视图处理后 具体逻辑源码参见：org.springframework.web.servlet.DispatcherServlet#doDispatch 方法。 三、注解配置 [ ] 演示基于注解配置mvc mapping // 注解方法 @RequestMapping(\"/hello.do\") public ModelAndView hello() { ModelAndView mv = new ModelAndView(\"userView\"); mv.addObject(\"name\", \"luban\"); return mv; } 提问 为什么基于 配置就能实现mvc 的整个配置了，之前所提到的 handlerMapping 、与handlerAdapter 组件都不适用了？ 只要查看以类的源就可以知晓其中原因： [ ] 认识 NamespaceHandler 接口 [ ] 查看 MvcNamespaceHandler [ ] 查看AnnotationDrivenBeanDefinitionParser 结论： 在 对应的解析器，自动向ioc 里面注册了两个BeanDefinition。分别是：RequestMappingHandlerMapping与BeanNameUrlHandlerMapping Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:15:19 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concurrent/":{"url":"concurrent/","title":"三、并发专题","keywords":"","body":"并发编程专题 本章主要介绍的是并发包里面工具的使用，java的内存模型，线程池，原子操作，阻塞队列等 1.Executor线程池详解 2.JMM&Lock&Tools详解 3.atomic&collections详解 4.Fork-join详解 5.BlockingQueue详解 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:40:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concurrent/executor.html":{"url":"concurrent/executor.html","title":"1.Executor线程池详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.什么是线程 2.线程实现的三种方式 3.线程的生命周期&状态 4.线程的执行顺序 5.线程与线程池对比 6.线程池体系介绍 7.线程池源码分析 1.什么是线程 线程是进程的一个实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源 (如程序计数器，一组寄存器和栈 )，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。 2.线程实现的三种方式 Runnable、 Thread、 Callable 总结 最后再来看看它们三个之间的总结。 实现 Runnable接口相比继承 Thread类有如下优势 1）可以避免由于 Java的单继承特性而带来的局限 2）增强程序的健壮性，代码能够被多个线程共享，代码与数据是独立的 3）线程池只能放入实现 Runable或 Callable类线程，不能直接放入继承 Thread的类 实现 Runnable接口和实现 Callable接口的区别 1）Runnable是自从 java1.1就有了，而 Callable是 1.5之后才加上去的 2）实现 Callable接口的任务线程能返回执行结果，而实现 Runnable接口的任务线程不能返回结果 3 Callable接口的 call()方法允许抛出异 常，而 Runnable接口的 run()方法的异常只能在内部消化，不能继 续上抛 4）加入线程池运行 Runnable使用 ExecutorService的 execute方法， Callable使用 submit方法 注： Callable接口支持返回执行结果，此时需要调用 FutureTask.get()方法实现，此方法会阻塞主线程直到获 取返回结果，当不调用此方法时，主线程不会阻塞 3.线程的生命周期&状态 4.线程的执行顺序 5.线程与线程池对比 6.线程池体系介绍 7.线程池源码分析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:46:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concurrent/JMM&Lock&Tools.html":{"url":"concurrent/JMM&Lock&Tools.html","title":"2.JMM&Lock&Tools详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.JMM 内存模型 2.Volatile 可见性 3.Synchronized 同步 4.Lock 锁 5.ReentrantLock 6.ReentrantReadWriteLock 7.AbstractQueuedSynchronizer 8.CountDownLatch Semaphore 要点： JMM Volatile Synchronized Lock ReentrantLock ReentrantReadWriteLock AbstractQueuedSynchronizer CountDownLatch Semaphore1.JMM 内存模型 2.Volatile 可见性 3.Synchronized 同步 4.Lock 锁 5.ReentrantLock 6.ReentrantReadWriteLock 7.AbstractQueuedSynchronizer 8.CountDownLatch Semaphore Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:46:37 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concurrent/atomic&collections.html":{"url":"concurrent/atomic&collections.html","title":"3.atomic&collections详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.Atomic体系介绍 2.CAS源码分析 3.CAS的ABA问题 4.HashMap 5.HashTable 6.ConcurrenthHashMap 7.ArrayList 8.CopyOnWriteArrayList Copyright &copy ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 要点： Atomic体系介绍 CAS源码分析 CAS的ABA问题 HashMap HashTable ConcurrenthHashMap ArrayList CopyOnWriteArrayList1.Atomic体系介绍 2.CAS源码分析 3.CAS的ABA问题 4.HashMap 5.HashTable 6.ConcurrenthHashMap 7.ArrayList 8.CopyOnWriteArrayList Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:46:47 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concurrent/Fork-join.html":{"url":"concurrent/Fork-join.html","title":"4.Fork-join详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.Fork-join编程介绍 2.Fork-join原理分析 3.Fork-join最佳实践 要点： Fork-join编程介绍 Fork-join原理分析 Fork-join最佳实践1.Fork-join编程介绍 2.Fork-join原理分析 3.Fork-join最佳实践 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:46:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concurrent/BlockingQueue.html":{"url":"concurrent/BlockingQueue.html","title":"5.BlockingQueue详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.基本概念介绍/DIY 并发/阻塞队列 2.JDK 阻塞队列实战与原理分析 2.1ArrayBlockingQueue 2.2LinkedBlockingQueue 2.3LinkedTransferQueue 2.4LinkedBlockingDeque 2.5SynchronousQueue 2.6PriorityBlockingQueue 2.7DelayQueue 3.高性能并发队列讨论 要点： 基本概念介绍/DIY 并发/阻塞队列 JDK 阻塞队列实战与原理分析 高性能并发队列讨论1.基本概念介绍/DIY 并发/阻塞队列 2.JDK 阻塞队列实战与原理分析 2.1ArrayBlockingQueue 基于数组结构的有界阻塞队列（长度不可变） 2.2LinkedBlockingQueue 基于链表结构的有界阻塞队列（默认容量 Integer.MAX_VALUE） 2.3LinkedTransferQueue 基于链表结构的无界阻塞/传递队列 2.4LinkedBlockingDeque 基于链表结构的有界阻塞双端队列（默认容量 Integer.MAX_VALUE） 2.5SynchronousQueue 不存储元素的阻塞/传递队列 2.6PriorityBlockingQueue 支持优先级排序的无界阻塞队列 2.7DelayQueue 支持延时获取元素的无界阻塞队列 3.高性能并发队列讨论 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:47:08 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/":{"url":"performance/","title":"四、调优专题","keywords":"","body":"性能调优专题 主要分析java内存模型，mysql关糸型数据库，nginx反向代理，tomcat jvm内存调优 1.整体结构介绍 2.性能调优监控工具 3.垃圾回收与调优 mysql关糸型数据库 1.Mysql基础 2.Mysql性能优化 3.索引底层数据结构 4.Explain详解 5.索引优化深入 6.Mysql锁与事务 nginx反向代理 1.Nginx核心模块与配置 2.Nginx性能优化 Nginx-黑马 1.基础 2.进阶篇 3.Rewrite功能配置 4.负载均衡 5.集群搭建 tomcat-Servlet容器 1.Tomcat生产环境应用 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:39:32 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/jvm/":{"url":"performance/jvm/","title":"jvm内存调优","keywords":"","body":"jvm内存调优 1.整体结构介绍 2.性能调优监控工具 3.垃圾回收与调优 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:47:35 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/jvm/base.html":{"url":"performance/jvm/base.html","title":"1.整体结构介绍","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.JVM整体架构 2.JVM类加载器 2.1类加载过程 2.2类加载器种类（4种） 2.3类加载机制 3.JVM内存结构 4.JVM执行引擎 1.JVM整体架构 JVM由三个主要的子系统构成 类加载器子系统 运行时数据区（内存结构） 执行引擎 2.JVM类加载器 2.1类加载过程 类加载：类加载器将class文件加载到虚拟机的内存 加载：在硬盘上查找并通过IO读入字节码文件 连接：执行校验、准备、解析（可选）步骤 校验：校验字节码文件的正确性 准备：给类的静态变量分配内存，并赋予默认值 解析：类装载器装入类所引用的其他所有类 初始化：对类的静态变量初始化为指定的值，执行静态代码块 2.2类加载器种类（4种） 启动类加载器：负责加载JRE的核心类库，如jre目标下的rt.jar,charsets.jar等 扩展类加载器：负责加载JRE扩展目录ext中JAR类包 系统类加载器：负责加载ClassPath路径下的类包 用户自定义加载器：负责加载用户自定义路径下的类包2.3类加载机制 全盘负责委托机制：当一个ClassLoader加载一个类时，除非显示的使用另一个ClassLoader，该类所依赖和引用的类也由这个ClassLoader载入 双亲委派机制：指先委托父类加载器寻找目标类，在找不到的情况下在自己的路径中查找并载入目标类 3.JVM内存结构 4.JVM执行引擎 执行引擎：读取运行时数据区的Java字节码并逐个执行 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:48:55 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/jvm/view.html":{"url":"performance/jvm/view.html","title":"2.性能调优监控工具","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.Jinfo 2.Jstat 3.Jmap 4.Jstack 5.远程连接jvisualvm 6.jstack找出占用cpu最高的堆栈信息 1.**Jinfo** 查看正在运行的Java应用程序的扩展参数 查看jvm的参数 查看java系统参数 2.Jstat jstat命令可以查看堆内存各部分的使用量，以及加载类的数量。命令的格式如下： jstat [-命令选项] [vmid] [间隔时间/毫秒] [查询次数] 注意：使用的jdk版本是jdk8. 类加载统计： Loaded：加载class的数量 Bytes：所占用空间大小 Unloaded：未加载数量 Bytes:未加载占用空间 Time：时间 垃圾回收统计 S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小(元空间) MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 堆内存统计 NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0C：第一个幸存区大小 S1C：第二个幸存区的大小 EC：伊甸园区的大小 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC:当前老年代大小 MCMN:最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代gc次数 FGC：老年代GC次数 新生代垃圾回收统计 S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 TT:对象在新生代存活的次数 MTT:对象在新生代存活的最大次数 DSS:期望的幸存区大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 新生代内存统计 NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0CMX：最大幸存1区大小 S0C：当前幸存1区大小 S1CMX：最大幸存2区大小 S1C：当前幸存2区大小 ECMX：最大伊甸园区大小 EC：当前伊甸园区大小 YGC：年轻代垃圾回收次数 FGC：老年代回收次数 老年代垃圾回收统计 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 OC：老年代大小 OU：老年代使用大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 老年代内存统计 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC：老年代大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 元数据空间统计 MCMN:最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 S0：幸存1区当前使用比例 S1：幸存2区当前使用比例 E：伊甸园区使用比例 O：老年代使用比例 M：元数据区使用比例 CCS：压缩使用比例 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 3.**Jmap** 此命令可以用来查看内存信息。 实例个数以及占用内存大小 打开log.txt，文件内容如下： num：序号 instances：实例数量 bytes：占用空间大小 class name：类名称 堆信息 堆内存dump 也可以设置内存溢出自动导出dump文件(内存很大的时候，可能会导不出来) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./ （路径） 可以用jvisualvm命令工具导入该dump文件分析 4.**Jstack** 用jstack查找死锁，见如下示例，也可以用jvisualvm查看死锁 ** 5.**远程连接jvisualvm** 启动普通的jar程序JMX端口配置： java -Dcom.sun.management.jmxremote.port=12345 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -jar foo.jar tomcat的JMX配置 JAVA_OPTS=-Dcom.sun.management.jmxremote.port=8999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false jvisualvm远程连接服务需要在远程服务器上配置host(连接ip 主机名)，并且要关闭防火墙 6.**jstack找出占用cpu最高的堆栈信息** 1，使用命令top -p ，显示你的java进程的内存情况，pid是你的java进程号，比如4977 2，按H，获取每个线程的内存情况 3，找到内存和cpu占用最高的线程tid，比如4977 4，转为十六进制得到 0x1371 ,此为线程id的十六进制表示 5，执行 jstack 4977|grep -A 10 1371，得到线程堆栈信息中1371这个线程所在行的后面10行 6，查看对应的堆栈信息找出可能存在问题的代码 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:49:07 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/jvm/gc.html":{"url":"performance/jvm/gc.html","title":"3.垃圾回收与调优","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.JVM内存分配与回收 1.1 对象优先在Eden区分配 1.2 大对象直接进入老年代 1.3 长期存活的对象将进入老年代 2.如何判断对象可以被回收 2.1 引用计数法 2.2 可达性分析算法 2.3 finalize()方法最终判定对象是否存活 2.4 如何判断一个常量是废弃常量 2.5 如何判断一个类是无用的类 3.垃圾收集算法 3.1 标记-清除算法 3.2 复制算法 3.3 标记-整理算法 3.4 分代收集算法 4.垃圾收集器 4.1 Serial收集器 4.2 ParNew收集器 4.3 Parallel Scavenge收集器 4.4.Serial Old收集器 4.5 Parallel Old收集器 4.6 CMS收集器(-XX:+UseConcMarkSweepGC(主要是old区使用)) 4.7 G1收集器(-XX:+UseG1GC) 5. 如何选择垃圾收集器 6. 实战调优 6.1JVM调优主要就是调整下面两个指标 6.2GC调优步骤 6.3G1调优相关 1.JVM内存分配与回收 1.1 对象优先在Eden区分配 大多数情况下，对象在新生代中 Eden 区分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次Minor GC。我们来进行实际测试一下。 在测试之前我们先来看看Minor Gc和Full GC 有什么不同呢？ 新生代GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。 老年代GC（Major GC/Full GC）:指发生在老年代的GC，出现了Major GC经常会伴随至少一次的Minor GC（并非绝对），Major GC的速度一般会比Minor GC的慢10倍以上。 测试： 通过以下方式运行： 添加的参数：**-XX:+PrintGCDetails** 运行结果： 从上图我们可以看出eden区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用至少2000多k内存）。假如我们再为allocation2分配内存会出现什么情况呢？ 简单解释一下为什么会出现这种情况：因为给allocation2分配内存的时候eden区内存几乎已经被分配完了，我们刚刚讲了当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC.GC期间虚拟机又发现allocation1无法存入Survior空间，所以只好通过分配担保机制把新生代的对象提前转移到老年代中去，老年代上的空间足够存放allocation1，所以不会出现Full GC。执行Minor GC后，后面分配的对象如果能够存在eden区的话，还是会在eden区分配内存。可以执行如下代码验证： 1.2 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？ 为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。 1.3 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别那些对象应放在新生代，那些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold来设置。 2.如何判断对象可以被回收 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断那些对象已经死亡（即不能再被任何途径使用的对象）。 2.1 引用计数法 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。所谓对象之间的相互引用问题，如下面代码所示：除了对象objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知 GC 回收器回收他们。 2.2 可达性分析算法 这个算法的基本思想就是通过一系列的称为“GC Roots”的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 GC Roots根节点：类加载器、Thread、虚拟机栈的本地变量表、static成员、常量引用、本地方法栈的变量等等 2.3 finalize()方法最终判定对象是否存活 即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历再次标记过程。 标记的前提是对象在进行可达性分析后发现没有与GC Roots相连接的引用链。 1. 第一次标记并进行一次筛选。 筛选的条件是此对象是否有必要执行finalize()方法。 当对象没有覆盖finalize方法，或者finzlize方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”，对象被回收。 2. 第二次标记 如果这个对象被判定为有必要执行finalize（）方法，那么这个对象将会被放置在一个名为：F-Queue的队列之中，并在稍后由一条虚拟机自动建立的、低优先级的Finalizer线程去执行。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束。这样做的原因是，如果一个对象finalize（）方法中执行缓慢，或者发生死循环（更极端的情况），将很可能会导致F-Queue队列中的其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。 finalize（）方法是对象脱逃死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模标记，如果对象要在finalize（）中成功拯救自己----只要重新与引用链上的任何的一个对象建立关联即可，譬如把自己赋值给某个类变量或对象的成员变量，那在第二次标记时它将移除出“即将回收”的集合。如果对象这时候还没逃脱，那基本上它就真的被回收了。 见示例程序： 2.4 如何判断一个常量是废弃常量 运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 \"abc\"，如果当前没有任何String对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池。 2.5 如何判断一个类是无用的类 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 3.垃圾收集算法 3.1 标记-清除算法 算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，效率也很高，但是会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 3.2 复制算法 为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 3.3 标记-整理算法 根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一段移动，然后直接清理掉端边界以外的内存。 3.4 分代收集算法 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 4.垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的HotSpot虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial收集器 Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的“单线程”的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（\"Stop The World\"），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。 4.2 ParNew收集器 ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。适合科学计算、后台处理等弱交互场景。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。适合Web应用。4.3 Parallel Scavenge收集器 Parallel Scavenge 收集器类似于ParNew 收集器，是Server 模式（内存大于2G，2个cpu）下的默认收集器，那么它有什么特别之处呢？ Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 4.4.Serial Old收集器 Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 4.5 Parallel Old收集器 Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 4.6 CMS收集器(-XX:+UseConcMarkSweepGC(**主要是old区使用**)) CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用，它是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种“标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记：暂停所有的其他线程(STW)，并记录下直接与root相连的对象，速度很快 ； 并发标记：同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除：开启用户线程，同时GC线程开始对未标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对CPU资源敏感（会和服务抢资源）； 无法处理浮动垃圾(在java业务程序线程与垃圾收集线程并发执行过程中又产生的垃圾，这种浮动垃圾只能等到下一次gc再清理了)； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 CMS的相关参数 -XX:+UseConcMarkSweepGC 启用cms -XX:ConcGCThreads:并发的GC线程数（并非STW时间，而是和服务一起执行的线程数） -XX:+UseCMSCompactAtFullCollection:FullGC之后做压缩（减少碎片） -XX:CMSFullGCsBeforeCompaction:多少次FullGC之后压缩一次（因压缩非常的消耗时间，所以不能每次FullGC都做） -XX:CMSInitiatingOccupancyFraction:触发FulGC条件（默认是92） -XX:+UseCMSInitiatingOccupancyOnly:是否动态调节 -XX:+CMSScavengeBeforeRemark:FullGC之前先做YGC（一般这个参数是打开的） -XX:+CMSClassUnloadingEnabled:启用回收Perm区（jdk1.7及以前）4.7 G1收集器(-XX:+UseG1GC) G1 (Garbage-First)是一款面向服务器的垃圾收集器,**主要针对配备多颗处理器及大容量内存的机器**. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征. G1将Java堆划分为多个大小相等的独立区域（Region），虽保留新生代和老年代的概念，但不再是物理隔阂了，它们都是（可以不连续）Region的集合。 分配大对象（直接进Humongous区，专门存放短期巨型对象，不用直接进老年代，避免Full GC的大量开销）不会因为无法找到连续空间而提前触发下一次GC。 被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备以下特点： 并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程来执行GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 空间整合：与CMS的“标记--清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内完成垃圾收集。 G1收集器的运作大致分为以下几个步骤： 初始标记（initial mark，STW）：在此阶段，G1 GC 对根进行标记。该阶段与常规的 (STW) 年轻代垃圾回收密切相关。 并发标记（Concurrent Marking）：G1 GC 在整个堆中查找可访问的（存活的）对象。 最终标记（Remark，STW）：该阶段是 STW 回收，帮助完成标记周期。 筛选回收（Cleanup，STW）：筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率。 G1垃圾收集分类 YoungGC 新对象进入Eden区 存活对象拷贝到Survivor区 存活时间达到年龄阈值时，对象晋升到Old区 MixedGC 不是FullGC，回收所有的Young和部分Old(根据期望的GC停顿时间确定old区垃圾收集的优先顺序) global concurrent marking （全局并发标记） Initial marking phase:标记GC Root，STW Root region scanning phase：标记存活Region Concurrent marking phase：标记存活的对象 Remark phase :重新标记,STW Cleanup phase:部分STW 相关参数 G1MixedGCLiveThresholdPercent Old区的region被回收的时候的存活对象占比 G1MixedGCCountTarget：一次global concurrent marking之后，最多执行Mixed GC的次数 G1OldCSetRegionThresholdPercent 一次Mixed GC中能被选入CSet的最多old区的region数量 触发的时机 InitiatingHeapOccupancyPercent:堆占有率达到这个值则触发global concurrent marking，默认45% G1HeapWastePercent:在global concurrent marking结束之后，可以知道区有多少空间要被回收，在每次YGC之后和再次发生Mixed GC之前，会检查垃圾占比是否达到了此参数，只有达到了，下次才会发生Mixed GC5. 如何选择垃圾收集器 优先调整堆的大小让服务器自己来选择 如果内存小于100M，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 如果允许停顿时间超过1秒，选择并行或者JVM自己选 如果响应时间最重要，并且不能超过1秒，使用并发收集器 下图有连线的可以搭配使用，官方推荐使用G1，因为性能高 6. 实战调优 6.1**JVM调优主要就是调整下面两个指标** 停顿时间**: 垃圾收集器做垃圾回收中断应用执行的时间。-XX:MaxGCPauseMillis** 吞吐量**：花在垃圾收集的时间和花在应用时间的占比 -XX:GCTimeRatio=,垃圾收集时间占比：1/(1+n)** 6.2**GC调优步骤** 打印GC日志 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:./gc.log 分析日志得到关键性指标 分析GC原因，调优JVM参数 1、Parallel Scavenge收集器(默认) 分析parallel-gc.log 第一次调优，设置Metaspace大小：增大元空间大小-XX:MetaspaceSize=64M -XX:MaxMetaspaceSize=64M 第二次调优，添加吞吐量和停顿时间参数：-XX:MaxGCPauseMillis=100 -XX:GCTimeRatio=99 第三次调优，修改动态扩容增量：-XX:YoungGenerationSizeIncrement=30 2、配置CMS收集器 -XX:+UseConcMarkSweepGC 分析cms-gc.log 3、配置G1收集器 -XX:+UseG1GC 分析g1-gc.log 查看发生MixedGC的阈值：jinfo -flag InitiatingHeapOccupancyPercent 进程id 分析工具：gceasy，GCViewer 6.3**G1调优相关** 常用参数 -XX:+UseG1GC 开启G1 -XX:G1HeapRegionSize=n,region的大小，1-32M，2048个 -XX:MaxGCPauseMillis=200 最大停顿时间 -XX:G1NewSizePercent -XX:G1MaxNewSizePercent -XX:G1ReservePercent=10 保留防止to space溢出（） -XX:ParallelGCThreads=n SWT线程数（停止应用程序） -XX:ConcGCThreads=n 并发线程数=1/4*并行 最佳实践 年轻代大小：避免使用-Xmn、-XX:NewRatio等显示设置Young区大小，会覆盖暂停时间目标（常用参数3） 暂停时间目标：暂停时间不要太严苛，其吞吐量目标是90%的应用程序时间和10%的垃圾回收时间，太严苛会直接影响到吞吐量 是否需要切换到G1 50%以上的堆被存活对象占用 对象分配和晋升的速度变化非常大 垃圾回收时间特别长，超过1秒 G1调优目标 6GB以上内存 停顿时间是500ms以内 吞吐量是90%以上 GC常用参数 堆栈设置 -Xss:每个线程的栈大小 -Xms:初始堆大小，默认物理内存的1/64 -Xmx:最大堆大小，默认物理内存的1/4 -Xmn:新生代大小 -XX:NewSize:设置新生代初始大小 -XX:NewRatio:默认2表示新生代占年老代的1/2，占整个堆内存的1/3。 -XX:SurvivorRatio:默认8表示一个survivor区占用1/8的Eden内存，即1/10的新生代内存。 -XX:MetaspaceSize:设置元空间大小 -XX:MaxMetaspaceSize:设置元空间最大允许大小，默认不受限制，JVM Metaspace会进行动态扩展。 垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParallelOldGC:老年代使用并行回收收集器 -XX:+UseParNewGC:在新生代使用并行收集器 -XX:+UseParalledlOldGC:设置并行老年代收集器 -XX:+UseConcMarkSweepGC:设置CMS并发收集器 -XX:+UseG1GC:设置G1收集器 -XX:ParallelGCThreads:设置用于垃圾回收的线程数 并行收集器设置 -XX:ParallelGCThreads:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis:设置并行收集最大暂停时间 -XX:GCTimeRatio:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) CMS收集器设置 -XX:+UseConcMarkSweepGC:设置CMS并发收集器 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads:设置并发收集器新生代收集方式为并行收集时，使用的CPU数。并行收集线程数。 -XX:CMSFullGCsBeforeCompaction:设定进行多少次CMS垃圾回收后，进行一次内存压缩 -XX:+CMSClassUnloadingEnabled:允许对类元数据进行回收 -XX:UseCMSInitiatingOccupancyOnly:表示只在到达阀值的时候，才进行CMS回收 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况 -XX:ParallelCMSThreads:设定CMS的线程数量 -XX:CMSInitiatingOccupancyFraction:设置CMS收集器在老年代空间被使用多少后触发 -XX:+UseCMSCompactAtFullCollection:设置CMS收集器在完成垃圾收集后是否要进行一次内存碎片的整理 G1收集器设置 -XX:+UseG1GC:使用G1收集器 -XX:ParallelGCThreads:指定GC工作的线程数量 -XX:G1HeapRegionSize:指定分区大小(1MB~32MB，且必须是2的幂)，默认将整堆划分为2048个分区 -XX:GCTimeRatio:吞吐量大小，0-100的整数(默认9)，值为n则系统将花费不超过1/(1+n)的时间用于垃圾收集 -XX:MaxGCPauseMillis:目标暂停时间(默认200ms) -XX:G1NewSizePercent:新生代内存初始空间(默认整堆5%) -XX:G1MaxNewSizePercent:新生代内存最大空间 -XX:TargetSurvivorRatio:Survivor填充容量(默认50%) -XX:MaxTenuringThreshold:最大任期阈值(默认15) -XX:InitiatingHeapOccupancyPercen:老年代占用空间超过整堆比IHOP阈值(默认45%),超过则执行混合收集 -XX:G1HeapWastePercent:堆废物百分比(默认5%) -XX:G1MixedGCCountTarget:参数混合周期的最大总次数(默认8) Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 14:49:19 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/":{"url":"performance/mysql/","title":"mysql关糸型数据库","keywords":"","body":"mysql关糸型数据库 1.Mysql基础 2.Mysql性能优化 3.索引底层数据结构 4.Explain详解 5.索引优化深入 6.Mysql锁与事务 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:47:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/use.html":{"url":"performance/mysql/use.html","title":"1.Mysql基础","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 安装MySQL 1.安装包准备 2.安装MySQL服务器 3.安装MySQL客户端 4.MySQL中user表中主机配置 5.开启bin-log 安装MySQL 注意：一定要用root用户操作如下步骤；先卸载MySQL再安装 1.安装包准备 （1）查看MySQL是否安装 rpm -qa|grep mysql （2）如果安装了MySQL，就先卸载 rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64 （3）上传mysql-libs.zip到hadoop102的/opt/software目录，并解压文件到当前目录 unzip mysql-libs.zip （4）进入到mysql-libs文件夹下 ll -rw-r--r--. 1 root root 18509960 3月 26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm -rw-r--r--. 1 root root 3575135 12月 1 2013 mysql-connector-java-5.1.27.tar.gz -rw-r--r--. 1 root root 55782196 3月 26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm 2.安装MySQL服务器 （1）安装MySQL服务端 rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm （2）查看产生的随机密码 cat /root/.mysql_secret OEXaQuS8IWkG19Xs （3）查看MySQL状态 service mysql status （4）启动MySQL service mysql start 3.安装MySQL客户端 （1）安装MySQL客户端 rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm （2）链接MySQL（密码替换成产生的随机密码） mysql -uroot -pOEXaQuS8IWkG19Xs （3）修改密码 mysql>SET PASSWORD=PASSWORD('000000'); （4）退出MySQL mysql>exit 4.MySQL中user表中主机配置 配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。 （1）进入MySQL mysql -uroot -p000000 （2）显示数据库 mysql>show databases; （3）使用MySQL数据库 mysql>use mysql; （4）展示MySQL数据库中的所有表 mysql>show tables; （5）展示user表的结构 mysql>desc user; （6）查询user表 mysql>select User, Host, Password from user; （7）修改user表，把Host表内容修改为% mysql>update user set host='%' where host='localhost'; （8）删除root用户的其他host mysql> delete from user where Host='hadoop102'; delete from user where Host='127.0.0.1'; delete from user where Host='::1'; （9）刷新 mysql>flush privileges; （10）退出 mysql>quit; 5.开启bin-log 打开mysql 的配置文件my.ini(别说找不到哦) 在mysqld配置项下面加上log_bin=mysql_bin [mysqld] log_bin = mysql_bin 重启mysql 在次show variables like 'log_bin' Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:14:39 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/high-use.html":{"url":"performance/mysql/high-use.html","title":"2.Mysql性能优化","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.Mysql架构介绍 1.1Mysql存储引擎 1.2Mysql逻辑架构 1.3Mysql物理文件体系结构 （1）binlog二进制日志文件 （2）redo重做日志文件 （3）共享表空间和独立表空间 （4）undo log （5）临时表空间 （6）error log （7）slow.log 2.InnoDB存储引擎体系结构 2.1缓冲池 2.2change buffer 2.3自适应哈希索引 2.4redo log buffer 2.5double write 2.6InnoDB后台线程 2.6.1InnoDB后台主线程 2.6.2InnoDB后台I/O线程 2.6.3InnoDB脏页刷新线程 2.6.4InnoDB purge线程 2.7redo log 2.8undo log 2.9Query Cache 3.Mysql事务和锁 3.1事务隔离级别 3.2InoDB锁机制介绍 3.3锁等待和死锁 3.3.1锁等待 3.3.2死锁 3.5锁问题的监控 4.Mysql服务器全面优化 4.1Mysql 5.7 InnoDB存储引擎增强特性 4.2硬件层面优化 4.3Linux操作系统层面优化 4.4Mysql配置参数优化 4.5设计规范 1.Mysql架构介绍 1.1Mysql存储引擎 InnoDB 支持行锁,事务 MyISAM 支持表锁,不支持事务 Memory 默认使用hash索引,比b+tree效力高 可通过show engines;命令来查看 1.2Mysql逻辑架构 1.3Mysql物理文件体系结构 （1）binlog二进制日志文件 不管使用的是哪一种存储引擎，都会产生binlog。如果开启了binlog二进制日志，就会有若干个二进制日志文件，如mysql-bin.000001、mysql-bin.000002、mysql-bin.00003等。binlog记录了MySQL对数据库执行更改的所有操作。查看当前binlog文件列表，如图1-7所示。 show master logs ; 。从MySQL 5.1版本开始，MySQL引入了binlog_format参数。这个参数有可选值statement和row：statement就是之前的格式，基于SQL语句来记录；row记录的则是行的更改情况，可以避免之前提到的数据不一致的问题。做MySQL主从复制，statement格式的binlog可能会导致主备不一致，所以要使用row格式。我们还需要借助mysqlbinlog工具来解析和查看binlog中的内容。如果需要用binlog来恢复数据，标准做法是用mysqlbinlog工具把binlog中的内容解析出来，然后把解析结果整个发给MySQL执行。 （2）redo重做日志文件 ib_logfile0、ib_logfile1是InnoDB引擎特有的、用于记录InnoDB引擎下事务的日志，它记录每页更改的物理情况。首先要搞明白的是已经有binlog了为什么还需要redo log，因为两者分工不同。binlog主要用来做数据归档，但是并不具备崩溃恢复的能力，也就是说如果系统突然崩溃，重启后可能会有部分数据丢失。Innodb将所有对页面的修改操作写入一个专门的文件，并在数据库启动时从此文件进行恢复操作，这个文件就是redo log file。redo log的存在可以完美解决这个问题。默认情况下，每个InnoDB引擎至少有一个重做日志组，每组下至少有两个重做日志文件，例如前面提到的ib_logfile0和ib_logfile1。重做日志组中的每个日志文件大小一致且循环写入，也就是说先写iblogfile0，写满了之后写iblogfile1，一旦iblogfile1写满了，就继续写iblogfile0。 （3）共享表空间和独立表空间 在MySQL 5.6.6之前的版本中，InnoDB默认会将所有的数据库InnoDB引擎的表数据存储在一个共享表空间ibdata1中，这样管理起来很困难，增删数据库的时候，ibdata1文件不会自动收缩，单个数据库的备份也将成为问题。为了优化上述问题，在MySQL 5.6.6之后的版本中，独立表空间innodb_file_per_table参数默认开启，每个数据库的每个表都有自已独立的表空间，每个表的数据和索引都会存在自己的表空间中。即便是innnodb表指定为独立表空间，用户自定义数据库中的某些元数据信息、回滚（undo）信息、插入缓冲（change buffer）、二次写缓冲（double write buffer）等还是存放在共享表空间，所以又称为系统表空间。 show variables like 'innodb_data%'; （4）undo log undo log是回滚日志，如果事务回滚，则需要依赖undo日志进行回滚操作。MySQL在进行事务操作的同时，会记录事务性操作修改数据之前的信息，就是undo日志，确保可以回滚到事务发生之前的状态。innodb的undo log存放在ibdata1共享表空间中，当开启事务后，事务所使用的undo log会存放在ibdata1中，即使这个事务被关闭，undo log也会一直占用空间。 show variables like '%undo%'; （5）临时表空间 存储临时对象的空间，比如临时表对象等。如图1-10所示，参数innodb_temp_data_file_path可以看到临时表空间的信息，上限设置为5GB。 show variables like 'innodb_temp_data_file_path%'; （6）error log 错误日志记录了MySQL Server每次启动和关闭的详细信息，以及运行过程中所有较为严重的警告和错误信息。 show variables like 'log_error%'; （7）slow.log 如果配置了MySQL的慢查询日志，MySQL就会将运行过程中的慢查询日志记录到slow_log文件中。MySQL的慢查询日志是MySQL提供的一种日志记录，用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL。 show variables like '%slow_query_log%'; 例如，mysqldumpslow -s r -t 20 /database/mysql/slow-log表示为得到返回记录集最多的20个查询；mysqldumpslow -s t -t 20 -g \"left join\" /database/mysql/slow-log表示得到按照时间排序的前20条里面含有左连接的查询语句。 2.InnoDB存储引擎体系结构 2.1缓冲池 InnoDB存储引擎是基于磁盘存储的。由于CPU速度和磁盘速度之间的鸿沟，InnoDB引擎使用缓冲池技术来提高数据库的整体性能。 show variables like 'innodb_buffer_pool_size%'; ，这个对InnoDB整体性能影响也最大，一般可以设置为50%到80%的内存大小。在专用数据库服务器上，可以将缓冲池大小设置得大些，多次访问相同的数据表数据所需要的磁盘I/O就更少 InnoDB存储引擎的缓存机制和MyISAM的最大区别就在于InnoDB缓冲池不仅仅缓存索引，还会缓存实际的数据。 InnoDB存储引擎的LRU列表中还加入了midpoint位置，即新读取的页并不是直接放到LRU列表首部，而是放到LRU列表的midpoint位置。这个算法在InnoDB存储引擎下称为midpoint insertion strategy，默认该位置在LRU列表长度的5/8处。midpoint的位置可由参数innodb_old_blocks_pct控制 show variables like 'innodb_old_blocks_pct%'; 在图中，innodb_old_blocks_pct默认值为37，表示新读取的页插入到LRU列表尾端37%的位置 MySQL 5.6之后的版本提供了一个新特性来快速预热buffer_pool缓冲池，如图2-4所示。参数innodb_buffer_pool_dump_at_shutdown=ON表示在关闭MySQL时会把内存中的热数据保存在磁盘里ib_buffer_pool文件中，其保存比率由参数innodb_buffer_pool_dump_pct控制，默认为25%。 2.2change buffer change buffer的主要目的是将对二级索引的数据操作缓存下来，以减少二级索引的随机I/O，并达到操作合并的效果。 2.3自适应哈希索引 InnoDB存储引擎会监控对表上二级索引的查找。如果发现某二级索引被频繁访问，二级索引就成为热数据；如果观察到建立哈希索引可以带来速度的提升，则建立哈希索引，所以称之为自适应（adaptive）的，即自适应哈希索引（Adaptive Hash Index，AHI）。 2.4redo log buffer redo log buffer是一块内存区域，存放将要写入redo log文件的数据 2.5double write double write（两次写）技术的引入是为了提高数据写入的可靠性 2.6InnoDB后台线程 2.6.1InnoDB后台主线程 master thread是InnoD存储引擎非常核心的一个在后台运行的主线程，相当重要。它做的主要工作包括但不限于：将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲等。 2.6.2InnoDB后台I/O线程 在InnoDB存储引擎中大量使用了AIO（Async I/O）来处理写I/O请求，这样可以极大地提高数据库的性能。I/O线程的工作主要是负责这些I/O请求的回调（call back）处理。InnoDB 1.0版本之前共有4个I/O线程，分别是write、read、insert buffer和log I/O thread。在Linux平台下，I/O线程的数量不能进行调整，但是在Windows平台下可以通过参数innodb_file_io_threads来增大I/O线程。从InnoDB 1.0.x版本开始，read thread和write thread分别增大到了4个，并且不再使用innodb_file_io_threads参数，而是分别使用innodb_read_io_threads和innodb_write_io_threads参数进行设置，如此调整后，在Linux平台上就可以根据CPU核数来更改相应的参数值了。 2.6.3InnoDB脏页刷新线程 MySQL 5.6版本以前，脏页的清理工作交由master线程处理。page cleaner thread是5.6.2版本引入的一个新线程，实现从master线程中卸下缓冲池刷脏页的工作。为了进一步提升扩展性和刷脏效率，在5.7.4版本里引入了多个page cleaner线程，从而达到并行刷脏的效果。 2.6.4InnoDB purge线程 purge thread负责回收已经使用并分配的undo页 2.7redo log 明确一下InnoDB修改数据的基本流程。当我们想要修改DB上某一行数据的时候，InnoDB是把数据从磁盘读取到内存的缓冲池上进行修改。这个时候数据在内存中被修改，与磁盘中相比就存在了差异，我们称这种有差异的数据为脏页。InnoDB对脏页的处理不是每次生成脏页就将脏页刷新回磁盘，因为这样会产生海量的I/O操作，严重影响InnoDB的处理性能。既然脏页与磁盘中的数据存在差异，那么如果在此期间DB出现故障就会造成数据丢失。为了解决这个问题，redo log就应运而生了。 2.8undo log undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，undo记录默认被记录到系统表空间（ibdata）中，但从MySQL 5.6开始，也可以使用独立的undo表空间。 2.9Query Cache 3.Mysql事务和锁 3.1事务隔离级别 数据库提供了4种隔离级别，由低到高依次为read uncommitted（读未提交）、read committed（读已提交）、repeatable read（可重复读取）、serializable（序列化）。 3.2InoDB锁机制介绍 锁机制是事务型数据库中为了保证数据的一致性手段 InnoDB主要使用两种级别的锁机制： 行级锁和表级锁。 InnoDB的行级锁类型主要有共享（S）锁（又称读锁）、排他（X）锁（又称写锁）。共享（S）锁允许持有该锁的事务读取行；排他（X）锁允许持有该锁的事务更新或删除行。 3.3锁等待和死锁 3.3.1锁等待 锁等待是指一个事务过程中产生的锁，其他事务需要等待上一个事务释放它的锁才能占用该资源。如果该事务一直不释放，就需要持续等待下去，直到超过了锁等待时间，会报一个等待超时的错误。在MySQL中通过innodb_lock_wait_timeout参数来控制锁等待时间，单位是秒。如图3-9所示，可以通过语句show variables like '%innodb_lock_wait%'来查看锁等待超时时间 当MySQL发生锁等待情况时，可以通过语句select * from sys.innodb_lock_waits来在线查看， 3.3.2死锁 在MySQL中，两个或两个以上的事务相互持有和请求锁，并形成一个循环的依赖关系，就会产生死锁，也就是锁资源请求产生了死循环现象。InnoDB会自动检测事务死锁，立即回滚其中某个事务，并且返回一个错误。它根据某种机制来选择那个最简单（代价最小）的事务来进行回滚。常见的死锁会报错“ERROR 1213 (40001):deadlock found when trying to get lock; try restarting transaction.”。偶然发生的死锁不必担心，InnoDB存储引擎有一个后台的锁监控线程，该线程负责查看可能的死锁问题，并自动告知用户。 3.5锁问题的监控 我们通过show full processlist是为了查看当前MySQL是否有压力、都在跑什么语句、当前语句耗时多久了，从中可以看到总共有多少链接数、哪些线程有问题，然后把有问题的线程kill掉，临时解决一些突发性的问题。 通过执行show engine innodb status命令来查看是否存在锁表情况，如 MySQL将事务和锁信息记录在了information_schema数据库中，我们只需要查询即可。涉及的表主要有3个，即innodb_trx、innodb_locks、innodb_lock_waits， select * from information_schema.INNODB_TRX; select * from information_schema.INNODB_LOCKS; select * from information_schema.INNODB_LOCK_WAITS; 4.Mysql服务器全面优化 4.1Mysql 5.7 InnoDB存储引擎增强特性 Online Alter Table以及索引 innodb_buffer_pool online change innodb_buffer_pool_dump和load的增强 InnoDB临时表优化 page clean的效率提升 undo log自动清除4.2硬件层面优化 （1）使用SSD或者PCIe SSD设备，至少获得数百倍甚至万倍的IOPS提升。 4.3Linux操作系统层面优化 文件系统选择推荐使用xfs 4.4Mysql配置参数优化 4.5设计规范 库表的设计规范 索引设计规范 sql语句的规范 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:20:32 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/mysql-index.html":{"url":"performance/mysql/mysql-index.html","title":"3.索引底层数据结构","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.索引到底是什么 2.索引底层数据结构与算法 2.1B-Tree 2.2B+Tree(B-Tree变种) 3.索引最左前缀原理 1.索引到底是什么 索引是帮助MySQL高效获取数据的排好序的数据结构 数据结构教学网站：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html 2.索引底层数据结构与算法 二叉树 红黑树 HASH BTREE 2.1B-Tree 度(Degree)-节点的数据存储个数 叶节点具有相同的深度 叶节点的指针为空 节点中的数据key从左到右递增排列 2.2B+Tree(B-Tree变种) 非叶子节点不存储data，只存储key，可以增大度 叶子节点不存储指针 顺序访问指针，提高区间访问的性能 3.索引最左前缀原理 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:01:37 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/Explain.html":{"url":"performance/mysql/Explain.html","title":"4.Explain详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.explain 中的列 1. id列 1）简单子查询 2）from子句中的子查询 3）union查询 2. select_type列 1）simple：简单查询。查询不包含子查询和union 2）primary：复杂查询中最外层的 select 3）subquery：包含在 select 中的子查询（不在 from 子句中） 4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） 5）union：在 union 中的第二个和随后的 select 6）union result：从 union 临时表检索结果的 select 3. table列 4. type列 5. possible_keys列 6. key列 7. key_len列 8. ref列 9. rows列 10. Extra列 2.索引最佳实践 1. 全值匹配 2.最佳左前缀法则 3.不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描 4.存储引擎不能使用索引中范围条件右边的列 5.尽量使用覆盖索引（只访问索引的查询（索引列包含查询列）），减少select *语句 6.mysql在使用不等于（！=或者<>）的时候无法使用索引会导致全表扫描 7.is null,is not null 也无法使用索引 8.like以通配符开头（'$abc...'）mysql索引失效会变成全表扫描操作 9.字符串不加单引号索引失效 10.少用or,用它连接时很多情况下索引会失效 使用EXPLAIN关键字可以模拟优化器执行SQL语句，从而知道MySQL是 如何处理你的SQL语句的。分析你的查询语句或是结构的性能瓶颈 下面是使用 explain 的例子： 在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询时，会返回执行计划的信息，而不是执行这条SQL（如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中） 使用的表 DROP TABLE IF EXISTS `actor`; CREATE TABLE `actor` ( `id` int(11) NOT NULL, `name` varchar(45) DEFAULT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,'a','2017-12-22 15:27:18'), (2,'b','2017-12-22 15:27:18'), (3,'c','2017-12-22 15:27:18'); DROP TABLE IF EXISTS `film`; CREATE TABLE `film` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film` (`id`, `name`) VALUES (3,'film0'),(1,'film1'),(2,'film2'); DROP TABLE IF EXISTS `film_actor`; CREATE TABLE `film_actor` ( `id` int(11) NOT NULL, `film_id` int(11) NOT NULL, `actor_id` int(11) NOT NULL, `remark` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_film_actor_id` (`film_id`,`actor_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1); mysql> explain select * from actor; 在查询中的每个表会输出一行，如果有两个表通过 join 连接查询，那么会输出两行。表的意义相当广泛：可以是子查询、一个 union 结果等。 explain 有两个变种： 1）explain extended：会在 explain 的基础上额外提供一些查询优化的信息。紧随其后通过 show warnings 命令可以 得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个半分比的值，rows * filtered/100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的表）。 mysql> explain extended select * from film where id = 1; mysql> show warnings; 2）explain partitions：相比 explain 多了个 partitions 字段，如果查询是基于分区表的话，会显示查询将访问的分区。 1.explain 中的列 接下来我们将展示 explain 中每个列的信息。 1. id列 id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。MySQL将 select 查询分为简单查询(SIMPLE)和复杂查询(PRIMARY)。 复杂查询分为三类：简单子查询、派生表（from语句中的子查询）、union 查询。 id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行 1）简单子查询 mysql> explain select (select 1 from actor limit 1) from film; 2）from子句中的子查询 mysql> explain select id from (select id from film) as der; 这个查询执行时有个临时表别名为der，外部 select 查询引用了这个临时表 3）union查询 mysql> explain select 1 union all select 1; union结果总是放在一个匿名临时表中，临时表不在SQL中出现，因此它的id是NULL。 2. select_type列 select_type 表示对应行是简单还是复杂的查询，如果是复杂的查询，又是上述三种复杂查询中的哪一种。 1）simple：简单查询。查询不包含子查询和union mysql> explain select * from film where id = 2; 2）primary：复杂查询中最外层的 select 3）subquery：包含在 select 中的子查询（不在 from 子句中） 4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） 用这个例子来了解 primary、subquery 和 derived 类型 mysql> explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; 5）union：在 union 中的第二个和随后的 select 6）union result：从 union 临时表检索结果的 select 用这个例子来了解 union 和 union result 类型： mysql> explain select 1 union all select 1; 3. table列 这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查询。 当有 union 时，UNION RESULT 的 table 列的值为，1和2表示参与 union 的 select 行id。 4. type列 这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 依次从最优到最差分别为：system > const > eq_ref > ref > range > index > ALL 一般来说，得保证查询达到range级别，最好达到ref NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表 mysql> explain select min(id) from film; const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是const的特例，表里只有一条元组匹配时为system mysql> explain extended select from (select from film where id = 1) tmp; mysql> show warnings; eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 mysql> explain select * from film_actor left join film on film_actor.film_id = film.id; ref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。 简单 select 查询，name是普通索引（非唯一索引） mysql> explain select * from film where name = \"film1\"; 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。 mysql> explain select film_id from film left join film_actor on film.id = film_actor.film_id; range：范围扫描通常出现在 in(), between ,> ,= 等操作中。使用一个索引来检索给定范围的行。 mysql> explain select * from actor where id > 1; index：扫描全表索引，这通常比ALL快一些。（index是从索引中读取的，而all是从硬盘中读取） mysql> explain select * from film; ALL：即全表扫描，意味着mysql需要从头到尾去查找所需要的行。通常情况下这需要增加索引来进行优化了 mysql> explain select * from actor; 5. possible_keys列 这一列显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果。 6. key列 这一列显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 7. key_len列 这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通过结果中的key_len=4可推断出查询使用了第一个列：film_id列来执行索引查找。 mysql> explain select * from film_actor where film_id = 2; key_len计算规则如下： 字符串 char(n)：n字节长度 varchar(n)：2字节存储字符串长度，如果是utf-8，则长度 3n + 2 数值类型 tinyint：1字节 smallint：2字节 int：4字节 bigint：8字节 时间类型 date：3字节 timestamp：4字节 datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。 8. ref列 这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id） 9. rows列 这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 10. Extra列 这一列展示的是额外信息。常见的重要值如下： Using index：查询的列被索引覆盖，并且where筛选条件是索引的前导列，是性能高的表现。一般是使用了覆盖索引(索引包含了所有查询的字段)。对于innodb来说，如果是辅助索引性能会有不少提高 mysql> explain select film_id from film_actor where film_id = 1; Using where：查询的列未被索引覆盖，where筛选条件非索引的前导列 mysql> explain select * from actor where name = 'a'; Using where Using index：查询的列被索引覆盖，并且where筛选条件是索引列之一但是不是索引的前导列，意味着无法直接通过索引查找来查询到符合条件的数据 mysql> explain select film_id from film_actor where actor_id = 1; NULL：查询的列未被索引覆盖，并且where筛选条件是索引的前导列，意味着用到了索引，但是部分字段未被索引覆盖，必须通过“回表”来实现，不是纯粹地用到了索引，也不是完全没用到索引 mysql>explain select * from film_actor where film_id = 1; Using index condition：与Using where类似，查询的列不完全被索引覆盖，where条件中是一个前导列的范围； mysql> explain select * from film_actor where film_id > 1; Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化。 actor.name没有索引，此时创建了张临时表来distinct mysql> explain select distinct name from actor; film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表 mysql> explain select distinct name from film; Using filesort：mysql 会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。此时mysql会根据联接类型浏览所有符合条件的记录，并保存排序关键字和行指针，然后排序关键字并按顺序检索行信息。这种情况下一般也是要考虑使用索引来优化的。 actor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 mysql> explain select * from actor order by name; film.name建立了idx_name索引,此时查询时extra是using indexmysql> explain select * from film order by name; 2.索引最佳实践 使用的表 CREATE TABLE `employees` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(24) NOT NULL DEFAULT '' COMMENT '姓名', `age` int(11) NOT NULL DEFAULT '0' COMMENT '年龄', `position` varchar(20) NOT NULL DEFAULT '' COMMENT '职位', `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间', PRIMARY KEY (`id`), KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COMMENT='员工记录表'; INSERT INTO employees(name,age,position,hire_time) VALUES('LiLei',22,'manager',NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES('HanMeimei', 23,'dev',NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES('Lucy',23,'dev',NOW()); 最佳实践 1. 全值匹配 EXPLAIN SELECT * FROM employees WHERE name= 'LiLei'; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 22; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 22 AND position ='manager'; 2.最佳左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 EXPLAIN SELECT * FROM employees WHERE age = 22 AND position ='manager'; EXPLAIN SELECT * FROM employees WHERE position = 'manager'; EXPLAIN SELECT * FROM employees WHERE name = 'LiLei'; 3.不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描 EXPLAIN SELECT * FROM employees WHERE name = 'LiLei'; EXPLAIN SELECT * FROM employees WHERE left(name,3) = 'LiLei'; 4.存储引擎不能使用索引中范围条件右边的列 EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 22 AND position ='manager'; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age > 22 AND position ='manager'; 5.尽量使用覆盖索引（只访问索引的查询（索引列包含查询列）），减少select *语句 EXPLAIN SELECT name,age FROM employees WHERE name= 'LiLei' AND age = 23 AND position ='manager'; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 23 AND position ='manager'; 6.mysql在使用不等于（！=或者<>）的时候无法使用索引会导致全表扫描 EXPLAIN SELECT * FROM employees WHERE name != 'LiLei' 7.is null,is not null 也无法使用索引 EXPLAIN SELECT * FROM employees WHERE name is null 8.like以通配符开头（'$abc...'）mysql索引失效会变成全表扫描操作 EXPLAIN SELECT * FROM employees WHERE name like '%Lei' EXPLAIN SELECT * FROM employees WHERE name like 'Lei%' 问题：解决like'%字符串%'索引不被使用的方法？ a）使用覆盖索引，查询字段必须是建立覆盖索引字段 EXPLAIN SELECT name,age,position FROM employees WHERE name like '%Lei%'; b）当覆盖索引指向的字段是varchar(380)及380以上的字段时，覆盖索引会失效！ 9.字符串不加单引号索引失效 EXPLAIN SELECT * FROM employees WHERE name = '1000'; EXPLAIN SELECT * FROM employees WHERE name = 1000; 10.少用or,用它连接时很多情况下索引会失效 EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' or name = 'HanMeimei'; 总结： like KK%相当于=常量，%KK和%KK% 相当于范围 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:01:48 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/high-index.html":{"url":"performance/mysql/high-index.html","title":"5.索引优化深入","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.创建test表（测试表） 2.创建索引 3.分析以下Case索引使用情况 3.1Case 1： 3.2Case 2： 3.2.1Case 2.1： 3.2.1Case 2.2： 3.3Case 3： 3.3.1Case 3.1： 3.3.2Case 3.2： 3.4Case 4： 3.4.1Case 4.1： 3.4.2Case 4.2： 3.4.3Case 4.3： 3.5Case 5： 3.5.1Case 5.1： 3.6Case 6： 3.7Case 7： 3.8Case 8： 4.总结： 1.order by语句使用索引最左前列。 2.使用where子句与order by子句条件列组合满足索引最左前列。 等价于： 等价于 A表与B表的ID字段应建立索引 1.创建test表（测试表） drop table if exists test; create table test( id int primary key auto_increment, c1 varchar(10), c2 varchar(10), c3 varchar(10), c4 varchar(10), c5 varchar(10) ) ENGINE=INNODB default CHARSET=utf8; insert into test(c1,c2,c3,c4,c5) values('a1','a2','a3','a4','a5'); insert into test(c1,c2,c3,c4,c5) values('b1','b2','b3','b4','b5'); insert into test(c1,c2,c3,c4,c5) values('c1','c2','c3','c4','c5'); insert into test(c1,c2,c3,c4,c5) values('d1','d2','d3','d4','d5'); insert into test(c1,c2,c3,c4,c5) values('e1','e2','e3','e4','e5'); 2.创建索引 3.分析以下Case索引使用情况 3.1Case 1： 分析： ①创建复合索引的顺序为c1,c2,c3,c4。 ②上述四组explain执行的结果都一样：type=ref，key_len=132，ref=const,const,const,const。 结论：在执行常量等值查询时，改变索引列的顺序并不会更改explain的执行结果，因为mysql底层优化器会进行优化，但是推荐按照索引顺序列编写sql语句。 3.2Case 2： 分析： 当出现范围的时候，type=range，key_len=99，比不用范围key_len=66增加了，说明使用上了索引，但对比Case1中执行结果，说明c4上索引失效。 结论：范围右边索引列失效，但是范围当前位置（c3）的索引是有效的，从key_len=99可证明。 3.2.1Case 2.1： 分析： 与上面explain执行结果对比，key_len=132说明索引用到了4个，因为对此sql语句mysql底层优化器会进行优化：范围右边索引列失效（c4右边已经没有索引列了），注意索引的顺序（c1,c2,c3,c4），所以c4右边不会出现失效的索引列，因此4个索引全部用上。 结论：范围右边索引列失效，是有顺序的：c1,c2,c3,c4，如果c3有范围，则c4失效；如果c4有范围，则没有失效的索引列，从而会使用全部索引。 3.2.1Case 2.2： 分析： 如果在c1处使用范围，则type=ALL，key=Null，索引失效，全表扫描，这里违背了最佳左前缀法则，带头大哥已死，因为c1主要用于范围，而不是查询。 解决方式使用覆盖索引。 结论：在最佳左前缀法则中，如果最左前列（带头大哥）的索引失效，则后面的索引都失效。 3.3Case 3： 分析： 利用最佳左前缀法则：中间兄弟不能断，因此用到了c1和c2索引（查找），从key_len=66，ref=const,const，c3索引列用在排序过程中。 3.3.1Case 3.1： 分析： 从explain的执行结果来看：key_len=66，ref=const,const，从而查找只用到c1和c2索引，c3索引用于排序。 3.3.2Case 3.2： 分析： 从explain的执行结果来看：key_len=66，ref=const,const，查询使用了c1和c2索引，由于用了c4进行排序，跳过了c3，出现了Using filesort。 3.4Case 4： 分析： 查找只用到索引c1，c2和c3用于排序，无Using filesort。 3.4.1Case 4.1： 分析： 和Case 4中explain的执行结果一样，但是出现了Using filesort，因为索引的创建顺序为c1,c2,c3,c4，但是排序的时候c2和c3颠倒位置了。 3.4.2Case 4.2： 分析： 在查询时增加了c5，但是explain的执行结果一样，因为c5并未创建索引。 3.4.3Case 4.3： 分析： 与Case 4.1对比，在Extra中并未出现Using filesort，因为c2为常量，在排序中被优化，所以索引未颠倒，不会出现Using filesort。 3.5Case 5： 分析： 只用到c1上的索引，因为c4中间间断了，根据最佳左前缀法则，所以key_len=33，ref=const，表示只用到一个索引。 3.5.1Case 5.1： 分析： 对比Case 5，在group by时交换了c2和c3的位置，结果出现Using temporary和Using filesort，极度恶劣。原因：c3和c2与索引创建顺序相反。 3.6Case 6： 分析： ①在c1,c2,c3,c4上创建了索引，直接在c1上使用范围，导致了索引失效，全表扫描：type=ALL，ref=Null。因为此时c1主要用于排序，并不是查询。 ②使用c1进行排序，出现了Using filesort。 ③解决方法：使用覆盖索引。 3.7Case 7： 分析： 虽然排序的字段列与索引顺序一样，且order by默认升序，这里c2 desc变成了降序，导致与索引的排序方式不同，从而产生Using filesort。 3.8Case 8： EXPLAIN extended select c1 from test where c1 in ('a1','b1') ORDER BY c2,c3; 分析： 对于排序来说，多个相等条件也是范围查询 4.总结： ①MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index效率高，filesort效率低。 ②order by满足两种情况会使用Using index。 1.order by语句使用索引最左前列。 2.使用where子句与order by子句条件列组合满足索引最左前列。 ③尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最佳左前缀法则。 ④如果order by的条件不在索引列上，就会产生Using filesort。 ⑤group by与order by很类似，其实质是先排序后分组，遵照索引创建顺序的最佳左前缀法则。注意where高于having，能写在where中的限定条件就不要去having限定了。 通俗理解口诀： 全值匹配我最爱，最左前缀要遵守； 带头大哥不能死，中间兄弟不能断； 索引列上少计算，范围之后全失效； LIKE百分写最右，覆盖索引不写星； 不等空值还有or，索引失效要少用。 补充：in和exsits优化 原则：小表驱动大表，即小的数据集驱动大的数据集 in：当B表的数据集必须小于A表的数据集时，in优于exists select * from A where id in (select id from B) 等价于： for select id from B for select * from A where A.id = B.id exists：当A表的数据集小于B表的数据集时，exists优于in 将主查询A的数据，放到子查询B中做条件验证，根据验证结果（true或false）来决定主查询的数据是否保留 select * from A where exists (select 1 from B where B.id = A.id) 等价于 for select * from A for select * from B where B.id = A.id A表与B表的ID字段应建立索引 1、EXISTS (subquery)只返回TRUE或FALSE,因此子查询中的SELECT * 也可以是SELECT 1或select X,官方说法是实际执行时会忽略SELECT清单,因此没有区别 2、EXISTS子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比 3、EXISTS子查询往往也可以用JOIN来代替，何种最优需要具体问题具体分析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:02:03 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/acid.html":{"url":"performance/mysql/acid.html","title":"6.Mysql锁与事务","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 １. 概述 1.1 定义 1.2 锁的分类 2. 锁 2.1 表锁（偏读） 2.1.1 基本操作 2.1.2 案例分析(加读锁） 2.1.3 案例分析(加写锁） 2.1.4 案例结论 2.2 行锁（偏写） 2.2.1 行锁支持事务 2.2.2 行锁案例分析 2.2.3******隔离级别案例分析** 2.2.4 案例结论 2.2.5 行锁分析 2.2.6 死锁 2.2.7 优化建议 １. 概述 1.1 定义 锁是计算机协调多个进程或线程并发访问某一资源的机制。 在数据库中，除了传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供需要用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。 1.2 锁的分类 从性能上分为乐观锁(用版本对比来实现)和悲观锁 从对数据库操作的类型分，分为读锁和写锁(都属于悲观锁) 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响 写锁（排它锁）：当前写操作没有完成前，它会阻断其他写锁和读锁 从对数据操作的粒度分，分为表锁和行锁2. 锁 2.1 表锁（**偏读**） 表锁偏向MyISAM存储引擎，开销小，加锁快，无思索，锁定粒度大，发生锁冲突的概率最高，并发度最低。 2.1.1 基本操作 建表SQL CREATE TABLE `mylock` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `NAME` VARCHAR (20) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE = MyISAM DEFAULT CHARSET = utf8; 插入数据 INSERT INTO`test`.`mylock`(`id`,`NAME`)VALUES('1','a'); INSERT INTO`test`.`mylock`(`id`,`NAME`)VALUES('2','b'); INSERT INTO`test`.`mylock`(`id`,`NAME`)VALUES('3','c'); INSERT INTO`test`.`mylock`(`id`,`NAME`)VALUES('4','d'); 手动增加表锁 lock table表名称read(write),表名称2read(write); 查看表上加过的锁 show opentables; 删除表锁 unlock tables; 2.1.2 案例分析(加读锁） 当前session和其他session都可以读该表 当前session中插入或者更新锁定的表都会报错，其他session插入或更新则会等待 2.1.3 案例分析(加写锁） 当前session对该表的增删改查都没有问题，其他session对该表的所有操作被阻塞 2.1.4 案例结论 MyISAM在执行查询语句(SELECT)前,会自动给涉及的所有表加读锁,在执行增删改操作前,会自动给涉及的表加写锁。 1、对MyISAM表的读操作(加读锁) ,不会阻寒其他进程对同一表的读请求,但会阻赛对同一表的写请求。只有当读锁释放后,才会执行其它进程的写操作。 2、对MylSAM表的写操作(加写锁) ,会阻塞其他进程对同一表的读和写操作,只有当写锁释放后,才会执行其它进程的读写操作 总结： 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。 2.2 行锁（**偏写**） 行锁偏向InnoDB存储引擎，开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。InnoDB与MYISAM的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。 2.2.1 行锁支持事务 事务（Transaction）及其ACID属性 事务是由一组SQL语句组成的逻辑处理单元,事务具有以下4个属性,通常简称为事务的ACID属性。 原子性(Atomicity)：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行。 一致性(Consistent)：在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性;事务结束时,所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。 隔离性(Isolation)：数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,反之亦然。 持久性(Durable)：事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持。 并发事务处理带来的问题 更新丢失（Lost Update） 当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题–最后的更新覆盖了由其他事务所做的更新。 脏读（Dirty Reads） 一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致的状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫做“脏读”。 一句话：事务A读取到了事务B已经修改但尚未提交的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。 不可重读（Non-Repeatable Reads） 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。 一句话：事务A读取到了事务B已经提交的修改数据，不符合隔离性 幻读（Phantom Reads） 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 一句话：事务A读取到了事务B提交的新增数据，不符合隔离性 脏读是事务B里面修改了数据 幻读是事务B里面新增了数据 事务隔离级别 脏读”、“不可重复读”和“幻读”,其实都是数据库读一致性问题,必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格,并发副作用越小,但付出的代价也就越大,因为事务隔离实质上就是使事务在一定程度上“串行化”进行,这显然与“并发”是矛盾的。 同时,不同的应用对读一致性和事务隔离程度的要求也是不同的,比如许多应用对“不可重复读\"和“幻读”并不敏感,可能更关心数据并发访问的能力。 常看当前数据库的事务隔离级别: show variables like 'tx_isolation'; 设置事务隔离级别：**set tx_isolation='REPEATABLE-READ';** 2.2.2 行锁案例分析 用下面的表演示，需要开启事务，Session_1更新某一行，Session_2更新同一行被阻塞，但是更新其他行正常2.2.3**隔离级别案例分析** CREATE TABLE `account` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `balance` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `test`.`account` (`name`, `balance`) VALUES ('lilei', '450'); INSERT INTO `test`.`account` (`name`, `balance`) VALUES ('hanmei', '16000'); INSERT INTO `test`.`account` (`name`, `balance`) VALUES ('lucy', '2400'); 1、读未提交： （1）打开一个客户端A，并设置当前事务模式为read uncommitted（未提交读），查询表account的初始值： set tx_isolation='read-uncommitted'; （2）在客户端A的事务提交之前，打开另一个客户端B，更新表account： （3）这时，虽然客户端B的事务还没提交，但是客户端A就可以查询到B已经更新的数据： （4）一旦客户端B的事务因为某种原因回滚，所有的操作都将会被撤销，那客户端A查询到的数据其实就是脏数据： （5）在客户端A执行更新语句update account set balance = balance - 50 where id =1，lilei的balance没有变成350，居然是400，是不是很奇怪，数据不一致啊，如果你这么想就太天真 了，在应用程序中，我们会用400-50=350，并不知道其他会话回滚了，要想解决这个问题可以采用读已提交的隔离级别 2、读已提交 （1）打开一个客户端A，并设置当前事务模式为read committed（未提交读），查询表account的所有记录： set tx_isolation='read-committed'; （2）在客户端A的事务提交之前，打开另一个客户端B，更新表account： （3）这时，客户端B的事务还没提交，客户端A不能查询到B已经更新的数据，解决了脏读问题： （4）客户端B的事务提交 （5）客户端A执行与上一步相同的查询，结果 与上一步不一致，即产生了不可重复读的问题 3、可重复读 （1）打开一个客户端A，并设置当前事务模式为repeatable read，查询表account的所有记录 set tx_isolation='repeatable-read'; （2）在客户端A的事务提交之前，打开另一个客户端B，更新表account并提交 （3）在客户端A查询表account的所有记录，与步骤（1）查询结果一致，没有出现不可重复读的问题 （4）在客户端A，接着执行update balance = balance - 50 where id = 1，balance没有变成400-50=350，lilei的balance值用的是步骤（2）中的350来算的，所以是300，数据的一致性倒是没有被破坏。可重复读的隔离级别下使用了MVCC机制，select操作不会更新版本号，是快照读（历史版本）；insert、update和delete会更新版本号，是当前读（当前版本）。 （5）重新打开客户端B，插入一条新数据后提交 （6）在客户端A查询表account的所有记录，没有 查出 新增数据，所以没有出现幻读 (7)验证幻读 在客户端A执行update account set balance=888 where id = 4;能更新成功，再次查询能查到客户端B新增的数据 4.串行化 （1）打开一个客户端A，并设置当前事务模式为serializable，查询表account的初始值： set tx_isolation='serializable'; mysql>setsessiontransactionisolationlevelserializable; Query OK,0rows affected (0.00sec) mysql>starttransaction; Query OK,0rows affected (0.00sec) mysql>selectfromaccount;+------+--------+---------+|id|name|balance|+------+--------+---------+|1|lilei|10000||2|hanmei|10000||3|lucy|10000||4|lily|10000|+------+--------+---------+4rowsinset(*0.00sec) （2）打开一个客户端B，并设置当前事务模式为serializable，插入一条记录报错，表被锁了插入失败，mysql中事务隔离级别为serializable时会锁表，因此不会出现幻读的情况，这种隔离级别并发性极低，开发中很少会用到。 mysql>setsessiontransactionisolationlevelserializable; Query OK,0rows affected (0.00sec) mysql>starttransaction; Query OK,0rows affected (0.00sec) mysql>insertintoaccountvalues(5,'tom',0); ERROR1205(HY000): Lock wait timeout exceeded; try restartingtransaction Mysql默认级别是repeatable-read，有办法解决幻读问题吗？ 间隙锁在某些情况下可以解决幻读问题 要避免幻读可以用间隙锁在Session_1下面执行update account set name = 'zhuge' where id > 10 and id 2.2.4 案例结论 Innodb存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一下，但是在整体并发处理能力方面要远远优于MYISAM的表级锁定的。当系统并发量高的时候，Innodb的整体性能和MYISAM相比就会有比较明显的优势了。 但是，Innodb的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让Innodb的整体性能表现不仅不能比MYISAM高，甚至可能会更差。 2.2.5 行锁分析 通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况 showstatuslike'innodb_row_lock%'; 对各个状态量的说明如下： Innodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg: 每次等待所花平均时间 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花时间 Innodb_row_lock_waits:系统启动后到现在总共等待的次数 对于这5个状态变量，比较重要的主要是： Innodb_row_lock_time_avg （等待平均时长） Innodb_row_lock_waits （等待总次数） Innodb_row_lock_time（等待总时长） 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 2.2.6 死锁 set tx_isolation='repeatable-read'; Session_1执行：select * from account where id=1 for update; Session_2执行：select * from account where id=2 for update; Session_1执行：select * from account where id=2 for update; Session_2执行：select * from account where id=1 for update; 查看近期死锁日志信息：show engine innodb status\\G; 大多数情况mysql可以自动检测死锁并回滚产生死锁的那个事务，但是有些情况mysql没法自动检测死锁 2.2.7 优化建议 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能减少检索条件，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可能低级别事务隔离 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:18:12 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/":{"url":"performance/nginx/","title":"nginx反向代理","keywords":"","body":"nginx反向代理 1.Nginx核心模块与配置 2.Nginx性能优化 Nginx-黑马 1.基础 2.进阶篇 3.Rewrite功能配置 4.负载均衡 5.集群搭建 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:48:03 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx-config.html":{"url":"performance/nginx/Nginx-config.html","title":"1.Nginx核心模块与配置","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Nginx 简介与安装 知识点： 1、Nginx简介： 2、编译与安装 二、Nginx 架构说明 三、Nginx 配置与使用 知识点 1、配置文件的语法格式： 2、配置第一个静态WEB服务 3、日志配置： 概要： Nginx 简介 Nginx 架构说明 Nginx 基础配置与使用一、Nginx 简介与安装 知识点： Nginx 简介 Nginx 编译与安装1、Nginx简介： Nginx是一个高性能WEB服务器，除它之外Apache、Tomcat、Jetty、IIS，它们都是Web服务器，或者叫做WWW（World Wide Web）服务器，相应地也都具备Web服务器的基本功能。Nginx 相对基它WEB服务有什么优势呢？ Tomcat、Jetty 面向java语言，先天就是重量级的WEB服务器，其性能与Nginx没有可比性。 IIS只能在Windows操作系统上运行。Windows作为服务器在稳定性与其他一些性能上都不如类UNIX操作系统，因此，在需要高性能Web服务器的场合下IIS并不占优。 Apache的发展时期很长，而且是目前毫无争议的世界第一大Web服务器，其有许多优点，如稳定、开源、跨平台等，但它出现的时间太长了，在它兴起的年代，互联网的产业规模远远比不上今天，所以它被设计成了一个重量级的、不支持高并发的Web服务器。在Apache服务器上，如果有数以万计的并发HTTP请求同时访问，就会导致服务器上消耗大量内存，操作系统内核对成百上千的Apache进程做进程间切换也会消耗大量CPU资源，并导致HTTP请求的平均响应速度降低，这些都决定了Apache不可能成为高性能Web服务器，这也促使了Lighttpd和Nginx的出现。 下图可以看出07年到17 年强劲增长势头。 2、编译与安装 安装环境准备： （1）linux 内核2.6及以上版本: 只有2.6之后才支持epool ，在此之前使用select或pool多路复用的IO模型，无法解决高并发压力的问题。通过命令uname -a 即可查看。 #查看 linux 内核 uname -a （2）GCC编译器 GCC（GNU Compiler Collection）可用来编译C语言程序。Nginx不会直接提供二进制可执行程序,只能下载源码进行编译。 （3）PCRE库 PCRE（Perl Compatible Regular Expressions，Perl兼容正则表达式）是由Philip Hazel开发的函数库，目前为很多软件所使用，该库支持正则表达式。 （4）zlib库 zlib库用于对HTTP包的内容做gzip格式的压缩，如果我们在nginx.conf里配置了gzip on，并指定对于某些类型（content-type）的HTTP响应使用gzip来进行压缩以减少网络传输量。 （5）OpenSSL开发库 如果我们的服务器不只是要支持HTTP，还需要在更安全的SSL协议上传输HTTP，那么就需要拥有OpenSSL了。另外，如果我们想使用MD5、SHA1等散列函数，那么也需要安装它。 上面几个库都是Nginx 基础功能所必需的，为简单起见我们可以通过yum 命令统一安装。 #yum 安装nginx 环境 yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel pcre pcre-devel 源码获取： nginx 下载页：http://nginx.org/en/download.html 。 # 下载nginx 最新稳定版本 wget http://nginx.org/download/nginx-1.14.0.tar.gz #解压 tar -zxvf nginx-1.14.0.tar.gz 最简单的安装： # 全部采用默认安装 ./configure make && make install 执行完成之后 nginx 运行文件 就会被安装在 /usr/local/nginx 下。 基于参数构建 ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-debug 控制命令： #默认方式启动： ./sbin/nginx #指定配置文件启动 ./sbing/nginx -c /tmp/nginx.conf #指定nginx程序目录启动 ./sbin/nginx -p /usr/local/nginx/ #快速停止 ./sbin/nginx -s stop #优雅停止 ./sbin/nginx -s quit # 热装载配置文件 ./sbin/nginx -s reload # 重新打开日志文件 ./sbin/nginx -s reopen 模块更新： 二、Nginx 架构说明 Nginx 架构图: 架构说明： 1）nginx启动时，会生成两种类型的进程，一个是主进程（Master），一个（windows版本的目前只有一个）和多个工作进程（Worker）。主进程并不处理网络请求，主要负责调度工作进程，也就是图示的三项：加载配置、启动工作进程及非停升级。所以，nginx启动以后，查看操作系统的进程列表，我们就能看到至少有两个nginx进程。 2）服务器实际处理网络请求及响应的是工作进程（worker），在类unix系统上，nginx可以配置多个worker，而每个worker进程都可以同时处理数以千计的网络请求。 3）模块化设计。nginx的worker，包括核心和功能性模块，核心模块负责维持一个运行循环（run-loop），执行网络请求处理的不同阶段的模块功能，如网络读写、存储读写、内容传输、外出过滤，以及将请求发往上游服务器等。而其代码的模块化设计，也使得我们可以根据需要对功能模块进行适当的选择和修改，编译成具有特定功能的服务器。 4）事件驱动、异步及非阻塞，可以说是nginx得以获得高并发、高性能的关键因素，同时也得益于对Linux、Solaris及类BSD等操作系统内核中事件通知及I/O性能增强功能的采用，如kqueue、epoll及event ports。 Nginx 核心模块： 三、Nginx 配置与使用 知识点 配置文件语法格式 配置第一个静态WEB服务 配置案例 动静分离实现 防盗链 多域名站点 下载限速 IP 黑名单 基于user-agent分流 日志配置1、配置文件的语法格式： 先来看一个简单的nginx 配置worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } location /nginx_status { stub_status on; access_log off; } } } 上述配置中的events、http、server、location、upstream等属于配置项块。而worker_processes 、worker_connections、include、listen 属于配置项块中的属性。 /nginx_status 属于配置块的特定参数参数。其中server块嵌套于http块，其可以直接继承访问Http块当中的参数。 | 配置块 | 名称开头用大口号包裹其对应属性 | |:----|:----| | 属性 | 基于空格切分属性名与属性值，属性值可能有多个项 都以空格进行切分 如： access_log logs/host.access.log main | | 参数 | 其配置在 块名称与大括号间，其值如果有多个也是通过空格进行拆 | 注意 如果配置项值中包括语法符号，比如空格符，那么需要使用单引号或双引号括住配置项值，否则Nginx会报语法错误。例如： log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; 2、配置第一个静态WEB服务 基础站点演示： [ ] 创建站点目录 mkdir -p /usr/www/luban [ ] 编写静态文件 [ ] 配置 nginx.conf [ ] 配置server [ ] 配置location 基本配置介绍说明： （1）监听端口 语法：listen address： 默认：listen 80; 配置块：server （2）主机名称 语法：server_name name[……]; 默认：server_name \"\"; 配置块：server server_name后可以跟多个主机名称，如server_name www.testweb.com、download.testweb.com;。 支持通配符与正则 （3）location 语法：location[=|～|～*|^～|@]/uri/{……} 配置块：server / 基于uri目录匹配 =表示把URI作为字符串，以便与参数中的uri做完全匹配。 ～表示正则匹配URI时是字母大小写敏感的。 ～*表示正则匹配URI时忽略字母大小写问题。 ^～表示正则匹配URI时只需要其前半部分与uri参数匹配即可。 动静分离演示： [ ] 创建静态站点 [ ] 配置 location /static [ ] 配置 ~* .(gif|png|css|js)$ 基于目录动静分离 server { listen 80; server_name *.luban.com; root /usr/www/luban; location / { index luban.html; } location /static { alias /usr/www/static; } } 基于正则动静分离 location ~* \\.(gif|jpg|png|css|js)$ { root /usr/www/static; } 防盗链配置演示： # 加入至指定location 即可实现 valid_referers none blocked *.luban.com; if ($invalid_referer) { return 403; } 下载限速： location /download { limit_rate 1m; limit_rate_after 30m; } 创建IP黑名单 # 创建黑名单文件 echo 'deny 192.168.0.132;' >> balck.ip #http 配置块中引入 黑名单文件 include black.ip; 3、日志配置： 日志格式： log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log logs/access.log main; #基于域名打印日志 access_log logs/$host.access.log main; error日志的设置 语法：error_log /path/file level; 默认：error_log logs/error.log error; level是日志的输出级别，取值范围是debug、info、notice、warn、error、crit、alert、emerg， 针对指定的客户端输出debug级别的日志 语法：debug_connection[IP|CIDR] events { debug_connection 192.168.0.147; debug_connection 10.224.57.0/200; } nginx.conf Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:05 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx-In-actual-combat.html":{"url":"performance/nginx/Nginx-In-actual-combat.html","title":"2.Nginx性能优化","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Nginx 反向代理实现 知识点： 2.负载均衡配置与参数解析 3.upstream 负载均衡算法介绍 二、Nginx 高速缓存 知识点： 1、案例分析： 2.Nginx 静态缓存基本配置 3.缓存的清除： 三、Nginx 性能参数调优 时间：2018/10/14 概要： Nginx 反向代理与负载均衡 Nginx 实现高速缓存 Nginx 性能参数调优 一、Nginx 反向代理实现 知识点： 反向代理基本配置 负载均衡配置与参数解析 负载均衡算法详解 反向代理基本配置 提问：什么是反向代理其与正向代理有什么区别？ 正向代理的概念： 正向代理是指客户端与目标服务器之间增加一个代理服务器，客户端直接访问代理服务器，在由代理服务器访问目标服务器并返回客户端并返回 。这个过程当中客户端需要知道代理服务器地址，并配置连接。 反向代理的概念： 反向代理是指 客户端访问目标服务器，在目标服务内部有一个统一接入网关将请求转发至后端真正处理的服务器并返回结果。这个过程当中客户端不需要知道代理服务器地址，代理对客户端而言是透明的。 反向代理与正向代理的区别 | | 正向代理 | 反向代理 | |:----|:----|:----| | 代理服务器位置 | 客户端与服务都能连接的们位置 | 目标服务器内部 | | 主要作用 | 屏蔽客户端IP、集中式缓存、解决客户端不能直连服务端的问题。 | 屏蔽服务端内部实现、负载均衡、缓存。 | | 应用场景 | 爬虫、翻墙、maven 的nexus 服务 | Nginx 、Apache负载均衡应用 | Nginx代理基本配置 Nginx 代理只需要配置 location 中配置proxy_pass 属性即可。其指向代理的服务器地址。 # 正向代理到baidu 服务 location = /baidu.html { proxy_pass http://www.baidu.com; } # 反向代理至 本机的8010服务 location /luban/ { proxy_pass http://127.0.0.1:8010; } 代理相关参数： proxy_pass # 代理服务 proxy_redirect off; # 是否允许重定向 proxy_set_header Host $host; # 传 header 参数至后端服务 proxy_set_header X-Forwarded-For $remote_addr; # 设置request header 即客户端IP 地址 proxy_connect_timeout 90; # 连接代理服务超时时间 proxy_send_timeout 90; # 请求发送最大时间 proxy_read_timeout 90; # 读取最大时间 proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; 2.负载均衡配置与参数解析 通过proxy_pass 可以把请求代理至后端服务，但是为了实现更高的负载及性能， 我们的后端服务通常是多个， 这个是时候可以通过upstream 模块实现负载均衡。 演示upstream 的实现。 upstream backend { server 127.0.0.1:8010 weight=1; server 127.0.0.1:8080 weight=2; } location / { proxy_pass http://backend; } upstream 相关参数: service 反向服务地址 加端口 weight 权重 max_fails 失败多少次 认为主机已挂掉则，踢出 fail_timeout 踢出后重新探测时间 backup 备用服务 max_conns 允许最大连接数 slow_start 当节点恢复，不立即加入,而是等待 slow_start 后加入服务对列。3.upstream 负载均衡算法介绍 ll+weight： 轮询加权重 (默认) ip_hash : 基于Hash 计算 ,用于保持session 一至性 url_hash: 静态资源缓存,节约存储，加快速度（第三方） least_conn ：最少链接（第三方） least_time ：最小的响应时间,计算节点平均响应时间，然后取响应最快的那个，分配更高权重（第三方）二、Nginx 高速缓存 知识点： 缓存案例分析 Nginx 静态缓存基本配置 缓存更新 1、案例分析： 某电商平台商品详情页需要实现 700+ QPS，如何着手去做？ 首先为分析一下一个商品详情页有哪些信息 从中得出 商品详情页依懒了 对于商品详情页涉及了如下主要服务： 商品详情页HTML页面渲染 价格服务 促销服务 库存状态/配送至服务 广告词服务 预售/秒杀服务 评价服务 试用服务 推荐服务 商品介绍服务 各品类相关的一些特殊服务 解决方案： 采用Ajax 动态加载 价格、广告、库存等服务 采用key value 缓存详情页主体html。 方案架构： 问题： 当达到500QPS 的时候很难继续压测上去。 分析原因：一个详情页html 主体达平均150 kb 那么在500QPS 已接近千M局域网宽带极限。必须减少内网通信。 基于Nginx 静态缓存的解决方案： 2.Nginx 静态缓存基本配置 一、在http元素下添加缓存区声明。 #proxy_cache_path 缓存路径 #levels 缓存层级及目录位数 #keys_zone 缓存区内存大小 #inactive 有效期 #max_size 硬盘大小 proxy_cache_path /data/nginx/cache_luban levels=1:2 keys_zone=cache_luban:500m inactive=20d max_size=1g; 二、为指定location 设定缓存策略。 # 指定缓存区 proxy_cache cache_luban; #以全路径md5值做做为Key proxy_cache_key $host$uri$is_args$args; #对不同的HTTP状态码设置不同的缓存时间 proxy_cache_valid 200 304 12h; 演示缓存生效过程 [ ] 配置声明缓存路径 [ ] 为location 配置缓存策略 [ ] 重启nginx（修改了） [ ] 查看缓存目录生成 缓存参数详细说明| 父元素 | 名称 | 描述 | |:----|:----|:----| | http | proxy_cache_path | 指定缓存区的根路径 | | | levels | 缓存目录层级最高三层，每层1~2个字符表示。如1:1:2 表示三层。 | | | keys_zone | 缓存块名称 及内存块大小。如 cache_item:500m 。表示声明一个名为cache_item 大小为500m。超出大小后最早的数据将会被清除。 | | | inactive | 最长闲置时间 如:10d 如果一个数据被闲置10天将会被清除 | | | max_size | 缓存区硬盘最大值。超出闲置数据将会被清除 | | location | proxy_cache | 指定缓存区，对应keys_zone 中设置的值 | | | proxy_cache_key | 通过参数拼装缓存key 如：$host$uri$is_args$args 则会以全路径md5值做做为Key | | | proxy_cache_valid | 为不同的状态码设置缓存有效期 | 3.缓存的清除： 该功能可以采用第三方模块 ngx_cache_purge 实现。 为nginx 添加 ngx_cache_purge 模块 #下载ngx_cache_purge 模块包 ,这里nginx 版本为1.6.2 purge 对应2.0版 wget http://labs.frickle.com/files/ngx_cache_purge-2.3.tar.gz #查看已安装模块 ./sbin/nginx -V #进入nginx安装包目录 重新安装 --add-module为模块解压的全路径 ./configure --prefix=/root/svr/nginx --with-http_stub_status_module --with-http_ssl_module --add-module=/root/svr/nginx/models/ngx_cache_purge-2.0 #重新编译 make #拷贝 安装目录/objs/nginx 文件用于替换原nginx 文件 #检测查看安装是否成功 nginx -t 清除配置： location ~ /clear(/.*) { #允许访问的IP allow 127.0.0.1; allow 192.168.0.193; #禁止访问的IP deny all; #配置清除指定缓存区和路径(与proxy_cache_key一至) proxy_cache_purge cache_item $host$1$is_args$args; } 配置好以后 直接访问 ： # 访问生成缓存文件 http://www.luban.com/?a=1 # 清除生成的缓存,如果指定缓存不存在 则会报404 错误。 http://www.luban.com/clear/?a=1 三、Nginx 性能参数调优 worker_processes number; 每个worker进程都是单线程的进程，它们会调用各个模块以实现多种多样的功能。如果这些模块确认不会出现阻塞式的调用，那么，有多少CPU内核就应该配置多少个进程；反之，如果有可能出现阻塞式调用，那么需要配置稍多一些的worker进程。例如，如果业务方面会致使用户请求大量读取本地磁盘上的静态资源文件，而且服务器上的内存较小，以至于大部分的请求访问静态资源文件时都必须读取磁盘（磁头的寻址是缓慢的），而不是内存中的磁盘缓存，那么磁盘I/O调用可能会阻塞住worker进程少量时间，进而导致服务整体性能下降。 每个worker 进程的最大连接数 语法：worker_connections number; 默认：worker_connections 1024 worker_cpu_affinity cpumask[cpumask……] 绑定Nginx worker进程到指定的CPU内核 为什么要绑定worker进程到指定的CPU内核呢？假定每一个worker进程都是非常繁忙的，如果多个worker进程都在抢同一个CPU，那么这就会出现同步问题。反之，如果每一个worker进程都独享一个CPU，就在内核的调度策略上实现了完全的并发。 例如，如果有4颗CPU内核，就可以进行如下配置： worker_processes 4; worker_cpu_affinity 1000 0100 0010 0001; 注意 worker_cpu_affinity配置仅对Linux操作系统有效。 Nginx worker 进程优先级设置 语法：worker_priority nice; 默认：worker_priority 0; 优先级由静态优先级和内核根据进程执行情况所做的动态调整（目前只有±5的调整）共同决定。nice值是进程的静态优先级，它的取值范围是–20～+19，–20是最高优先级，+19是最低优先级。因此，如果用户希望Nginx占有更多的系统资源，那么可以把nice值配置得更小一些，但不建议比内核进程的nice值（通常为–5）还要小 Nginx worker进程可以打开的最大句柄描述符个数 语法： worker_rlimit_nofile limit; 默认：空 更改worker进程的最大打开文件数限制。如果没设置的话，这个值为操作系统的限制。设置后你的操作系统和Nginx可以处理比“ulimit -a”更多的文件，所以把这个值设高，这样nginx就不会有“too many open files”问题了。 是否打开accept锁 语法：accept_mutex[on|off] 默认：accept_mutext on; accept_mutex是Nginx的负载均衡锁，当某一个worker进程建立的连接数量达到worker_connections配置的最大连接数的7/8时，会大大地减小该worker进程试图建立新TCP连接的机会，accept锁默认是打开的，如果关闭它，那么建立TCP连接的耗时会更短，但worker进程之间的负载会非常不均衡，因此不建议关闭它。 使用accept锁后到真正建立连接之间的延迟时间 语法：accept_mutex_delay Nms; 默认：accept_mutex_delay 500ms; 在使用accept锁后，同一时间只有一个worker进程能够取到accept锁。这个accept锁不是堵塞锁，如果取不到会立刻返回。如果只有一个worker进程试图取锁而没有取到，他至少要等待accept_mutex_delay定义的时间才能再次试图取锁。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-07 10:28:05 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx_day00.html":{"url":"performance/nginx/Nginx_day00.html","title":"Nginx-黑马","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 09:20:55 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx_day01.html":{"url":"performance/nginx/Nginx_day01.html","title":"1.基础","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 背景介绍 名词解释 常见服务器对比 IIS Tomcat Apache Lighttpd 其他的服务器 Nginx的优点 (1)速度更快、并发更高 (2)配置简单，扩展性强 (3)高可靠性 (4)热部署 (5)成本低、BSD许可证 Nginx的功能特性及常用功能 基本HTTP服务 高级HTTP服务 邮件服务 Nginx常用的功能模块 Nginx环境准备 Nginx版本介绍 获取Nginx源码 1580610584036 准备服务器系统 Nginx安装方式介绍 GCC编译器 PCRE zlib OpenSSL 方案一：Nginx的源码简单安装 方案二：yum安装 源码简单安装和yum安装的差异： 解压Nginx目录 方案三:Nginx的源码复杂安装 Nginx目录结构分析 Nginx服务器启停命令 方式一:Nginx服务的信号控制 方式二:Nginx的命令行控制 Nginx服务器版本升级和新增模块 环境准备 方案一:使用Nginx服务信号进行升级 方案二:使用Nginx安装目录的make命令完成升级 Nginx核心配置文件结构 全局块 user指令 work process指令 其他指令 events块 events指令配置实例 http块 定义MIME-Type 自定义服务日志 其他配置指令 server块和location块 Nginx简介 背景介绍 Nginx（“engine x”）一个具有高性能的【HTTP】和【反向代理】的【WEB服务器】，同时也是一个【POP3/SMTP/IMAP代理服务器】，是由伊戈尔·赛索耶夫(俄罗斯人)使用C语言编写的，Nginx的第一个版本是2004年10月4号发布的0.1.0版本。另外值得一提的是伊戈尔·赛索耶夫将Nginx的源码进行了开源，这也为Nginx的发展提供了良好的保障。 名词解释 WEB服务器： WEB服务器也叫网页服务器，英文名叫Web Server，主要功能是为用户提供网上信息浏览服务。 HTTP: HTTP是超文本传输协议的缩写，是用于从WEB服务器传输超文本到本地浏览器的传输协议，也是互联网上应用最为广泛的一种网络协议。HTTP是一个客户端和服务器端请求和应答的标准，客户端是终端用户，服务端是网站，通过使用Web浏览器、网络爬虫或者其他工具，客户端发起一个到服务器上指定端口的HTTP请求。 POP3/SMTP/IMAP： POP3(Post Offic Protocol 3)邮局协议的第三个版本， SMTP(Simple Mail Transfer Protocol)简单邮件传输协议， IMAP(Internet Mail Access Protocol)交互式邮件存取协议， 通过上述名词的解释，我们可以了解到Nginx也可以作为电子邮件代理服务器。 反向代理 正向代理 反向代理 常见服务器对比 在介绍这一节内容之前，我们先来认识一家公司叫Netcraft。 Netcraft公司于1994年底在英国成立，多年来一直致力于互联网市场以及在线安全方面的咨询服务，其中在国际上最具影响力的当属其针对网站服务器、SSL市场所做的客观严谨的分析研究，公司官网每月公布的调研数据（Web Server Survey）已成为当今人们了解全球网站数量以及服务器市场分额情况的主要参考依据，时常被诸如华尔街杂志，英国BBC，Slashdot等媒体报道或引用。 我们先来看一组数据，我们先打开Nginx的官方网站 http://nginx.org/,找到Netcraft公司公布的数据，对当前主流服务器产品进行介绍。 上面这张图展示了2019年全球主流Web服务器的市场情况，其中有Apache、Microsoft-IIS、google Servers、Nginx、Tomcat等，而我们在了解新事物的时候，往往习惯通过类比来帮助自己理解事物的概貌。所以下面我们把几种常见的服务器来给大家简单介绍下： IIS ​ 全称(Internet Information Services)即互联网信息服务，是由微软公司提供的基于windows系统的互联网基本服务。windows作为服务器在稳定性与其他一些性能上都不如类UNIX操作系统，因此在需要高性能Web服务器的场合下，IIS可能就会被\"冷落\". Tomcat ​ Tomcat是一个运行Servlet和JSP的Web应用软件，Tomcat技术先进、性能稳定而且开放源代码，因此深受Java爱好者的喜爱并得到了部分软件开发商的认可，成为目前比较流行的Web应用服务器。但是Tomcat天生是一个重量级的Web服务器，对静态文件和高并发的处理比较弱。 Apache ​ Apache的发展时期很长，同时也有过一段辉煌的业绩。从上图可以看出大概在2014年以前都是市场份额第一的服务器。Apache有很多优点，如稳定、开源、跨平台等。但是它出现的时间太久了，在它兴起的年代，互联网的产业规模远远不如今天，所以它被设计成一个重量级的、不支持高并发的Web服务器。在Apache服务器上，如果有数以万计的并发HTTP请求同时访问，就会导致服务器上消耗大量能存，操作系统内核对成百上千的Apache进程做进程间切换也会消耗大量的CUP资源，并导致HTTP请求的平均响应速度降低，这些都决定了Apache不可能成为高性能的Web服务器。这也促使了Lighttpd和Nginx的出现。 Lighttpd ​ Lighttpd是德国的一个开源的Web服务器软件，它和Nginx一样，都是轻量级、高性能的Web服务器，欧美的业界开发者比较钟爱Lighttpd,而国内的公司更多的青睐Nginx，同时网上Nginx的资源要更丰富些。 其他的服务器 Google Servers，Weblogic, Webshpere(IBM)... 经过各个服务器的对比，种种迹象都表明，Nginx将以性能为王。这也是我们为什么选择Nginx的理由。 Nginx的优点 (1)速度更快、并发更高 单次请求或者高并发请求的环境下，Nginx都会比其他Web服务器响应的速度更快。一方面在正常情况下，单次请求会得到更快的响应，另一方面，在高峰期(如有数以万计的并发请求)，Nginx比其他Web服务器更快的响应请求。Nginx之所以有这么高的并发处理能力和这么好的性能原因在于Nginx采用了多进程和I/O多路复用(epoll)的底层实现。 (2)配置简单，扩展性强 Nginx的设计极具扩展性，它本身就是由很多模块组成，这些模块的使用可以通过配置文件的配置来添加。这些模块有官方提供的也有第三方提供的模块，如果需要完全可以开发服务自己业务特性的定制模块。 (3)高可靠性 Nginx采用的是多进程模式运行，其中有一个master主进程和N多个worker进程，worker进程的数量我们可以手动设置，每个worker进程之间都是相互独立提供服务，并且master主进程可以在某一个worker进程出错时，快速去\"拉起\"新的worker进程提供服务。 (4)热部署 现在互联网项目都要求以7*24小时进行服务的提供，针对于这一要求，Nginx也提供了热部署功能，即可以在Nginx不停止的情况下，对Nginx进行文件升级、更新配置和更换日志文件等功能。 (5)成本低、BSD许可证 BSD是一个开源的许可证，世界上的开源许可证有很多，现在比较流行的有六种分别是GPL、BSD、MIT、Mozilla、Apache、LGPL。这六种的区别是什么，我们可以通过下面一张图来解释下： Nginx本身是开源的，我们不仅可以免费的将Nginx应用在商业领域，而且还可以在项目中直接修改Nginx的源码来定制自己的特殊要求。这些点也都是Nginx为什么能吸引无数开发者继续为Nginx来贡献自己的智慧和青春。OpenRestry [Nginx+Lua] Tengine[淘宝] Nginx的功能特性及常用功能 Nginx提供的基本功能服务从大体上归纳为\"基本HTTP服务\"、“高级HTTP服务”和\"邮件服务\"等三大类。 基本HTTP服务 Nginx可以提供基本HTTP服务，可以作为HTTP代理服务器和反向代理服务器，支持通过缓存加速访问，可以完成简单的负载均衡和容错，支持包过滤功能，支持SSL等。 处理静态文件、处理索引文件以及支持自动索引； 提供反向代理服务器，并可以使用缓存加上反向代理，同时完成负载均衡和容错； 提供对FastCGI、memcached等服务的缓存机制，，同时完成负载均衡和容错； 使用Nginx的模块化特性提供过滤器功能。Nginx基本过滤器包括gzip压缩、ranges支持、chunked响应、XSLT、SSI以及图像缩放等。其中针对包含多个SSI的页面，经由FastCGI或反向代理，SSI过滤器可以并行处理。 支持HTTP下的安全套接层安全协议SSL. 支持基于加权和依赖的优先权的HTTP/2 高级HTTP服务 支持基于名字和IP的虚拟主机设置 支持HTTP/1.0中的KEEP-Alive模式和管线(PipeLined)模型连接 自定义访问日志格式、带缓存的日志写操作以及快速日志轮转。 提供3xx~5xx错误代码重定向功能 支持重写（Rewrite)模块扩展 支持重新加载配置以及在线升级时无需中断正在处理的请求 支持网络监控 支持FLV和MP4流媒体传输 邮件服务 Nginx提供邮件代理服务也是其基本开发需求之一，主要包含以下特性： 支持IMPA/POP3代理服务功能 支持内部SMTP代理服务功能 Nginx常用的功能模块 静态资源部署 Rewrite地址重写 正则表达式 反向代理 负载均衡 轮询、加权轮询、ip_hash、url_hash、fair Web缓存 环境部署 高可用的环境 用户认证模块... Nginx的核心组成 nginx二进制可执行文件 nginx.conf配置文件 error.log错误的日志记录 access.log访问日志记录 Nginx环境准备 Nginx版本介绍 Nginx的官方网站为: http://nginx.org 打开源码可以看到如下的页面内容 Nginx的官方下载网站为http://nginx.org/en/download.html，当然你也可以之间在首页选中右边的download进入版本下载网页。在下载页面我们会看到如下内容： 获取Nginx源码 http://nginx.org/download/ 打开上述网站，就可以查看到Nginx的所有版本，选中自己需要的版本进行下载。下载我们可以直接在windows上下载然后上传到服务器，也可以直接从服务器上下载，这个时候就需要准备一台服务器。 准备服务器系统 环境准备 VMware WorkStation Centos7 MobaXterm xsheel,SecureCRT 网络 (1)确认centos的内核 准备一个内核为2.6及以上版本的操作系统，因为linux2.6及以上内核才支持epoll,而Nginx需要解决高并发压力问题是需要用到epoll，所以我们需要有这样的版本要求。 我们可以使用uname -a命令来查询linux的内核版本。 (2)确保centos能联网 ping www.baidu.com (3)确认关闭防火墙 这一项的要求仅针对于那些对linux系统的防火墙设置规则不太清楚的，建议大家把防火墙都关闭掉，因为我们此次课程主要的内容是对Nginx的学习，把防火墙关闭掉，可以省掉后续Nginx学习过程中遇到的诸多问题。 关闭的方式有如下两种： systemctl stop firewalld 关闭运行的防火墙，系统重新启动后，防火墙将重新打开 systemctl disable firewalld 永久关闭防火墙，，系统重新启动后，防火墙依然关闭 systemctl status firewalld 查看防火墙状态 （4）确认停用selinux selinux(security-enhanced linux),美国安全局对于强制访问控制的实现，在linux2.6内核以后的版本中，selinux已经成功内核中的一部分。可以说selinux是linux史上最杰出的新安全子系统之一。虽然有了selinux，我们的系统会更安全，但是对于我们的学习Nginx的历程中，会多很多设置，所以这块建议大家将selinux进行关闭。 sestatus查看状态 如果查看不是disabled状态，我们可以通过修改配置文件来进行设置,修改SELINUX=disabled，然后重启下系统即可生效。 vim /etc/selinux/config Nginx安装方式介绍 Nginx的安装方式有两种分别是: 通过Nginx源码 通过Nginx源码简单安装 (1) 通过Nginx源码复杂安装 (3) 通过yum安装 (2) 如果通过Nginx源码安装需要提前准备的内容： GCC编译器 Nginx是使用C语言编写的程序，因此想要运行Nginx就需要安装一个编译工具。GCC就是一个开源的编译器集合，用于处理各种各样的语言，其中就包含了C语言。 使用命令yum install -y gcc来安装 安装成功后，可以通过gcc --version来查看gcc是否安装成功 PCRE Nginx在编译过程中需要使用到PCRE库（perl Compatible Regular Expressoin 兼容正则表达式库)，因为在Nginx的Rewrite模块和http核心模块都会使用到PCRE正则表达式语法。 可以使用命令yum install -y pcre pcre-devel来进行安装 安装成功后，可以通过rpm -qa pcre pcre-devel来查看是否安装成功 zlib zlib库提供了开发人员的压缩算法，在Nginx的各个模块中需要使用gzip压缩，所以我们也需要提前安装其库及源代码zlib和zlib-devel 可以使用命令yum install -y zlib zlib-devel来进行安装 安装成功后，可以通过rpm -qa zlib zlib-devel来查看是否安装成功 OpenSSL OpenSSL是一个开放源代码的软件库包，应用程序可以使用这个包进行安全通信，并且避免被窃听。 SSL:Secure Sockets Layer安全套接协议的缩写，可以在Internet上提供秘密性传输，其目标是保证两个应用间通信的保密性和可靠性。在Nginx中，如果服务器需要提供安全网页时就需要用到OpenSSL库，所以我们需要对OpenSSL的库文件及它的开发安装包进行一个安装。 可以使用命令yum install -y openssl openssl-devel来进行安装 安装成功后，可以通过rpm -qa openssl openssl-devel来查看是否安装成功 上述命令，一个个来的话比较麻烦，我们也可以通过一条命令来进行安装 yum install -y gcc pcre pcre-devel zlib zlib-devel openssl openssl-devel进行全部安装。 方案一：Nginx的源码简单安装 (1)进入官网查找需要下载版本的链接地址，然后使用wget命令进行下载 wget http://nginx.org/download/nginx-1.16.1.tar.gz (2)建议大家将下载的资源进行包管理 mkdir -p nginx/core mv nginx-1.16.1.tar.gz nginx/core (3)解压缩 tar -xzf nginx-1.16.1.tar.gz (4)进入资源文件中，发现configure ./configure (5)编译 make (6)安装 make install 方案二：yum安装 使用源码进行简单安装，我们会发现安装的过程比较繁琐，需要提前准备GCC编译器、PCRE兼容正则表达式库、zlib压缩库、OpenSSL安全通信的软件库包，然后才能进行Nginx的安装。 （1）安装yum-utils sudo yum install -y yum-utils （2）添加yum源文件 vim /etc/yum.repos.d/nginx.repo [nginx-stable] name=nginx stable repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true [nginx-mainline] name=nginx mainline repo baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/ gpgcheck=1 enabled=0 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true （3）查看是否安装成功 yum list | grep nginx （4）使用yum进行安装 yun install -y nginx （5）查看nginx的安装位置 whereis nginx （6）启动测试 源码简单安装和yum安装的差异： 这里先介绍一个命令: ./nginx -V,通过该命令可以查看到所安装Nginx的版本及相关配置信息。 简单安装 yum安装 解压Nginx目录 执行tar -zxvf nginx-1.16.1.tar.gz对下载的资源进行解压缩，进入压缩后的目录，可以看到如下结构 内容解释： auto:存放的是编译相关的脚本 CHANGES:版本变更记录 CHANGES.ru:俄罗斯文的版本变更记录 conf:nginx默认的配置文件 configure:nginx软件的自动脚本程序,是一个比较重要的文件，作用如下： ​ （1）检测环境及根据环境检测结果生成C代码 ​ （2）生成编译代码需要的Makefile文件 contrib:存放的是几个特殊的脚本文件，其中README中对脚本有着详细的说明 html:存放的是Nginx自带的两个html页面，访问Nginx的首页和错误页面 LICENSE:许可证的相关描述文件 man:nginx的man手册 README:Nginx的阅读指南 src:Nginx的源代码 方案三:Nginx的源码复杂安装 这种方式和简单的安装配置不同的地方在第一步，通过./configure来对编译参数进行设置，需要我们手动来指定。那么都有哪些参数可以进行设置，接下来我们进行一个详细的说明。 PATH:是和路径相关的配置信息 with:是启动模块，默认是关闭的 without:是关闭模块，默认是开启的 我们先来认识一些简单的路径配置已经通过这些配置来完成一个简单的编译： --prefix=PATH 指向Nginx的安装目录，默认值为/usr/local/nginx --sbin-path=PATH 指向(执行)程序文件(nginx)的路径,默认值为/sbin/nginx --modules-path=PATH 指向Nginx动态模块安装目录，默认值为/modules --conf-path=PATH 指向配置文件(nginx.conf)的路径,默认值为/conf/nginx.conf --error-log-path=PATH 指向错误日志文件的路径,默认值为/logs/error.log --http-log-path=PATH 指向访问日志文件的路径,默认值为/logs/access.log --pid-path=PATH 指向Nginx启动后进行ID的文件路径，默认值为/logs/nginx.pid --lock-path=PATH 指向Nginx锁文件的存放路径,默认值为/logs/nginx.lock 要想使用可以通过如下命令 ./configure --prefix=/usr/local/nginx \\ --sbin-path=/usr/local/nginx/sbin/nginx \\ --modules-path=/usr/local/nginx/modules \\ --conf-path=/usr/local/nginx/conf/nginx.conf \\ --error-log-path=/usr/local/nginx/logs/error.log \\ --http-log-path=/usr/local/nginx/logs/access.log \\ --pid-path=/usr/local/nginx/logs/nginx.pid \\ --lock-path=/usr/local/nginx/logs/nginx.lock 在使用上述命令之前，需要将之前服务器已经安装的nginx进行卸载，卸载的步骤分为三步骤： 步骤一：需要将nginx的进程关闭 ./nginx -s stop 步骤二:将安装的nginx进行删除 rm -rf /usr/local/nginx 步骤三:将安装包之前编译的环境清除掉 make clean Nginx目录结构分析 在使用Nginx之前，我们先对安装好的Nginx目录文件进行一个分析，在这块给大家介绍一个工具tree，通过tree我们可以很方面的去查看centos系统上的文件目录结构，当然，如果想使用tree工具，就得先通过yum install -y tree来进行安装，安装成功后，可以通过执行tree /usr/local/nginx(tree后面跟的是Nginx的安装目录)，获取的结果如下： conf:nginx所有配置文件目录 ​ CGI(Common Gateway Interface)通用网关【接口】，主要解决的问题是从客户端发送一个请求和数据，服务端获取到请求和数据后可以调用调用CGI【程序】处理及相应结果给客户端的一种标准规范。 ​ fastcgi.conf:fastcgi相关配置文件 ​ fastcgi.conf.default:fastcgi.conf的备份文件 ​ fastcgi_params:fastcgi的参数文件 ​ fastcgi_params.default:fastcgi的参数备份文件 ​ scgi_params:scgi的参数文件 ​ scgi_params.default：scgi的参数备份文件 ​ uwsgi_params:uwsgi的参数文件 ​ uwsgi_params.default:uwsgi的参数备份文件 ​ mime.types:记录的是HTTP协议中的Content-Type的值和文件后缀名的对应关系 ​ mime.types.default:mime.types的备份文件 ​ nginx.conf:这个是Nginx的核心配置文件，这个文件非常重要，也是我们即将要学习的重点 ​ nginx.conf.default:nginx.conf的备份文件 ​ koi-utf、koi-win、win-utf这三个文件都是与编码转换映射相关的配置文件，用来将一种编码转换成另一种编码 html:存放nginx自带的两个静态的html页面 ​ 50x.html:访问失败后的失败页面 ​ index.html:成功访问的默认首页 logs:记录入门的文件，当nginx服务器启动后，这里面会有 access.log error.log 和nginx.pid三个文件出现。 sbin:是存放执行程序文件nginx ​ nginx是用来控制Nginx的启动和停止等相关的命令。 Nginx服务器启停命令 Nginx安装完成后，接下来我们要学习的是如何启动、重启和停止Nginx的服务。 对于Nginx的启停在linux系统中也有很多种方式，我们本次课程介绍两种方式： Nginx服务的信号控制 Nginx的命令行控制 方式一:Nginx服务的信号控制 Nginx中的master和worker进程? Nginx的工作方式? 如何获取进程的PID? 信号有哪些? 如何通过信号控制Nginx的启停等相关操作? 前面在提到Nginx的高性能，其实也和它的架构模式有关。Nginx默认采用的是多进程的方式来工作的，当将Nginx启动后，我们通过ps -ef | grep nginx命令可以查看到如下内容： 从上图中可以看到,Nginx后台进程中包含一个master进程和多个worker进程，master进程主要用来管理worker进程，包含接收外界的信息，并将接收到的信号发送给各个worker进程，监控worker进程的状态，当worker进程出现异常退出后，会自动重新启动新的worker进程。而worker进程则是专门用来处理用户请求的，各个worker进程之间是平等的并且相互独立，处理请求的机会也是一样的。nginx的进程模型，我们可以通过下图来说明下： 我们现在作为管理员，只需要通过给master进程发送信号就可以来控制Nginx,这个时候我们需要有两个前提条件，一个是要操作的master进程，一个是信号。 （1）要想操作Nginx的master进程，就需要获取到master进程的进程号ID。获取方式简单介绍两个， 方式一：通过ps -ef | grep nginx； 方式二：在讲解nginx的./configure的配置参数的时候，有一个参数是--pid-path=PATH默认是/usr/local/nginx/logs/nginx.pid,所以可以通过查看该文件来获取nginx的master进程ID. （2）信号 信号 作用 TERM/INT 立即关闭整个服务 QUIT \"优雅\"地关闭整个服务 HUP 重读配置文件并使用服务对新配置项生效 USR1 重新打开日志文件，可以用来进行日志切割 USR2 平滑升级到最新版的nginx WINCH 所有子进程不在接收处理新连接，相当于给work进程发送QUIT指令 调用命令为kill -signal PID signal:即为信号；PID即为获取到的master线程ID 发送TERM/INT信号给master进程，会将Nginx服务立即关闭。 kill -TERM PID / kill -TERM `cat /usr/local/nginx/logs/nginx.pid` kill -INT PID / kill -INT `cat /usr/local/nginx/logs/nginx.pid` 发送QUIT信号给master进程，master进程会控制所有的work进程不再接收新的请求，等所有请求处理完后，在把进程都关闭掉。 kill -QUIT PID / kill -TERM `cat /usr/local/nginx/logs/nginx.pid` 发送HUP信号给master进程，master进程会把控制旧的work进程不再接收新的请求，等处理完请求后将旧的work进程关闭掉，然后根据nginx的配置文件重新启动新的work进程 kill -HUP PID / kill -TERM `cat /usr/local/nginx/logs/nginx.pid` 发送USR1信号给master进程，告诉Nginx重新开启日志文件 kill -USR1 PID / kill -TERM `cat /usr/local/nginx/logs/nginx.pid` 发送USR2信号给master进程，告诉master进程要平滑升级，这个时候，会重新开启对应的master进程和work进程，整个系统中将会有两个master进程，并且新的master进程的PID会被记录在/usr/local/nginx/logs/nginx.pid而之前的旧的master进程PID会被记录在/usr/local/nginx/logs/nginx.pid.oldbin文件中，接着再次发送QUIT信号给旧的master进程，让其处理完请求后再进行关闭 kill -USR2 PID / kill -USR2 `cat /usr/local/nginx/logs/nginx.pid` kill -QUIT PID / kill -QUIT `cat /usr/local/nginx/logs/nginx.pid.oldbin` 发送WINCH信号给master进程,让master进程控制不让所有的work进程在接收新的请求了，请求处理完后关闭work进程。注意master进程不会被关闭掉 kill -WINCH PID /kill -WINCH`cat /usr/local/nginx/logs/nginx.pid` 方式二:Nginx的命令行控制 此方式是通过Nginx安装目录下的sbin下的可执行文件nginx来进行Nginx状态的控制，我们可以通过nginx -h来查看都有哪些参数可以用： -?和-h:显示帮助信息 -v:打印版本号信息并退出 -V:打印版本号信息和配置信息并退出 -t:测试nginx的配置文件语法是否正确并退出 -T:测试nginx的配置文件语法是否正确并列出用到的配置文件信息然后退出 -q:在配置测试期间禁止显示非错误消息 -s:signal信号，后面可以跟 ： ​ stop[快速关闭，类似于TERM/INT信号的作用] ​ quit[优雅的关闭，类似于QUIT信号的作用] ​ reopen[重新打开日志文件类似于USR1信号的作用] ​ reload[类似于HUP信号的作用] -p:prefix，指定Nginx的prefix路径，(默认为: /usr/local/nginx/) -c:filename,指定Nginx的配置文件路径,(默认为: conf/nginx.conf) -g:用来补充Nginx配置文件，向Nginx服务指定启动时应用全局的配置 Nginx服务器版本升级和新增模块 如果想对Nginx的版本进行更新，或者要应用一些新的模块，最简单的做法就是停止当前的Nginx服务，然后开启新的Nginx服务。但是这样会导致在一段时间内，用户是无法访问服务器。为了解决这个问题，我们就需要用到Nginx服务器提供的平滑升级功能。这个也是Nginx的一大特点，使用这种方式，就可以使Nginx在7*24小时不间断的提供服务了。接下来我们分析下需求： 需求：Nginx的版本最开始使用的是Nginx-1.14.2,由于服务升级，需要将Nginx的版本升级到Nginx-1.16.1,要求Nginx不能中断提供服务。 为了应对上述的需求，这里我们给大家提供两种解决方案: 方案一:使用Nginx服务信号完成Nginx的升级 方案二:使用Nginx安装目录的make命令完成升级 环境准备 （1）先准备两个版本的Nginx分别是 1.14.2和1.16.1 （2）使用Nginx源码安装的方式将1.14.2版本安装成功并正确访问 进入安装目录 ./configure make && make install （3）将Nginx1.16.1进行参数配置和编译，不需要进行安装。 进入安装目录 ./configure make 方案一:使用Nginx服务信号进行升级 第一步:将1.14.2版本的sbin目录下的nginx进行备份 cd /usr/local/nginx/sbin mv nginx nginxold 第二步:将Nginx1.16.1安装目录编译后的objs目录下的nginx文件，拷贝到原来/usr/local/nginx/sbin目录下 cd ~/nginx/core/nginx-1.16.1/objs cp nginx /usr/local/nginx/sbin 第三步:发送信号USR2给Nginx的1.14.2版本对应的master进程 第四步:发送信号QUIT给Nginx的1.14.2版本对应的master进程 kill -QUIT `more /usr/local/logs/nginx.pid.oldbin` 方案二:使用Nginx安装目录的make命令完成升级 第一步:将1.14.2版本的sbin目录下的nginx进行备份 cd /usr/local/nginx/sbin mv nginx nginxold 第二步:将Nginx1.16.1安装目录编译后的objs目录下的nginx文件，拷贝到原来/usr/local/nginx/sbin目录下 cd ~/nginx/core/nginx-1.16.1/objs cp nginx /usr/local/nginx/sbin 第三步:进入到安装目录，执行make upgrade 第四步:查看是否更新成功 ./nginx -v 在整个过程中，其实Nginx是一直对外提供服务的。并且当Nginx的服务器启动成功后，我们是可以通过浏览器进行直接访问的，同时我们可以通过更改html目录下的页面来修改我们在页面上所看到的内容，那么问题来了，为什么我们要修改html目录下的文件，能不能多添加一些页面是Nginx的功能更加丰富，还有前面聊到Nginx的前端功能又是如何来实现的，这就需要我们对Nginx的核心配置文件进行一个详细的学习。 Nginx核心配置文件结构 从前面的内容学习中，我们知道Nginx的核心配置文件默认是放在/usr/local/nginx/conf/nginx.conf，这一节，我们就来学习下nginx.conf的内容和基本配置方法。 读取Nginx自带的Nginx配置文件，我们将其中的注释部分【学习一个技术点就是在Nginx的配置文件中可以使用#来注释】删除掉后，就剩下下面内容: worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 指令名 指令值; #全局块，主要设置Nginx服务器整体运行的配置指令 #events块,主要设置,Nginx服务器与用户的网络连接,这一部分对Nginx服务器的性能影响较大 events { 指令名 指令值; } #http块，是Nginx服务器配置中的重要部分，代理、缓存、日志记录、第三方模块配置... http { 指令名 指令值; server { #server块，是Nginx配置和虚拟主机相关的内容 指令名 指令值; location / { #location块，基于Nginx服务器接收请求字符串与location后面的值进行匹配，对特定请求进行处理 指令名 指令值; } } ... } 简单小结下: nginx.conf配置文件中默认有三大块：全局块、events块、http块 http块中可以配置多个server块，每个server块又可以配置多个location块。 全局块 user指令 （1）user:用于配置运行Nginx服务器的worker进程的用户和用户组。 语法 user user [group] 默认值 nobody 位置 全局块 该属性也可以在编译的时候指定，语法如下./configure --user=user --group=group,如果两个地方都进行了设置，最终生效的是配置文件中的配置。 该指令的使用步骤: (1)设置一个用户信息\"www\" user www; (2) 创建一个用户 useradd www (3)修改user属性 user www (4)创建/root/html/index.html页面，添加如下内容 Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. I am WWW (5)修改nginx.conf location / { root /root/html; index index.html index.htm; } (5)测试启动访问 页面会报403拒绝访问的错误 (6)分析原因 因为当前用户没有访问/root/html目录的权限 (7)将文件创建到 /home/www/html/index.html,修改配置 location / { root /home/www/html; index index.html index.htm; } (8)再次测试启动访问 能正常访问。 综上所述，使用user指令可以指定启动运行工作进程的用户及用户组，这样对于系统的权限访问控制的更加精细，也更加安全。 work process指令 master_process:用来指定是否开启工作进程。 语法 master_process on\\ off; 默认值 master_process on; 位置 全局块 worker_processes:用于配置Nginx生成工作进程的数量，这个是Nginx服务器实现并发处理服务的关键所在。理论上来说workder process的值越大，可以支持的并发处理量也越多，但事实上这个值的设定是需要受到来自服务器自身的限制，建议将该值和服务器CPU的内核数保存一致。 语法 worker_processes num/auto; 默认值 1 位置 全局块 如果将worker_processes设置成2，则会看到如下内容: 其他指令 daemon：设定Nginx是否以守护进程的方式启动。 守护式进程是linux后台执行的一种服务进程，特点是独立于控制终端，不会随着终端关闭而停止。 语法 daemon on\\ off; 默认值 daemon on; 位置 全局块 pid:用来配置Nginx当前master进程的进程号ID存储的文件路径。 语法 pid file; 默认值 默认为:/usr/local/nginx/logs/nginx.pid 位置 全局块 该属性可以通过./configure --pid-path=PATH来指定 error_log:用来配置Nginx的错误日志存放路径 语法 error_log file [日志级别]; 默认值 error_log logs/error.log error; 位置 全局块、http、server、location 该属性可以通过./configure --error-log-path=PATH来指定 其中日志级别的值有：debug|info|notice|warn|error|crit|alert|emerg，翻译过来为试|信息|通知|警告|错误|临界|警报|紧急，这块建议大家设置的时候不要设置成info以下的等级，因为会带来大量的磁盘I/O消耗，影响Nginx的性能。 （5）include:用来引入其他配置文件，使Nginx的配置更加灵活 语法 include file; 默认值 无 位置 any events块 （1）accept_mutex:用来设置Nginx网络连接序列化 语法 accept_mutex on\\ off; 默认值 accept_mutex on; 位置 events 这个配置主要可以用来解决常说的\"惊群\"问题。大致意思是在某一个时刻，客户端发来一个请求连接，Nginx后台是以多进程的工作模式，也就是说有多个worker进程会被同时唤醒，但是最终只会有一个进程可以获取到连接，如果每次唤醒的进程数目太多，就会影响Nginx的整体性能。如果将上述值设置为on(开启状态)，将会对多个Nginx进程接收连接进行序列号，一个个来唤醒接收，就防止了多个进程对连接的争抢。 （2）multi_accept:用来设置是否允许同时接收多个网络连接 语法 multi_accept on\\ off; 默认值 multi_accept off; 位置 events 如果multi_accept被禁止了，nginx一个工作进程只能同时接受一个新的连接。否则，一个工作进程可以同时接受所有的新连接 （3）worker_connections：用来配置单个worker进程最大的连接数 语法 worker_connections number; 默认值 worker_commections 512; 位置 events 这里的连接数不仅仅包括和前端用户建立的连接数，而是包括所有可能的连接数。另外，number值不能大于操作系统支持打开的最大文件句柄数量。 （4）use:用来设置Nginx服务器选择哪种事件驱动来处理网络消息。 语法 use method; 默认值 根据操作系统定 位置 events 注意：此处所选择事件处理模型是Nginx优化部分的一个重要内容，method的可选值有select/poll/epoll/kqueue等，之前在准备centos环境的时候，我们强调过要使用linux内核在2.6以上，就是为了能使用epoll函数来优化Nginx。 另外这些值的选择，我们也可以在编译的时候使用 --with-select_module、--without-select_module、 --with-poll_module、--without-poll_module来设置是否需要将对应的事件驱动模块编译到Nginx的内核。 events指令配置实例 打开Nginx的配置文件 nginx.conf,添加如下配置 events{ accept_mutex on; multi_accept on; worker_commections 1024; use epoll; } 启动测试 ./nginx -t ./nginx -s reload http块 定义MIME-Type 我们都知道浏览器中可以显示的内容有HTML、XML、GIF等种类繁多的文件、媒体等资源，浏览器为了区分这些资源，就需要使用MIME Type。所以说MIME Type是网络资源的媒体类型。Nginx作为web服务器，也需要能够识别前端请求的资源类型。 在Nginx的配置文件中，默认有两行配置 include mime.types; default_type application/octet-stream; （1）default_type:用来配置Nginx响应前端请求默认的MIME类型。 语法 default_type mime-type; 默认值 default_type text/plain； 位置 http、server、location 在default_type之前还有一句include mime.types,include之前我们已经介绍过，相当于把mime.types文件中MIMT类型与相关类型文件的文件后缀名的对应关系加入到当前的配置文件中。 举例来说明： 有些时候请求某些接口的时候需要返回指定的文本字符串或者json字符串，如果逻辑非常简单或者干脆是固定的字符串，那么可以使用nginx快速实现，这样就不用编写程序响应请求了，可以减少服务器资源占用并且响应性能非常快。 如何实现: location /get_text { #这里也可以设置成text/plain default_type text/html; return 200 \"This is nginx's text\"; } location /get_json{ default_type application/json; return 200 '{\"name\":\"TOM\",\"age\":18}'; } 自定义服务日志 Nginx中日志的类型分access.log、error.log。 access.log:用来记录用户所有的访问请求。 error.log:记录nginx本身运行时的错误信息，不会记录用户的访问请求。 Nginx服务器支持对服务日志的格式、大小、输出等进行设置，需要使用到两个指令，分别是access_log和log_format指令。 （1）access_log:用来设置用户访问日志的相关属性。 语法 access_log path[format[buffer=size]] 默认值 access_log logs/access.log combined; 位置 http, server, location （2）log_format:用来指定日志的输出格式。 语法 log_format name [escape=default\\ json\\ none] string....; 默认值 log_format combined \"...\"; 位置 http 其他配置指令 （1）sendfile:用来设置Nginx服务器是否使用sendfile()传输文件，该属性可以大大提高Nginx处理静态资源的性能 语法 sendfile on\\ off； 默认值 sendfile off; 位置 http、server、location （2）keepalive_timeout:用来设置长连接的超时时间。 》为什么要使用keepalive？ 我们都知道HTTP是一种无状态协议，客户端向服务端发送一个TCP请求，服务端响应完毕后断开连接。 如何客户端向服务端发送多个请求，每个请求都需要重新创建一次连接，效率相对来说比较多，使用keepalive模式，可以告诉服务器端在处理完一个请求后保持这个TCP连接的打开状态，若接收到来自这个客户端的其他请求，服务端就会利用这个未被关闭的连接，而不需要重新创建一个新连接，提升效率，但是这个连接也不能一直保持，这样的话，连接如果过多，也会是服务端的性能下降，这个时候就需要我们进行设置其的超时时间。 语法 keepalive_timeout time; 默认值 keepalive_timeout 75s; 位置 http、server、location （3）keepalive_requests:用来设置一个keep-alive连接使用的次数。 语法 keepalive_requests number; 默认值 keepalive_requests 100; 位置 http、server、location server块和location块 server块和location块都是我们要重点讲解和学习的内容，因为我们后面会对Nginx的功能进行详细讲解，所以这块内容就放到静态资源部署的地方给大家详细说明。 本节我们主要来认识下Nginx默认给的nginx.conf中的相关内容，以及server块与location块在使用的时候需要注意的一些内容。 server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 404 /50x.html; location = /50x.html { root html; } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 10:21:15 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx_day02.html":{"url":"performance/nginx/Nginx_day02.html","title":"2.进阶篇","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 Nginx服务器基础配置实例 Nginx服务操作的问题 Nginx配置成系统服务 Nginx命令配置到系统环境 Nginx静态资源部署 Nginx静态资源概述 Nginx静态资源的配置指令 listen指令 server_name指令 匹配执行顺序 location指令 设置请求资源的目录root / alias index指令 error_page指令 静态资源优化配置语法 Nginx静态资源压缩实战 Gzip模块配置指令 Gzip压缩功能的实例配置 Gzip和sendfile共存问题 gzip_static指令 添加模块到Nginx的实现步骤 gzip_static测试使用 静态资源的缓存处理 什么是缓存 什么是web缓存 web缓存的种类 浏览器缓存 为什么要用浏览器缓存 浏览器缓存的执行流程 浏览器缓存相关指令 expires指令 add_header指令 Nginx的跨域问题解决 同源策略 跨域问题 跨域问题的案例演示 解决方案 静态资源防盗链 什么是资源盗链 Nginx防盗链的实现原理： 针对目录进行防盗链 Rewrite功能配置 \"地址重写\"与\"地址转发\" Rewrite规则 set指令 Rewrite常用全局变量 if指令 break指令 return指令 rewrite指令 rewrite_log指令 Rewrite的案例 域名跳转 域名镜像 独立域名 目录自动添加\"/\" 合并目录 防盗链 Nginx进阶篇 Nginx服务器基础配置实例 前面我们已经对Nginx服务器默认配置文件的结构和涉及的基本指令做了详细的阐述。通过这些指令的合理配置，我们就可以让一台Nginx服务器正常工作，并且提供基本的web服务器功能。 接下来我们将通过一个比较完整和最简单的基础配置实例，来巩固下前面所学习的指令及其配置。 需求如下: （1）有如下访问： http://192.168.200.133:8081/server1/location1 访问的是：index_sr1_location1.html http://192.168.200.133:8081/server1/location2 访问的是：index_sr1_location2.html http://192.168.200.133:8082/server2/location1 访问的是：index_sr2_location1.html http://192.168.200.133:8082/server2/location2 访问的是：index_sr2_location2.html （2）如果访问的资源不存在， 返回自定义的404页面 （3）将/server1和/server2的配置使用不同的配置文件分割 将文件放到/home/www/conf.d目录下，然后使用include进行合并 （4）为/server1和/server2各自创建一个访问日志文件 准备相关文件，目录如下： 配置的内容如下: ##全局块 begin## #配置允许运行Nginx工作进程的用户和用户组 user www; #配置运行Nginx进程生成的worker进程数 worker_processes 2; #配置Nginx服务器运行对错误日志存放的路径 error_log logs/error.log; #配置Nginx服务器允许时记录Nginx的master进程的PID文件路径和名称 pid logs/nginx.pid; #配置Nginx服务是否以守护进程方法启动 #daemon on; ##全局块 end## ##events块 begin## events{ #设置Nginx网络连接序列化 accept_mutex on; #设置Nginx的worker进程是否可以同时接收多个请求 multi_accept on; #设置Nginx的worker进程最大的连接数 worker_connections 1024; #设置Nginx使用的事件驱动模型 use epoll; } ##events块 end## ##http块 start## http{ #定义MIME-Type include mime.types; default_type application/octet-stream; #配置允许使用sendfile方式运输 sendfile on; #配置连接超时时间 keepalive_timeout 65; #配置请求处理日志格式 log_format server1 '===>server1 access log'; log_format server2 '===>server2 access log'; ##server块 开始## include /home/www/conf.d/*.conf; ##server块 结束## } ##http块 end## server1.conf server{ #配置监听端口和主机名称 listen 8081; server_name localhost; #配置请求处理日志存放路径 access_log /home/www/myweb/server1/logs/access.log server1; #配置错误页面 error_page 404 /404.html; #配置处理/server1/location1请求的location location /server1/location1{ root /home/www/myweb; index index_sr1_location1.html; } #配置处理/server1/location2请求的location location /server1/location2{ root /home/www/myweb; index index_sr1_location2.html; } #配置错误页面转向 location = /404.html { root /home/www/myweb; index 404.html; } } server2.conf server{ #配置监听端口和主机名称 listen 8082; server_name localhost; #配置请求处理日志存放路径 access_log /home/www/myweb/server2/logs/access.log server2; #配置错误页面,对404.html做了定向配置 error_page 404 /404.html; #配置处理/server1/location1请求的location location /server2/location1{ root /home/www/myweb; index index_sr2_location1.html; } #配置处理/server2/location2请求的location location /server2/location2{ root /home/www/myweb; index index_sr2_location2.html; } #配置错误页面转向 location = /404.html { root /home/www/myweb; index 404.html; } } 访问测试： Nginx服务操作的问题 经过前面的操作，我们会发现，如果想要启动、关闭或重新加载nginx配置文件，都需要先进入到nginx的安装目录的sbin目录，然后使用nginx的二级制可执行文件来操作，相对来说操作比较繁琐，这块该如何优化？另外如果我们想把Nginx设置成随着服务器启动就自动完成启动操作，又该如何来实现?这就需要用到接下来我们要讲解的两个知识点： Nginx配置成系统服务 Nginx命令配置到系统环境 Nginx配置成系统服务 把Nginx应用服务设置成为系统服务，方便对Nginx服务的启动和停止等相关操作，具体实现步骤: (1) 在/usr/lib/systemd/system目录下添加nginx.service,内容如下: vim /usr/lib/systemd/system/nginx.service [Unit] Description=nginx web service Documentation=http://nginx.org/en/docs/ After=network.target [Service] Type=forking PIDFile=/usr/local/nginx/logs/nginx.pid ExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.conf ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s stop PrivateTmp=true [Install] WantedBy=default.target (2)添加完成后如果权限有问题需要进行权限设置 chmod 755 /usr/lib/systemd/system/nginx.service (3)使用系统命令来操作Nginx服务 启动: systemctl start nginx 停止: systemctl stop nginx 重启: systemctl restart nginx 重新加载配置文件: systemctl reload nginx 查看nginx状态: systemctl status nginx 开机启动: systemctl enable nginx Nginx命令配置到系统环境 前面我们介绍过Nginx安装目录下的二级制可执行文件nginx的很多命令，要想使用这些命令前提是需要进入sbin目录下才能使用，很不方便，如何去优化，我们可以将该二进制可执行文件加入到系统的环境变量，这样的话在任何目录都可以使用nginx对应的相关命令。具体实现步骤如下: 演示可删除 /usr/local/nginx/sbin/nginx -V cd /usr/local/nginx/sbin nginx -V 如何优化？？？ (1)修改/etc/profile文件 vim /etc/profile 在最后一行添加 export PATH=$PATH:/usr/local/nginx/sbin (2)使之立即生效 source /etc/profile (3)执行nginx命令 nginx -V Nginx静态资源部署 Nginx静态资源概述 上网去搜索访问资源对于我们来说并不陌生，通过浏览器发送一个HTTP请求实现从客户端发送请求到服务器端获取所需要内容后并把内容回显展示在页面的一个过程。这个时候，我们所请 求的内容就分为两种类型，一类是静态资源、一类是动态资源。 静态资源即指在服务器端真实存在并且能直接拿来展示的一些文件，比如常见的html页面、css文件、js文件、图 片、视频等资源； 动态资源即指在服务器端真实存在但是要想获取需要经过一定的业务逻辑处理，根据不同的条件展示在页面不同这 一部分内容，比如说报表数据展示、根据当前登录用户展示相关具体数据等资源； Nginx处理静态资源的内容，我们需要考虑下面这几个问题： （1）静态资源的配置指令 （2）静态资源的配置优化 （3）静态资源的压缩配置指令 （4）静态资源的缓存处理 （5）静态资源的访问控制，包括跨域问题和防盗链问题 Nginx静态资源的配置指令 listen指令 listen:用来配置监听端口。 语法 listen address[:port] [default_server]...;listen port [default_server]...; 默认值 listen *:80 \\ *:8000 位置 server listen的设置比较灵活，我们通过几个例子来把常用的设置方式熟悉下： listen 127.0.0.1:8000; // listen localhost:8000 监听指定的IP和端口 listen 127.0.0.1; 监听指定IP的所有端口 listen 8000; 监听指定端口上的连接 listen *:8000; 监听指定端口上的连接 default_server属性是标识符，用来将此虚拟主机设置成默认主机。所谓的默认主机指的是如果没有匹配到对应的address:port，则会默认执行的。如果不指定默认使用的是第一个server。 server{ listen 8080; server_name 127.0.0.1; location /{ root html; index index.html; } } server{ listen 8080 default_server; server_name localhost; default_type text/plain; return 444 'This is a error request'; } server_name指令 server_name：用来设置虚拟主机服务名称。 127.0.0.1 、 localhost 、域名[www.baidu.com | www.jd.com] 语法 server_name name ...;name可以提供多个中间用空格分隔 默认值 server_name \"\"; 位置 server 关于server_name的配置方式有三种，分别是： 精确匹配 通配符匹配 正则表达式匹配 配置方式一：精确匹配 如： server { listen 80; server_name www.itcast.cn www.itheima.cn; ... } 补充小知识点: hosts是一个没有扩展名的系统文件，可以用记事本等工具打开，其作用就是将一些常用的网址域名与其对应的IP地址建立一个关联“数据库”，当用户在浏览器中输入一个需要登录的网址时，系统会首先自动从hosts文件中寻找对应的IP地址，一旦找到，系统会立即打开对应网页，如果没有找到，则系统会再将网址提交DNS域名解析服务器进行IP地址的解析。 windows:C:\\Windows\\System32\\drivers\\etc centos：/etc/hosts 因为域名是要收取一定的费用，所以我们可以使用修改hosts文件来制作一些虚拟域名来使用。需要修改 /etc/hosts文件来添加 vim /etc/hosts 127.0.0.1 www.itcast.cn 127.0.0.1 www.itheima.cn 配置方式二:使用通配符配置 server_name中支持通配符\"*\",但需要注意的是通配符不能出现在域名的中间，只能出现在首段或尾段，如： server { listen 80; server_name *.itcast.cn www.itheima.*; # www.itcast.cn abc.itcast.cn www.itheima.cn www.itheima.com ... } 下面的配置就会报错 server { listen 80; server_name www.*.cn www.itheima.c* ... } 配置三:使用正则表达式配置 server_name中可以使用正则表达式，并且使用~作为正则表达式字符串的开始标记。 常见的正则表达式 代码 说明 ^ 匹配搜索字符串开始位置 $ 匹配搜索字符串结束位置 . 匹配除换行符\\n之外的任何单个字符 \\ 转义字符，将下一个字符标记为特殊字符 [xyz] 字符集，与任意一个指定字符匹配 [a-z] 字符范围，匹配指定范围内的任何字符 \\w 与以下任意字符匹配 A-Z a-z 0-9 和下划线,等效于[A-Za-z0-9_] \\d 数字字符匹配，等效于[0-9] {n} 正好匹配n次 {n,} 至少匹配n次 {n,m} 匹配至少n次至多m次 * 零次或多次，等效于{0,} + 一次或多次，等效于{1,} ? 零次或一次，等效于{0,1} 配置如下： server{ listen 80; server_name ~^www\\.(\\w+)\\.com$; default_type text/plain; return 200 $1 $2 ..; } 注意 ~后面不能加空格，括号可以取值 匹配执行顺序 由于server_name指令支持通配符和正则表达式，因此在包含多个虚拟主机的配置文件中，可能会出现一个名称被多个虚拟主机的server_name匹配成功，当遇到这种情况，当前的请求交给谁来处理呢？ server{ listen 80; server_name ~^www\\.\\w+\\.com$; default_type text/plain; return 200 'regex_success'; } server{ listen 80; server_name www.itheima.*; default_type text/plain; return 200 'wildcard_after_success'; } server{ listen 80; server_name *.itheima.com; default_type text/plain; return 200 'wildcard_before_success'; } server{ listen 80; server_name www.itheima.com; default_type text/plain; return 200 'exact_success'; } server{ listen 80 default_server; server_name _; default_type text/plain; return 444 'default_server not found server'; } 结论： exact_success wildcard_before_success wildcard_after_success regex_success default_server not found server!! No1:准确匹配server_name No2:通配符在开始时匹配server_name成功 No3:通配符在结束时匹配server_name成功 No4:正则表达式匹配server_name成功 No5:被默认的default_server处理，如果没有指定默认找第一个server location指令 server{ listen 80; server_name localhost; location / { } location /abc{ } ... } location:用来设置请求的URI 语法 location [ = \\ ~ \\ ~* \\ ^~ \\ @ ] uri{...} 默认值 — 位置 server,location uri变量是待匹配的请求字符串，可以不包含正则表达式，也可以包含正则表达式，那么nginx服务器在搜索匹配location的时候，是先使用不包含正则表达式进行匹配，找到一个匹配度最高的一个，然后在通过包含正则表达式的进行匹配，如果能匹配到直接访问，匹配不到，就使用刚才匹配度最高的那个location来处理请求。 属性介绍: 不带符号，要求必须以指定模式开始 server { listen 80; server_name 127.0.0.1; location /abc{ default_type text/plain; return 200 \"access success\"; } } 以下访问都是正确的 http://192.168.200.133/abc http://192.168.200.133/abc?p1=TOM http://192.168.200.133/abc/ http://192.168.200.133/abcdef = : 用于不包含正则表达式的uri前，必须与指定的模式精确匹配 server { listen 80; server_name 127.0.0.1; location =/abc{ default_type text/plain; return 200 \"access success\"; } } 可以匹配到 http://192.168.200.133/abc http://192.168.200.133/abc?p1=TOM 匹配不到 http://192.168.200.133/abc/ http://192.168.200.133/abcdef ~ ： 用于表示当前uri中包含了正则表达式，并且区分大小写 ~*: 用于表示当前uri中包含了正则表达式，并且不区分大小写 换句话说，如果uri包含了正则表达式，需要用上述两个符合来标识 server { listen 80; server_name 127.0.0.1; location ~^/abc\\w${ default_type text/plain; return 200 \"access success\"; } } server { listen 80; server_name 127.0.0.1; location ~*^/abc\\w${ default_type text/plain; return 200 \"access success\"; } } ^~: 用于不包含正则表达式的uri前，功能和不加符号的一致，唯一不同的是，如果模式匹配，那么就停止搜索其他模式了。 server { listen 80; server_name 127.0.0.1; location ^~/abc{ default_type text/plain; return 200 \"access success\"; } } 设置请求资源的目录root / alias root：设置请求的根目录 语法 root path; 默认值 root html; 位置 http、server、location path为Nginx服务器接收到请求以后查找资源的根目录路径。 alias：用来更改location的URI 语法 alias path; 默认值 — 位置 location path为修改后的根路径。 以上两个指令都可以来指定访问资源的路径，那么这两者之间的区别是什么? 举例说明： （1）在/usr/local/nginx/html目录下创建一个 images目录,并在目录下放入一张图片mv.png图片 location /images { root /usr/local/nginx/html; } 访问图片的路径为: http://192.168.200.133/images/mv.png （2）如果把root改为alias location /images { alias /usr/local/nginx/html; } 再次访问上述地址，页面会出现404的错误，查看错误日志会发现是因为地址不对，所以验证了： root的处理结果是: root路径+location路径 /usr/local/nginx/html/images/mv.png alias的处理结果是:使用alias路径替换location路径 /usr/local/nginx/html/images 需要在alias后面路径改为 location /images { alias /usr/local/nginx/html/images; } （3）如果location路径是以/结尾,则alias也必须是以/结尾，root没有要求 将上述配置修改为 location /images/ { alias /usr/local/nginx/html/images; } 访问就会出问题，查看错误日志还是路径不对，所以需要把alias后面加上 / 小结： root的处理结果是: root路径+location路径 alias的处理结果是:使用alias路径替换location路径 alias是一个目录别名的定义，root则是最上层目录的含义。 如果location路径是以/结尾,则alias也必须是以/结尾，root没有要求 index指令 index:设置网站的默认首页 语法 index file ...; 默认值 index index.html; 位置 http、server、location index后面可以跟多个设置，如果访问的时候没有指定具体访问的资源，则会依次进行查找，找到第一个为止。 举例说明： location / { root /usr/local/nginx/html; index index.html index.htm; } 访问该location的时候，可以通过 http://ip:port/，地址后面如果不添加任何内容，则默认依次访问index.html和index.htm，找到第一个来进行返回 error_page指令 error_page:设置网站的错误页面 语法 error_page code ... [=[response]] uri; 默认值 — 位置 http、server、location...... 当出现对应的响应code后，如何来处理。 举例说明： （1）可以指定具体跳转的地址 server { error_page 404 http://www.itcast.cn; } （2）可以指定重定向地址 server{ error_page 404 /50x.html; error_page 500 502 503 504 /50x.html; location =/50x.html{ root html; } } （3）使用location的@符合完成错误信息展示 server{ error_page 404 @jump_to_error; location @jump_to_error { default_type text/plain; return 404 'Not Found Page...'; } } 可选项=[response]的作用是用来将相应代码更改为另外一个 server{ error_page 404 =200 /50x.html; location =/50x.html{ root html; } } 这样的话，当返回404找不到对应的资源的时候，在浏览器上可以看到，最终返回的状态码是200，这块需要注意下，编写error_page后面的内容，404后面需要加空格，200前面不能加空格 静态资源优化配置语法 Nginx对静态资源如何进行优化配置。这里从三个属性配置进行优化： sendfile on; tcp_nopush on; tcp_nodeplay on; （1）sendﬁle，用来开启高效的文件传输模式。 语法 sendﬁle on \\ oﬀ; 默认值 sendﬁle oﬀ; 位置 http、server、location... 请求静态资源的过程：客户端通过网络接口向服务端发送请求，操作系统将这些客户端的请求传递给服务器端应用程序，服务器端应用程序会处理这些请求，请求处理完成以后，操作系统还需要将处理得到的结果通过网络适配器传递回去。 如： server { listen 80; server_name localhost； location / { root html; index index.html; } } 在html目录下有一个welcome.html页面，访问地址 http://192.168.200.133/welcome.html （2）tcp_nopush：该指令必须在sendfile打开的状态下才会生效，主要是用来提升网络包的传输'效率' 语法 tcp_nopush on\\ off; 默认值 tcp_nopush oﬀ; 位置 http、server、location （3）tcp_nodelay：该指令必须在keep-alive连接开启的情况下才生效，来提高网络包传输的'实时性' 语法 tcp_nodelay on\\ off; 默认值 tcp_nodelay on; 位置 http、server、location 经过刚才的分析，\"tcp_nopush\"和”tcp_nodelay“看起来是\"互斥的\"，那么为什么要将这两个值都打开，这个大家需要知道的是在linux2.5.9以后的版本中两者是可以兼容的，三个指令都开启的好处是，sendfile可以开启高效的文件传输模式，tcp_nopush开启可以确保在发送到客户端之前数据包已经充分“填满”， 这大大减少了网络开销，并加快了文件发送的速度。 然后，当它到达最后一个可能因为没有“填满”而暂停的数据包时，Nginx会忽略tcp_nopush参数， 然后，tcp_nodelay强制套接字发送数据。由此可知，TCP_NOPUSH可以与TCP_NODELAY一起设置，它比单独配置TCP_NODELAY具有更强的性能。所以我们可以使用如下配置来优化Nginx静态资源的处理 sendfile on; tcp_nopush on; tcp_nodelay on; Nginx静态资源压缩实战 经过上述内容的优化，我们再次思考一个问题，假如在满足上述优化的前提下，我们传送一个1M的数据和一个10M的数据那个效率高?，答案显而易见，传输内容小，速度就会快。那么问题又来了，同样的内容，如果把大小降下来，我们脑袋里面要蹦出一个词就是\"压缩\"，接下来，我们来学习Nginx的静态资源压缩模块。 在Nginx的配置文件中可以通过配置gzip来对静态资源进行压缩，相关的指令可以配置在http块、server块和location块中，Nginx可以通过 ngx_http_gzip_module模块 ngx_http_gzip_static_module模块 ngx_http_gunzip_module模块 对这些指令进行解析和处理。 接下来我们从以下内容进行学习 （1）Gzip各模块支持的配置指令 （2）Gzip压缩功能的配置 （3）Gzip和sendfile的冲突解决 （4）浏览器不支持Gzip的解决方案 Gzip模块配置指令 接下来所学习的指令都来自ngx_http_gzip_module模块，该模块会在nginx安装的时候内置到nginx的安装环境中，也就是说我们可以直接使用这些指令。 gzip指令：该指令用于开启或者关闭gzip功能 语法 gzip on\\ off; 默认值 gzip off; 位置 http、server、location... 注意只有该指令为打开状态，下面的指令才有效果 http{ gzip on; } gzip_types指令：该指令可以根据响应页的MIME类型选择性地开启Gzip压缩功能 语法 gzip_types mime-type ...; 默认值 gzip_types text/html; 位置 http、server、location 所选择的值可以从mime.types文件中进行查找，也可以使用\"*\"代表所有。 http{ gzip_types application/javascript; } gzip_comp_level指令：该指令用于设置Gzip压缩程度，级别从1-9,1表示要是程度最低，要是效率最高，9刚好相反，压缩程度最高，但是效率最低最费时间。 语法 gzip_comp_level level; 默认值 gzip_comp_level 1; 位置 http、server、location http{ gzip_comp_level 6; } gzip_vary指令：该指令用于设置使用Gzip进行压缩发送是否携带“Vary:Accept-Encoding”头域的响应头部。主要是告诉接收方，所发送的数据经过了Gzip压缩处理 语法 gzip_vary on\\ off; 默认值 gzip_vary off; 位置 http、server、location gzip_buffers指令：该指令用于处理请求压缩的缓冲区数量和大小。 语法 gzip_buffers number size; 默认值 gzip_buffers 32 4k\\ 16 8k; 位置 http、server、location 其中number:指定Nginx服务器向系统申请缓存空间个数，size指的是每个缓存空间的大小。主要实现的是申请number个每个大小为size的内存空间。这个值的设定一般会和服务器的操作系统有关，所以建议此项不设置，使用默认值即可。 gzip_buffers 4 16K; #缓存空间大小 gzip_disable指令：针对不同种类客户端发起的请求，可以选择性地开启和关闭Gzip功能。 语法 gzip_disable regex ...; 默认值 — 位置 http、server、location regex:根据客户端的浏览器标志(user-agent)来设置，支持使用正则表达式。指定的浏览器标志不使用Gzip.该指令一般是用来排除一些明显不支持Gzip的浏览器。 gzip_disable \"MSIE [1-6]\\.\"; gzip_http_version指令：针对不同的HTTP协议版本，可以选择性地开启和关闭Gzip功能。 语法 gzip_http_version 1.0\\ 1.1; 默认值 gzip_http_version 1.1; 位置 http、server、location 该指令是指定使用Gzip的HTTP最低版本，该指令一般采用默认值即可。 gzip_min_length指令：该指令针对传输数据的大小，可以选择性地开启和关闭Gzip功能 语法 gzip_min_length length; 默认值 gzip_min_length 20; 位置 http、server、location nignx计量大小的单位：bytes[字节] / kb[千字节] / M[兆] 例如: 1024 / 10k|K / 10m|M Gzip压缩功能对大数据的压缩效果明显，但是如果要压缩的数据比较小的化，可能出现越压缩数据量越大的情况，因此我们需要根据响应内容的大小来决定是否使用Gzip功能，响应页面的大小可以通过头信息中的Content-Length来获取。但是如何使用了Chunk编码动态压缩，该指令将被忽略。建议设置为1K或以上。 gzip_proxied指令：该指令设置是否对服务端返回的结果进行Gzip压缩。 语法 gzip_proxied off\\ expired\\ no-cache\\ no-store\\ private\\ no_last_modified\\ no_etag\\ auth\\ any; 默认值 gzip_proxied off; 位置 http、server、location off - 关闭Nginx服务器对后台服务器返回结果的Gzip压缩 expired - 启用压缩，如果header头中包含 \"Expires\" 头信息 no-cache - 启用压缩，如果header头中包含 \"Cache-Control:no-cache\" 头信息 no-store - 启用压缩，如果header头中包含 \"Cache-Control:no-store\" 头信息 private - 启用压缩，如果header头中包含 \"Cache-Control:private\" 头信息 no_last_modified - 启用压缩,如果header头中不包含 \"Last-Modified\" 头信息 no_etag - 启用压缩 ,如果header头中不包含 \"ETag\" 头信息 auth - 启用压缩 , 如果header头中包含 \"Authorization\" 头信息 any - 无条件启用压缩 Gzip压缩功能的实例配置 gzip on; #开启gzip功能 gzip_types *; #压缩源文件类型,根据具体的访问资源类型设定 gzip_comp_level 6; #gzip压缩级别 gzip_min_length 1024; #进行压缩响应页面的最小长度,content-length gzip_buffers 4 16K; #缓存空间大小 gzip_http_version 1.1; #指定压缩响应所需要的最低HTTP请求版本 gzip_vary on; #往头信息中添加压缩标识 gzip_disable \"MSIE [1-6]\\.\"; #对IE6以下的版本都不进行压缩 gzip_proxied off； #nginx作为反向代理压缩服务端返回数据的条件 这些配置在很多地方可能都会用到，所以我们可以将这些内容抽取到一个配置文件中，然后通过include指令把配置文件再次加载到nginx.conf配置文件中，方法使用。 nginx_gzip.conf gzip on; gzip_types *; gzip_comp_level 6; gzip_min_length 1024; gzip_buffers 4 16K; gzip_http_version 1.1; gzip_vary on; gzip_disable \"MSIE [1-6]\\.\"; gzip_proxied off; nginx.conf include nginx_gzip.conf Gzip和sendfile共存问题 前面在讲解sendfile的时候，提到过，开启sendfile以后，在读取磁盘上的静态资源文件的时候，可以减少拷贝的次数，可以不经过用户进程将静态文件通过网络设备发送出去，但是Gzip要想对资源压缩，是需要经过用户进程进行操作的。所以如何解决两个设置的共存问题。 可以使用ngx_http_gzip_static_module模块的gzip_static指令来解决。 gzip_static指令 gzip_static: 检查与访问资源同名的.gz文件时，response中以gzip相关的header返回.gz文件的内容。 语法 gzip_static on \\ off \\ always; 默认值 gzip_static off; 位置 http、server、location 添加上述命令后，会报一个错误，unknown directive \"gzip_static\"主要的原因是Nginx默认是没有添加ngx_http_gzip_static_module模块。如何来添加? 添加模块到Nginx的实现步骤 (1)查询当前Nginx的配置参数 nginx -V (2)将nginx安装目录下sbin目录中的nginx二进制文件进行更名 cd /usr/local/nginx/sbin mv nginx nginxold (3) 进入Nginx的安装目录 cd /root/nginx/core/nginx-1.16.1 (4)执行make clean清空之前编译的内容 make clean (5)使用configure来配置参数 ./configure --with-http_gzip_static_module (6)使用make命令进行编译 make (7) 将objs目录下的nginx二进制执行文件移动到nginx安装目录下的sbin目录中 mv objs/nginx /usr/local/nginx/sbin (8)执行更新命令 make upgrade gzip_static测试使用 (1)直接访问http://192.168.200.133/jquery.js (2)使用gzip命令进行压缩 cd /usr/local/nginx/html gzip jquery.js (3)再次访问http://192.168.200.133/jquery.js 静态资源的缓存处理 什么是缓存 缓存（cache），原始意义是指访问速度比一般随机存取存储器（RAM）快的一种高速存储器，通常它不像系统主存那样使用DRAM技术，而使用昂贵但较快速的SRAM技术。缓存的设置是所有现代计算机系统发挥高性能的重要因素之一。 什么是web缓存 Web缓存是指一个Web资源（如html页面，图片，js，数据等）存在于Web服务器和客户端（浏览器）之间的副本。缓存会根据进来的请求保存输出内容的副本；当下一个请求来到的时候，如果是相同的URL，缓存会根据缓存机制决定是直接使用副本响应访问请求，还是向源服务器再次发送请求。比较常见的就是浏览器会缓存访问过网站的网页，当再次访问这个URL地址的时候，如果网页没有更新，就不会再次下载网页，而是直接使用本地缓存的网页。只有当网站明确标识资源已经更新，浏览器才会再次下载网页 web缓存的种类 客户端缓存 浏览器缓存 服务端缓存 Nginx / Redis / Memcached等 浏览器缓存 是为了节约网络的资源加速浏览，浏览器在用户磁盘上对最近请求过的文档进行存储，当访问者再次请求这个页面时，浏览器就可以从本地磁盘显示文档，这样就可以加速页面的阅览. 为什么要用浏览器缓存 成本最低的一种缓存实现 减少网络带宽消耗 降低服务器压力 减少网络延迟，加快页面打开速度 浏览器缓存的执行流程 HTTP协议中和页面缓存相关的字段，我们先来认识下： header 说明 Expires 缓存过期的日期和时间 Cache-Control 设置和缓存相关的配置信息 Last-Modified 请求资源最后修改时间 ETag 请求变量的实体标签的当前值，比如文件的MD5值 （1）用户首次通过浏览器发送请求到服务端获取数据，客户端是没有对应的缓存，所以需要发送request请求来获取数据； （2）服务端接收到请求后，获取服务端的数据及服务端缓存的允许后，返回200的成功状态码并且在响应头上附上对应资源以及缓存信息； （3）当用户再次访问相同资源的时候，客户端会在浏览器的缓存目录中查找是否存在响应的缓存文件 （4）如果没有找到对应的缓存文件，则走(2)步 （5）如果有缓存文件，接下来对缓存文件是否过期进行判断，过期的判断标准是(Expires), （6）如果没有过期，则直接从本地缓存中返回数据进行展示 （7）如果Expires过期，接下来需要判断缓存文件是否发生过变化 （8）判断的标准有两个，一个是ETag(Entity Tag),一个是Last-Modified （9）判断结果是未发生变化，则服务端返回304，直接从缓存文件中获取数据 （10）如果判断是发生了变化，重新从服务端获取数据，并根据缓存协商(服务端所设置的是否需要进行缓存数据的设置)来进行数据缓存。 浏览器缓存相关指令 Nginx需要进行缓存相关设置，就需要用到如下的指令 expires指令 expires:该指令用来控制页面缓存的作用。可以通过该指令控制HTTP应答中的“Expires\"和”Cache-Control\" 语法 expires [modified] timeexpires epoch\\ max\\ off; 默认值 expires off; 位置 http、server、location time:可以整数也可以是负数，指定过期时间，如果是负数，Cache-Control则为no-cache,如果为整数或0，则Cache-Control的值为max-age=time; epoch: 指定Expires的值为'1 January,1970,00:00:01 GMT'(1970-01-01 00:00:00)，Cache-Control的值no-cache max:指定Expires的值为'31 December2037 23:59:59GMT' (2037-12-31 23:59:59) ，Cache-Control的值为10年 off:默认不缓存。 add_header指令 add_header指令是用来添加指定的响应头和响应值。 语法 add_header name value [always]; 默认值 — 位置 http、server、location... Cache-Control作为响应头信息，可以设置如下值： 缓存响应指令： Cache-control: must-revalidate Cache-control: no-cache Cache-control: no-store Cache-control: no-transform Cache-control: public Cache-control: private Cache-control: proxy-revalidate Cache-Control: max-age= Cache-control: s-maxage= 指令 说明 must-revalidate 可缓存但必须再向源服务器进行确认 no-cache 缓存前必须确认其有效性 no-store 不缓存请求或响应的任何内容 no-transform 代理不可更改媒体类型 public 可向任意方提供响应的缓存 private 仅向特定用户返回响应 proxy-revalidate 要求中间缓存服务器对缓存的响应有效性再进行确认 max-age= 响应最大Age值 s-maxage= 公共缓存服务器响应的最大Age值 max-age=[秒]： Nginx的跨域问题解决 这块内容，我们主要从以下方面进行解决： 什么情况下会出现跨域问题? 实例演示跨域问题 具体的解决方案是什么? 同源策略 浏览器的同源策略：是一种约定，是浏览器最核心也是最基本的安全功能，如果浏览器少了同源策略，则浏览器的正常功能可能都会受到影响。 同源: 协议、域名(IP)、端口相同即为同源 http://192.168.200.131/user/1 https://192.168.200.131/user/1 不 http://192.168.200.131/user/1 http://192.168.200.132/user/1 不 http://192.168.200.131/user/1 http://192.168.200.131:8080/user/1 不 http://www.nginx.com/user/1 http://www.nginx.org/user/1 不 http://192.168.200.131/user/1 http://192.168.200.131:8080/user/1 不 http://www.nginx.org:80/user/1 http://www.nginx.org/user/1 满足 跨域问题 简单描述下: 有两台服务器分别为A,B,如果从服务器A的页面发送异步请求到服务器B获取数据，如果服务器A和服务器B不满足同源策略，则就会出现跨域问题。 跨域问题的案例演示 出现跨域问题会有什么效果?,接下来通过一个需求来给大家演示下： （1）nginx的html目录下新建一个a.html 跨域问题演示 $(function(){ $(\"#btn\").click(function(){ $.get('http://192.168.200.133:8080/getUser',function(data){ alert(JSON.stringify(data)); }); }); }); （2）在nginx.conf配置如下内容 server{ listen 8080; server_name localhost; location /getUser{ default_type application/json; return 200 '{\"id\":1,\"name\":\"TOM\",\"age\":18}'; } } server{ listen 80; server_name localhost; location /{ root html; index index.html; } } (3)通过浏览器访问测试 解决方案 使用add_header指令，该指令可以用来添加一些头信息 语法 add_header name value... 默认值 — 位置 http、server、location 此处用来解决跨域问题，需要添加两个头信息，一个是Access-Control-Allow-Origin,Access-Control-Allow-Methods Access-Control-Allow-Origin: 直译过来是允许跨域访问的源地址信息，可以配置多个(多个用逗号分隔)，也可以使用*代表所有源 Access-Control-Allow-Methods:直译过来是允许跨域访问的请求方式，值可以为 GET POST PUT DELETE...,可以全部设置，也可以根据需要设置，多个用逗号分隔 具体配置方式 location /getUser{ add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE; default_type application/json; return 200 '{\"id\":1,\"name\":\"TOM\",\"age\":18}'; } 静态资源防盗链 什么是资源盗链 资源盗链指的是此内容不在自己服务器上，而是通过技术手段，绕过别人的限制将别人的内容放到自己页面上最终展示给用户。以此来盗取大网站的空间和流量。简而言之就是用别人的东西成就自己的网站。 效果演示 京东:https://img14.360buyimg.com/n7/jfs/t1/101062/37/2153/254169/5dcbd410E6d10ba22/4ddbd212be225fcd.jpg 百度:https://pics7.baidu.com/feed/cf1b9d16fdfaaf516f7e2011a7cda1e8f11f7a1a.jpeg?token=551979a23a0995e5e5279b8fa1a48b34&s=BD385394D2E963072FD48543030030BB 我们自己准备一个页面，在页面上引入这两个图片查看效果 从上面的效果，可以看出来，下面的图片地址添加了防止盗链的功能，京东这边我们可以直接使用其图片。 Nginx防盗链的实现原理： 了解防盗链的原理之前，我们得先学习一个HTTP的头信息Referer,当浏览器向web服务器发送请求的时候，一般都会带上Referer,来告诉浏览器该网页是从哪个页面链接过来的。 后台服务器可以根据获取到的这个Referer信息来判断是否为自己信任的网站地址，如果是则放行继续访问，如果不是则可以返回403(服务端拒绝访问)的状态信息。 在本地模拟上述的服务器效果： Nginx防盗链的具体实现: valid_referers:nginx会通就过查看referer自动和valid_referers后面的内容进行匹配，如果匹配到了就将$invalid_referer变量置0，如果没有匹配到，则将$invalid_referer变量置为1，匹配的过程中不区分大小写。 语法 valid_referers none\\ blocked\\ server_names\\ string... 默认值 — 位置 server、location none: 如果Header中的Referer为空，允许访问 blocked:在Header中的Referer不为空，但是该值被防火墙或代理进行伪装过，如不带\"http://\" 、\"https://\"等协议头的资源允许访问。 server_names:指定具体的域名或者IP string: 可以支持正则表达式和*的字符串。如果是正则表达式，需要以~开头表示，例如 location ~*\\.(png|jpg|gif){ valid_referers none blocked www.baidu.com 192.168.200.222 *.example.com example.* www.example.org ~\\.google\\.; if ($invalid_referer){ return 403; } root /usr/local/nginx/html; } 遇到的问题:图片有很多，该如何批量进行防盗链？ 针对目录进行防盗链 配置如下： location /images { valid_referers none blocked www.baidu.com 192.168.200.222 *.example.com example.* www.example.org ~\\.google\\.; if ($invalid_referer){ return 403; } root /usr/local/nginx/html; } 这样我们可以对一个目录下的所有资源进行翻到了操作。 遇到的问题：Referer的限制比较粗，比如随意加一个Referer，上面的方式是无法进行限制的。那么这个问题改如何解决？ 此处我们需要用到Nginx的第三方模块ngx_http_accesskey_module，第三方模块如何实现盗链，如果在Nginx中使用第三方模块的功能，这些我们在后面的Nginx的模块篇再进行详细的讲解。 Rewrite功能配置 Rewrite是Nginx服务器提供的一个重要基本功能，是Web服务器产品中几乎必备的功能。主要的作用是用来实现URL的重写。 注意:Nginx服务器的Rewrite功能的实现依赖于PCRE的支持，因此在编译安装Nginx服务器之前，需要安装PCRE库。Nginx使用的是ngx_http_rewrite_module模块来解析和处理Rewrite功能的相关配置。 \"地址重写\"与\"地址转发\" 重写和转发的区别: 地址重写浏览器地址会发生变化而地址转发则不变 一次地址重写会产生两次请求而一次地址转发只会产生一次请求 地址重写到的页面必须是一个完整的路径而地址转发则不需要 地址重写因为是两次请求所以request范围内属性不能传递给新页面而地址转发因为是一次请求所以可以传递值 地址转发速度快于地址重写 Rewrite规则 set指令 该指令用来设置一个新的变量。 语法 set $variable value; 默认值 — 位置 server、location、if variable:变量的名称，该变量名称要用\"$\"作为变量的第一个字符，且不能与Nginx服务器预设的全局变量同名。 value:变量的值，可以是字符串、其他变量或者变量的组合等。 Rewrite常用全局变量 变量 说明 $args 变量中存放了请求URL中的请求指令。比如http://192.168.200.133:8080?arg1=value1&args2=value2中的\"arg1=value1&arg2=value2\"，功能和$query_string一样 $http_user_agent 变量存储的是用户访问服务的代理信息(如果通过浏览器访问，记录的是浏览器的相关版本信息) $host 变量存储的是访问服务器的server_name值 $document_uri 变量存储的是当前访问地址的URI。比如http://192.168.200.133/server?id=10&name=zhangsan中的\"/server\"，功能和$uri一样 $document_root 变量存储的是当前请求对应location的root值，如果未设置，默认指向Nginx自带html目录所在位置 $content_length 变量存储的是请求头中的Content-Length的值 $content_type 变量存储的是请求头中的Content-Type的值 $http_cookie 变量存储的是客户端的cookie信息，可以通过add_header Set-Cookie 'cookieName=cookieValue'来添加cookie数据 $limit_rate 变量中存储的是Nginx服务器对网络连接速率的限制，也就是Nginx配置中对limit_rate指令设置的值，默认是0，不限制。 $remote_addr 变量中存储的是客户端的IP地址 $remote_port 变量中存储了客户端与服务端建立连接的端口号 $remote_user 变量中存储了客户端的用户名，需要有认证模块才能获取 $scheme 变量中存储了访问协议 $server_addr 变量中存储了服务端的地址 $server_name 变量中存储了客户端请求到达的服务器的名称 $server_port 变量中存储了客户端请求到达服务器的端口号 $server_protocol 变量中存储了客户端请求协议的版本，比如\"HTTP/1.1\" $request_body_file 变量中存储了发给后端服务器的本地文件资源的名称 $request_method 变量中存储了客户端的请求方式，比如\"GET\",\"POST\"等 $request_filename 变量中存储了当前请求的资源文件的路径名 $request_uri 变量中存储了当前请求的URI，并且携带请求参数，比如http://192.168.200.133/server?id=10&name=zhangsan中的\"/server?id=10&name=zhangsan\" if指令 该指令用来支持条件判断，并根据条件判断结果选择不同的Nginx配置。 语法 if (condition){...} 默认值 — 位置 server、location condition为判定条件，可以支持以下写法： 变量名。如果变量名对应的值为空或者是0，if都判断为false,其他条件为true。 if ($param){ } 2. 使用\"=\"和\"!=\"比较变量和字符串是否相等，满足条件为true，不满足为false if ($request_method = POST){ return 405; } 注意：此处和Java不太一样的地方是字符串不需要添加引号。 使用正则表达式对变量进行匹配，匹配成功返回true，否则返回false。变量与正则表达式之间使用\"~\",\"~\",\"!~\",\"!~\\\"来连接。 \"~\"代表匹配正则表达式过程中区分大小写， \"~*\"代表匹配正则表达式过程中不区分大小写 \"!~\"和\"!~*\"刚好和上面取相反值，如果匹配上返回false,匹配不上返回true if ($http_user_agent ~ MSIE){ #$http_user_agent的值中是否包含MSIE字符串，如果包含返回true } 注意：正则表达式字符串一般不需要加引号，但是如果字符串中包含\"}\"或者是\";\"等字符时，就需要把引号加上。 判断请求的文件是否存在使用\"-f\"和\"!-f\", 当使用\"-f\"时，如果请求的文件存在返回true，不存在返回false。 当使用\"!f\"时，如果请求文件不存在，但该文件所在目录存在返回true,文件和目录都不存在返回false,如果文件存在返回false if (-f $request_filename){ #判断请求的文件是否存在 } if (!-f $request_filename){ #判断请求的文件是否不存在 } 判断请求的目录是否存在使用\"-d\"和\"!-d\", 当使用\"-d\"时，如果请求的目录存在，if返回true，如果目录不存在则返回false 当使用\"!-d\"时，如果请求的目录不存在但该目录的上级目录存在则返回true，该目录和它上级目录都不存在则返回false,如果请求目录存在也返回false. 判断请求的目录或者文件是否存在使用\"-e\"和\"!-e\" 当使用\"-e\",如果请求的目录或者文件存在时，if返回true,否则返回false. 当使用\"!-e\",如果请求的文件和文件所在路径上的目录都不存在返回true,否则返回false 判断请求的文件是否可执行使用\"-x\"和\"!-x\" 当使用\"-x\",如果请求的文件可执行，if返回true,否则返回false 当使用\"!-x\",如果请求文件不可执行，返回true,否则返回false break指令 该指令用于中断当前相同作用域中的其他Nginx配置。与该指令处于同一作用域的Nginx配置中，位于它前面的指令配置生效，位于后面的指令配置无效。 语法 break; 默认值 — 位置 server、location、if 例子: location /{ if ($param){ set $id $1; break; limit_rate 10k; } } return指令 该指令用于完成对请求的处理，直接向客户端返回响应状态代码。在return后的所有Nginx配置都是无效的。 语法 return code [text];return code URL;return URL; 默认值 — 位置 server、location、if code:为返回给客户端的HTTP状态代理。可以返回的状态代码为0~999的任意HTTP状态代理 text:为返回给客户端的响应体内容，支持变量的使用 URL:为返回给客户端的URL地址 rewrite指令 该指令通过正则表达式的使用来改变URI。可以同时存在一个或者多个指令，按照顺序依次对URL进行匹配和处理。 URL和URI的区别： URI:统一资源标识符 URL:统一资源定位符 语法 rewrite regex replacement [flag]; 默认值 — 位置 server、location、if regex:用来匹配URI的正则表达式 replacement:匹配成功后，用于替换URI中被截取内容的字符串。如果该字符串是以\"http://\"或者\"https://\"开头的，则不会继续向下对URI进行其他处理，而是直接返回重写后的URI给客户端。 flag:用来设置rewrite对URI的处理行为，可选值有如下： last: break redirect permanent rewrite_log指令 该指令配置是否开启URL重写日志的输出功能。 语法 rewrite_log on\\ off; 默认值 rewrite_log off; 位置 http、server、location、if 开启后，URL重写的相关日志将以notice级别输出到error_log指令配置的日志文件汇总。 Rewrite的案例 域名跳转 》问题分析 先来看一个效果，如果我们想访问京东网站，大家都知道我们可以输入www.jd.com,但是同样的我们也可以输入www.360buy.com同样也都能访问到京东网站。这个其实是因为京东刚开始的时候域名就是www.360buy.com，后面由于各种原因把自己的域名换成了www.jd.com, 虽然说域名变量，但是对于以前只记住了www.360buy.com的用户来说，我们如何把这部分用户也迁移到我们新域名的访问上来，针对于这个问题，我们就可以使用Nginx中Rewrite的域名跳转来解决。 》环境准备 准备两个域名 www.360buy.com | www.jd.com vim /etc/hosts 192.168.200.133 www.360buy.com 192.168.200.133 www.jd.com 在/usr/local/nginx/html/hm目录下创建一个访问页面 欢迎来到我们的网站 通过Nginx实现当访问www.访问到系统的首页 server { listen 80; server_name www.hm.com; location /{ root /usr/local/nginx/html/hm; index index.html; } } 》通过Rewrite完成将www.360buy.com的请求跳转到www.jd.com server { listen 80; server_name www.360buy.com; rewrite ^/ http://www.jd.com permanent; } 问题描述:如何在域名跳转的过程中携带请求的URI？ 修改配置信息 server { listen 80; server_name www.itheima.com; rewrite ^(.*) http://www.hm.com$1 permanent; } 问题描述:我们除了上述说的www.jd.com 、www.360buy.com其实还有我们也可以通过www.jingdong.com来访问，那么如何通过Rewrite来实现多个域名的跳转? 添加域名 vim /etc/hosts 192.168.200.133 www.jingdong.com 修改配置信息 server{ listen 80; server_name www.360buy.com www.jingdong.com; rewrite ^(.*) http://www.jd.com$1 permanent; } 域名镜像 上述案例中，将www.360buy.com 和 www.jingdong.com都能跳转到www.jd.com，那么www.jd.com我们就可以把它起名叫主域名，其他两个就是我们所说的镜像域名，当然如果我们不想把整个网站做镜像，只想为其中某一个子目录下的资源做镜像，我们可以在location块中配置rewrite功能，比如: server { listen 80; server_name rewrite.myweb.com; location ^~ /source1{ rewrite ^/resource1(.*) http://rewrite.myweb.com/web$1 last; } location ^~ /source2{ rewrite ^/resource2(.*) http://rewrite.myweb.com/web$1 last; } } 独立域名 一个完整的项目包含多个模块，比如购物网站有商品商品搜索模块、商品详情模块已经购物车模块等，那么我们如何为每一个模块设置独立的域名。 需求： http://search.hm.com 访问商品搜索模块 http://item.hm.com 访问商品详情模块 http://cart.hm.com 访问商品购物车模块 server{ listen 80; server_name search.hm.com; rewrite ^(.*) http://www.hm.com/bbs$1 last; } server{ listen 81; server_name item.hm.com; rewrite ^(.*) http://www.hm.com/item$1 last; } server{ listen 82; server_name cart.hm.com; rewrite ^(.*) http://www.hm.com/cart$1 last; } 目录自动添加\"/\" 问题描述 通过一个例子来演示下问题: server { listen 80; server_name localhost; location / { root html; index index.html; } } 要想访问上述资源，很简单，只需要通过http://192.168.200.133直接就能访问，地址后面不需要加/,但是如果将上述的配置修改为如下内容: server { listen 80; server_name localhost; location /hm { root html; index index.html; } } 这个时候，要想访问上述资源，按照上述的访问方式，我们可以通过http://192.168.200.133/hm/来访问,但是如果地址后面不加斜杠，页面就会出问题。如果不加斜杠，Nginx服务器内部会自动做一个301的重定向，重定向的地址会有一个指令叫server_name_in_redirect on|off;来决定重定向的地址： 如果该指令为on 重定向的地址为: http://server_name/目录名/; 如果该指令为off 重定向的地址为: http://原URL中的域名/目录名/; 所以就拿刚才的地址来说，http://192.168.200.133/hm如果不加斜杠，那么按照上述规则，如果指令server_name_in_redirect为on，则301重定向地址变为 http://localhost/hm/,如果为off，则301重定向地址变为http://192.168.200.133/ht/。后面这个是正常的，前面地址就有问题。 注意server_name_in_redirect指令在Nginx的0.8.48版本之前默认都是on，之后改成了off,所以现在我们这个版本不需要考虑这个问题，但是如果是0.8.48以前的版本并且server_name_in_redirect设置为on，我们如何通过rewrite来解决这个问题？ 解决方案 我们可以使用rewrite功能为末尾没有斜杠的URL自动添加一个斜杠 server { listen 80; server_name localhost; server_name_in_redirect on; location /hm { if (-d $request_filename){ rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; } } } 合并目录 搜索引擎优化(SEO)是一种利用搜索引擎的搜索规则来提供目的网站的有关搜索引擎内排名的方式。我们在创建自己的站点时，可以通过很多中方式来有效的提供搜索引擎优化的程度。其中有一项就包含URL的目录层级一般不要超过三层，否则的话不利于搜索引擎的搜索也给客户端的输入带来了负担，但是将所有的文件放在一个目录下又会导致文件资源管理混乱并且访问文件的速度也会随着文件增多而慢下来，这两个问题是相互矛盾的，那么使用rewrite如何解决上述问题? 举例，网站中有一个资源文件的访问路径时 /server/11/22/33/44/20.html,也就是说20.html存在于第5级目录下，如果想要访问该资源文件，客户端的URL地址就要写成 http://www.web.name/server/11/22/33/44/20.html, server { listen 80; server_name www.web.name; location /server{ root html; } } 但是这个是非常不利于SEO搜索引擎优化的，同时客户端也不好记.使用rewrite我们可以进行如下配置: server { listen 80; server_name www.web.name; location /server{ rewrite ^/server-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)\\.html$ /server/$1/$2/$3/$4/$5.html last; } } 这样的花，客户端只需要输入http://www.web.name/server-11-22-33-44-20.html就可以访问到20.html页面了。这里也充分利用了rewrite指令支持正则表达式的特性。 防盗链 防盗链之前我们已经介绍过了相关的知识，在rewrite中的防盗链和之前将的原理其实都是一样的，只不过通过rewrite可以将防盗链的功能进行完善下，当出现防盗链的情况，我们可以使用rewrite将请求转发到自定义的一张图片和页面，给用户比较好的提示信息。下面我们就通过根据文件类型实现防盗链的一个配置实例: server{ listen 80; server_name www.web.com; locatin ~* ^.+\\.(gif|jpg|png|swf|flv|rar|zip)${ valid_referers none blocked server_names *.web.com; if ($invalid_referer){ rewrite ^/ http://www.web.com/images/forbidden.png; } } } 根据目录实现防盗链配置： server{ listen 80; server_name www.web.com; location /file/{ root /server/file/; valid_referers none blocked server_names *.web.com; if ($invalid_referer){ rewrite ^/ http://www.web.com/images/forbidden.png; } } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 10:31:44 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx_day03.html":{"url":"performance/nginx/Nginx_day03.html","title":"3.Rewrite功能配置","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 Rewrite的相关指令 set指令 Rewrite常用全局变量 if指令 break指令 return指令 rewrite指令 rewrite_log指令 Rewrite的案例 域名跳转 域名镜像 独立域名 目录自动添加\"/\" 合并目录 防盗链 Nginx反向代理 Nginx反向代理概述 Nginx反向代理的配置语法 proxy_pass proxy_set_header proxy_redirect Nginx反向代理实战 Nginx的安全控制 如何使用SSL对流量进行加密 nginx添加SSL的支持 Nginx的SSL相关指令 生成证书 开启SSL实例 反向代理系统调优 Rewrite功能配置 Rewrite是Nginx服务器提供的一个重要基本功能，是Web服务器产品中几乎必备的功能。主要的作用是用来实现URL的重写。www.jd.com 注意:Nginx服务器的Rewrite功能的实现依赖于PCRE的支持，因此在编译安装Nginx服务器之前，需要安装PCRE库。Nginx使用的是ngx_http_rewrite_module模块来解析和处理Rewrite功能的相关配置。 Rewrite的相关命令 set指令 if指令 break指令 return指令 rewrite指令 rewrite_log指令 Rewrite的应用场景 域名跳转 域名镜像 独立域名 目录自动添加\"/\" 合并目录 防盗链的实现 Rewrite的相关指令 set指令 该指令用来设置一个新的变量。 语法 set $variable value; 默认值 — 位置 server、location、if variable:变量的名称，该变量名称要用\"$\"作为变量的第一个字符，且不要与Nginx服务器预设的全局变量同名。 value:变量的值，可以是字符串、其他变量或者变量的组合等。 Rewrite常用全局变量 变量 说明 $args 变量中存放了请求URL中的请求参数。比如http://192.168.200.133/server?arg1=value1&args2=value2中的\"arg1=value1&arg2=value2\"，功能和$query_string一样 $http_user_agent 变量存储的是用户访问服务的代理信息(如果通过浏览器访问，记录的是浏览器的相关版本信息) $host 变量存储的是访问服务器的server_name值 $document_uri 变量存储的是当前访问地址的URI。比如http://192.168.200.133/server?id=10&name=zhangsan中的\"/server\"，功能和$uri一样 $document_root 变量存储的是当前请求对应location的root值，如果未设置，默认指向Nginx自带html目录所在位置 $content_length 变量存储的是请求头中的Content-Length的值 $content_type 变量存储的是请求头中的Content-Type的值 $http_cookie 变量存储的是客户端的cookie信息，可以通过add_header Set-Cookie 'cookieName=cookieValue'来添加cookie数据 $limit_rate 变量中存储的是Nginx服务器对网络连接速率的限制，也就是Nginx配置中对limit_rate指令设置的值，默认是0，不限制。 $remote_addr 变量中存储的是客户端的IP地址 $remote_port 变量中存储了客户端与服务端建立连接的端口号 $remote_user 变量中存储了客户端的用户名，需要有认证模块才能获取 $scheme 变量中存储了访问协议 $server_addr 变量中存储了服务端的地址 $server_name 变量中存储了客户端请求到达的服务器的名称 $server_port 变量中存储了客户端请求到达服务器的端口号 $server_protocol 变量中存储了客户端请求协议的版本，比如\"HTTP/1.1\" $request_body_file 变量中存储了发给后端服务器的本地文件资源的名称 $request_method 变量中存储了客户端的请求方式，比如\"GET\",\"POST\"等 $request_filename 变量中存储了当前请求的资源文件的路径名 $request_uri 变量中存储了当前请求的URI，并且携带请求参数，比如http://192.168.200.133/server?id=10&name=zhangsan中的\"/server?id=10&name=zhangsan\" 上述参数还可以在日志文件中使用，这个就要用到前面我们介绍的log_format指令 log_format main '$remote_addr - $request - $status-$request_uri $http_user_agent'; access_log logs/access.log main; if指令 该指令用来支持条件判断，并根据条件判断结果选择不同的Nginx配置。 语法 if (condition){...} 默认值 — 位置 server、location condition为判定条件，可以支持以下写法： 变量名。如果变量名对应的值为空字符串或\"0\"，if都判断为false,其他条件为true。 if ($param){ } 使用\"=\"和\"!=\"比较变量和字符串是否相等，满足条件为true，不满足为false if ($request_method = POST){ return 405; } 注意：此处和Java不太一样的地方是字符串不需要添加引号,并且等号和不等号前后到需要加空格。 使用正则表达式对变量进行匹配，匹配成功返回true，否则返回false。变量与正则表达式之间使用\"~\",\"~\",\"!~\",\"!~\\\"来连接。 \"~\"代表匹配正则表达式过程中区分大小写， \"~*\"代表匹配正则表达式过程中不区分大小写 \"!~\"和\"!~*\"刚好和上面取相反值，如果匹配上返回false,匹配不上返回true if ($http_user_agent ~ MSIE){ #$http_user_agent的值中是否包含MSIE字符串，如果包含返回true } 注意：正则表达式字符串一般不需要加引号，但是如果字符串中包含\"}\"或者是\";\"等字符时，就需要把引号加上。 判断请求的文件是否存在使用\"-f\"和\"!-f\", if (-f $request_filename){ #判断请求的文件是否存在 } if (!-f $request_filename){ #判断请求的文件是否不存在 } 判断请求的目录是否存在使用\"-d\"和\"!-d\" 判断请求的目录或者文件是否存在使用\"-e\"和\"!-e\" 判断请求的文件是否可执行使用\"-x\"和\"!-x\" break指令 该指令用于中断当前相同作用域中的其他Nginx配置。与该指令处于同一作用域的Nginx配置中，位于它前面的指令配置生效，位于后面的指令配置无效。并且break还有另外一个功能就是终止当前的匹配并把当前的URI在本location进行重定向访问处理。 语法 break; 默认值 — 位置 server、location、if 例子: location /testbreak{ default_type text/plain; set $username TOM; if ($args){ Set $username JERRY; break; set $username ROSE; } add_header username $username; return 200 $username; } return指令 该指令用于完成对请求的处理，直接向客户端返回。在return后的所有Nginx配置都是无效的。 语法 return code [text];return code URL;return URL; 默认值 — 位置 server、location、if code:为返回给客户端的HTTP状态代理。可以返回的状态代码为0~999的任意HTTP状态代理 text:为返回给客户端的响应体内容，支持变量的使用 URL:为返回给客户端的URL地址 location /testreturn { return 200 success; } location /testreturn { return https://www.baidu.com; // 302重定向到百度 } location /testreturn { return 302 https://www.baidu.com; } location /testreturn { return 302 www.baidu.com;//不允许这么写 } rewrite指令 该指令通过正则表达式的使用来改变URI。可以同时存在一个或者多个指令，按照顺序依次对URL进行匹配和处理。 语法 rewrite regex replacement [flag]; 默认值 — 位置 server、location、if regex:用来匹配URI的正则表达式 replacement:匹配成功后，用于替换URI中被截取内容的字符串。如果该字符串是以\"http://\"或者\"https://\"开头的，则不会继续向下对URI进行其他处理，而是直接返回重写后的URI给客户端。 location rewrite { rewrite ^/rewrite/url\\w*$ https://www.baidu.com; rewrite ^/rewrite/(test)\\w*$ /$1; rewrite ^/rewrite/(demo)\\w*$ /$1; } location /test{ default_type text/plain; return 200 test_success; } location /demo{ default_type text/plain; return 200 demo_success; } flag:用来设置rewrite对URI的处理行为，可选值有如下： last:终止继续在本location块中处理接收到的URI，并将此处重写的URI作为一个新的URI，使用各location块进行处理。该标志将重写后的URI重写在server块中执行，为重写后的URI提供了转入到其他location块的机会。 location rewrite { rewrite ^/rewrite/(test)\\w*$ /$1 last; rewrite ^/rewrite/(demo)\\w*$ /$1 last; } location /test{ default_type text/plain; return 200 test_success; } location /demo{ default_type text/plain; return 200 demo_success; } 访问 http://192.168.200.133:8081/rewrite/testabc,能正确访问 break：将此处重写的URI作为一个新的URI,在本块中继续进行处理。该标志将重写后的地址在当前的location块中执行，不会将新的URI转向其他的location块。 location rewrite { #/test /usr/local/nginx/html/test/index.html rewrite ^/rewrite/(test)\\w*$ /$1 break; rewrite ^/rewrite/(demo)\\w*$ /$1 break; } location /test{ default_type text/plain; return 200 test_success; } location /demo{ default_type text/plain; return 200 demo_success; } 访问 http://192.168.200.133:8081/rewrite/demoabc,页面报404错误 redirect：将重写后的URI返回给客户端，状态码为302，指明是临时重定向URI,主要用在replacement变量不是以\"http://\"或者\"https://\"开头的情况。 location rewrite { rewrite ^/rewrite/(test)\\w*$ /$1 redirect; rewrite ^/rewrite/(demo)\\w*$ /$1 redirect; } location /test{ default_type text/plain; return 200 test_success; } location /demo{ default_type text/plain; return 200 demo_success; } 访问http://192.168.200.133:8081/rewrite/testabc请求会被临时重定向，浏览器地址也会发生改变 permanent：将重写后的URI返回给客户端，状态码为301，指明是永久重定向URI,主要用在replacement变量不是以\"http://\"或者\"https://\"开头的情况。 location rewrite { rewrite ^/rewrite/(test)\\w*$ /$1 permanent; rewrite ^/rewrite/(demo)\\w*$ /$1 permanent; } location /test{ default_type text/plain; return 200 test_success; } location /demo{ default_type text/plain; return 200 demo_success; } 访问http://192.168.200.133:8081/rewrite/testabc请求会被永久重定向，浏览器地址也会发生改变 rewrite_log指令 该指令配置是否开启URL重写日志的输出功能。 语法 rewrite_log on\\ off; 默认值 rewrite_log off; 位置 http、server、location、if 开启后，URL重写的相关日志将以notice级别输出到error_log指令配置的日志文件汇总。 rewrite_log on; error_log logs/error.log notice; Rewrite的案例 域名跳转 》问题分析 先来看一个效果，如果我们想访问京东网站，大家都知道我们可以输入www.jd.com,但是同样的我们也可以输入www.360buy.com同样也都能访问到京东网站。这个其实是因为京东刚开始的时候域名就是www.360buy.com，后面由于各种原因把自己的域名换成了www.jd.com, 虽然说域名变量，但是对于以前只记住了www.360buy.com的用户来说，我们如何把这部分用户也迁移到我们新域名的访问上来，针对于这个问题，我们就可以使用Nginx中Rewrite的域名跳转来解决。 》环境准备 准备三个域名： vim /etc/hosts 127.0.0.1 www.itcast.cn 127.0.0.1 www.itheima.cn 127.0.0.1 www.itheima.com 通过Nginx实现访问www.itcast.cn server { listen 80; server_name www.itcast.cn; location /{ default_type text/html; return 200 'welcome to itcast'; } } 》通过Rewrite完成将www.ithema.com和www.itheima.cn的请求跳转到www.itcast.com server { listen 80; server_name www.itheima.com www.itheima.cn; rewrite ^/ http://www.itcast.cn; } 问题描述:如何在域名跳转的过程中携带请求的URI？ 修改配置信息 server { listen 80; server_name www.itheima.com www.itheima.cn; rewrite ^(.*) http://www.itcast.cn$1； } 域名镜像 镜像网站指定是将一个完全相同的网站分别放置到几台服务器上，并分别使用独立的URL进行访问。其中一台服务器上的网站叫主站，其他的为镜像网站。镜像网站和主站没有太大的区别，可以把镜像网站理解为主站的一个备份节点。可以通过镜像网站提供网站在不同地区的响应速度。镜像网站可以平衡网站的流量负载、可以解决网络宽带限制、封锁等。 而我们所说的域名镜像和网站镜像比较类似，上述案例中，将www.itheima.com和 www.itheima.cn都能跳转到www.itcast.cn，那么www.itcast.cn我们就可以把它起名叫主域名，其他两个就是我们所说的镜像域名，当然如果我们不想把整个网站做镜像，只想为其中某一个子目录下的资源做镜像，我们可以在location块中配置rewrite功能，比如: server { listen 80; server_name www.itheima.cn www.itheima.com; location /user { rewrite ^/user(.*)$ http://www.itcast.cn$1; } location /emp{ default_type text/html; return 200 'emp_success'; } } 独立域名 一个完整的项目包含多个模块，比如购物网站有商品搜索模块、商品详情模块和购物车模块等，那么我们如何为每一个模块设置独立的域名。 需求： http://search.itcast.com:81 访问商品搜索模块 http://item.itcast.com:82 访问商品详情模块 http://cart.itcast.com:83 访问商品购物车模块 server{ listen 81; server_name search.itcast.com; rewrite ^(.*) http://www.itcast.cn/search$1; } server{ listen 82; server_name item.itcast.com; rewrite ^(.*) http://www.itcast.cn/item$1; } server{ listen 83; server_name cart.itcast.com; rewrite ^(.*) http://www.itcast.cn/cart$1; } 目录自动添加\"/\" 问题描述 通过一个例子来演示下问题: server { listen 8082; server_name localhost; location /heima { root html; index index.html; } } 通过http://192.168.200.133:8082/heima和通过http://192.168.200.133:8082/heima/访问的区别？ 如果不加斜杠，Nginx服务器内部会自动做一个301的重定向，重定向的地址会有一个指令叫server_name_in_redirect on|off;来决定重定向的地址： 如果该指令为on 重定向的地址为: http://server_name:8082/目录名/; http://localhost:8082/heima/ 如果该指令为off 重定向的地址为: http://原URL中的域名:8082/目录名/; http://192.168.200.133:8082/heima/ 所以就拿刚才的地址来说，http://192.168.200.133:8082/heima如果不加斜杠，那么按照上述规则，如果指令server_name_in_redirect为on，则301重定向地址变为 http://localhost:8082/heima/,如果为off，则301重定向地址变为http://192.168.200.133:8082/heima/。后面这个是正常的，前面地址就有问题。 注意server_name_in_redirect指令在Nginx的0.8.48版本之前默认都是on，之后改成了off,所以现在我们这个版本不需要考虑这个问题，但是如果是0.8.48以前的版本并且server_name_in_redirect设置为on，我们如何通过rewrite来解决这个问题？ 解决方案 我们可以使用rewrite功能为末尾没有斜杠的URL自动添加一个斜杠 server { listen 80; server_name localhost; server_name_in_redirect on; location /heima { if (-d $request_filename){ rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; } } } 合并目录 搜索引擎优化(SEO)是一种利用搜索引擎的搜索规则来提高目的网站在有关搜索引擎内排名的方式。我们在创建自己的站点时，可以通过很多中方式来有效的提供搜索引擎优化的程度。其中有一项就包含URL的目录层级一般不要超过三层，否则的话不利于搜索引擎的搜索也给客户端的输入带来了负担，但是将所有的文件放在一个目录下又会导致文件资源管理混乱并且访问文件的速度也会随着文件增多而慢下来，这两个问题是相互矛盾的，那么使用rewrite如何解决上述问题? 举例，网站中有一个资源文件的访问路径时 /server/11/22/33/44/20.html,也就是说20.html存在于第5级目录下，如果想要访问该资源文件，客户端的URL地址就要写成 http://192.168.200.133/server/11/22/33/44/20.html, server { listen 8083; server_name localhost; location /server{ root html; } } 但是这个是非常不利于SEO搜索引擎优化的，同时客户端也不好记.使用rewrite我们可以进行如下配置: server { listen 8083; server_name localhost; location /server{ rewrite ^/server-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)\\.html$ /server/$1/$2/$3/$4/$5.html last; } } 这样的花，客户端只需要输入http://www.web.name/server-11-22-33-44-20.html就可以访问到20.html页面了。这里也充分利用了rewrite指令支持正则表达式的特性。 防盗链 防盗链之前我们已经介绍过了相关的知识，在rewrite中的防盗链和之前将的原理其实都是一样的，只不过通过rewrite可以将防盗链的功能进行完善下，当出现防盗链的情况，我们可以使用rewrite将请求转发到自定义的一张图片和页面，给用户比较好的提示信息。下面我们就通过根据文件类型实现防盗链的一个配置实例: location /images { root html; valid_referers none blocked www.baidu.com; if ($invalid_referer){ #return 403; rewrite ^/ /images/forbidden.png break; } } Nginx反向代理 Nginx反向代理概述 关于正向代理和反向代理，我们在前面的章节已经通过一张图给大家详细的介绍过了，简而言之就是正向代理代理的对象是客户端，反向代理代理的是服务端，这是两者之间最大的区别。 Nginx即可以实现正向代理，也可以实现反向代理。 我们先来通过一个小案例演示下Nginx正向代理的简单应用。 先提需求： (1)服务端的设置： http { log_format main 'client send request=>clientIp=$remote_addr serverIp=>$host'; server{ listen 80; server_name localhost; access_log logs/access.log main; location { root html; index index.html index.htm; } } } (2)使用客户端访问服务端，打开日志查看结果 (3)代理服务器设置： server { listen 82; resolver 8.8.8.8; location /{ proxy_pass http://$host$request_uri; } } (4)查看代理服务器的IP(192.168.200.146)和Nginx配置监听的端口(82) (5)在客户端配置代理服务器 (6)设置完成后，再次通过浏览器访问服务端 通过对比，上下两次的日志记录，会发现虽然我们是客户端访问服务端，但是如何使用了代理，那么服务端能看到的只是代理发送过去的请求，这样的化，就使用Nginx实现了正向代理的设置。 但是Nginx正向代理，在实际的应用中不是特别多，所以我们简单了解下，接下来我们继续学习Nginx的反向代理，这是Nginx比较重要的一个功能。 Nginx反向代理的配置语法 Nginx反向代理模块的指令是由ngx_http_proxy_module模块进行解析，该模块在安装Nginx的时候已经自己加装到Nginx中了，接下来我们把反向代理中的常用指令一一介绍下： proxy_pass proxy_set_header proxy_redirect proxy_pass 该指令用来设置被代理服务器地址，可以是主机名称、IP地址加端口号形式。 语法 proxy_pass URL; 默认值 — 位置 location URL:为要设置的被代理服务器地址，包含传输协议(http,https://)、主机名称或IP地址加端口号、URI等要素。 举例： proxy_pass http://www.baidu.com; location /server{} proxy_pass http://192.168.200.146; http://192.168.200.146/server/index.html proxy_pass http://192.168.200.146/; http://192.168.200.146/index.html 大家在编写proxy_pass的时候，后面的值要不要加\"/\"? 接下来通过例子来说明刚才我们提到的问题： server { listen 80; server_name localhost; location /{ #proxy_pass http://192.168.200.146; proxy_pass http://192.168.200.146/; } } 当客户端访问 http://localhost/index.html,效果是一样的 server{ listen 80; server_name localhost; location /server{ #proxy_pass http://192.168.200.146; proxy_pass http://192.168.200.146/; } } 当客户端访问 http://localhost/server/index.html 这个时候，第一个proxy_pass就变成了http://localhost/server/index.html 第二个proxy_pass就变成了http://localhost/index.html效果就不一样了。 proxy_set_header 该指令可以更改Nginx服务器接收到的客户端请求的请求头信息，然后将新的请求头发送给代理的服务器 语法 proxy_set_header field value; 默认值 proxy_set_header Host $proxy_host;proxy_set_header Connection close; 位置 http、server、location 需要注意的是，如果想要看到结果，必须在被代理的服务器上来获取添加的头信息。 被代理服务器： [192.168.200.146] server { listen 8080; server_name localhost; default_type text/plain; return 200 $http_username; } 代理服务器: [192.168.200.133] server { listen 8080; server_name localhost; location /server { proxy_pass http://192.168.200.146:8080/; proxy_set_header username TOM; } } 访问测试 proxy_redirect 该指令是用来重置头信息中的\"Location\"和\"Refresh\"的值。 语法 proxy_redirect redirect replacement;proxy_redirect default;proxy_redirect off; 默认值 proxy_redirect default; 位置 http、server、location 》为什么要用该指令? 服务端[192.168.200.146] server { listen 8081; server_name localhost; if (!-f $request_filename){ return 302 http://192.168.200.146; } } 代理服务端[192.168.200.133] server { listen 8081; server_name localhost; location / { proxy_pass http://192.168.200.146:8081/; proxy_redirect http://192.168.200.146 http://192.168.200.133; } } 》该指令的几组选项 proxy_redirect redirect replacement; redirect:目标,Location的值 replacement:要替换的值 proxy_redirect default; default; 将location块的uri变量作为replacement, 将proxy_pass变量作为redirect进行替换 proxy_redirect off; 关闭proxy_redirect的功能 Nginx反向代理实战 服务器1,2,3存在两种情况 第一种情况: 三台服务器的内容不一样。 第二种情况: 三台服务器的内容是一样。 如果服务器1、服务器2和服务器3的内容不一样，那我们可以根据用户请求来分发到不同的服务器。 代理服务器 server { listen 8082; server_name localhost; location /server1 { proxy_pass http://192.168.200.146:9001/; } location /server2 { proxy_pass http://192.168.200.146:9002/; } location /server3 { proxy_pass http://192.168.200.146:9003/; } } 服务端 server1 server { listen 9001; server_name localhost; default_type text/html; return 200 '192.168.200.146:9001' } server2 server { listen 9002; server_name localhost; default_type text/html; return 200 '192.168.200.146:9002' } server3 server { listen 9003; server_name localhost; default_type text/html; return 200 '192.168.200.146:9003' } 如果服务器1、服务器2和服务器3的内容是一样的，该如何处理? Nginx的安全控制 关于web服务器的安全是比较大的一个话题，里面所涉及的内容很多，Nginx反向代理是如何来提升web服务器的安全呢？ 安全隔离 什么是安全隔离? 通过代理分开了客户端到应用程序服务器端的连接，实现了安全措施。在反向代理之前设置防火墙，仅留一个入口供代理服务器访问。 如何使用SSL对流量进行加密 翻译成大家能熟悉的说法就是将我们常用的http请求转变成https请求，那么这两个之间的区别简单的来说两个都是HTTP协议，只不过https是身披SSL外壳的http. HTTPS是一种通过计算机网络进行安全通信的传输协议。它经由HTTP进行通信，利用SSL/TLS建立全通信，加密数据包，确保数据的安全性。 SSL(Secure Sockets Layer)安全套接层 TLS(Transport Layer Security)传输层安全 上述这两个是为网络通信提供安全及数据完整性的一种安全协议，TLS和SSL在传输层和应用层对网络连接进行加密。 总结来说为什么要使用https: http协议是明文传输数据，存在安全问题，而https是加密传输，相当于http+ssl，并且可以防止流量劫持。 Nginx要想使用SSL，需要满足一个条件即需要添加一个模块--with-http_ssl_module,而该模块在编译的过程中又需要OpenSSL的支持，这个我们之前已经准备好了。 nginx添加SSL的支持 （1）完成 --with-http_ssl_module模块的增量添加 》将原有/usr/local/nginx/sbin/nginx进行备份 》拷贝nginx之前的配置信息 》在nginx的安装源码进行配置指定对应模块 ./configure --with-http_ssl_module 》通过make模板进行编译 》将objs下面的nginx移动到/usr/local/nginx/sbin下 》在源码目录下执行 make upgrade进行升级，这个可以实现不停机添加新模块的功能 Nginx的SSL相关指令 因为刚才我们介绍过该模块的指令都是通过ngx_http_ssl_module模块来解析的。 》ssl:该指令用来在指定的服务器开启HTTPS,可以使用 listen 443 ssl,后面这种方式更通用些。 语法 ssl on \\ off; 默认值 ssl off; 位置 http、server server{ listen 443 ssl; } 》ssl_certificate:为当前这个虚拟主机指定一个带有PEM格式证书的证书。 语法 ssl_certificate file; 默认值 — 位置 http、server 》ssl_certificate_key:该指令用来指定PEM secret key文件的路径 语法 ssl_ceritificate_key file; 默认值 — 位置 http、server 》ssl_session_cache:该指令用来配置用于SSL会话的缓存 语法 ssl_sesion_cache off\\ none\\ [builtin[:size]] [shared:name:size] 默认值 ssl_session_cache none; 位置 http、server off:禁用会话缓存，客户端不得重复使用会话 none:禁止使用会话缓存，客户端可以重复使用，但是并没有在缓存中存储会话参数 builtin:内置OpenSSL缓存，仅在一个工作进程中使用。 shared:所有工作进程之间共享缓存，缓存的相关信息用name和size来指定 》ssl_session_timeout：开启SSL会话功能后，设置客户端能够反复使用储存在缓存中的会话参数时间。 语法 ssl_session_timeout time; 默认值 ssl_session_timeout 5m; 位置 http、server 》ssl_ciphers:指出允许的密码，密码指定为OpenSSL支持的格式 语法 ssl_ciphers ciphers; 默认值 ssl_ciphers HIGH:!aNULL:!MD5; 位置 http、server 可以使用openssl ciphers查看openssl支持的格式。 》ssl_prefer_server_ciphers：该指令指定是否服务器密码优先客户端密码 语法 ssl_perfer_server_ciphers on\\ off; 默认值 ssl_perfer_server_ciphers off; 位置 http、server 生成证书 方式一：使用阿里云/腾讯云等第三方服务进行购买。 方式二:使用openssl生成证书 先要确认当前系统是否有安装openssl openssl version 安装下面的命令进行生成 mkdir /root/cert cd /root/cert openssl genrsa -des3 -out server.key 1024 openssl req -new -key server.key -out server.csr cp server.key server.key.org openssl rsa -in server.key.org -out server.key openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt 开启SSL实例 server { listen 443 ssl; server_name localhost; ssl_certificate server.cert; ssl_certificate_key server.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / { root html; index index.html index.htm; } } （4）验证 反向代理系统调优 反向代理值Buffer和Cache Buffer翻译过来是\"缓冲\"，Cache翻译过来是\"缓存\"。 总结下： 相同点: 两种方式都是用来提供IO吞吐效率，都是用来提升Nginx代理的性能。 不同点: 缓冲主要用来解决不同设备之间数据传递速度不一致导致的性能低的问题，缓冲中的数据一旦此次操作完成后，就可以删除。 缓存主要是备份，将被代理服务器的数据缓存一份到代理服务器，这样的话，客户端再次获取相同数据的时候，就只需要从代理服务器上获取，效率较高，缓存中的数据可以重复使用，只有满足特定条件才会删除. （1）Proxy Buffer相关指令 》proxy_buffering :该指令用来开启或者关闭代理服务器的缓冲区； 语法 proxy_buffering on\\ off; 默认值 proxy_buffering on; 位置 http、server、location 》proxy_buffers:该指令用来指定单个连接从代理服务器读取响应的缓存区的个数和大小。 语法 proxy_buffers number size; 默认值 proxy_buffers 8 4k \\ 8K;(与系统平台有关) 位置 http、server、location number:缓冲区的个数 size:每个缓冲区的大小，缓冲区的总大小就是number*size 》proxy_buffer_size:该指令用来设置从被代理服务器获取的第一部分响应数据的大小。保持与proxy_buffers中的size一致即可，当然也可以更小。 语法 proxy_buffer_size size; 默认值 proxy_buffer_size 4k \\ 8k;(与系统平台有关) 位置 http、server、location 》proxy_busy_buffers_size：该指令用来限制同时处于BUSY状态的缓冲总大小。 语法 proxy_busy_buffers_size size; 默认值 proxy_busy_buffers_size 8k\\ 16K; 位置 http、server、location 》proxy_temp_path:当缓冲区存满后，仍未被Nginx服务器完全接受，响应数据就会被临时存放在磁盘文件上，该指令设置文件路径 语法 proxy_temp_path path; 默认值 proxy_temp_path proxy_temp; 位置 http、server、location 注意path最多设置三层。 》proxy_temp_file_write_size：该指令用来设置磁盘上缓冲文件的大小。 语法 proxy_temp_file_write_size size; 默认值 proxy_temp_file_write_size 8K\\ 16K; 位置 http、server、location 通用网站的配置 proxy_buffering on; proxy_buffer_size 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; 根据项目的具体内容进行相应的调节。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 10:41:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx_day04.html":{"url":"performance/nginx/Nginx_day04.html","title":"4.负载均衡","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 负载均衡概述 负载均衡的原理及处理流程 负载均衡的作用 负载均衡常用的处理方式 方式一:用户手动选择 方式二:DNS轮询方式 方式三:四/七层负载均衡 Nginx七层负载均衡 Nginx七层负载均衡的指令 upstream指令 server指令 Nginx七层负载均衡的实现流程 负载均衡状态 down backup max_conns max_fails和fail_timeout 负载均衡策略 轮询 weight加权[加权轮询] ip_hash least_conn url_hash fair 负载均衡案例 案例一：对所有请求实现一般轮询规则的负载均衡 案例二：对所有请求实现加权轮询规则的负载均衡 案例三：对特定资源实现负载均衡 案例四：对不同域名实现负载均衡 案例五：实现带有URL重写的负载均衡 Nginx四层负载均衡 添加stream模块的支持 Nginx四层负载均衡的指令 stream指令 upstream指令 四层负载均衡的案例 需求分析 Nginx缓存集成 缓存的概念 Nginx的web缓存服务 Nginx缓存设置的相关指令 proxy_cache_path proxy_cache proxy_cache_key proxy_cache_valid proxy_cache_min_uses proxy_cache_methods Nginx缓存设置案例 需求分析 步骤实现 Nginx缓存的清除 方式一:删除对应的缓存目录 方式二:使用第三方扩展模块 ngx_cache_purge Nginx设置资源不缓存 $cookie_nocache、$arg_nocache、$arg_comment 案例实现 Nginx负载均衡 负载均衡概述 早期的网站流量和业务功能都比较简单，单台服务器足以满足基本的需求，但是随着互联网的发展，业务流量越来越大并且业务逻辑也跟着越来越复杂，单台服务器的性能及单点故障问题就凸显出来了，因此需要多台服务器进行性能的水平扩展及避免单点故障出现。那么如何将不同用户的请求流量分发到不同的服务器上呢？ 负载均衡的原理及处理流程 系统的扩展可以分为纵向扩展和横向扩展。 纵向扩展是从单机的角度出发，通过增加系统的硬件处理能力来提升服务器的处理能力 横向扩展是通过添加机器来满足大型网站服务的处理能力。 这里面涉及到两个重要的角色分别是\"应用集群\"和\"负载均衡器\"。 应用集群：将同一应用部署到多台机器上，组成处理集群，接收负载均衡设备分发的请求，进行处理并返回响应的数据。 负载均衡器:将用户访问的请求根据对应的负载均衡算法，分发到集群中的一台服务器进行处理。 负载均衡的作用 1、解决服务器的高并发压力，提高应用程序的处理性能。 2、提供故障转移，实现高可用。 3、通过添加或减少服务器数量，增强网站的可扩展性。 4、在负载均衡器上进行过滤，可以提高系统的安全性。 负载均衡常用的处理方式 方式一:用户手动选择 这种方式比较原始，只要实现的方式就是在网站主页上面提供不同线路、不同服务器链接方式，让用户来选择自己访问的具体服务器，来实现负载均衡。 方式二:DNS轮询方式 DNS 域名系统（服务）协议（DNS）是一种分布式网络目录服务，主要用于域名与 IP 地址的相互转换。 大多域名注册商都支持对同一个主机名添加多条A记录，这就是DNS轮询，DNS服务器将解析请求按照A记录的顺序，随机分配到不同的IP上，这样就能完成简单的负载均衡。DNS轮询的成本非常低，在一些不重要的服务器，被经常使用。 如下是我们为某一个域名添加的IP地址，用2台服务器来做负载均衡。 验证: ping www.nginx521.cn 清空本地的dns缓存 ipconfig/flushdns 我们发现使用DNS来实现轮询，不需要投入过多的成本，虽然DNS轮询成本低廉，但是DNS负载均衡存在明显的缺点。 1.可靠性低 假设一个域名DNS轮询多台服务器，如果其中的一台服务器发生故障，那么所有的访问该服务器的请求将不会有所回应，即使你将该服务器的IP从DNS中去掉，但是由于各大宽带接入商将众多的DNS存放在缓存中，以节省访问时间，导致DNS不会实时更新。所以DNS轮流上一定程度上解决了负载均衡问题，但是却存在可靠性不高的缺点。 2.负载均衡不均衡 DNS负载均衡采用的是简单的轮询负载算法，不能区分服务器的差异，不能反映服务器的当前运行状态，不能做到为性能好的服务器多分配请求，另外本地计算机也会缓存已经解析的域名到IP地址的映射，这也会导致使用该DNS服务器的用户在一定时间内访问的是同一台Web服务器，从而引发Web服务器减的负载不均衡。 负载不均衡则会导致某几台服务器负荷很低，而另外几台服务器负荷确很高，处理请求的速度慢，配置高的服务器分配到的请求少，而配置低的服务器分配到的请求多。 方式三:四/七层负载均衡 介绍四/七层负载均衡之前，我们先了解一个概念，OSI(open system interconnection),叫开放式系统互联模型，这个是由国际标准化组织ISO指定的一个不基于具体机型、操作系统或公司的网络体系结构。该模型将网络通信的工作分为七层。 应用层：为应用程序提供网络服务。 表示层：对数据进行格式化、编码、加密、压缩等操作。 会话层：建立、维护、管理会话连接。 传输层：建立、维护、管理端到端的连接，常见的有TCP/UDP。 网络层：IP寻址和路由选择 数据链路层：控制网络层与物理层之间的通信。 物理层：比特流传输。 所谓四层负载均衡指的是OSI七层模型中的传输层，主要是基于IP+PORT的负载均衡 实现四层负载均衡的方式： 硬件：F5 BIG-IP、Radware等 软件：LVS、Nginx、Hayproxy等 所谓的七层负载均衡指的是在应用层，主要是基于虚拟的URL或主机IP的负载均衡 实现七层负载均衡的方式： 软件：Nginx、Hayproxy等 四层和七层负载均衡的区别 四层负载均衡数据包是在底层就进行了分发，而七层负载均衡数据包则在最顶端进行分发，所以四层负载均衡的效率比七层负载均衡的要高。 四层负载均衡不识别域名，而七层负载均衡识别域名。 处理四层和七层负载以为其实还有二层、三层负载均衡，二层是在数据链路层基于mac地址来实现负载均衡，三层是在网络层一般采用虚拟IP地址的方式实现负载均衡。 实际环境采用的模式 四层负载(LVS)+七层负载(Nginx) Nginx七层负载均衡 Nginx要实现七层负载均衡需要用到proxy_pass代理模块配置。Nginx默认安装支持这个模块，我们不需要再做任何处理。Nginx的负载均衡是在Nginx的反向代理基础上把用户的请求根据指定的算法分发到一组【upstream虚拟服务池】。 Nginx七层负载均衡的指令 upstream指令 该指令是用来定义一组服务器，它们可以是监听不同端口的服务器，并且也可以是同时监听TCP和Unix socket的服务器。服务器可以指定不同的权重，默认为1。 语法 upstream name {...} 默认值 — 位置 http server指令 该指令用来指定后端服务器的名称和一些参数，可以使用域名、IP、端口或者unix socket 语法 server name [paramerters] 默认值 — 位置 upstream Nginx七层负载均衡的实现流程 服务端设置 server { listen 9001; server_name localhost; default_type text/html; location /{ return 200 '192.168.200.146:9001'; } } server { listen 9002; server_name localhost; default_type text/html; location /{ return 200 '192.168.200.146:9002'; } } server { listen 9003; server_name localhost; default_type text/html; location /{ return 200 '192.168.200.146:9003'; } } 负载均衡器设置 upstream backend{ server 192.168.200.146:9091; server 192.168.200.146:9092; server 192.168.200.146:9093; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 负载均衡状态 代理服务器在负责均衡调度中的状态有以下几个： 状态 概述 down 当前的server暂时不参与负载均衡 backup 预留的备份服务器 max_fails 允许请求失败的次数 fail_timeout 经过max_fails失败后, 服务暂停时间 max_conns 限制最大的接收连接数 down down:将该服务器标记为永久不可用，那么该代理服务器将不参与负载均衡。 upstream backend{ server 192.168.200.146:9001 down; server 192.168.200.146:9002 server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 该状态一般会对需要停机维护的服务器进行设置。 backup backup:将该服务器标记为备份服务器，当主服务器不可用时，将用来传递请求。 upstream backend{ server 192.168.200.146:9001 down; server 192.168.200.146:9002 backup; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 此时需要将9094端口的访问禁止掉来模拟下唯一能对外提供访问的服务宕机以后，backup的备份服务器就要开始对外提供服务，此时为了测试验证，我们需要使用防火墙来进行拦截。 介绍一个工具firewall-cmd,该工具是Linux提供的专门用来操作firewall的。 查询防火墙中指定的端口是否开放 firewall-cmd --query-port=9001/tcp 如何开放一个指定的端口 firewall-cmd --permanent --add-port=9002/tcp 批量添加开发端口 firewall-cmd --permanent --add-port=9001-9003/tcp 如何移除一个指定的端口 firewall-cmd --permanent --remove-port=9003/tcp 重新加载 firewall-cmd --reload 其中 ​ --permanent表示设置为持久 ​ --add-port表示添加指定端口 ​ --remove-port表示移除指定端口 max_conns max_conns=number:用来设置代理服务器同时活动链接的最大数量，默认为0，表示不限制，使用该配置可以根据后端服务器处理请求的并发量来进行设置，防止后端服务器被压垮。 max_fails和fail_timeout max_fails=number:设置允许请求代理服务器失败的次数，默认为1。 fail_timeout=time:设置经过max_fails失败后，服务暂停的时间，默认是10秒。 upstream backend{ server 192.168.200.133:9001 down; server 192.168.200.133:9002 backup; server 192.168.200.133:9003 max_fails=3 fail_timeout=15; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 负载均衡策略 介绍完Nginx负载均衡的相关指令后，我们已经能实现将用户的请求分发到不同的服务器上，那么除了采用默认的分配方式以外，我们还能采用什么样的负载算法? Nginx的upstream支持如下六种方式的分配算法，分别是: 算法名称 说明 轮询 默认方式 weight 权重方式 ip_hash 依据ip分配方式 least_conn 依据最少连接方式 url_hash 依据URL分配方式 fair 依据响应时间方式 轮询 是upstream模块负载均衡默认的策略。每个请求会按时间顺序逐个分配到不同的后端服务器。轮询不需要额外的配置。 upstream backend{ server 192.168.200.146:9001 weight=1; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } weight加权[加权轮询] weight=number:用来设置服务器的权重，默认为1，权重数据越大，被分配到请求的几率越大；该权重值，主要是针对实际工作环境中不同的后端服务器硬件配置进行调整的，所有此策略比较适合服务器的硬件配置差别比较大的情况。 upstream backend{ server 192.168.200.146:9001 weight=10; server 192.168.200.146:9002 weight=5; server 192.168.200.146:9003 weight=3; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } ip_hash 当对后端的多台动态应用服务器做负载均衡时，ip_hash指令能够将某个客户端IP的请求通过哈希算法定位到同一台后端服务器上。这样，当来自某一个IP的用户在后端Web服务器A上登录后，在访问该站点的其他URL，能保证其访问的还是后端web服务器A。 语法 ip_hash; 默认值 — 位置 upstream upstream backend{ ip_hash; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 需要额外多说一点的是使用ip_hash指令无法保证后端服务器的负载均衡，可能导致有些后端服务器接收到的请求多，有些后端服务器接收的请求少，而且设置后端服务器权重等方法将不起作用。 least_conn 最少连接，把请求转发给连接数较少的后端服务器。轮询算法是把请求平均的转发给各个后端，使它们的负载大致相同；但是，有些请求占用的时间很长，会导致其所在的后端负载较高。这种情况下，least_conn这种方式就可以达到更好的负载均衡效果。 upstream backend{ least_conn; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 此负载均衡策略适合请求处理时间长短不一造成服务器过载的情况。 url_hash 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，要配合缓存命中来使用。同一个资源多次请求，可能会到达不同的服务器上，导致不必要的多次下载，缓存命中率不高，以及一些资源时间的浪费。而使用url_hash，可以使得同一个url（也就是同一个资源请求）会到达同一台服务器，一旦缓存住了资源，再此收到请求，就可以从缓存中读取。 upstream backend{ hash &request_uri; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 访问如下地址： http://192.168.200.133:8083/a http://192.168.200.133:8083/b http://192.168.200.133:8083/c fair fair采用的不是内建负载均衡使用的轮换的均衡算法，而是可以根据页面大小、加载时间长短智能的进行负载均衡。那么如何使用第三方模块的fair负载均衡策略。 upstream backend{ fair; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 但是如何直接使用会报错，因为fair属于第三方模块实现的负载均衡。需要添加nginx-upstream-fair,如何添加对应的模块: 下载nginx-upstream-fair模块 下载地址为: https://github.com/gnosek/nginx-upstream-fair 将下载的文件上传到服务器并进行解压缩 unzip nginx-upstream-fair-master.zip 重命名资源 mv nginx-upstream-fair-master fair 使用./configure命令将资源添加到Nginx模块中 ./configure --add-module=/root/fair 编译 make 编译可能会出现如下错误，ngx_http_upstream_srv_conf_t结构中缺少default_port 解决方案: 在Nginx的源码中 src/http/ngx_http_upstream.h,找到ngx_http_upstream_srv_conf_s，在模块中添加添加default_port属性 in_port_t default_port 然后再进行make. 更新Nginx ​ 6.1 将sbin目录下的nginx进行备份 mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginxold ​ 6.2 将安装目录下的objs中的nginx拷贝到sbin目录 cd objs cp nginx /usr/local/nginx/sbin ​ 6.3 更新Nginx cd ../ make upgrade 编译测试使用Nginx 上面介绍了Nginx常用的负载均衡的策略，有人说是5种，是把轮询和加权轮询归为一种，也有人说是6种。那么在咱们以后的开发中到底使用哪种，这个需要根据实际项目的应用场景来决定的。 负载均衡案例 案例一：对所有请求实现一般轮询规则的负载均衡 upstream backend{ server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 案例二：对所有请求实现加权轮询规则的负载均衡 upstream backend{ server 192.168.200.146:9001 weight=7; server 192.168.200.146:9002 weight=5; server 192.168.200.146:9003 weight=3; } server { listen 8083; server_name localhost; location /{ proxy_pass http://backend; } } 案例三：对特定资源实现负载均衡 upstream videobackend{ server 192.168.200.146:9001; server 192.168.200.146:9002; } upstream filebackend{ server 192.168.200.146:9003; server 192.168.200.146:9004; } server { listen 8084; server_name localhost; location /video/ { proxy_pass http://videobackend; } location /file/ { proxy_pass http://filebackend; } } 案例四：对不同域名实现负载均衡 upstream itcastbackend{ server 192.168.200.146:9001; server 192.168.200.146:9002; } upstream itheimabackend{ server 192.168.200.146:9003; server 192.168.200.146:9004; } server { listen 8085; server_name www.itcast.cn; location / { proxy_pass http://itcastbackend; } } server { listen 8086; server_name www.itheima.cn; location / { proxy_pass http://itheimabackend; } } 案例五：实现带有URL重写的负载均衡 upstream backend{ server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003; } server { listen 80; server_name localhost; location /file/ { rewrite ^(/file/.*) /server/$1 last; } location / { proxy_pass http://backend; } } Nginx四层负载均衡 Nginx在1.9之后，增加了一个stream模块，用来实现四层协议的转发、代理、负载均衡等。stream模块的用法跟http的用法类似，允许我们配置一组TCP或者UDP等协议的监听，然后通过proxy_pass来转发我们的请求，通过upstream添加多个后端服务，实现负载均衡。 四层协议负载均衡的实现，一般都会用到LVS、HAProxy、F5等，要么很贵要么配置很麻烦，而Nginx的配置相对来说更简单，更能快速完成工作。 添加stream模块的支持 Nginx默认是没有编译这个模块的，需要使用到stream模块，那么需要在编译的时候加上--with-stream。 完成添加--with-stream的实现步骤: 》将原有/usr/local/nginx/sbin/nginx进行备份 》拷贝nginx之前的配置信息 》在nginx的安装源码进行配置指定对应模块 ./configure --with-stream 》通过make模板进行编译 》将objs下面的nginx移动到/usr/local/nginx/sbin下 》在源码目录下执行 make upgrade进行升级，这个可以实现不停机添加新模块的功能 Nginx四层负载均衡的指令 stream指令 该指令提供在其中指定流服务器指令的配置文件上下文。和http指令同级。 语法 stream { ... } 默认值 — 位置 main upstream指令 该指令和http的upstream指令是类似的。 四层负载均衡的案例 需求分析 实现步骤 (1)准备Redis服务器,在一条服务器上准备三个Redis，端口分别是6379,6378 1.上传redis的安装包，redis-4.0.14.tar.gz 2.将安装包进行解压缩 tar -zxf redis-4.0.14.tar.gz 3.进入redis的安装包 cd redis-4.0.14 4.使用make和install进行编译和安装 make PREFIX=/usr/local/redis/redis01 install 5.拷贝redis配置文件redis.conf到/usr/local/redis/redis01/bin目录中 cp redis.conf /usr/local/redis/redis01/bin 6.修改redis.conf配置文件 port 6379 #redis的端口 daemonize yes #后台启动redis 7.将redis01复制一份为redis02 cd /usr/local/redis cp -r redis01 redis02 8.将redis02文件文件夹中的redis.conf进行修改 port 6378 #redis的端口 daemonize yes #后台启动redis 9.分别启动，即可获取两个Redis.并查看 ps -ef | grep redis 使用Nginx将请求分发到不同的Redis服务器上。 (2)准备Tomcat服务器. 1.上传tomcat的安装包，apache-tomcat-8.5.56.tar.gz 2.将安装包进行解压缩 tar -zxf apache-tomcat-8.5.56.tar.gz 3.进入tomcat的bin目录 cd apache-tomcat-8.5.56/bin ./startup nginx.conf配置 stream { upstream redisbackend { server 192.168.200.146:6379; server 192.168.200.146:6378; } upstream tomcatbackend { server 192.168.200.146:8080; } server { listen 81; proxy_pass redisbackend; } server { listen 82; proxy_pass tomcatbackend; } } 访问测试。 Nginx缓存集成 缓存的概念 缓存就是数据交换的缓冲区(称作:Cache),当用户要获取数据的时候，会先从缓存中去查询获取数据，如果缓存中有就会直接返回给用户，如果缓存中没有，则会发请求从服务器重新查询数据，将数据返回给用户的同时将数据放入缓存，下次用户就会直接从缓存中获取数据。 缓存其实在很多场景中都有用到，比如： 场景 作用 操作系统磁盘缓存 减少磁盘机械操作 数据库缓存 减少文件系统的IO操作 应用程序缓存 减少对数据库的查询 Web服务器缓存 减少对应用服务器请求次数 浏览器缓存 减少与后台的交互次数 缓存的优点 ​ 1.减少数据传输，节省网络流量，加快响应速度，提升用户体验； ​ 2.减轻服务器压力； ​ 3.提供服务端的高可用性； 缓存的缺点 ​ 1.数据的不一致 ​ 2.增加成本 本次课程注解讲解的是Nginx,Nginx作为web服务器，Nginx作为Web缓存服务器，它介于客户端和应用服务器之间，当用户通过浏览器访问一个URL时，web缓存服务器会去应用服务器获取要展示给用户的内容，将内容缓存到自己的服务器上，当下一次请求到来时，如果访问的是同一个URL，web缓存服务器就会直接将之前缓存的内容返回给客户端，而不是向应用服务器再次发送请求。web缓存降低了应用服务器、数据库的负载，减少了网络延迟，提高了用户访问的响应速度，增强了用户的体验。 Nginx的web缓存服务 Nginx是从0.7.48版开始提供缓存功能。Nginx是基于Proxy Store来实现的，其原理是把URL及相关组合当做Key,在使用MD5算法对Key进行哈希，得到硬盘上对应的哈希目录路径，从而将缓存内容保存在该目录中。它可以支持任意URL连接，同时也支持404/301/302这样的非200状态码。Nginx即可以支持对指定URL或者状态码设置过期时间，也可以使用purge命令来手动清除指定URL的缓存。 Nginx缓存设置的相关指令 Nginx的web缓存服务主要是使用ngx_http_proxy_module模块相关指令集来完成，接下来我们把常用的指令来进行介绍下。 proxy_cache_path 该指定用于设置缓存文件的存放路径 语法 proxy_cache_path path [levels=number] keys_zone=zone_name:zone_size [inactive=time][max_size=size]; 默认值 — 位置 http path:缓存路径地址,如： /usr/local/proxy_cache levels: 指定该缓存空间对应的目录，最多可以设置3层，每层取值为1|2如 : levels=1:2 缓存空间有两层目录，第一次是1个字母，第二次是2个字母 举例说明: itheima[key]通过MD5加密以后的值为 43c8233266edce38c2c9af0694e2107d levels=1:2 最终的存储路径为/usr/local/proxy_cache/d/07 levels=2:1:2 最终的存储路径为/usr/local/proxy_cache/7d/0/21 levels=2:2:2 最终的存储路径为??/usr/local/proxy_cache/7d/10/e2 keys_zone:用来为这个缓存区设置名称和指定大小，如： keys_zone=itcast:200m 缓存区的名称是itcast,大小为200M,1M大概能存储8000个keys inactive:指定缓存的数据多次时间未被访问就将被删除，如： inactive=1d 缓存数据在1天内没有被访问就会被删除 max_size:设置最大缓存空间，如果缓存空间存满，默认会覆盖缓存时间最长的资源，如: max_size=20g 配置实例: http{ proxy_cache_path /usr/local/proxy_cache keys_zone=itcast:200m levels=1:2:1 inactive=1d max_size=20g; } proxy_cache 该指令用来开启或关闭代理缓存，如果是开启则自定使用哪个缓存区来进行缓存。 语法 proxy_cache zone_name\\ off; 默认值 proxy_cache off; 位置 http、server、location zone_name：指定使用缓存区的名称 proxy_cache_key 该指令用来设置web缓存的key值，Nginx会根据key值MD5哈希存缓存。 语法 proxy_cache_key key; 默认值 proxy_cache_key $scheme$proxy_host$request_uri; 位置 http、server、location proxy_cache_valid 该指令用来对不同返回状态码的URL设置不同的缓存时间 语法 proxy_cache_valid [code ...] time; 默认值 — 位置 http、server、location 如： proxy_cache_valid 200 302 10m; proxy_cache_valid 404 1m; 为200和302的响应URL设置10分钟缓存，为404的响应URL设置1分钟缓存 proxy_cache_valid any 1m; 对所有响应状态码的URL都设置1分钟缓存 proxy_cache_min_uses 该指令用来设置资源被访问多少次后被缓存 语法 proxy_cache_min_uses number; 默认值 proxy_cache_min_uses 1; 位置 http、server、location proxy_cache_methods 该指令用户设置缓存哪些HTTP方法 语法 proxy_cache_methods GET\\ HEAD\\ POST; 默认值 proxy_cache_methods GET HEAD; 位置 http、server、location 默认缓存HTTP的GET和HEAD方法，不缓存POST方法。 Nginx缓存设置案例 需求分析 步骤实现 1.环境准备 应用服务器的环境准备 （1）在192.168.200.146服务器上的tomcat的webapps下面添加一个js目录，并在js目录中添加一个jquery.js文件 （2）启动tomcat （3）访问测试 http://192.168.200.146:8080/js/jquery.js Nginx的环境准备 （1）完成Nginx反向代理配置 http{ upstream backend{ server 192.168.200.146:8080; } server { listen 8080; server_name localhost; location / { proxy_pass http://backend/js/; } } } （2）完成Nginx缓存配置 4.添加缓存配置 http{ proxy_cache_path /usr/local/proxy_cache levels=2:1 keys_zone=itcast:200m inactive=1d max_size=20g; upstream backend{ server 192.168.200.146:8080; } server { listen 8080; server_name localhost; location / { proxy_cache itcast; proxy_cache_key itheima; proxy_cache_min_uses 5; proxy_cache_valid 200 5d; proxy_cache_valid 404 30s; proxy_cache_valid any 1m; add_header nginx-cache \"$upstream_cache_status\"; proxy_pass http://backend/js/; } } } Nginx缓存的清除 方式一:删除对应的缓存目录 rm -rf /usr/local/proxy_cache/...... 方式二:使用第三方扩展模块 ngx_cache_purge （1）下载ngx_cache_purge模块对应的资源包，并上传到服务器上。 ngx_cache_purge-2.3.tar.gz （2）对资源文件进行解压缩 tar -zxf ngx_cache_purge-2.3.tar.gz （3）修改文件夹名称，方便后期配置 mv ngx_cache_purge-2.3 purge （4）查询Nginx的配置参数 nginx -V （5）进入Nginx的安装目录，使用./configure进行参数配置 ./configure --add-module=/root/nginx/module/purge （6）使用make进行编译 make （7）将nginx安装目录的nginx二级制可执行文件备份 mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginxold （8）将编译后的objs中的nginx拷贝到nginx的sbin目录下 cp objs/nginx /usr/local/nginx/sbin （9）使用make进行升级 make upgrade （10）在nginx配置文件中进行如下配置 server{ location ~/purge(/.*) { proxy_cache_purge itcast itheima; } } Nginx设置资源不缓存 前面咱们已经完成了Nginx作为web缓存服务器的使用。但是我们得思考一个问题就是不是所有的数据都适合进行缓存。比如说对于一些经常发生变化的数据。如果进行缓存的话，就很容易出现用户访问到的数据不是服务器真实的数据。所以对于这些资源我们在缓存的过程中就需要进行过滤，不进行缓存。 Nginx也提供了这块的功能设置，需要使用到如下两个指令 proxy_no_cache 该指令是用来定义不将数据进行缓存的条件。 语法 proxy_no_cache string ...; 默认值 — 位置 http、server、location 配置实例 proxy_no_cache $cookie_nocache $arg_nocache $arg_comment; proxy_cache_bypass 该指令是用来设置不从缓存中获取数据的条件。 语法 proxy_cache_bypass string ...; 默认值 — 位置 http、server、location 配置实例 proxy_cache_bypass $cookie_nocache $arg_nocache $arg_comment; 上述两个指令都有一个指定的条件，这个条件可以是多个，并且多个条件中至少有一个不为空且不等于\"0\",则条件满足成立。上面给的配置实例是从官方网站获取的，里面使用到了三个变量，分别是$cookie_nocache、$arg_nocache、$arg_comment $cookie_nocache、$arg_nocache、$arg_comment 这三个参数分别代表的含义是: $cookie_nocache 指的是当前请求的cookie中键的名称为nocache对应的值 $arg_nocache和$arg_comment 指的是当前请求的参数中属性名为nocache和comment对应的属性值 案例演示下: log_format params $cookie_nocache | $arg_nocache | $arg_comment； server{ listen 8081; server_name localhost; location /{ access_log logs/access_params.log params; add_header Set-Cookie 'nocache=999'; root html; index index.html; } } 案例实现 设置不缓存资源的配置方案 server{ listen 8080; server_name localhost; location / { if ($request_uri ~ /.*\\.js$){ set $nocache 1; } proxy_no_cache $nocache $cookie_nocache $arg_nocache $arg_comment; proxy_cache_bypass $nocache $cookie_nocache $arg_nocache $arg_comment; } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 10:47:54 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx_day05.html":{"url":"performance/nginx/Nginx_day05.html","title":"5.集群搭建","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 Nginx与Tomcat部署 环境准备(Tomcat) 环境准备(Nginx) Nginx实现动静分离 需求分析 动静分离实现步骤 Nginx实现Tomcat集群搭建 Nginx高可用解决方案 Keepalived VRRP介绍 环境搭建 Keepalived配置文件介绍 访问测试 keepalived之vrrp_script Nginx制作下载站点 Nginx的用户认证模块 Nginx的扩展模块 Lua 概念 特性 应用场景 Lua的安装 Lua的语法 第一个Lua程序 Lua的注释 标识符 关键字 运算符 全局变量&局部变量 Lua数据类型 nil boolean number string table function thread userdata Lua控制结构 if then elseif else while循环 repeat循环 for循环 ngx_lua模块概念 ngx_lua模块环境准备 方式一:lua-nginx-module 方式二:OpenRestry 概述 安装 ngx_lua的使用 init_by_lua* init_worker_by_lua* set_by_lua* rewrite_by_lua* access_by_lua* content_by_lua* header_filter_by_lua* body_filter_by_lua* log_by_lua* balancer_by_lua* ssl_certificate_by_* 需求: ngx_lua操作Redis lua-resty-redis环境准备 ngx_lua操作Mysql lua-resty-mysql 使用lua-resty-mysql实现数据库的查询 使用lua-cjson处理查询结果 lua-resty-mysql实现数据库的增删改 综合小案例 Nginx实现服务器端集群搭建 Nginx与Tomcat部署 前面课程已经将Nginx的大部分内容进行了讲解，我们都知道了Nginx在高并发场景和处理静态资源是非常高性能的，但是在实际项目中除了静态资源还有就是后台业务代码模块，一般后台业务都会被部署在Tomcat，weblogic或者是websphere等web服务器上。那么如何使用Nginx接收用户的请求并把请求转发到后台web服务器？ 步骤分析: 1.准备Tomcat环境，并在Tomcat上部署一个web项目 2.准备Nginx环境，使用Nginx接收请求，并把请求分发到Tomat上 环境准备(Tomcat) 浏览器访问: http://192.168.200.146:8080/demo/index.html 获取动态资源的链接地址: http://192.168.200.146:8080/demo/getAddress 本次课程将采用Tomcat作为后台web服务器 （1）在Centos上准备一个Tomcat 1.Tomcat官网地址:https://tomcat.apache.org/ 2.下载tomcat,本次课程使用的是apache-tomcat-8.5.59.tar.gz 3.将tomcat进行解压缩 mkdir web_tomcat tar -zxf apache-tomcat-8.5.59.tar.gz -C /web_tomcat （2）准备一个web项目，将其打包为war 1.将资料中的demo.war上传到tomcat8目录下的webapps包下 2.将tomcat进行启动，进入tomcat8的bin目录下 ./startup.sh （3）启动tomcat进行访问测试。 静态资源: http://192.168.200.146:8080/demo/index.html 动态资源: http://192.168.200.146:8080/demo/getAddress 环境准备(Nginx) （1）使用Nginx的反向代理，将请求转给Tomcat进行处理。 upstream webservice { server 192.168.200.146:8080; } server{ listen 80; server_name localhost; location /demo { proxy_pass http://webservice; } } （2）启动访问测试 学习到这，可能大家会有一个困惑，明明直接通过tomcat就能访问，为什么还需要多加一个nginx，这样不是反而是系统的复杂度变高了么? 那接下来我们从两个方便给大家分析下这个问题， 第一个使用Nginx实现动静分离 第二个使用Nginx搭建Tomcat的集群 Nginx实现动静分离 什么是动静分离? 动:后台应用程序的业务处理 静:网站的静态资源(html,javaScript,css,images等文件) 分离:将两者进行分开部署访问，提供用户进行访问。举例说明就是以后所有和静态资源相关的内容都交给Nginx来部署访问，非静态内容则交个类似于Tomcat的服务器来部署访问。 为什么要动静分离? ​ 前面我们介绍过Nginx在处理静态资源的时候，效率是非常高的，而且Nginx的并发访问量也是名列前茅，而Tomcat则相对比较弱一些，所以把静态资源交个Nginx后，可以减轻Tomcat服务器的访问压力并提高静态资源的访问速度。 ​ 动静分离以后，降低了动态资源和静态资源的耦合度。如动态资源宕机了也不影响静态资源的展示。 如何实现动静分离? 实现动静分离的方式很多，比如静态资源可以部署到CDN、Nginx等服务器上，动态资源可以部署到Tomcat,weblogic或者websphere上。本次课程只要使用Nginx+Tomcat来实现动静分离。 需求分析 动静分离实现步骤 1.将demo.war项目中的静态资源都删除掉，重新打包生成一个war包，在资料中有提供。 2.将war包部署到tomcat中，把之前部署的内容删除掉 进入到tomcat的webapps目录下，将之前的内容删除掉 将新的war包复制到webapps下 将tomcat启动 3.在Nginx所在服务器创建如下目录，并将对应的静态资源放入指定的位置 其中index.html页面的内容如下: Title $(function(){ $.get('http://192.168.200.133/demo/getAddress',function(data){ $(\"#msg\").html(data); }); }); Nginx如何将请求转发到后端服务器 4.配置Nginx的静态资源与动态资源的访问 upstream webservice{ server 192.168.200.146:8080; } server { listen 80; server_name localhost; #动态资源 location /demo { proxy_pass http://webservice; } #静态资源 location ~/.*\\.(png|jpg|gif|js){ root html/web; gzip on; } location / { root html/web; index index.html index.htm; } } 5.启动测试，访问http://192.168.200.133/index.html 假如某个时间点，由于某个原因导致Tomcat后的服务器宕机了，我们再次访问Nginx,会得到如下效果，用户还是能看到页面，只是缺失了访问次数的统计，这就是前后端耦合度降低的效果，并且整个请求只和后的服务器交互了一次，js和images都直接从Nginx返回，提供了效率，降低了后的服务器的压力。 Nginx实现Tomcat集群搭建 在使用Nginx和Tomcat部署项目的时候，我们使用的是一台Nginx服务器和一台Tomcat服务器，效果图如下: 那么问题来了，如果Tomcat的真的宕机了，整个系统就会不完整，所以如何解决上述问题，一台服务器容易宕机，那就多搭建几台Tomcat服务器，这样的话就提升了后的服务器的可用性。这也就是我们常说的集群，搭建Tomcat的集群需要用到了Nginx的反向代理和赋值均衡的知识，具体如何来实现?我们先来分析下原理 环境准备： (1)准备3台tomcat,使用端口进行区分[实际环境应该是三台服务器]，修改server.ml，将端口修改分别修改为8080,8180,8280 (2)启动tomcat并访问测试， http://192.168.200.146:8080/demo/getAddress http://192.168.200.146:8180/demo/getAddress http://192.168.200.146:8280/demo/getAddress (3)在Nginx对应的配置文件中添加如下内容: upstream webservice{ server 192.168.200.146:8080; server 192.168.200.146:8180; server 192.168.200.146:8280; } 好了，完成了上述环境的部署，我们已经解决了Tomcat的高可用性，一台服务器宕机，还有其他两条对外提供服务，同时也可以实现后台服务器的不间断更新。但是新问题出现了，上述环境中，如果是Nginx宕机了呢，那么整套系统都将服务对外提供服务了，这个如何解决？ Nginx高可用解决方案 针对于上面提到的问题，我们来分析下要想解决上述问题，需要面临哪些问题? 需要两台以上的Nginx服务器对外提供服务，这样的话就可以解决其中一台宕机了，另外一台还能对外提供服务，但是如果是两台Nginx服务器的话，会有两个IP地址，用户该访问哪台服务器，用户怎么知道哪台是好的，哪台是宕机了的? Keepalived 使用Keepalived来解决，Keepalived 软件由 C 编写的，最初是专为 LVS 负载均衡软件设计的，Keepalived 软件主要是通过 VRRP 协议实现高可用功能。 VRRP介绍 VRRP（Virtual Route Redundancy Protocol）协议，翻译过来为虚拟路由冗余协议。VRRP协议将两台或多台路由器设备虚拟成一个设备，对外提供虚拟路由器IP,而在路由器组内部，如果实际拥有这个对外IP的路由器如果工作正常的话就是MASTER,MASTER实现针对虚拟路由器IP的各种网络功能。其他设备不拥有该虚拟IP，状态为BACKUP,处了接收MASTER的VRRP状态通告信息以外，不执行对外的网络功能。当主机失效时，BACKUP将接管原先MASTER的网络功能。 从上面的介绍信息获取到的内容就是VRRP是一种协议，那这个协议是用来干什么的？ 1.选择协议 VRRP可以把一个虚拟路由器的责任动态分配到局域网上的 VRRP 路由器中的一台。其中的虚拟路由即Virtual路由是由VRRP路由群组创建的一个不真实存在的路由，这个虚拟路由也是有对应的IP地址。而且VRRP路由1和VRRP路由2之间会有竞争选择，通过选择会产生一个Master路由和一个Backup路由。 2.路由容错协议 Master路由和Backup路由之间会有一个心跳检测，Master会定时告知Backup自己的状态，如果在指定的时间内，Backup没有接收到这个通知内容，Backup就会替代Master成为新的Master。Master路由有一个特权就是虚拟路由和后端服务器都是通过Master进行数据传递交互的，而备份节点则会直接丢弃这些请求和数据，不做处理，只是去监听Master的状态 用了Keepalived后，解决方案如下: 环境搭建 环境准备 VIP IP 主机名 主/从 192.168.200.133 keepalived1 Master 192.168.200.222 192.168.200.122 keepalived2 Backup keepalived的安装 步骤1:从官方网站下载keepalived,官网地址https://keepalived.org/ 步骤2:将下载的资源上传到服务器 keepalived-2.0.20.tar.gz 步骤3:创建keepalived目录，方便管理资源 mkdir keepalived 步骤4:将压缩文件进行解压缩，解压缩到指定的目录 tar -zxf keepalived-2.0.20.tar.gz -C keepalived/ 步骤5:对keepalived进行配置，编译和安装 cd keepalived/keepalived-2.0.20 ./configure --sysconf=/etc --prefix=/usr/local make && make install 安装完成后，有两个文件需要我们认识下，一个是 /etc/keepalived/keepalived.conf(keepalived的系统配置文件，我们主要操作的就是该文件)，一个是/usr/local/sbin目录下的keepalived,是系统配置脚本，用来启动和关闭keepalived Keepalived配置文件介绍 打开keepalived.conf配置文件 这里面会分三部，第一部分是global全局配置、第二部分是vrrp相关配置、第三部分是LVS相关配置。 本次课程主要是使用keepalived实现高可用部署，没有用到LVS，所以我们重点关注的是前两部分 global全局部分： global_defs { #通知邮件，当keepalived发送切换时需要发email给具体的邮箱地址 notification_email { tom@itcast.cn jerry@itcast.cn } #设置发件人的邮箱信息 notification_email_from zhaomin@itcast.cn #指定smpt服务地址 smtp_server 192.168.200.1 #指定smpt服务连接超时时间 smtp_connect_timeout 30 #运行keepalived服务器的一个标识，可以用作发送邮件的主题信息 router_id LVS_DEVEL #默认是不跳过检查。检查收到的VRRP通告中的所有地址可能会比较耗时，设置此命令的意思是，如果通告与接收的上一个通告来自相同的master路由器，则不执行检查(跳过检查) vrrp_skip_check_adv_addr #严格遵守VRRP协议。 vrrp_strict #在一个接口发送的两个免费ARP之间的延迟。可以精确到毫秒级。默认是0 vrrp_garp_interval 0 #在一个网卡上每组na消息之间的延迟时间，默认为0 vrrp_gna_interval 0 } VRRP部分，该部分可以包含以下四个子模块 1. vrrp_script 2. vrrp_sync_group 3. garp_group 4. vrrp_instance 我们会用到第一个和第四个， #设置keepalived实例的相关信息，VI_1为VRRP实例名称 vrrp_instance VI_1 { state MASTER #有两个值可选MASTER主 BACKUP备 interface ens33 #vrrp实例绑定的接口，用于发送VRRP包[当前服务器使用的网卡名称] virtual_router_id 51#指定VRRP实例ID，范围是0-255 priority 100 #指定优先级，优先级高的将成为MASTER advert_int 1 #指定发送VRRP通告的间隔，单位是秒 authentication { #vrrp之间通信的认证信息 auth_type PASS #指定认证方式。PASS简单密码认证(推荐) auth_pass 1111 #指定认证使用的密码，最多8位 } virtual_ipaddress { #虚拟IP地址设置虚拟IP地址，供用户访问使用，可设置多个，一行一个 192.168.200.222 } } 配置内容如下: 服务器1 global_defs { notification_email { tom@itcast.cn jerry@itcast.cn } notification_email_from zhaomin@itcast.cn smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keepalived1 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.200.222 } } 服务器2 ! Configuration File for keepalived global_defs { notification_email { tom@itcast.cn jerry@itcast.cn } notification_email_from zhaomin@itcast.cn smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keepalived2 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.200.222 } } 访问测试 启动keepalived之前，咱们先使用命令 ip a,查看192.168.200.133和192.168.200.122这两台服务器的IP情况。 分别启动两台服务器的keepalived cd /usr/local/sbin ./keepalived 再次通过 ip a查看ip 当把192.168.200.133服务器上的keepalived关闭后，再次查看ip 通过上述的测试，我们会发现，虚拟IP(VIP)会在MASTER节点上，当MASTER节点上的keepalived出问题以后，因为BACKUP无法收到MASTER发出的VRRP状态通过信息，就会直接升为MASTER。VIP也会\"漂移\"到新的MASTER。 上面测试和Nginx有什么关系? 我们把192.168.200.133服务器的keepalived再次启动下，由于它的优先级高于服务器192.168.200.122的，所有它会再次成为MASTER，VIP也会\"漂移\"过去，然后我们再次通过浏览器访问: http://192.168.200.222/ 如果把192.168.200.133服务器的keepalived关闭掉，再次访问相同的地址 效果实现了以后， 我们会发现要想让vip进行切换，就必须要把服务器上的keepalived进行关闭，而什么时候关闭keepalived呢?应该是在keepalived所在服务器的nginx出现问题后，把keepalived关闭掉，就可以让VIP执行另外一台服务器，但是现在这所有的操作都是通过手动来完成的，我们如何能让系统自动判断当前服务器的nginx是否正确启动，如果没有，要能让VIP自动进行\"漂移\"，这个问题该如何解决? keepalived之vrrp_script keepalived只能做到对网络故障和keepalived本身的监控，即当出现网络故障或者keepalived本身出现问题时，进行切换。但是这些还不够，我们还需要监控keepalived所在服务器上的其他业务，比如Nginx,如果Nginx出现异常了，仅仅keepalived保持正常，是无法完成系统的正常工作的，因此需要根据业务进程的运行状态决定是否需要进行主备切换，这个时候，我们可以通过编写脚本对业务进程进行检测监控。 实现步骤: 在keepalived配置文件中添加对应的配置像 vrrp_script 脚本名称 { script \"脚本位置\" interval 3 #执行时间间隔 weight -20 #动态调整vrrp_instance的优先级 } 编写脚本 ck_nginx.sh #!/bin/bash num=`ps -C nginx --no-header | wc -l` if [ $num -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header | wc -l` -eq 0 ]; then killall keepalived fi fi Linux ps命令用于显示当前进程 (process) 的状态。 -C(command) :指定命令的所有进程 --no-header 排除标题 为脚本文件设置权限 chmod 755 ck_nginx.sh 将脚本添加到 vrrp_script ck_nginx { script \"/etc/keepalived/ck_nginx.sh\" #执行脚本的位置 interval 2 #执行脚本的周期，秒为单位 weight -20 #权重的计算方式 } vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 10 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.200.111 } track_script { ck_nginx } } 如果效果没有出来，可以使用 tail -f /var/log/messages查看日志信息，找对应的错误信息。 测试 问题思考: 通常如果master服务死掉后backup会变成master，但是当master服务又好了的时候 master此时会抢占VIP，这样就会发生两次切换对业务繁忙的网站来说是不好的。所以我们要在配置文件加入 nopreempt 非抢占，但是这个参数只能用于state 为backup，故我们在用HA的时候最好master 和backup的state都设置成backup 让其通过priority来竞争。 Nginx制作下载站点 首先我们先要清楚什么是下载站点? 我们先来看一个网站http://nginx.org/download/这个我们刚开始学习Nginx的时候给大家看过这样的网站，该网站主要就是用来提供用户来下载相关资源的网站，就叫做下载网站。 如何制作一个下载站点: nginx使用的是模块ngx_http_autoindex_module来实现的，该模块处理以斜杠(\"/\")结尾的请求，并生成目录列表。 nginx编译的时候会自动加载该模块，但是该模块默认是关闭的，我们需要使用下来指令来完成对应的配置 （1）autoindex:启用或禁用目录列表输出 语法 autoindex on\\ off; 默认值 autoindex off; 位置 http、server、location （2）autoindex_exact_size:对应HTLM格式，指定是否在目录列表展示文件的详细大小 默认为on，显示出文件的确切大小，单位是bytes。 改为off后，显示出文件的大概大小，单位是kB或者MB或者GB 语法 autoindex_exact_size on\\ off; 默认值 autoindex_exact_size on; 位置 http、server、location （3）autoindex_format：设置目录列表的格式 语法 autoindex_format html\\ xml\\ json\\ jsonp; 默认值 autoindex_format html; 位置 http、server、location 注意:该指令在1.7.9及以后版本中出现 （4）autoindex_localtime:对应HTML格式，是否在目录列表上显示时间。 默认为off，显示的文件时间为GMT时间。 改为on后，显示的文件时间为文件的服务器时间 语法 autoindex_localtime on \\ off; 默认值 autoindex_localtime off; 位置 http、server、location 配置方式如下: location /download{ root /usr/local; autoindex on; autoindex_exact_size on; autoindex_format html; autoindex_localtime on; } XML/JSON格式[一般不用这两种方式] Nginx的用户认证模块 对应系统资源的访问，我们往往需要限制谁能访问，谁不能访问。这块就是我们通常所说的认证部分，认证需要做的就是根据用户输入的用户名和密码来判定用户是否为合法用户，如果是则放行访问，如果不是则拒绝访问。 Nginx对应用户认证这块是通过ngx_http_auth_basic_module模块来实现的，它允许通过使用\"HTTP基本身份验证\"协议验证用户名和密码来限制对资源的访问。默认情况下nginx是已经安装了该模块，如果不需要则使用--without-http_auth_basic_module。 该模块的指令比较简单， （1）auth_basic:使用“ HTTP基本认证”协议启用用户名和密码的验证 语法 auth_basic string\\ off; 默认值 auth_basic off; 位置 http,server,location,limit_except 开启后，服务端会返回401，指定的字符串会返回到客户端，给用户以提示信息，但是不同的浏览器对内容的展示不一致。 （2）auth_basic_user_file:指定用户名和密码所在文件 语法 auth_basic_user_file file; 默认值 — 位置 http,server,location,limit_except 指定文件路径，该文件中的用户名和密码的设置，密码需要进行加密。可以采用工具自动生成 实现步骤: 1.nginx.conf添加如下内容 location /download{ root /usr/local; autoindex on; autoindex_exact_size on; autoindex_format html; autoindex_localtime on; auth_basic 'please input your auth'; auth_basic_user_file htpasswd; } 2.我们需要使用htpasswd工具生成 yum install -y httpd-tools htpasswd -c /usr/local/nginx/conf/htpasswd username //创建一个新文件记录用户名和密码 htpasswd -b /usr/local/nginx/conf/htpasswd username password //在指定文件新增一个用户名和密码 htpasswd -D /usr/local/nginx/conf/htpasswd username //从指定文件删除一个用户信息 htpasswd -v /usr/local/nginx/conf/htpasswd username //验证用户名和密码是否正确 上述方式虽然能实现用户名和密码的验证，但是大家也看到了，所有的用户名和密码信息都记录在文件里面，如果用户量过大的话，这种方式就显得有点麻烦了，这时候我们就得通过后台业务代码来进行用户权限的校验了。 Nginx的扩展模块 Nginx是可扩展的，可用于处理各种使用场景。本节中，我们将探讨使用Lua扩展Nginx的功能。 Lua 概念 Lua是一种轻量、小巧的脚本语言，用标准C语言编写并以源代码形式开发。设计的目的是为了嵌入到其他应用程序中，从而为应用程序提供灵活的扩展和定制功能。 特性 跟其他语言进行比较，Lua有其自身的特点： （1）轻量级 Lua用标准C语言编写并以源代码形式开发，编译后仅仅一百余千字节，可以很方便的嵌入到其他程序中。 （2）可扩展 Lua提供非常丰富易于使用的扩展接口和机制，由宿主语言(通常是C或C++)提供功能，Lua可以使用它们，就像内置的功能一样。 （3）支持面向过程编程和函数式编程 应用场景 Lua在不同的系统中得到大量应用，场景的应用场景如下: 游戏开发、独立应用脚本、web应用脚本、扩展和数据库插件、系统安全上。 Lua的安装 在linux上安装Lua非常简单，只需要下载源码包并在终端解压、编译即可使用。 Lua的官网地址为:https://www.lua.org 点击download可以找到对应版本的下载地址，我们本次课程采用的是lua-5.3.5,其对应的资源链接地址为https://www.lua.org/ftp/lua-5.4.1.tar.gz,也可以使用wget命令直接下载: wget https://www.lua.org/ftp/lua-5.4.1.tar.gz 编译安装 cd lua-5.4.1 make linux test make install 如果在执行make linux test失败，报如下错误: 说明当前系统缺少libreadline-dev依赖包，需要通过命令来进行安装 yum install -y readline-devel 验证是否安装成功 lua -v Lua的语法 Lua和C/C++语法非常相似，整体上比较清晰，简洁。条件语句、循环语句、函数调用都与C/C++基本一致。如果对C/C++不太熟悉的同学来说，也没关系，因为天下语言是一家，基本上理解起来都不会太困难。我们一点点来讲。 第一个Lua程序 大家需要知道的是，Lua有两种交互方式，分别是:交互式和脚本式，这两者的区别，下面我们分别来讲解下： 交互式之HELLOWORLD 交互式是指可以在命令行输入程序，然后回车就可以看到运行的效果。 Lua交互式编程模式可以通过命令lua -i 或lua来启用: 在命令行中key输入如下命令，并按回车,会有输出在控制台： 脚本式之HELLOWORLD 脚本式是将代码保存到一个以lua为扩展名的文件中并执行的方式。 方式一: 我们需要一个文件名为 hello.lua,在文件中添加要执行的代码，然后通过命令 lua hello.lua来执行，会在控制台输出对应的结果。 hello.lua print(\"Hello World!!\") 方式二: 将hello.lua做如下修改 #!/usr/local/bin/lua print(\"Hello World!!!\") 第一行用来指定Lua解释器所在位置为 /usr/local/bin/lua，加上#号标记解释器会忽略它。一般情况下#!就是用来指定用哪个程序来运行本文件。但是hello.lua并不是一个可执行文件，需要通过chmod来设置可执行权限，最简单的方式为: chmod 755 hello.lua 然后执行该文件 ./hello.lua 补充一点，如果想在交互式中运行脚本式的hello.lua中的内容，我们可以使用一个dofile函数，如： dofile(\"lua_demo/hello.lua\") 注意:在Lua语言中，连续语句之间的分隔符并不是必须的，也就是说后面不需要加分号，当然加上也不会报错， 在Lua语言中，表达式之间的换行也起不到任何作用。如以下四个写法，其实都是等效的 写法一 a=1 b=a+2 写法二 a=1; b=a+2; 写法三 a=1; b=a+2; 写法四 a=1 b=a+2 不建议使用第四种方式，可读性太差。 Lua的注释 关于Lua的注释要分两种，第一种是单行注释，第二种是多行注释。 单行注释的语法为： --注释内容 多行注释的语法为: --[[ 注释内容 注释内容 --]] 如果想取消多行注释，只需要在第一个--之前在加一个-即可，如： ---[[ 注释内容 注释内容 --]] 标识符 换句话说标识符就是我们的变量名，Lua定义变量名以一个字母 A 到 Z 或 a 到 z 或下划线 _ 开头后加上0个或多个字母，下划线，数字（0到9）。这块建议大家最好不要使用下划线加大写字母的标识符，因为Lua的保留字也是这样定义的，容易发生冲突。注意Lua是区分大小写字母的。 A0 关键字 下列是Lua的关键字，大家在定义常量、变量或其他用户自定义标识符都要避免使用以下这些关键字： and break do else elseif end false for function if in local nil not or repeat return then true until while goto 一般约定，以下划线开头连接一串大写字母的名字（比如 _VERSION）被保留用于 Lua 内部全局变量。这个也是上面我们不建议这么定义标识符的原因。 运算符 Lua中支持的运算符有算术运算符、关系运算符、逻辑运算符、其他运算符。 算术运算符: + 加法 - 减法 * 乘法 / 除法 % 取余 ^ 乘幂 - 负号 例如: 10+20 -->30 20-10 -->10 10*20 -->200 20/10 -->2 3%2 -->1 10^2 -->100 -10 -->-10 关系运算符 == 等于 ~= 不等于 > 大于 = 大于等于 例如: 10==10 -->true 10~=10 -->false 20>10 -->true 20false 20>=10 -->true 20false 逻辑运算符 and 逻辑与 A and B && or 逻辑或 A or B || not 逻辑非 取反，如果为true,则返回false ! 逻辑运算符可以作为if的判断条件，返回的结果如下: A = true B = true A and B -->true A or B -->true not A -->false A = true B = false A and B -->false A or B -->true not A -->false A = false B = true A and B -->false A or B -->true not A -->true 其他运算符 .. 连接两个字符串 # 一元预算法，返回字符串或表的长度 例如: > \"HELLO \"..\"WORLD\" -->HELLO WORLD > #\"HELLO\" -->5 全局变量&局部变量 在Lua语言中，全局变量无须声明即可使用。在默认情况下，变量总是认为是全局的，如果未提前赋值，默认为nil: 要想声明一个局部变量，需要使用local来声明 Lua数据类型 Lua有8个数据类型 nil(空，无效值) boolean(布尔，true/false) number(数值) string(字符串) function(函数) table（表） thread(线程) userdata（用户数据） 可以使用type函数测试给定变量或者的类型： print(type(nil)) -->nil print(type(true)) --> boolean print(type(1.1*1.1)) --> number print(type(\"Hello world\")) --> string print(type(io.stdin)) -->userdata print(type(print)) --> function print(type(type)) -->function print(type{}) -->table print(type(type(X))) --> string nil nil是一种只有一个nil值的类型，它的作用可以用来与其他所有值进行区分，也可以当想要移除一个变量时，只需要将该变量名赋值为nil,垃圾回收就会会释放该变量所占用的内存。 boolean boolean类型具有两个值，true和false。boolean类型一般被用来做条件判断的真与假。在Lua语言中，只会将false和nil视为假，其他的都视为真，特别是在条件检测中0和空字符串都会认为是真，这个和我们熟悉的大多数语言不太一样。 number 在Lua5.3版本开始，Lua语言为数值格式提供了两种选择:integer(整型)和float(双精度浮点型)[和其他语言不太一样，float不代表单精度类型]。 数值常量的表示方式: >4 -->4 >0.4 -->0.4 >4.75e-3 -->0.00475 >4.75e3 -->4750 不管是整型还是双精度浮点型，使用type()函数来取其类型，都会返回的是number >type(3) -->number >type(3.3) -->number 所以它们之间是可以相互转换的，同时，具有相同算术值的整型值和浮点型值在Lua语言中是相等的 string Lua语言中的字符串即可以表示单个字符，也可以表示一整本书籍。在Lua语言中，操作100K或者1M个字母组成的字符串的程序很常见。 可以使用单引号或双引号来声明字符串 >a = \"hello\" >b = 'world' >print(a) -->hello >print(b) -->world 如果声明的字符串比较长或者有多行，则可以使用如下方式进行声明 html = [[ Lua-string Lua ]] table ​ table是Lua语言中最主要和强大的数据结构。使用表， Lua 语言可以以一种简单、统一且高效的方式表示数组、集合、记录和其他很多数据结构。 Lua语言中的表本质上是一种辅助数组。这种数组比Java中的数组更加灵活，可以使用数值做索引，也可以使用字符串或其他任意类型的值作索引(除nil外)。 创建表的最简单方式: > a = {} 创建数组: ​ 我们都知道数组就是相同数据类型的元素按照一定顺序排列的集合，那么使用table如何创建一个数组呢? >arr = {\"TOM\",\"JERRY\",\"ROSE\"} ​ 要想获取数组中的值，我们可以通过如下内容来获取: print(arr[0]) nil print(arr[1]) TOM print(arr[2]) JERRY print(arr[3]) ROSE ​ 从上面的结果可以看出来，数组的下标默认是从1开始的。所以上述创建数组，也可以通过如下方式来创建 >arr = {} >arr[1] = \"TOM\" >arr[2] = \"JERRY\" >arr[3] = \"ROSE\" 上面我们说过了，表的索引即可以是数字，也可以是字符串等其他的内容，所以我们也可以将索引更改为字符串来创建 >arr = {} >arr[\"X\"] = 10 >arr[\"Y\"] = 20 >arr[\"Z\"] = 30 当然，如果想要获取这些数组中的值，可以使用下面的方式 方式一 >print(arr[\"X\"]) >print(arr[\"Y\"]) >print(arr[\"Z\"]) 方式二 >print(arr.X) >print(arr.Y) >print(arr.Z) 当前table的灵活不进于此，还有更灵活的声明方式 >arr = {\"TOM\",X=10,\"JERRY\",Y=20,\"ROSE\",Z=30} 如何获取上面的值? TOM : arr[1] 10 : arr[\"X\"] | arr.X JERRY: arr[2] 20 : arr[\"Y\"] | arr.Y ROESE? function 在 Lua语言中，函数（ Function ）是对语句和表达式进行抽象的主要方式。 定义函数的语法为: function functionName(params) end 函数被调用的时候，传入的参数个数与定义函数时使用的参数个数不一致的时候，Lua 语言会通过 抛弃多余参数和将不足的参数设为 nil 的方式来调整参数的个数。 function f(a,b) print(a,b) end f() --> nil nil f(2) --> 2 nil f(2,6) --> 2 6 f(2.6.8) --> 2 6 (8被丢弃) 可变长参数函数 function add(...) a,b,c=... print(a) print(b) print(c) end add(1,2,3) --> 1 2 3 函数返回值可以有多个，这点和Java不太一样 function f(a,b) return a,b end x,y=f(11,22) --> x=11,y=22 thread thread翻译过来是线程的意思，在Lua中，thread用来表示执行的独立线路，用来执行协同程序。 userdata userdata是一种用户自定义数据，用于表示一种由应用程序或C/C++语言库所创建的类型。 Lua控制结构 Lua 语言提供了一组精简且常用的控制结构，包括用于条件执行的证 以及用于循环的 while、 repeat 和 for。 所有的控制结构语法上都有一个显式的终结符： end 用于终结 if、 for 及 while 结构， until 用于终结 repeat 结构。 if then elseif else if语句先测试其条件，并根据条件是否满足执行相应的 then 部分或 else 部分。 else 部分 是可选的。 function testif(a) if a>0 then print(\"a是正数\") end end function testif(a) if a>0 then print(\"a是正数\") else print(\"a是负数\") end end 如果要编写嵌套的 if 语句，可以使用 elseif。 它类似于在 else 后面紧跟一个if。根据传入的年龄返回不同的结果，如 age18 , age 45 , age60 老年人 function show(age) if age18 and age45 and age60 then return \"老年人\" end end while循环 顾名思义，当条件为真时 while 循环会重复执行其循环体。 Lua 语言先测试 while 语句 的条件，若条件为假则循环结束；否则， Lua 会执行循环体并不断地重复这个过程。 语法： while 条件 do 循环体 end 例子:实现数组的循环 function testWhile() local i = 1 while i repeat循环 顾名思义， repeat-until语句会重复执行其循环体直到条件为真时结束。 由于条件测试在循环体之后执行，所以循环体至少会执行一次。 语法 repeat 循环体 until 条件 function testRepeat() local i = 10 repeat print(i) i=i-1 until i for循环 数值型for循环 语法 for param=exp1,exp2,exp3 do 循环体 end param的值从exp1变化到exp2之前的每次循环会执行 循环体，并在每次循环结束后将步长(step)exp3增加到param上。exp3可选，如果不设置默认为1 for i = 1,100,10 do print(i) end 泛型for循环 泛型for循环通过一个迭代器函数来遍历所有值，类似于java中的foreach语句。 语法 for i,v in ipairs(x) do 循环体 end i是数组索引值，v是对应索引的数组元素值，ipairs是Lua提供的一个迭代器函数，用来迭代数组，x是要遍历的数组。 例如: arr = {\"TOME\",\"JERRY\",\"ROWS\",\"LUCY\"} for i,v in ipairs(arr) do print(i,v) end 上述实例输出的结果为 1 TOM 2 JERRY 3 ROWS 4 LUCY 但是如果将arr的值进行修改为 arr = {\"TOME\",\"JERRY\",\"ROWS\",x=\"JACK\",\"LUCY\"} 同样的代码在执行的时候，就只能看到和之前一样的结果，而其中的x为JACK就无法遍历出来，缺失了数据，如果解决呢? 我们可以将迭代器函数变成pairs,如 for i,v in pairs(arr) do print(i,v) end 上述实例就输出的结果为 1 TOM 2 JERRY 3 ROWS 4 LUCY x JACK ngx_lua模块概念 淘宝开发的ngx_lua模块通过将lua解释器集成进Nginx，可以采用lua脚本实现业务逻辑，由于lua的紧凑、快速以及内建协程，所以在保证高并发服务能力的同时极大地降低了业务逻辑实现成本。 ngx_lua模块环境准备 方式一:lua-nginx-module LuaJIT是采用C语言编写的Lua代表的解释器。 官网地址为:http://luajit.org/ 在官网上找到对应的下载地址:http://luajit.org/download/LuaJIT-2.0.5.tar.gz 在centos上使用wget来下载: wget http://luajit.org/download/LuaJIT-2.0.5.tar.gz 将下载的资源进行解压: tar -zxf LuaJIT-2.0.5.tar.gz 进入解压的目录: cd LuaJIT-2.0.5 执行编译和安装: make && make install 下载lua-nginx-module 下载地址:https://github.com/openresty/lua-nginx-module/archive/v0.10.16rc4.tar.gz 在centos上使用wget来下载: wget https://github.com/openresty/lua-nginx-module/archive/v0.10.16rc4.tar.gz 将下载的资源进行解压: tar -zxf lua-nginx-module-0.10.16rc4.tar.gz 更改目录名:mv lua-nginx-module-0.10.16rc4 lua-nginx-module 导入环境变量，告诉Nginx去哪里找luajit export LUAJIT_LIB=/usr/local/lib export LUAJIT_INC=/usr/local/include/luajit-2.0 进入Nginx的目录执行如下命令: ./configure --prefix=/usr/local/nginx --add-module=../lua-nginx-module make && make install 注意事项: （1）如果启动Nginx出现如下错误: 解决方案: 设置软链接，使用如下命令 ln -s /usr/local/lib/libluajit-5.1.so.2 /lib64/libluajit-5.1.so.2 （2）如果启动Nginx出现以下错误信息 分析原因:因为lua-nginx-module是来自openrestry,错误中提示的resty.core是openrestry的核心模块，对其下的很多函数进行了优化等工作。以前的版本默认不会把该模块编译进去，所以需要使用的话，我们得手动安装，或者禁用就可以。但是最新的lua-nginx-module模块已经强制性安装了该模块，所以此处因为缺少resty模块导致的报错信息。 解决方案有两个:一种是下载对应的模块，另一种则是禁用掉restry模块，禁用的方式为: http{ lua_load_resty_core off; } 测试 在nginx.conf下配置如下内容: location /lua{ default_type 'text/html'; content_by_lua 'ngx.say(\"HELLO,LUA\")'; } 配置成功后，启动nginx,通过浏览器进行访问，如果获取到如下结果，则证明安装成功。 方式二:OpenRestry 概述 ​ 前面我们提到过，OpenResty是由淘宝工程师开发的，所以其官方网站(http://openresty.org/)我们读起来是非常的方便。OpenResty是一个基于Nginx与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。所以本身OpenResty内部就已经集成了Nginx和Lua，所以我们使用起来会更加方便。 安装 (1) 下载OpenResty：https://openresty.org/download/openresty-1.15.8.2.tar.gz (2)使用wget下载: wget https://openresty.org/download/openresty-1.15.8.2.tar.gz (3)解压缩: tar -zxf openresty-1.15.8.2.tar.gz (4)进入OpenResty目录: cd openresty-1.15.8.2 (5) 执行命令:./configure (6) 执行命令:make && make install (7)进入OpenResty的目录，找到nginx：cd /usr/local/openresty/nginx/ (8)在conf目录下的nginx.conf添加如下内容 location /lua{ default_type 'text/html'; content_by_lua 'ngx.say(\"HELLO,OpenRestry\")'; } (9)在sbin目录下启动nginx (10)通过浏览器访问测试 ngx_lua的使用 使用Lua编写Nginx脚本的基本构建块是指令。指令用于指定何时运行用户Lua代码以及如何使用结果。下图显示了执行指令的顺序。 先来解释下*的作用 *：无 ， 即 xxx_by_lua ,指令后面跟的是 lua指令 *:_file，即 xxx_by_lua_file 指令后面跟的是 lua文件 *:_block,即 xxx_by_lua_block 在0.9.17版后替换init_by_lua_file init_by_lua* 该指令在每次Nginx重新加载配置时执行，可以用来完成一些耗时模块的加载，或者初始化一些全局配置。 init_worker_by_lua* 该指令用于启动一些定时任务，如心跳检查、定时拉取服务器配置等。 set_by_lua* 该指令只要用来做变量赋值，这个指令一次只能返回一个值，并将结果赋值给Nginx中指定的变量。 rewrite_by_lua* 该指令用于执行内部URL重写或者外部重定向，典型的如伪静态化URL重写，本阶段在rewrite处理阶段的最后默认执行。 access_by_lua* 该指令用于访问控制。例如，如果只允许内网IP访问。 content_by_lua* 该指令是应用最多的指令，大部分任务是在这个阶段完成的，其他的过程往往为这个阶段准备数据，正式处理基本都在本阶段。 header_filter_by_lua* 该指令用于设置应答消息的头部信息。 body_filter_by_lua* 该指令是对响应数据进行过滤，如截断、替换。 log_by_lua* 该指令用于在log请求处理阶段，用Lua代码处理日志，但并不替换原有log处理。 balancer_by_lua* 该指令主要的作用是用来实现上游服务器的负载均衡器算法 sslcertificate_by* 该指令作用在Nginx和下游服务开始一个SSL握手操作时将允许本配置项的Lua代码。 需求: http://192.168.200.133?name=张三&gender=1 Nginx接收到请求后，根据gender传入的值，如果gender传入的是1，则在页面上展示 张三先生,如果gender传入的是0，则在页面上展示张三女士,如果未传或者传入的不是1和2则在页面上展示张三。 实现代码 location /getByGender { default_type 'text/html'; set_by_lua $name \" local uri_args = ngx.req.get_uri_args() gender = uri_args['gender'] name = uri_args['name'] if gender=='1' then return name..'先生' elseif gender=='0' then return name..'女士' else return name end \"; header_filter_by_lua \" ngx.header.aaa='bbb' \"; return 200 $name; } ngx_lua操作Redis Redis在系统中经常作为数据缓存、内存数据库使用，在大型系统中扮演着非常重要的作用。在Nginx核心系统中，Redis是常备组件。Nginx支持3种方法访问Redis,分别是HttpRedis模块、HttpRedis2Module、lua-resty-redis库。这三种方式中HttpRedis模块提供的指令少，功能单一，适合做简单缓存，HttpRedis2Module模块比HttpRedis模块操作更灵活，功能更强大。而Lua-resty-redis库是OpenResty提供的一个操作Redis的接口库，可根据自己的业务情况做一些逻辑处理，适合做复杂的业务逻辑。所以本次课程将主要以Lua-resty-redis来进行讲解。 lua-resty-redis环境准备 步骤一:准备一个Redis环境 连接地址 host= 192.168.200.111 port=6379 步骤二:准备对应的API lua-resty-redis提供了访问Redis的详细API，包括创建对接、连接、操作、数据处理等。这些API基本上与Redis的操作一一对应。 （1）redis = require \"resty.redis\" （2）new 语法: redis,err = redis:new(),创建一个Redis对象。 （3）connect 语法:ok,err=redis:connect(host,port[,options_table]),设置连接Redis的连接信息。 ok:连接成功返回 1，连接失败返回nil err:返回对应的错误信息 （4）set_timeoutproperties 语法: redis:set_timeout(time) ，设置请求操作Redis的超时时间。 （5）close 语法: ok,err = redis:close(),关闭当前连接，成功返回1，失败返回nil和错误信息 （6）redis命令对应的方法 在lua-resty-redis中，所有的Redis命令都有自己的方法，方法名字和命令名字相同，只是全部为小写。 步骤三:效果实现 location / { default_type \"text/html\"; content_by_lua_block{ local redis = require \"resty.redis\" -- 引入Redis local redisObj = redis:new() --创建Redis对象 redisObj:set_timeout(1000) --设置超时数据为1s local ok,err = redisObj:connect(\"192.168.200.1\",6379) --设置redis连接信息 if not ok then --判断是否连接成功 ngx.say(\"failed to connection redis\",err) return end ok,err = redisObj:set(\"username\",\"TOM\")--存入数据 if not ok then --判断是否存入成功 ngx.say(\"failed to set username\",err) return end local res,err = redisObj:get(\"username\") --从redis中获取数据 ngx.say(res) --将数据写会消息体中 redisObj:close() } } 步骤四:运行测试效果 ngx_lua操作Mysql MySQL是一个使用广泛的关系型数据库。在ngx_lua中，MySQL有两种访问模式,分别是使 （1）用ngx_lua模块和lua-resty-mysql模块：这两个模块是安装OpenResty时默认安装的。 （2）使用drizzle_nginx_module(HttpDrizzleModule)模块：需要单独安装，这个库现不在OpenResty中。 lua-resty-mysql lua-resty-mysql是OpenResty开发的模块，使用灵活、功能强大，适合复杂的业务场景，同时支持存储过程的访问。 使用lua-resty-mysql实现数据库的查询 步骤一: 准备MYSQL host: 192.168.200.111 port: 3306 username:root password:123456 创建一个数据库表及表中的数据。 create database nginx_db; use nginx_db; create table users( id int primary key auto_increment, username varchar(30), birthday date, salary double ); insert into users(id,username,birthday,salary) values(null,\"TOM\",\"1988-11-11\",10000.0); insert into users(id,username,birthday,salary) values(null,\"JERRY\",\"1989-11-11\",20000.0); insert into users(id,username,birthday,salary) values(null,\"ROWS\",\"1990-11-11\",30000.0); insert into users(id,username,birthday,salary) values(null,\"LUCY\",\"1991-11-11\",40000.0); insert into users(id,username,birthday,salary) values(null,\"JACK\",\"1992-11-11\",50000.0); 数据库连接四要素: driverClass=com.mysql.jdbc.Driver url=jdbc:mysql://192.168.200.111:3306/nginx_db username=root password=123456 步骤二:API学习 （1）引入\"resty.mysql\"模块 local mysql = require \"resty.mysql\" （2）new 创建一个MySQL连接对象，遇到错误时，db为nil，err为错误描述信息 语法: db,err = mysql:new() （3）connect 尝试连接到一个MySQL服务器 语法:ok,err=db:connect(options),options是一个参数的Lua表结构，里面包含数据库连接的相关信息 host:服务器主机名或IP地址 port:服务器监听端口，默认为3306 user:登录的用户名 password:登录密码 database:使用的数据库名 （4）set_timeout 设置子请求的超时时间(ms)，包括connect方法 语法:db:set_timeout(time) （5）close 关闭当前MySQL连接并返回状态。如果成功，则返回1；如果出现任何错误，则将返回nil和错误描述。 语法:db:close() （6）send_query 异步向远程MySQL发送一个查询。如果成功则返回成功发送的字节数；如果错误，则返回nil和错误描述 语法:bytes,err=db:send_query(sql) （7）read_result 从MySQL服务器返回结果中读取一行数据。res返回一个描述OK包或结果集包的Lua表,语法: res, err, errcode, sqlstate = db:read_result() res, err, errcode, sqlstate = db:read_result(rows) :rows指定返回结果集的最大值，默认为4 如果是查询，则返回一个容纳多行的数组。每行是一个数据列的key-value对，如 { {id=1,username=\"TOM\",birthday=\"1988-11-11\",salary=10000.0}, {id=2,username=\"JERRY\",birthday=\"1989-11-11\",salary=20000.0} } 如果是增删改，则返回类上如下数据 { insert_id = 0, server_status=2, warning_count=1, affected_rows=2, message=nil } 返回值: res:操作的结果集 err:错误信息 errcode:MySQL的错误码，比如1064 sqlstate:返回由5个字符组成的标准SQL错误码，比如42000 步骤三:效果实现 location /{ content_by_lua_block{ local mysql = require \"resty.mysql\" local db = mysql:new() local ok,err = db:connect{ host=\"192.168.200.111\", port=3306, user=\"root\", password=\"123456\", database=\"nginx_db\" } db:set_timeout(1000) db:send_query(\"select * from users where id =1\") local res,err,errcode,sqlstate = db:read_result() ngx.say(res[1].id..\",\"..res[1].username..\",\"..res[1].birthday..\",\"..res[1].salary) db:close() } } 问题: 1.如何获取返回数据的内容 2.如何实现查询多条数据 3.如何实现数据库的增删改操作 使用lua-cjson处理查询结果 通过上述的案例学习，read_result()得到的结果res都是table类型，要想在页面上展示，就必须知道table的具体数据结构才能进行遍历获取。处理起来比较麻烦，接下来我们介绍一种简单方式cjson，使用它就可以将table类型的数据转换成json字符串，把json字符串展示在页面上即可。具体如何使用? 步骤一：引入cjson llocal cjson = require \"cjson\" 步骤二：调用cjson的encode方法进行类型转换 cjson.encode(res) 步骤三:使用 location /{ content_by_lua_block{ local mysql = require \"resty.mysql\" local cjson = require \"cjson\" local db = mysql:new() local ok,err = db:connect{ host=\"192.168.200.111\", port=3306, user=\"root\", password=\"123456\", database=\"nginx_db\" } db:set_timeout(1000) --db:send_query(\"select * from users where id = 2\") db:send_query(\"select * from users\") local res,err,errcode,sqlstate = db:read_result() ngx.say(cjson.encode(res)) for i,v in ipairs(res) do ngx.say(v.id..\",\"..v.username..\",\"..v.birthday..\",\"..v.salary) end db:close() } } lua-resty-mysql实现数据库的增删改 优化send_query和read_result 本方法是send_query和read_result组合的快捷方法。 语法: res, err, errcode, sqlstate = db:query(sql[,rows]) 有了该API，上面的代码我们就可以进行对应的优化，如下: location /{ content_by_lua_block{ local mysql = require \"resty.mysql\" local db = mysql:new() local ok,err = db:connect{ host=\"192.168.200.1\", port=3306, user=\"root\", password=\"123456\", database=\"nginx_db\", max_packet_size=1024, compact_arrays=false } db:set_timeout(1000) local res,err,errcode,sqlstate = db:query(\"select * from users\") --local res,err,errcode,sqlstate = db:query(\"insert into users(id,username,birthday,salary) values(null,'zhangsan','2020-11-11',32222.0)\") --local res,err,errcode,sqlstate = db:query(\"update users set username='lisi' where id = 6\") --local res,err,errcode,sqlstate = db:query(\"delete from users where id = 6\") db:close() } } 综合小案例 使用ngx_lua模块完成Redis缓存预热。 分析: （1）先得有一张表(users) （2）浏览器输入如下地址 http://191.168.200.133?username=TOM （3）从表中查询出符合条件的记录，此时获取的结果为table类型 （4）使用cjson将table数据转换成json字符串 （5）将查询的结果数据存入Redis中 init_by_lua_block{ redis = require \"resty.redis\" mysql = require \"resty.mysql\" cjson = require \"cjson\" } location /{ default_type \"text/html\"; content_by_lua_block{ --获取请求的参数username local param = ngx.req.get_uri_args()[\"username\"] --建立mysql数据库的连接 local db = mysql:new() local ok,err = db:connect{ host=\"192.168.200.111\", port=3306, user=\"root\", password=\"123456\", database=\"nginx_db\" } if not ok then ngx.say(\"failed connect to mysql:\",err) return end --设置连接超时时间 db:set_timeout(1000) --查询数据 local sql = \"\"; if not param then sql=\"select * from users\" else sql=\"select * from users where username=\"..\"'\"..param..\"'\" end local res,err,errcode,sqlstate=db:query(sql) if not res then ngx.say(\"failed to query from mysql:\",err) return end --连接redis local rd = redis:new() ok,err = rd:connect(\"192.168.200.111\",6379) if not ok then ngx.say(\"failed to connect to redis:\",err) return end rd:set_timeout(1000) --循环遍历数据 for i,v in ipairs(res) do rd:set(\"user_\"..v.username,cjson.encode(v)) end ngx.say(\"success\") rd:close() db:close() } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 10:38:54 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/tomcat/":{"url":"performance/tomcat/","title":"tomcat-Servlet容器","keywords":"","body":"Servlet容器 1.Tomcat生产环境应用 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:48:11 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/tomcat/Tomcat-pro-use.html":{"url":"performance/tomcat/Tomcat-pro-use.html","title":"1.Tomcat生产环境应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Tomcat各组件认知 2.Tomcat 各组件及关系 3.Tomcat启动参数说明 二、Tomcat server.xml 配置详解 三、Tomcat IO模型介绍 1、Tomcat支持的IO模型说明 2、BIO 与NIO有什么区别 概要： Tomcat各核心组件认知 Tomcat server.xml 配置详解 Tomcat IO模型介绍 一、Tomcat各组件认知 知识点： Tomcat架构说明 Tomcat组件及关系详情介绍 Tomcat启动参数说明 Tomcat架构说明 Tomcat是一个基于JAVA的WEB容器，其实现了JAVA EE中的 Servlet 与 jsp 规范，与Nginx apache 服务器不同在于一般用于动态请求处理。在架构设计上采用面向组件的方式设计。即整体功能是通过组件的方式拼装完成。另外每个组件都可以被替换以保证灵活性。 那么是哪些组件组成了Tomcat呢？ 2.Tomcat 各组件及关系 Server 和 Service Connector 连接器 HTTP 1.1 SSL https AJP（ Apache JServ Protocol） apache 私有协议，用于apache 反向代理Tomcat Container Engine 引擎 catalina Host 虚拟机 基于域名 分发请求 Context 隔离各个WEB应用 每个Context的 ClassLoader都是独立 Component Manager （管理器） logger （日志管理） loader （载入器） pipeline (管道) valve （管道中的阀） 3.Tomcat启动参数说明 我们平时启动Tomcat过程是怎么样的？ 复制WAR包至Tomcat webapp 目录。 执行starut.bat 脚本启动。 启动过程中war 包会被自动解压装载。 但是我们在Eclipse 或idea 中启动WEB项目的时候 也是把War包复杂至webapps 目录解压吗？显然不是，其真正做法是在Tomcat程序文件之外创建了一个部署目录，在一般生产环境中也是这么做的 即：Tomcat 程序目录和部署目录分开 。 我们只需要在启动时指定CATALINA_HOME 与 CATALINA_BASE 参数即可实现。 启动参数 描述说明 JAVA_OPTS jvm 启动参数 , 设置内存 编码等 -Xms100m -Xmx200m -Dfile.encoding=UTF-8 JAVA_HOME 指定jdk 目录，如果未设置从java 环境变量当中去找。 CATALINA_HOME Tomcat 程序根目录 CATALINA_BASE 应用部署目录，默认为$CATALINA_HOME CATALINA_OUT 应用日志输出目录：默认$CATALINA_BASE/log CATALINA_TMPDIR 应用临时目录：默认：$CATALINA_BASE/temp 可以编写一个脚本 来实现自定义配置： 演示自定义启动Tomcat [ ] 下载并解压Tomcat [ ] 创建并拷贝应用目录 [ ] 创建Tomcat.sh [ ] 编写Tomcat.sh [ ] chmod +x tomcat.sh 添加执行权限 [ ] 拷贝conf 、webapps 、logs至应用目录。 [ ] 执行启动测试。 ```powershell!/bin/bash export JAVA_OPTS=\"-Xms100m -Xmx200m\" export JAVA_HOME=/root/svr/jdk/ export CATALINA_HOME=/usr/local/apache-tomcat-8.5.34 export CATALINA_BASE=\"pwd\" case $1 in start) $CATALINA_HOME/bin/catalina.sh start echo start success!! ;; stop) $CATALINA_HOME/bin/catalina.sh stop echo stop success!! ;; restart) $CATALINA_HOME/bin/catalina.sh stop echo stop success!! sleep 2 $CATALINA_HOME/bin/catalina.sh start echo start success!! ;; version) $CATALINA_HOME/bin/catalina.sh version ;; configtest) $CATALINA_HOME/bin/catalina.sh configtest ;; esac exit 0 ## 二、Tomcat server.xml 配置详解 --- Server 的基本基本配置： ```xml 元素说明： server root元素：server 的顶级配置 主要属性: port：执行关闭命令的端口号 shutdown：关闭命令 [ ] 演示shutdown的用法#基于telent 执行SHUTDOWN 命令即可关闭 telent 127.0.0.1 8005 SHUTDOWN service 服务：将多个connector 与一个Engine组合成一个服务，可以配置多个服务。 Connector 连接器：用于接收 指定协议下的连接 并指定给唯一的Engine 进行处理。 主要属性： protocol 监听的协议，默认是http/1.1 port 指定服务器端要创建的端口号 minThread 服务器启动时创建的处理请求的线程数 maxThread 最大可以创建的处理请求的线程数 enableLookups 如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址 redirectPort 指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号 acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理 connectionTimeout 指定超时的时间数(以毫秒为单位) SSLEnabled 是否开启 sll 验证，在Https 访问时需要开启。 [ ] 演示配置多个Connector Engine 引擎：用于处理连接的执行器，默认的引擎是catalina。一个service 中只能配置一个Engine。 主要属性：name 引擎名称 defaultHost 默认host Host 虚拟机：基于域名匹配至指定虚拟机。类似于nginx 当中的server,默认的虚拟机是localhost. 主要属性： [ ] 演示配置多个Host Context 应用上下文：一个host 下可以配置多个Context ，每个Context 都有其独立的classPath。相互隔离，以免造成ClassPath 冲突。 主要属性： [ ] 演示配置多个Context Valve 阀门：可以理解成request 的过滤器，具体配置要基于具体的Valve 接口的子类。以下即为一个访问日志的Valve. 三、Tomcat IO模型介绍 知识点： Tomcat支持的IO模型说明 BIO 与NIO的区别 IO模型源码解读1、Tomcat支持的IO模型说明 | | 描述 | |:----|:----| | BIO | 阻塞式IO，即Tomcat使用传统的java.io进行操作。该模式下每个请求都会创建一个线程，对性能开销大，不适合高并发场景。优点是稳定，适合连接数目小且固定架构。 | | NIO | 非阻塞式IO，jdk1.4 之后实现的新IO。该模式基于多路复用选择器监测连接状态在通知线程处理，从而达到非阻塞的目的。比传统BIO能更好的支持并发性能。Tomcat 8.0之后默认采用该模式 | | APR | 全称是 Apache Portable Runtime/Apache可移植运行库)，是Apache HTTP服务器的支持库。可以简单地理解为，Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作。使用需要编译安装APR 库 | | AIO | 异步非阻塞式IO，jdk1.7后之支持 。与nio不同在于不需要多路复用选择器，而是请求处理线程执行完程进行回调调知，已继续执行后续操作。Tomcat 8之后支持。 | | | | 使用指定IO模型的配置方式: 配置 server.xml 文件当中的 修改即可。 默认配置 8.0 protocol=“HTTP/1.1” 8.0 之前是 BIO 8.0 之后是NIO BIO protocol=“org.apache.coyote.http11.Http11Protocol“ NIO protocol=”org.apache.coyote.http11.Http11NioProtocol“ AIO protocol=”org.apache.coyote.http11.Http11Nio2Protocol“ APR protocol=”org.apache.coyote.http11.Http11AprProtocol“ 2、BIO 与NIO有什么区别 分别演示在高并发场景下BIO与NIO的线程数的变化？ 演示数据： | | 每秒提交数 | BIO执行线程 | NIO执行线程 | | |:----|:----|:----|:----|:----| | 预测 | 200 | 200 | 50 | | | 实验环境 | 200 | 48 | 37 | | | 生产环境 | 200 | 419 | 23 | | 结论： BIO 线程模型讲解 NIO 线程模型讲解 BIO 源码解读 Http11Protocol Http BIO协议解析器 JIoEndpoint Acceptor implements Runnable SocketProcessor implements Runnable Http11NioProtocol Http Nio协议解析器 NioEndpoint Acceptor implements Runnable Poller implements Runnable SocketProcessor implements Runnable Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/":{"url":"distributed/","title":"五、分布式专题","keywords":"","body":"分布式专题 介绍比较常用的分布式框架 zookeeper协调框架 1.特性与节点说明 2.客户端使用与集群特性 3.典型使用场景实践 4.ZAB协议实现源码分析 netty网络框架 1.分布式之netty Netty-黑马 Netty01-nio Netty02-入门 Netty03-进阶 Netty04-优化与源码 dubbo远程调用 1.从0到1整体认知分布式系统 2.快速掌握Dubbo常规应用 3.Dubbo企业级应用进阶 2.Dubb调用模块详解 3.Dubbo协议模块源码剖析 rocketmq消息中间件 1.初识消息中间件 2.特性详解&场景介绍 3.源码分析 rabbitmq消息中间件 1.JVM整体结构介绍 2.JVM性能调优监控工具 3.JVM垃圾回收与调优 kafka消息中间件 1.大数据技术之Kafka mongo文档数据库 1.准备数据 2.快速上手 3.企业应用 redis缓存 1.特性介绍 2.集群部署 3.原理分析 4.源码分析 5.Redis6.0 shardingsphere数据库中间件 1.概述 2.核心概念 elk日志监控 1.分布式之ELK Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 17:09:01 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/":{"url":"distributed/zookeeper/","title":"zookeeper协调框架","keywords":"","body":"zookeeper协调框架 1.特性与节点说明 2.客户端使用与集群特性 3.典型使用场景实践 4.ZAB协议实现源码分析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:51:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-base-use.html":{"url":"distributed/zookeeper/zookeeper-base-use.html","title":"1.特性与节点说明","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、zookeeper概要、背景及作用 zookeeper产生背景： zookeeper概要： znode 节点 二、部署与常规配置 版本说明： 常规配置文件说明： 客户端命令： 三、Zookeeper节点介绍 知识点： 节点类型 节点属性 节点的监听： acl权限设置 主题概要 zookeeper概要、背景及作用 部署与常规配置 节点类型 一、zookeeper概要、背景及作用 zookeeper产生背景： 项目从单体到分布式转变之后，将会产生多个节点之间协同的问题。如： 每天的定时任务由谁哪个节点来执行？ RPC调用时的服务发现? 如何保证并发请求的幂等 .... 这些问题可以统一归纳为多节点协调问题，如果靠节点自身进行协调这是非常不可靠的，性能上也不可取。必须由一个独立的服务做协调工作，它必须可靠，而且保证性能。 zookeeper概要： ZooKeeper是用于分布式应用程序的协调服务。它公开了一组简单的API，分布式应用程序可以基于这些API用于同步，节点状态、配置等信息、服务注册等信息。其由JAVA编写，支持JAVA 和C两种语言的客户端。 znode 节点 zookeeper 中数据基本单元叫节点，节点之下可包含子节点，最后以树级方式程现。每个节点拥有唯一的路径path。客户端基于PATH上传节点数据，zookeeper 收到后会实时通知对该路径进行监听的客户端。 二、部署与常规配置 zookeeper 基于JAVA开发，下载后只要有对应JVM环境即可运行。其默认的端口号是2181运行前得保证其不冲突。 版本说明： 2019年5月20日发行的3.5.5是3.5分支的第一个稳定版本。此版本被认为是3.4稳定分支的后续版本，可以用于生产。基于3.4它包含以下新功能 动态重新配置 本地会议 新节点类型：容器，TTL 原子广播协议的SSL支持 删除观察者的能力 多线程提交处理器 升级到Netty 4.1 Maven构建 另请注意：建议的最低JDK版本为1.8 文件说明： apache-zookeeper-xxx-tar.gz 代表源代码 apache-zookeeper-xxx-bin.tar.gz 运行版本 下载地址：https://zookeeper.apache.org/releases.html#download 具体部署流程： #下载 wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/current/apache-zookeeper-3.5.5-bin.tar.gz #解压 tar -zxvf apache-zookeeper-3.5.5-bin.tar.gz #拷贝默认配置 cd {zookeeper_home}/conf cp zoo_sample.cfg zoo.cfg #启动 {zookeeper_home}/bin/zkServer.sh 常规配置文件说明： # zookeeper时间配置中的基本单位 (毫秒) tickTime=2000 # 允许follower初始化连接到leader最大时长，它表示tickTime时间倍数 即:initLimit*tickTime initLimit=10 # 允许follower与leader数据同步最大时长,它表示tickTime时间倍数 syncLimit=5 #zookeper 数据存储目录 dataDir=/tmp/zookeeper #对客户端提供的端口号 clientPort=2181 #单个客户端与zookeeper最大并发连接数 maxClientCnxns=60 # 保存的数据快照数量，之外的将会被清除 autopurge.snapRetainCount=3 #自动触发清除任务时间间隔，小时为单位。默认为0，表示不自动清除。 autopurge.purgeInterval=1 客户端命令： 基本命令列表 close 关闭当前会话 connect host:port 重新连接指定Zookeeper服务 create [-s] [-e] [-c] [-t ttl] path [data] [acl] 创建节点 delete [-v version] path 删除节点，(不能存在子节点） deleteall path 删除路径及所有子节点 setquota -n|-b val path 设置节点限额 -n 子节点数 -b 字节数 listquota path 查看节点限额 delquota [-n|-b] path 删除节点限额 get [-s] [-w] path 查看节点数据 -s 包含节点状态 -w 添加监听 getAcl [-s] path ls [-s] [-w] [-R] path 列出子节点 -s状态 -R 递归查看所有子节点 -w 添加监听 printwatches on|off 是否打印监听事件 quit 退出客户端 history 查看执行的历史记录 redo cmdno 重复 执行命令，history 中命令编号确定 removewatches path [-c|-d|-a] [-l] 删除指定监听 set [-s] [-v version] path data 设置值 setAcl [-s] [-v version] [-R] path acl 为节点设置ACL权限 stat [-w] path 查看节点状态 -w 添加监听 sync path 强制同步节点 node数据的增删改查 # 列出子节点 ls / #创建节点 create /luban \"luban is good man\" # 查看节点 get /luban # 创建子节点 create /luban/sex \"man\" # 删除节点 delete /luban/sex # 删除所有节点 包括子节点 deleteall /luban 三、Zookeeper节点介绍 知识点： 节点类型 节点的监听(watch) 节点属性说明(stat) 权限设置(acl) zookeeper 中节点叫znode存储结构上跟文件系统类似，以树级结构进行存储。不同之外在于znode没有目录的概念，不能执行类似cd之类的命令。znode结构包含如下： path:唯一路径 childNode：子节点 stat:状态属性 type:节点类型 节点类型 | 类型 | 描述 | |:----|:----| | PERSISTENT | 持久节点 | | PERSISTENT_SEQUENTIAL | 持久序号节点 | | EPHEMERAL | 临时节点(不可在拥有子节点) | | EPHEMERAL_SEQUENTIAL | 临时序号节点(不可在拥有子节点) | PERSISTENT（持久节点） 持久化保存的节点，也是默认创建的 #默认创建的就是持久节点 create /test PERSISTENT_SEQUENTIAL(持久序号节点) 创建时zookeeper 会在路径上加上序号作为后缀，。非常适合用于分布式锁、分布式选举等场景。创建时添加 -s 参数即可。 #创建序号节点 create -s /test #返回创建的实际路径 Created /test0000000001 create -s /test #返回创建的实际路径2 Created /test0000000002 EPHEMERAL（临时节点） 临时节点会在客户端会话断开后自动删除。适用于心跳，服务发现等场景。创建时添加参数-e 即可。 #创建临时节点， 断开会话 在连接将会自动删除 create -e /temp EPHEMERAL_SEQUENTIAL（临时序号节点） 与持久序号节点类似，不同之处在于EPHEMERAL_SEQUENTIAL是临时的会在会话断开后删除。创建时添加 -e -s create -e -s /temp/seq 节点属性 # 查看节点属性 stat /luban 其属性说明如下表： #创建节点的事物ID cZxid = 0x385 #创建时间 ctime = Tue Sep 24 17:26:28 CST 2019 #修改节点的事物ID mZxid = 0x385 #最后修改时间 mtime = Tue Sep 24 17:26:28 CST 2019 # 子节点变更的事物ID pZxid = 0x385 #这表示对此znode的子节点进行的更改次数（不包括子节点） cversion = 0 # 数据版本，变更次数 dataVersion = 0 #权限版本，变更次数 aclVersion = 0 #临时节点所属会话ID ephemeralOwner = 0x0 #数据长度 dataLength = 17 #子节点数(不包括子子节点) numChildren = 0 节点的监听： 客户添加 -w 参数可实时监听节点与子节点的变化，并且实时收到通知。非常适用保障分布式情况下的数据一至性。其使用方式如下： | 命令 | 描述 | |:----|:----| | ls -w path | 监听子节点的变化（增，删） | | get -w path | 监听节点数据的变化 | | stat -w path | 监听节点属性的变化 | | printwatches on|off | 触发监听后，是否打印监听事件(默认on) | acl权限设置 ACL全称为Access Control List（访问控制列表），用于控制资源的访问权限。ZooKeeper使用ACL来控制对其znode的防问。基于scheme:id:permission的方式进行权限控制。scheme表示授权模式、id模式对应值、permission即具体的增删改权限位。 scheme:认证模型 | 方案 | 描述 | |:----|:----| | world | 开放模式，world表示全世界都可以访问（这是默认设置） | | ip | ip模式，限定客户端IP防问 | | auth | 用户密码认证模式，只有在会话中添加了认证才可以防问 | | digest | 与auth类似，区别在于auth用明文密码，而digest 用sha-1+base64加密后的密码。在实际使用中digest 更常见。 | permission权限位 | 权限位 | 权限 | 描述 | |:----|:----|:----| | c | CREATE | 可以创建子节点 | | d | DELETE | 可以删除子节点（仅下一级节点） | | r | READ | 可以读取节点数据及显示子节点列表 | | w | WRITE | 可以设置节点数据 | | a | ADMIN | 可以设置节点访问控制列表权限 | acl 相关命令： | 命令 | 使用方式 | 描述 | |:----|:----|:----| | getAcl | getAcl | 读取ACL权限 | | setAcl | setAcl | 设置ACL权限 | | addauth | addauth | 添加认证用户 | world权限**示例** 语法： setAcl world:anyone: 注：world模式中anyone是唯一的值,表示所有人 查看默认节点权限： #创建一个节点 create -e /testAcl #查看节点权限 getAcl /testAcl #返回的默认权限表示 ，所有人拥有所有权限。 'world,'anyone: cdrwa 修改默认权限为 读写 #设置为rw权限 setAcl /testAcl world:anyone:rw # 可以正常读 get /testAcl # 无法正常创建子节点 create -e /testAcl/t \"hi\" # 返回没有权限的异常 Authentication is not valid : /testAcl/t IP权限示例： 语法： setAcl ip:: auth模式示例: 语法： setAcl auth::: addauth digest : digest 权限示例： 语法： setAcl digest ::: addauth digest : 注1：密钥 通过sha1与base64组合加密码生成，可通过以下命令生成 echo -n : | openssl dgst -binary -sha1 | openssl base64 注2：为节点设置digest 权限后，访问前必须执行addauth，当前会话才可以防问。 设置digest 权限 #先 sha1 加密，然后base64加密 echo -n luban:123456 | openssl dgst -binary -sha1 | openssl base64 #返回密钥 2Rz3ZtRZEs5RILjmwuXW/wT13Tk= #设置digest权限 setAcl /luban digest:luban:2Rz3ZtRZEs5RILjmwuXW/wT13Tk=:cdrw 查看节点将显示没有权限 #查看节点 get /luban #显示没有权限访问 org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /luban 给当前会话添加认证后在次查看 #给当前会话添加权限帐户 addauth digest luban:123456 #在次查看 get /luban #获得返回结果 luban is good man ACL的特殊说明： 权限仅对当前节点有效，不会让子节点继承。如限制了IP防问A节点，但不妨碍该IP防问A的子节点 /A/B。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-client-use.html":{"url":"distributed/zookeeper/zookeeper-client-use.html","title":"2.客户端使用与集群特性","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、客户端API常规应用 2.创建、查看节点 3.监听节点 4.设置节点ACL权限 5.第三方客户端ZkClient 二、Zookeeper集群 3.选举机制 5.四字运维命令 主要内容： 客户端 zookeeper客户端简介，C客户端 客户端连接参数说明 客户端CRUD 客户端监听 集群 集群架构说明 集群配置及参数说明 选举投票机制 主从复制机制一、客户端API常规应用 zookeeper 提供了java与C两种语言的客户端。我们要学习的就是java客户端。引入最新的maven依赖： org.apache.zookeeper zookeeper 3.5.5 知识点： 初始连接 创建、查看节点 监听节点 设置节点权限 第三方客户端ZkClient 初始连接： 常规的客户端类是 org.apache.zookeeper.ZooKeeper，实例化该类之后将会自动与集群建立连接。构造参数说明如下： | 参数名称 | 类型 | 说明 | |:----|:----|:----|:----:| | connectString | String | 连接串，包括ip+端口 ,集群模式下用逗号隔开 192.168.0.149:2181,192.168.0.150:2181 | | sessionTimeout | int | 会话超时时间，该值不能超过服务端所设置的 minSessionTimeout 和maxSessionTimeout | | watcher | Watcher | 会话监听器，服务端事件将会触该监听 | | sessionId | long | 自定义会话ID | | sessionPasswd | byte[] | 会话密码 | | canBeReadOnly | boolean | 该连接是否为只读的 | | hostProvider | HostProvider | 服务端地址提供者，指示客户端如何选择某个服务来调用，默认采用StaticHostProvider实现 | 2.创建、查看节点 创建节点 通过org.apache.zookeeper.ZooKeeper#create()即可创建节点，其参数说明如下： | 参数名称 | 类型 | 说明 | |:----|:----|:----| | path | String | | | data | byte[] | | | acl | List | | | createMode | CreateMode | | | cb | StringCallback | | | ctx | Object | | 查看节点： 通过org.apache.zookeeper.ZooKeeper#getData()即可创建节点，其参数说明如下： | 参数名称 | 类型 | 说明 | |:----|:----|:----| | path | String | | | watch | boolean | | | watcher | Watcher | | | cb | DataCallback | | | ctx | Object | | 查看子节点： 通过org.apache.zookeeper.ZooKeeper#getChildren()即可获取子节点，其参数说明如下： | 参数名称 | 类型 | 说明 | |:----:|:----:|:----| | path | String | | | watch | boolean | | | watcher | Watcher | | | cb | Children2Callback | | | ctx | Object | | 3.监听节点 在getData() 与getChildren()两个方法中可分别设置监听数据变化和子节点变化。通过设置watch为true，当前事件触发时会调用zookeeper()构建函数中Watcher.process()方法。也可以添加watcher参数来实现自定义监听。一般采用后者。 注：所有的监听都是一次性的，如果要持续监听需要触发后在添加一次监听。 4.设置节点ACL权限 ACL包括结构为scheme:id:permission（有关ACL的介绍参照第一节课关于ACL的讲解） 客户端中由org.apache.zookeeper.data.ACL 类表示，类结构如下： ACL Id scheme // 对应权限模式scheme id // 对应模式中的id值 perms // 对应权限位permission 关于权限位的表示方式： 每个权限位都是一个唯一数字，将其合时通过或运行生成一个全新的数字即可 @InterfaceAudience.Public public interface Perms { int READ = 1 5.第三方客户端ZkClient zkClient 是在zookeeper客户端基础之上封装的，使用上更加友好。主要变化如下： 可以设置持久监听，或删除某个监听 可以插入JAVA对象，自动进行序列化和反序列化 简化了基本的增删改查操作。 二、Zookeeper集群 知识点： 集群部署 集群角色说明 选举机制 数据提交机制 集群配置说明 zookeeper集群的目的是为了保证系统的性能承载更多的客户端连接设专门提供的机制。通过集群可以实现以下功能： 读写分离：提高承载，为更多的客户端提供连接，并保障性能。 主从自动切换：提高服务容错性，部分节点故障不会影响整个服务集群。 半数以上运行机制说明： 集群至少需要三台服务器，并且强烈建议使用奇数个服务器。因为zookeeper 通过判断大多数节点的存活来判断整个服务是否可用。比如3个节点，挂掉了2个表示整个集群挂掉，而用偶数4个，挂掉了2个也表示其并不是大部分存活，因此也会挂掉。 集群部署 配置语法： server.=:: 节点**ID**：服务id手动指定1至125之间的数字，并写到对应服务节点的 {dataDir}/myid 文件中。 IP地址：节点的远程IP地址，可以相同。但生产环境就不能这么做了，因为在同一台机器就无法达到容错的目的。所以这种称作为伪集群。 数据同步端口：主从同时数据复制端口，（做伪集群时端口号不能重复）。 远举端口：主从节点选举端口，（做伪集群时端口号不能重复）。 配置文件示例： tickTime=2000 dataDir=/var/lib/zookeeper/ clientPort=2181 initLimit=5 syncLimit=2 #以下为集群配置，必须配置在所有节点的zoo.cfg文件中 server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 集群配置流程： 分别创建3个data目录用于存储各节点数据mkdir data mkdir data/1 mkdir data/3 mkdir data/3 编写myid文件echo 1 > data/1/myid echo 3 > data/3/myid echo 2 > data/2/myid 3、编写配置文件 conf/zoo1.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=data/1 clientPort=2181 #集群配置 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 conf/zoo2.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=data/2 clientPort=2182 #集群配置 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 conf/zoo3.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=data/3 clientPort=2183 #集群配置 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 4.分别启动 ./bin/zkServer.sh start conf/zoo1.cfg ./bin/zkServer.sh start conf/zoo2.cfg ./bin/zkServer.sh start conf/zoo3.cfg 5.分别查看状态 ./bin/zkServer.sh status conf/zoo1.cfg Mode: follower ./bin/zkServer.sh status conf/zoo2.cfg Mode: leader ./bin/zkServer.sh status conf/zoo3.cfg Mode: follower 检查集群复制情况： 1、分别连接指定节点 zkCli.sh 后加参数-server 表示连接指定IP与端口。 ./bin/zkCli.sh -server 127.0.0.1:2181 ./bin/zkCli.sh -server 127.0.0.1:2182 ./bin/zkCli.sh -server 127.0.0.1:2183 [ ] 任意节点中创建数据，查看其它节点已经同步成功。 注意： -server参数后同时连接多个服务节点，并用逗号隔开 127.0.0.1:2181,127.0.0.1:2182 集群角色说明 zookeeper 集群中总共有三种角色，分别是leader（主节点）follower(子节点) observer（次级子节点） | 角色 | 描述 | |:----|:----| | leader | 主节点，又名领导者。用于写入数据，通过选举产生，如果宕机将会选举新的主节点。 | | follower | 子节点，又名追随者。用于实现数据的读取。同时他也是主节点的备选节点，并用拥有投票权。 | | observer | 次级子节点，又名观察者。用于读取数据，与fllower区别在于没有投票权，不能选为主节点。并且在计算集群可用状态时不会将observer计算入内。 | observer配置： 只要在集群配置中加上observer后缀即可，示例如下： server.3=127.0.0.1:2889:3889:observer 3.选举机制 通过 ./bin/zkServer.sh status 命令可以查看到节点状态 ./bin/zkServer.sh status conf/zoo1.cfg Mode: follower ./bin/zkServer.sh status conf/zoo2.cfg Mode: leader ./bin/zkServer.sh status conf/zoo3.cfg Mode: follower 可以发现中间的2182 是leader状态.其选举机制如下图： 投票机制说明： 第一轮投票全部投给自己 第二轮投票给myid比自己大的相邻节点 如果得票超过半数，选举结束。 选举触发： 当集群中的服务器出现已下两种情况时会进行Leader的选举 服务节点初始化启动 半数以上的节点无法和Leader建立连接 当节点初始起动时会在集群中寻找Leader节点，如果找到则与Leader建立连接，其自身状态变化follower或observer。如果没有找到Leader，当前节点状态将变化LOOKING，进入选举流程。 在集群运行其间如果有follower或observer节点宕机只要不超过半数并不会影响整个集群服务的正常运行。但如果leader宕机，将暂停对外服务，所有follower将进入LOOKING 状态，进入选举流程。 数据同步机制 zookeeper 的数据同步是为了保证各节点中数据的一至性，同步时涉及两个流程，一个是正常的客户端数据提交，另一个是集群某个节点宕机在恢复后的数据同步。 客户端写入请求： 写入请求的大至流程是，收leader接收客户端写请求，并同步给各个子节点。如下图： 但实际情况要复杂的多，比如client 它并不知道哪个节点是leader 有可能写的请求会发给follower ，由follower在转发给leader进行同步处理 客户端写入流程说明： client向zk中的server发送写请求，如果该server不是leader，则会将该写请求转发给leader server，leader将请求事务以proposal形式分发给follower； 当follower收到收到leader的proposal时，根据接收的先后顺序处理proposal； 当Leader收到follower针对某个proposal过半的ack后，则发起事务提交，重新发起一个commit的proposal Follower收到commit的proposal后，记录事务提交，并把数据更新到内存数据库； 当写成功后，反馈给client。 服务节点初始化同步： 在集群运行过程当中如果有一个follower节点宕机，由于宕机节点没过半，集群仍然能正常服务。当leader 收到新的客户端请求，此时无法同步给宕机的节点。造成数据不一至。为了解决这个问题，当节点启动时，第一件事情就是找当前的Leader，比对数据是否一至。不一至则开始同步,同步完成之后在进行对外提供服务。 如何比对Leader的数据版本呢，这里通过ZXID事物ID来确认。比Leader就需要同步。 ZXID说明： ZXID是一个长度64位的数字，其中低32位是按照数字递增，任何数据的变更都会导致,低32位的数字简单加1。高32位是leader周期编号，每当选举出一个新的leader时，新的leader就从本地事物日志中取出ZXID,然后解析出高32位的周期编号，进行加1，再将低32位的全部设置为0。这样就保证了每次新选举的leader后，保证了ZXID的唯一性而且是保证递增的。 思考题： 如果leader 节点宕机，在恢复后它还能被选为leader吗？ 5.四字运维命令 ZooKeeper响应少量命令。每个命令由四个字母组成。可通过telnet或nc向ZooKeeper发出命令。 这些命令默认是关闭的，需要配置4lw.commands.whitelist来打开，可打开部分或全部示例如下： #打开指定命令 4lw.commands.whitelist=stat, ruok, conf, isro #打开全部 4lw.commands.whitelist=* 安装Netcat工具，已使用nc命令 #安装Netcat 工具 yum install -y nc #查看服务器及客户端连接状态 echo stat | nc localhost 2181 命令列表 conf：3.3.0中的新增功能：打印有关服务配置的详细信息。 缺点：3.3.0中的新增功能：列出了连接到该服务器的所有客户端的完整连接/会话详细信息。包括有关已接收/已发送的数据包数量，会话ID，操作等待时间，最后执行的操作等信息。 crst：3.3.0中的新增功能：重置所有连接的连接/会话统计信息。 dump：列出未完成的会话和临时节点。这仅适用于领导者。 envi：打印有关服务环境的详细信息 ruok：测试服务器是否以非错误状态运行。如果服务器正在运行，它将以imok响应。否则，它将完全不响应。响应“ imok”不一定表示服务器已加入仲裁，只是服务器进程处于活动状态并绑定到指定的客户端端口。使用“ stat”获取有关状态仲裁和客户端连接信息的详细信息。 srst：重置服务器统计信息。 srvr：3.3.0中的新功能：列出服务器的完整详细信息。 stat：列出服务器和连接的客户端的简要详细信息。 wchs：3.3.0中的新增功能：列出有关服务器监视的简要信息。 wchc：3.3.0中的新增功能：按会话列出有关服务器监视的详细信息。这将输出具有相关监视（路径）的会话（连接）列表。请注意，根据手表的数量，此操作可能会很昂贵（即影响服务器性能），请小心使用。 dirs：3.5.1中的新增功能：以字节为单位显示快照和日志文件的总大小 wchp：3.3.0中的新增功能：按路径列出有关服务器监视的详细信息。这将输出具有关联会话的路径（znode）列表。请注意，根据手表的数量，此操作可能会很昂贵（即影响服务器性能），请小心使用。 mntr：3.4.0中的新增功能：输出可用于监视集群运行状况的变量列表。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-sence-use.html":{"url":"distributed/zookeeper/zookeeper-sence-use.html","title":"3.典型使用场景实践","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、 分布式集群管理 分布式集群管理的需求： 架构设计： 功能实现： 二 、分布式注册中心 Dubbo 对zookeeper的使用 Dubbo Zookeeper注册中心存储结构： 示例演示： 三、分布式JOB 分布式JOB需求： 架构设计： 四、分布式锁 锁的的基本概念： 锁的获取： 关于羊群效应： 示例演示： 课程概要： 分布式集群管理 分布式注册中心 分布式JOB 分布式锁 一、 分布式集群管理 分布式集群管理的需求： 主动查看线上服务节点 查看服务节点资源使用情况 服务离线通知 服务资源（CPU、内存、硬盘）超出阀值通知架构设计： 节点结构： tuling-manger // 根节点 server00001 : //服务节点 1 server00002 ://服务节点 2 server........n ://服务节点 n 服务状态信息: ip cpu memory disk功能实现： 数据生成与上报： 创建临时节点： 定时变更节点状态信息： 主动查询： 1、实时查询 zookeeper 获取集群节点的状态信息。 被动通知： 监听根节点下子节点的变化情况,如果CPU 等硬件资源低于警告位则发出警报。 关键示例代码： package com.tuling; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.tuling.os.CPUMonitorCalc; import com.tuling.os.OsBean; import org.I0Itec.zkclient.IZkChildListener; import org.I0Itec.zkclient.ZkClient; import java.io.IOException; import java.lang.instrument.Instrumentation; import java.lang.management.ManagementFactory; import java.lang.management.MemoryUsage; import java.net.InetAddress; import java.net.UnknownHostException; import java.util.ArrayList; import java.util.List; import java.util.stream.Collectors; /** * @author Tommy * Created by Tommy on 2019/9/22 **/ public class Agent { private String server = \"192.168.0.149:2181\"; ZkClient zkClient; private static Agent instance; private static final String rootPath = \"/tuling-manger\"; private static final String servicePath = rootPath + \"/service\"; private String nodePath; private Thread stateThread; List list = new ArrayList<>(); public static void premain(String args, Instrumentation instrumentation) { instance = new Agent(); if (args != null) { instance.server = args; } instance.init(); } // 初始化连接 public void init() { zkClient = new ZkClient(server, 5000, 10000); System.out.println(\"zk连接成功\" + server); buildRoot(); createServerNode(); stateThread = new Thread(() -> { while (true) { updateServerNode(); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } }, \"zk_stateThread\"); stateThread.setDaemon(true); stateThread.start(); } // 构建根节点 public void buildRoot() { if (!zkClient.exists(rootPath)) { zkClient.createPersistent(rootPath); } } // 生成服务节点 public void createServerNode() { nodePath = zkClient.createEphemeralSequential(servicePath, getOsInfo()); System.out.println(\"创建节点:\" + nodePath); } // 监听服务节点状态改变 public void updateServerNode() { zkClient.writeData(nodePath, getOsInfo()); } // 更新服务节点状态 public String getOsInfo() { OsBean bean = new OsBean(); bean.lastUpdateTime = System.currentTimeMillis(); bean.ip = getLocalIp(); bean.cpu = CPUMonitorCalc.getInstance().getProcessCpu(); MemoryUsage memoryUsag = ManagementFactory.getMemoryMXBean().getHeapMemoryUsage(); bean.usableMemorySize = memoryUsag.getUsed() / 1024 / 1024; bean.usableMemorySize = memoryUsag.getMax() / 1024 / 1024; ObjectMapper mapper = new ObjectMapper(); try { return mapper.writeValueAsString(bean); } catch (JsonProcessingException e) { throw new RuntimeException(e); } } public void updateNode(String path, Object data) { if (zkClient.exists(path)) { zkClient.writeData(path, data); } else { zkClient.createEphemeral(path, data); } } public static String getLocalIp() { InetAddress addr = null; try { addr = InetAddress.getLocalHost(); } catch (UnknownHostException e) { throw new RuntimeException(e); } return addr.getHostAddress(); } } 实现效果图： 二 、分布式注册中心 在单体式服务中，通常是由多个客户端去调用一个服务，只要在客户端中配置唯一服务节点地址即可，当升级到分布式后，服务节点变多，像阿里一线大厂服务节点更是上万之多，这么多节点不可能手动配置在客户端，这里就需要一个中间服务，专门用于帮助客户端发现服务节点，即许多技术书籍经常提到的服务发现。 一个完整的注册中心涵盖以下功能特性： 服务注册：提供者上线时将自提供的服务提交给注册中心。 服务注销：通知注册心提供者下线。 服务订阅：动态实时接收服务变更消息。 可靠：注册服务本身是集群的，数据冗余存储。避免单点故障，及数据丢失。 容错：当服务提供者出现宕机，断电等极情况时，注册中心能够动态感知并通知客户端服务提供者的状态。Dubbo 对zookeeper的使用 阿里著名的开源项目Dubbo 是一个基于JAVA的RCP框架，其中必不可少的注册中心可基于多种第三方组件实现，但其官方推荐的还是Zookeeper做为注册中心服务。 Dubbo Zookeeper注册中心存储结构： 节点说明： | 类别 | 属性 | 说明 | |:----|:----|:----| | Root | 持久节点 | 根节点名称，默认是 \"dubbo\" | | Service | 持久节点 | 服务名称，完整的服务类名 | | type | 持久节点 | 可选值：providers(提供者)、consumers（消费者）、configurators(动态配置)、routers | | URL | 临时节点 | url名称 包含服务提供者的 IP 端口 及配置等信息。 | 流程说明： 服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。示例演示： 服务端代码： package com.tuling.zk.dubbo; import com.alibaba.dubbo.config.ApplicationConfig; import com.alibaba.dubbo.config.ProtocolConfig; import com.alibaba.dubbo.config.RegistryConfig; import com.alibaba.dubbo.config.ServiceConfig; import java.io.IOException; /** * @author Tommy * Created by Tommy on 2019/10/8 **/ public class Server { public void openServer(int port) { // 构建应用 ApplicationConfig config = new ApplicationConfig(); config.setName(\"simple-app\"); // 通信协议 ProtocolConfig protocolConfig = new ProtocolConfig(\"dubbo\", port); protocolConfig.setThreads(200); ServiceConfig serviceConfig = new ServiceConfig(); serviceConfig.setApplication(config); serviceConfig.setProtocol(protocolConfig); serviceConfig.setRegistry(new RegistryConfig(\"zookeeper://192.168.0.149:2181\")); serviceConfig.setInterface(UserService.class); UserServiceImpl ref = new UserServiceImpl(); serviceConfig.setRef(ref); //开始提供服务 开张做生意 serviceConfig.export(); System.out.println(\"服务已开启!端口:\"+serviceConfig.getExportedUrls().get(0).getPort()); ref.setPort(serviceConfig.getExportedUrls().get(0).getPort()); } public static void main(String[] args) throws IOException { new Server().openServer(-1); System.in.read(); } } 客户端代码： package com.tuling.zk.dubbo; import com.alibaba.dubbo.config.ApplicationConfig; import com.alibaba.dubbo.config.ReferenceConfig; import com.alibaba.dubbo.config.RegistryConfig; import java.io.IOException; /** * @author Tommy * Created by Tommy on 2018/11/20 **/ public class Client { UserService service; // URL 远程服务的调用地址 public UserService buildService(String url) { ApplicationConfig config = new ApplicationConfig(\"young-app\"); // 构建一个引用对象 ReferenceConfig referenceConfig = new ReferenceConfig<>(); referenceConfig.setApplication(config); referenceConfig.setInterface(UserService.class); // referenceConfig.setUrl(url); referenceConfig.setRegistry(new RegistryConfig(\"zookeeper://192.168.0.149:2181\")); referenceConfig.setTimeout(5000); // 透明化 this.service = referenceConfig.get(); return service; } static int i = 0; public static void main(String[] args) throws IOException { Client client1 = new Client(); client1.buildService(\"\"); String cmd; while (!(cmd = read()).equals(\"exit\")) { UserVo u = client1.service.getUser(Integer.parseInt(cmd)); System.out.println(u); } } private static String read() throws IOException { byte[] b = new byte[1024]; int size = System.in.read(b); return new String(b, 0, size).trim(); } } 查询zk 实际存储内容： /dubbo /dubbo/com.tuling.zk.dubbo.UserService /dubbo/com.tuling.zk.dubbo.UserService/configurators /dubbo/com.tuling.zk.dubbo.UserService/routers /dubbo/com.tuling.zk.dubbo.UserService/providers /dubbo/com.tuling.zk.dubbo.UserService/providers/dubbo://192.168.0.132:20880/com.tuling.zk.dubbo.UserService?anyhost=true&application=simple-app&dubbo=2.6.2&generic=false&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=11128&side=provider&threads=200&timestamp=1570518302772 /dubbo/com.tuling.zk.dubbo.UserService/providers/dubbo://192.168.0.132:20881/com.tuling.zk.dubbo.UserService?anyhost=true&application=simple-app&dubbo=2.6.2&generic=false&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=12956&side=provider&threads=200&timestamp=1570518532382 /dubbo/com.tuling.zk.dubbo.UserService/providers/dubbo://192.168.0.132:20882/com.tuling.zk.dubbo.UserService?anyhost=true&application=simple-app&dubbo=2.6.2&generic=false&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=2116&side=provider&threads=200&timestamp=1570518537021 /dubbo/com.tuling.zk.dubbo.UserService/consumers /dubbo/com.tuling.zk.dubbo.UserService/consumers/consumer://192.168.0.132/com.tuling.zk.dubbo.UserService?application=young-app&category=consumers&check=false&dubbo=2.6.2&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=9200&side=consumer&timeout=5000&timestamp=1570518819628 三、分布式JOB 分布式JOB需求： 多个服务节点只允许其中一个主节点运行JOB任务。 当主节点挂掉后能自动切换主节点，继续执行JOB任务。架构设计： node结构： tuling-master server0001:master server0002:slave server000n:slave 选举流程： 服务启动： 在tuling-maste下创建server子节点，值为slave 获取所有tuling-master 下所有子节点 判断是否存在master 节点 如果没有设置自己为master节点 子节点删除事件触发： 获取所有tuling-master 下所有子节点 判断是否存在master 节点 如果没有设置最小值序号为master 节点四、分布式锁 锁的的基本概念： 开发中锁的概念并不陌生，通过锁可以实现在多个线程或多个进程间在争抢资源时，能够合理的分配置资源的所有权。在单体应用中我们可以通过 synchronized 或ReentrantLock 来实现锁。但在分布式系统中，仅仅是加synchronized 是不够的，需要借助第三组件来实现。比如一些简单的做法是使用 关系型数据行级锁来实现不同进程之间的互斥，但大型分布式系统的性能瓶颈往往集中在数据库操作上。为了提高性能得采用如Redis、Zookeeper之内的组件实现分布式锁。 共享锁：也称作只读锁，当一方获得共享锁之后，其它方也可以获得共享锁。但其只允许读取。在共享锁全部释放之前，其它方不能获得写锁。 排它锁：也称作读写锁，获得排它锁后，可以进行数据的读写。在其释放之前，其它方不能获得任何锁。 锁的获取： 某银行帐户，可以同时进行帐户信息的读取，但读取其间不能修改帐户数据。其帐户ID为:888 获得读锁流程： 1、基于资源ID创建临时序号读锁节点 /lock/888.R0000000002 Read 2、获取 /lock 下所有子节点，判断其最小的节点是否为读锁，如果是则获锁成功 3、最小节点不是读锁，则阻塞等待。添加lock/ 子节点变更监听。 4、当节点变更监听触发，执行第2步 数据结构： 获得写锁： 1、基于资源ID创建临时序号写锁节点 /lock/888.R0000000002 Write 2、获取 /lock 下所有子节点，判断其最小的节点是否为自己，如果是则获锁成功 3、最小节点不是自己，则阻塞等待。添加lock/ 子节点变更监听。 4、当节点变更监听触发，执行第2步 释放锁： 读取完毕后，手动删除临时节点，如果获锁期间宕机，则会在会话失效后自动删除。 关于羊群效应： 在等待锁获得期间，所有等待节点都在监听 Lock节点，一但lock 节点变更所有等待节点都会被触发，然后在同时反查Lock 子节点。如果等待对例过大会使用Zookeeper承受非常大的流量压力。 为了改善这种情况，可以采用监听链表的方式，每个等待对列只监听前一个节点，如果前一个节点释放锁的时候，才会被触发通知。这样就形成了一个监听链表。 示例演示： package com.tuling.zookeeper.lock; import org.I0Itec.zkclient.IZkDataListener; import org.I0Itec.zkclient.ZkClient; import java.util.List; import java.util.stream.Collectors; /** * @author Tommy * Created by Tommy on 2019/9/23 **/ public class ZookeeperLock { private String server = \"192.168.0.149:2181\"; private ZkClient zkClient; private static final String rootPath = \"/tuling-lock\"; public ZookeeperLock() { zkClient = new ZkClient(server, 5000, 20000); buildRoot(); } // 构建根节点 public void buildRoot() { if (!zkClient.exists(rootPath)) { zkClient.createPersistent(rootPath); } } public Lock lock(String lockId, long timeout) { Lock lockNode = createLockNode(lockId); lockNode = tryActiveLock(lockNode);// 尝试激活锁 if (!lockNode.isActive()) { try { synchronized (lockNode) { lockNode.wait(timeout); } } catch (InterruptedException e) { throw new RuntimeException(e); } } if (!lockNode.isActive()) { throw new RuntimeException(\" lock timeout\"); } return lockNode; } public void unlock(Lock lock) { if (lock.isActive()) { zkClient.delete(lock.getPath()); } } // 尝试激活锁 private Lock tryActiveLock(Lock lockNode) { // 判断当前是否为最小节点 List list = zkClient.getChildren(rootPath) .stream() .sorted() .map(p -> rootPath + \"/\" + p) .collect(Collectors.toList()); String firstNodePath = list.get(0); if (firstNodePath.equals(lockNode.getPath())) { lockNode.setActive(true); } else { String upNodePath = list.get(list.indexOf(lockNode.getPath()) - 1); zkClient.subscribeDataChanges(upNodePath, new IZkDataListener() { @Override public void handleDataChange(String dataPath, Object data) throws Exception { } @Override public void handleDataDeleted(String dataPath) throws Exception { // 事件处理 与心跳 在同一个线程，如果Debug时占用太多时间，将导致本节点被删除，从而影响锁逻辑。 System.out.println(\"节点删除:\" + dataPath); Lock lock = tryActiveLock(lockNode); synchronized (lockNode) { if (lock.isActive()) { lockNode.notify(); } } zkClient.unsubscribeDataChanges(upNodePath, this); } }); } return lockNode; } public Lock createLockNode(String lockId) { String nodePath = zkClient.createEphemeralSequential(rootPath + \"/\" + lockId, \"lock\"); return new Lock(lockId, nodePath); } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-zab.html":{"url":"distributed/zookeeper/zookeeper-zab.html","title":"4.ZAB协议实现源码分析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、启动流程 1.工程结构介绍 2.启动宏观流程图： 3.集群启动详细流程 4.netty 服务启动流程： 二、快照与事务日志存储结构 概要: 存储结构: 快照相关配置： 快照装载流程： 课程概要： 启动流程源码分析 快照与事物日志的存储结构一、启动流程 知识点： 工程结构介绍 启动流程宏观图 集群启动详细流程 netty 服务工作机制1.工程结构介绍 项目地址:https://github.com/apache/zookeeper.git 分支tag ：3.5.5 zookeeper-recipes: 示例源码 zookeeper-client: C语言客户端 zookeeper-server：主体源码 2.启动宏观流程图： [ ] 启动示例演示： 服务端：ZooKeeperServerMain 客户端：ZooKeeperMain 3.集群启动详细流程 装载配置： # zookeeper 启动流程堆栈 >QuorumPeerMain#initializeAndRun //启动工程 >QuorumPeerConfig#parse // 加载config 配置 >QuorumPeerConfig#parseProperties// 解析config配置 >new DatadirCleanupManager // 构造一个数据清器 >DatadirCleanupManager#start // 启动定时任务 清除过期的快照 代码堆栈 ： >QuorumPeerMain#main //启动main方法 >QuorumPeerConfig#parse // 加载zoo.cfg 文件 >QuorumPeerConfig#parseProperties // 解析配置 >DatadirCleanupManager#start // 启动定时任务清除日志 >QuorumPeerConfig#isDistributed // 判断是否为集群模式 >ServerCnxnFactory#createFactory() // 创建服务默认为NIO，推荐netty //***创建 初始化集群管理器**/ >QuorumPeerMain#getQuorumPeer >QuorumPeer#setTxnFactory >new FileTxnSnapLog // 数据文件管理器，用于检测快照与日志文件 /** 初始化数据库*/ >new ZKDatabase >ZKDatabase#createDataTree //创建数据树，所有的节点都会存储在这 // 启动集群：同时启动线程 > QuorumPeer#start // > QuorumPeer#loadDataBase // 从快照文件以及日志文件 加载节点并填充到dataTree中去 > QuorumPeer#startServerCnxnFactory // 启动netty 或java nio 服务，对外开放2181 端口 > AdminServer#start// 启动管理服务，netty http服务，默认端口是8080 > QuorumPeer#startLeaderElection // 开始执行选举流程 > quorumPeer.join() // 防止主进程退出 流程说明: main方法启动 加载zoo.cfg 配置文件 解析配置 创建服务工厂 创建集群管理线程 设置数据库文件管理器 设置数据库 ....设置设置 start启动集群管理线程 加载数据节点至内存 启动netty 服务，对客户端开放端口 启动管理员Http服务，默认8080端口 启动选举流程 join 管理线程，防止main 进程退出 4.netty 服务启动流程： 服务UML类图 设置netty启动参数 -Dzookeeper.serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory 初始化： 关键代码： #初始化管道流 #channelHandler 是一个内部类是具体的消息处理器。 protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); if (secure) { initSSL(pipeline); } pipeline.addLast(\"servercnxnfactory\", channelHandler); } channelHandler 类结构 执行堆栈： NettyServerCnxnFactory#NettyServerCnxnFactory // 初始化netty服务工厂 > NettyUtils.newNioOrEpollEventLoopGroup // 创建IO线程组 > NettyUtils#newNioOrEpollEventLoopGroup() // 创建工作线程组 >ServerBootstrap#childHandler(io.netty.channel.ChannelHandler) // 添加管道流 >NettyServerCnxnFactory#start // 绑定端口，并启动netty服务 创建连接： 每当有客户端新连接进来，就会进入该方法 创建 NettyServerCnxn对象。并添加至cnxns对例 执行堆栈 CnxnChannelHandler#channelActive >new NettyServerCnxn // 构建连接器 >NettyServerCnxnFactory#addCnxn // 添加至连接器，并根据客户端IP进行分组 >ipMap.get(addr) // 基于IP进行分组 读取消息： 执行堆栈 CnxnChannelHandler#channelRead >NettyServerCnxn#processMessage // 处理消息 >NettyServerCnxn#receiveMessage // 接收消息 >ZooKeeperServer#processPacket //处理消息包 >org.apache.zookeeper.server.Request // 封装request 对象 >org.apache.zookeeper.server.ZooKeeperServer#submitRequest // 提交request >org.apache.zookeeper.server.RequestProcessor#processRequest // 处理请求 二、快照与事务日志存储结构 概要: ZK中所有的数据都是存储在内存中，即zkDataBase中。但同时所有对ZK数据的变更都会记录到事物日志中，并且当写入到一定的次数就会进行一次快照的生成。已保证数据的备份。其后缀就是ZXID（唯一事物ID）。 事物日志：每次增删改，的记录日志都会保存在文件当中 快照日志：存储了在指定时间节点下的所有的数据存储结构: zkDdataBase 是zk数据库基类，所有节点都会保存在该类当中，而对Zk进行任何的数据变更都会基于该类进行。zk数据的存储是通过DataTree 对象进行，其用了一个map 来进行存储。 UML 类图： 读取快照日志： org.apache.zookeeper.server.SnapshotFormatter 读取事物日志： org.apache.zookeeper.server.LogFormatter 快照相关配置： dataLogDir 事物日志目录 zookeeper.preAllocSize 预先开辟磁盘空间，用于后续写入事务日志，默认64M zookeeper.snapCount 每进行snapCount次事务日志输出后，触发一次快照，默认是100,000 autopurge.snapRetainCount 自动清除时 保留的快照数 autopurge.purgeInterval 清除时间间隔，小时为单位 -1 表示不自动清除。 快照装载流程： >ZooKeeperServer#loadData // 加载数据 >FileTxnSnapLog#restore // 恢复数据 >FileSnap#deserialize() // 反序列化数据 >FileSnap#findNValidSnapshots // 查找有效的快照 >Util#sortDataDir // 基于后缀排序文件 >persistence.Util#isValidSnapshot // 验证是否有效快照文件 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/":{"url":"distributed/netty/","title":"netty网络框架","keywords":"","body":"netty网络框架 1.分布式之netty Netty-黑马 Netty01-nio Netty02-入门 Netty03-进阶 Netty04-优化与源码 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:51:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/netty.html":{"url":"distributed/netty/netty.html","title":"1.分布式之netty","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.Java IO 与 NIO 1.1Linux I/O 模型介绍 1.1.1Linux I/O 流程 1.1.2 将 I/O 模型划分为以下五种类型： 1.1.3 各种 I/O 模型的比较 1.2Java I/O 1.3Java NIO 1.3.1java io 和 java nio 对比 1.3.2Java NIO 主要由3 部分核心组件组成 1.3.3NIO 带来了什么 2.Netty 编程实践 2.1Netty 介绍 2.1.1特性 2.2Netty 主要组件介绍 2.2.1Bootstrap 和 ServerBootstrap 2.2.2Transport Channel 2.2.3EventLoop和EventLoopGroup 2.2.4ChannelHandler和ChannelPipeline 2.2.5ByteBuf 2.3Netty 编程示例 3.Netty 线程模型解析 3.1Reactor 模式及其与 Netty 的对应关系 3.1.1单线程Reactor 3.1.2多线程Reactor 3.1.3Multiple Reactor 3.1.4主从Reactor 3.2Netty EventLoop 源码解析 3.2.1NioEventLoopGroup整体结构 3.2.2NioEventLoop创建分析 3.2.3NioEventLoop启动流程分析 3.2.4NioEventLoop执行流程分析 4.Netty 编解码编程实战 4.1 半包粘包问题示例与分析 4.2Netty 半包粘包问题解决 4.2.1LineBasedFrameDecoder（\\n, \\r\\n) 4.2.2DelimiterBasedFrameDecoder 4.2.3FixedLengthFrameDecoder 4.2.4LengthFieldBasedFrameDecoder 4.3Netty 编解码器分析 5.基于 Netty 实现高性能弹幕系统 5.1 弹幕系统概要设计 5.1.1弹幕系统特点 5.1.2弹幕系统架构设计 5.2Netty 对 Http 协议解析实现 5.2.1http报文解析方案： 5.2.2netty关于http 的解决方案： 5.2.3Netty Http的请求处理流程 5.3WebScoket 协议解析实现 5.3.1webSocket 协议简介： 5.3.2webSocket特点如下： 5.3.3WebSocket 协议报文格式： 6.基于 Netty 实现 RPC 框架 6.1RPC构建需要考虑的主要因素 6..1.2RPC框架 1.Java IO 与 NIO 1.1Linux I/O 模型介绍 1.1.1Linux I/O 流程 1.1.2 将 I/O 模型划分为以下五种类型： 阻塞式 I/O 模型 非阻塞式 I/O 模型 I/O 复用 信号驱动式 I/O 异步 I/O 1.1.3 各种 I/O 模型的比较 1.2Java I/O 1.3Java NIO 1.3.1java io 和 java nio 对比 1.3.2Java NIO 主要由3 部分核心组件组成 a. Buffer 一个 Buffer 本质上是内存中的一块， 可以将数据写入这块内存， 从这块内存获取数据 java.nio 定义了以下几个 Buffer 的实现 Java NIO Buffer 三大核心概念：position、limit、capacity 最好理解的当然是 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。 一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值。 b. Channel 所有的 NIO 操作始于通道，通道是数据来源或数据写入的目的地，主要地，java.nio 包中主要实现的以下几个 Channel： c. Selector Selector 是 Java NIO 中的一个组件，用于检查一个或多个 NIO Channel 的状态是否处于可读、可写 如此可以实现单线程管理多个 channels,也就是可以管理多个网络链接 1.3.3NIO 带来了什么 事件驱动模型 避免多线程 单线程处理多任务 非阻塞 IO,IO 读写不再阻塞,而是返回 0 基于 block 的传输,通常比基于流的传输更高效 更高级的 IO 函数,zero-copy IO 多路复用大大提高了 java 网络应用的可伸缩性和实用性 注意 使用NIO = 高性能 NIO不一定更快的场景 客户端应用 连接数 并发程度不高 局域网环境下 NIO完全屏蔽了平台差异(Linux poll/select/epoll, FreeBSD Kqueue) NIO仍然是基于各个OS平台的IO系统实现的,差异仍然存在 使用NIO做网络编程很容易 离散的事件驱动模型，编程困难2.Netty 编程实践 2.1Netty 介绍 2.1.1特性 设计 统一的API,适用于不同的协议(阻塞和非阻塞) 基于灵活、可扩展的事件驱动模型（SEDA） 高度可定制的线程模型 可靠的无连接数据Socket支持(UDP) 性能 更好的吞吐量,低延迟 更省资源 尽量减少不必要的内存拷贝 安全 完整的SSL/ TLS和STARTTLS的支持 易用 完善的Java doc,用户指南和样例 仅依赖于JDK1.6（netty 4.x)2.2Netty 主要组件介绍 2.2.1Bootstrap 和 ServerBootstrap Netty Server启动主要流程： 设置服务端ServerBootStrap启动参数 group(parentGroup, childGroup): channel(NioServerSocketChannel): 设置通道类型 handler()：设置NioServerSocketChannel的ChannelHandlerPipeline childHandler(): 设置NioSocketChannel的ChannelHandlerPipeline 通过ServerBootStrap的bind方法启动服务端，bind方法会在parentGroup中注册NioServerScoketChannel，监听客户端的连接请求 会创建一个NioServerSocketChannel实例，并将其在parentGroup中进行注册 Netty Server执行主要流程： Client发起连接CONNECT请求，parentGroup中的NioEventLoop不断轮循是否有新的客户端请求，如果有，ACCEPT事件触发 ACCEPT事件触发后，parentGroup中NioEventLoop会通过NioServerSocketChannel获取到对应的代表客户端的NioSocketChannel，并将其注册到childGroup中 childGroup中的NioEventLoop不断检测自己管理的NioSocketChannel是否有读写事件准备好 2.2.2Transport Channel 提供了统一的API，支持不同类型的传输层： OIO -阻塞IO NIO - Java NIO Epoll - Linux Epoll(JNI) Local Transport - IntraVM调用 Embedded Transport - 供测试使用的嵌入传输 UDS - Unix套接字的本地传输 2.2.3EventLoop和EventLoopGroup EventLoopGroup 包括多个EventLoop 多个EventLoop之间不交互 EventLoop： 每个EventLoop对应一个线程 所有连接(channel)都将注册到一个EventLoop，并且只注册到一个，整个生命周期中都不会变化 每个EventLoop管理着多个连接(channel) EventLoop来处理连接(Channel)上的读写事件 ServerBootstrap包括2个不同类型的EventLoopGroup: Parent EventLoop:负责处理Accept事件，接收请求 Child EventLoop：负责处理读写事件 ByteBuf通过两个索引（reader index、writer index）划分为三个区域： reader index前面的数据是已经读过的数据，这些数据可以丢弃 从reader index开始，到writer index之前的数据是可读数据 从writer index开始，为可写区域2.2.4ChannelHandler和ChannelPipeline ChannelHandler - 业务处理核心逻辑，用户自定义 Netty 提供2个重要的 ChannelHandler 子接口： ChannelInboundHandler - 处理进站数据和所有状态更改事件 ChannelOutboundHandler - 处理出站数据，允许拦截各种操作 ChannelPipeline 是ChannelHandler容器 包括一系列的ChannelHandler 实例,用于拦截流经一个 Channel 的入站和出站事件 每个Channel都有一个其ChannelPipeline 可以修改 ChannelPipeline 通过动态添加和删除 ChannelHandler 定义了丰富的API调用来回应入站和出站事件 ChannelHandlerContext表示 ChannelHandler 和ChannelPipeline 之间的关联 在 ChannelHandler 添加到 ChannelPipeline 时创建 ChannelHandlerContext表示 ChannelHandler 和ChannelPipeline 之间的关联 2.2.5ByteBuf 相比JDK ByteBuffer， 更加易于使用： 为读/写分别维护单独的指针，不需要通过flip()进行读/写模式切换 容量自动伸缩（类似于 ArrayList，StringBuilder） Fluent API (链式调用） 更好的性能： 通过内置的CompositeBuffer来减少数据拷贝（Zero copy） 支持内存池，减少GC压力2.3Netty 编程示例 3.Netty 线程模型解析 3.1Reactor 模式及其与 Netty 的对应关系 3.1.1单线程Reactor 3.1.2多线程Reactor 3.1.3Multiple Reactor 3.1.4主从Reactor 3.2Netty EventLoop 源码解析 3.2.1NioEventLoopGroup整体结构 3.2.2NioEventLoop创建分析 3.2.3NioEventLoop启动流程分析 3.2.4NioEventLoop执行流程分析 4.Netty 编解码编程实战 4.1 半包粘包问题示例与分析 4.2Netty 半包粘包问题解决 4.2.1LineBasedFrameDecoder（\\n, \\r\\n) 回车换行解码器 配合StringDecoder 4.2.2DelimiterBasedFrameDecoder 分隔符解码器 4.2.3FixedLengthFrameDecoder 固定长度解码器 4.2.4LengthFieldBasedFrameDecoder 基于'长度'解码器(私有协议最常用) 4.3Netty 编解码器分析 5.基于 Netty 实现高性能弹幕系统 5.1 弹幕系统概要设计 5.1.1弹幕系统特点 1.实时性高：你发我收， 毫秒之差 2.并发量大：一人吐槽，万人观看 5.1.2弹幕系统架构设计 业务架构 实现方案一 实现方案二 5.2Netty 对 Http 协议解析实现 request 报文 response 报文 5.2.1http报文解析方案： 1：请求行的边界是CRLF(回车)，如果读取到CRLF(回车)，则意味着请求行的信息已经读取完成。 2：Header的边界是CRLF，如果连续读取两个CRLF，则意味着header的信息读取完成。 3：body的长度是有Content-Length 来进行确定。 5.2.2netty关于http 的解决方案： // 解析请求 很多http server的实现都是基于servlet标准，但是netty对http实现并没有基于servlet。所以在使用上比Servlet复杂很多。比如在servlet 中直接可以通过 HttpServletRequest 获取 请求方法、请求头、请求参数。而netty 确需要通过如下对象自行解析获取。 HttpMethod：主要是对method的封装，包含method序列化的操作 HttpVersion: 对version的封装，netty包含1.0和1.1的版本 QueryStringDecoder: 主要是对urI进行解析，解析path和url上面的参数。 HttpPostRequestDecoder：对post 中body 内容进行解析获取 form 参数。 HttpHeaders：包含对header的内容进行封装及操作 HttpContent：是对body进行封装，本质上就是一个ByteBuf。如果ByteBuf的长度是固定的，则请求的body过大，可能包含多个HttpContent，其中最后一个为LastHttpContent(空的HttpContent),用来说明body的结束。 HttpRequest：主要包含对Request Line和Header的组合 FullHttpRequest： 主要包含对HttpRequest和httpContent的组合 5.2.3Netty Http的请求处理流程 从图中可以看出做为服务端的Netty 就是在做 编码和解码操作。其分别通过以下两个ChannelHandler对象实现： HttpRequestDecoder :用于从byteBuf 获取数据并解析封装成HttpRequest 对象 HttpResponseEncoder：用于将业务返回数据编码成 Response报文并发送到ByteBuf。 将以上两个对象添加进 Netty 的 pipeline 即可实现最简单的http 服务。 Decoder 流程 encode 流程 5.3WebScoket 协议解析实现 5.3.1webSocket 协议简介： webSocket 是html5 开始提供的一种浏览器与服务器间进行全双工二进制通信协议，其基于TCP双向全双工作进行消息传递，同一时刻即可以发又可以接收消息，相比Http的半双工协议性能有很大的提升， 5.3.2webSocket特点如下： 1.单一TCP长连接，采用全双工通信模式 2.对代理、防火墙透明 3.无头部信息、消息更精简 4.通过ping/pong 来保活 5.服务器可以主动推送消息给客户端，不在需要客户轮询 5.3.3WebSocket 协议报文格式： 我们知道，任何应用协议都有其特有的报文格式，比如Http协议通过 空格 换行组成其报文。如http 协议不同在于WebSocket属于二进制协议，通过规范进二进位来组成其报文。具体组成如下图： 通过javaScript 中的API可以直接操作WebSocket 对象，其示例如下： var ws = new WebSocket(“ws://localhost:8080”); ws.onopen = function()// 建立成功之后触发的事件 { console.log(“打开连接”); ws.send(\"ddd\"); // 发送消息 }; ws.onmessage = function(evt) { // 接收服务器消息 console.log(evt.data); }; ws.onclose = function(evt) { console.log(“WebSocketClosed!”); // 关闭连接 }; ws.onerror = function(evt) { console.log(“WebSocketError!”); // 连接异常 }; 6.基于 Netty 实现 RPC 框架 6.1RPC构建需要考虑的主要因素 通信协议 文本协议或二进制协议（RESTful with JSON or RPC with Binary Encoding） 支持的调用方式：单向、双向、Streaming API容错、可伸缩性6..1.2RPC框架 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:27:02 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/Netty00.html":{"url":"distributed/netty/Netty00.html","title":"Netty-黑马","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 三大组件 1.1 Channel & Buffer 1.2 Selector 多线程版设计 ⚠️ 多线程版缺点 线程池版设计 ⚠️ 线程池版缺点 selector 版设计 2. ByteBuffer 2.1 ByteBuffer 正确使用姿势 2.2 ByteBuffer 结构 &#x1F4A1; 调试工具类 2.3 ByteBuffer 常见方法 分配空间 向 buffer 写入数据 从 buffer 读取数据 mark 和 reset 字符串与 ByteBuffer 互转 ⚠️ Buffer 的线程安全 2.4 Scattering Reads 2.5 Gathering Writes 2.6 练习 3. 文件编程 3.1 FileChannel ⚠️ FileChannel 工作模式 获取 读取 写入 关闭 位置 大小 强制写入 3.2 两个 Channel 传输数据 3.3 Path 3.4 Files ⚠️ 删除很危险 4. 网络编程 4.1 非阻塞 vs 阻塞 阻塞 非阻塞 多路复用 4.2 Selector 创建 绑定 Channel 事件 监听 Channel 事件 &#x1F4A1; select 何时不阻塞 4.3 处理 accept 事件 &#x1F4A1; 事件发生后能否不处理 4.4 处理 read 事件 &#x1F4A1; 为何要 iter.remove() &#x1F4A1; cancel 的作用 ⚠️ 不处理边界的问题 处理消息的边界 ByteBuffer 大小分配 4.5 处理 write 事件 一次无法写完例子 &#x1F4A1; write 为何要取消 4.6 更进一步 &#x1F4A1; 利用多线程优化 &#x1F4A1; 如何拿到 cpu 个数 4.7 UDP 5. NIO vs BIO 5.1 stream vs channel 5.2 IO 模型 &#x1F516; 参考 5.3 零拷贝 传统 IO 问题 NIO 优化 5.3 AIO 文件 AIO &#x1F4A1; 守护线程 网络 AIO 一. NIO 基础 non-blocking io 非阻塞 IO 1. 三大组件 1.1 Channel & Buffer channel 有一点类似于 stream，它就是读写数据的双向通道，可以从 channel 将数据读入 buffer，也可以将 buffer 的数据写入 channel，而之前的 stream 要么是输入，要么是输出，channel 比 stream 更为底层 graph LR channel --> buffer buffer --> channel 常见的 Channel 有 FileChannel DatagramChannel SocketChannel ServerSocketChannel buffer 则用来缓冲读写数据，常见的 buffer 有 ByteBuffer MappedByteBuffer DirectByteBuffer HeapByteBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer CharBuffer 1.2 Selector selector 单从字面意思不好理解，需要结合服务器的设计演化来理解它的用途 多线程版设计 graph TD subgraph 多线程版 t1(thread) --> s1(socket1) t2(thread) --> s2(socket2) t3(thread) --> s3(socket3) end ⚠️ 多线程版缺点 内存占用高 线程上下文切换成本高 只适合连接数少的场景 线程池版设计 graph TD subgraph 线程池版 t4(thread) --> s4(socket1) t5(thread) --> s5(socket2) t4(thread) -.-> s6(socket3) t5(thread) -.-> s7(socket4) end ⚠️ 线程池版缺点 阻塞模式下，线程仅能处理一个 socket 连接 仅适合短连接场景 selector 版设计 selector 的作用就是配合一个线程来管理多个 channel，获取这些 channel 上发生的事件，这些 channel 工作在非阻塞模式下，不会让线程吊死在一个 channel 上。适合连接数特别多，但流量低的场景（low traffic） graph TD subgraph selector 版 thread --> selector selector --> c1(channel) selector --> c2(channel) selector --> c3(channel) end 调用 selector 的 select() 会阻塞直到 channel 发生了读写就绪事件，这些事件发生，select 方法就会返回这些事件交给 thread 来处理 2. ByteBuffer 有一普通文本文件 data.txt，内容为 1234567890abcd 使用 FileChannel 来读取文件内容 @Slf4j public class ChannelDemo1 { public static void main(String[] args) { try (RandomAccessFile file = new RandomAccessFile(\"helloword/data.txt\", \"rw\")) { FileChannel channel = file.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(10); do { // 向 buffer 写入 int len = channel.read(buffer); log.debug(\"读到字节数：{}\", len); if (len == -1) { break; } // 切换 buffer 读模式 buffer.flip(); while(buffer.hasRemaining()) { log.debug(\"{}\", (char)buffer.get()); } // 切换 buffer 写模式 buffer.clear(); } while (true); } catch (IOException e) { e.printStackTrace(); } } } 输出 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：10 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 1 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 2 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 3 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 4 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 5 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 6 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 7 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 8 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 9 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 0 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：4 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - a 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - b 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - c 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - d 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：-1 2.1 ByteBuffer 正确使用姿势 向 buffer 写入数据，例如调用 channel.read(buffer) 调用 flip() 切换至读模式 从 buffer 读取数据，例如调用 buffer.get() 调用 clear() 或 compact() 切换至写模式 重复 1~4 步骤 2.2 ByteBuffer 结构 ByteBuffer 有以下重要属性 capacity position limit 一开始 写模式下，position 是写入位置，limit 等于容量，下图表示写入了 4 个字节后的状态 flip 动作发生后，position 切换为读取位置，limit 切换为读取限制 读取 4 个字节后，状态 clear 动作发生后，状态 compact 方法，是把未读完的部分向前压缩，然后切换至写模式 &#x1F4A1; 调试工具类 public class ByteBufferUtil { private static final char[] BYTE2CHAR = new char[256]; private static final char[] HEXDUMP_TABLE = new char[256 * 4]; private static final String[] HEXPADDING = new String[16]; private static final String[] HEXDUMP_ROWPREFIXES = new String[65536 >>> 4]; private static final String[] BYTE2HEX = new String[256]; private static final String[] BYTEPADDING = new String[16]; static { final char[] DIGITS = \"0123456789abcdef\".toCharArray(); for (int i = 0; i >> 4 & 0x0F]; HEXDUMP_TABLE[(i = 0x7f) { BYTE2CHAR[i] = '.'; } else { BYTE2CHAR[i] = (char) i; } } } /** * 打印所有内容 * @param buffer */ public static void debugAll(ByteBuffer buffer) { int oldlimit = buffer.limit(); buffer.limit(buffer.capacity()); StringBuilder origin = new StringBuilder(256); appendPrettyHexDump(origin, buffer, 0, buffer.capacity()); System.out.println(\"+--------+-------------------- all ------------------------+----------------+\"); System.out.printf(\"position: [%d], limit: [%d]\\n\", buffer.position(), oldlimit); System.out.println(origin); buffer.limit(oldlimit); } /** * 打印可读取内容 * @param buffer */ public static void debugRead(ByteBuffer buffer) { StringBuilder builder = new StringBuilder(256); appendPrettyHexDump(builder, buffer, buffer.position(), buffer.limit() - buffer.position()); System.out.println(\"+--------+-------------------- read -----------------------+----------------+\"); System.out.printf(\"position: [%d], limit: [%d]\\n\", buffer.position(), buffer.limit()); System.out.println(builder); } private static void appendPrettyHexDump(StringBuilder dump, ByteBuffer buf, int offset, int length) { if (isOutOfBounds(offset, length, buf.capacity())) { throw new IndexOutOfBoundsException( \"expected: \" + \"0 >> 4; final int remainder = length & 0xF; // Dump the rows which have 16 bytes. for (int row = 0; row 2.3 ByteBuffer 常见方法 分配空间 可以使用 allocate 方法为 ByteBuffer 分配空间，其它 buffer 类也有该方法 Bytebuffer buf = ByteBuffer.allocate(16); 向 buffer 写入数据 有两种办法 调用 channel 的 read 方法 调用 buffer 自己的 put 方法 int readBytes = channel.read(buf); 和 buf.put((byte)127); 从 buffer 读取数据 同样有两种办法 调用 channel 的 write 方法 调用 buffer 自己的 get 方法 int writeBytes = channel.write(buf); 和 byte b = buf.get(); get 方法会让 position 读指针向后走，如果想重复读取数据 可以调用 rewind 方法将 position 重新置为 0 或者调用 get(int i) 方法获取索引 i 的内容，它不会移动读指针 mark 和 reset mark 是在读取时，做一个标记，即使 position 改变，只要调用 reset 就能回到 mark 的位置 注意 rewind 和 flip 都会清除 mark 位置 字符串与 ByteBuffer 互转 ByteBuffer buffer1 = StandardCharsets.UTF_8.encode(\"你好\"); ByteBuffer buffer2 = Charset.forName(\"utf-8\").encode(\"你好\"); debug(buffer1); debug(buffer2); CharBuffer buffer3 = StandardCharsets.UTF_8.decode(buffer1); System.out.println(buffer3.getClass()); System.out.println(buffer3.toString()); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| e4 bd a0 e5 a5 bd |...... | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| e4 bd a0 e5 a5 bd |...... | +--------+-------------------------------------------------+----------------+ class java.nio.HeapCharBuffer 你好 ⚠️ Buffer 的线程安全 Buffer 是非线程安全的 2.4 Scattering Reads 分散读取，有一个文本文件 3parts.txt onetwothree 使用如下方式读取，可以将数据填充至多个 buffer try (RandomAccessFile file = new RandomAccessFile(\"helloword/3parts.txt\", \"rw\")) { FileChannel channel = file.getChannel(); ByteBuffer a = ByteBuffer.allocate(3); ByteBuffer b = ByteBuffer.allocate(3); ByteBuffer c = ByteBuffer.allocate(5); channel.read(new ByteBuffer[]{a, b, c}); a.flip(); b.flip(); c.flip(); debug(a); debug(b); debug(c); } catch (IOException e) { e.printStackTrace(); } 结果 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6f 6e 65 |one | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 74 77 6f |two | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 74 68 72 65 65 |three | +--------+-------------------------------------------------+----------------+ 2.5 Gathering Writes 使用如下方式写入，可以将多个 buffer 的数据填充至 channel try (RandomAccessFile file = new RandomAccessFile(\"helloword/3parts.txt\", \"rw\")) { FileChannel channel = file.getChannel(); ByteBuffer d = ByteBuffer.allocate(4); ByteBuffer e = ByteBuffer.allocate(4); channel.position(11); d.put(new byte[]{'f', 'o', 'u', 'r'}); e.put(new byte[]{'f', 'i', 'v', 'e'}); d.flip(); e.flip(); debug(d); debug(e); channel.write(new ByteBuffer[]{d, e}); } catch (IOException e) { e.printStackTrace(); } 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 6f 75 72 |four | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 69 76 65 |five | +--------+-------------------------------------------------+----------------+ 文件内容 onetwothreefourfive 2.6 练习 网络上有多条数据发送给服务端，数据之间使用 \\n 进行分隔 但由于某种原因这些数据在接收时，被进行了重新组合，例如原始数据有3条为 Hello,world\\n I'm zhangsan\\n How are you?\\n 变成了下面的两个 byteBuffer (黏包，半包) Hello,world\\nI'm zhangsan\\nHo w are you?\\n 现在要求你编写程序，将错乱的数据恢复成原始的按 \\n 分隔的数据 public static void main(String[] args) { ByteBuffer source = ByteBuffer.allocate(32); // 11 24 source.put(\"Hello,world\\nI'm zhangsan\\nHo\".getBytes()); split(source); source.put(\"w are you?\\nhaha!\\n\".getBytes()); split(source); } private static void split(ByteBuffer source) { source.flip(); int oldLimit = source.limit(); for (int i = 0; i 3. 文件编程 3.1 FileChannel ⚠️ FileChannel 工作模式 FileChannel 只能工作在阻塞模式下 获取 不能直接打开 FileChannel，必须通过 FileInputStream、FileOutputStream 或者 RandomAccessFile 来获取 FileChannel，它们都有 getChannel 方法 通过 FileInputStream 获取的 channel 只能读 通过 FileOutputStream 获取的 channel 只能写 通过 RandomAccessFile 是否能读写根据构造 RandomAccessFile 时的读写模式决定 读取 会从 channel 读取数据填充 ByteBuffer，返回值表示读到了多少字节，-1 表示到达了文件的末尾 int readBytes = channel.read(buffer); 写入 写入的正确姿势如下， SocketChannel ByteBuffer buffer = ...; buffer.put(...); // 存入数据 buffer.flip(); // 切换读模式 while(buffer.hasRemaining()) { channel.write(buffer); } 在 while 中调用 channel.write 是因为 write 方法并不能保证一次将 buffer 中的内容全部写入 channel 关闭 channel 必须关闭，不过调用了 FileInputStream、FileOutputStream 或者 RandomAccessFile 的 close 方法会间接地调用 channel 的 close 方法 位置 获取当前位置 long pos = channel.position(); 设置当前位置 long newPos = ...; channel.position(newPos); 设置当前位置时，如果设置为文件的末尾 这时读取会返回 -1 这时写入，会追加内容，但要注意如果 position 超过了文件末尾，再写入时在新内容和原末尾之间会有空洞（00） 大小 使用 size 方法获取文件的大小 强制写入 操作系统出于性能的考虑，会将数据缓存，不是立刻写入磁盘。可以调用 force(true) 方法将文件内容和元数据（文件的权限等信息）立刻写入磁盘 3.2 两个 Channel 传输数据 String FROM = \"helloword/data.txt\"; String TO = \"helloword/to.txt\"; long start = System.nanoTime(); try (FileChannel from = new FileInputStream(FROM).getChannel(); FileChannel to = new FileOutputStream(TO).getChannel(); ) { from.transferTo(0, from.size(), to); } catch (IOException e) { e.printStackTrace(); } long end = System.nanoTime(); System.out.println(\"transferTo 用时：\" + (end - start) / 1000_000.0); 输出 transferTo 用时：8.2011 超过 2g 大小的文件传输 public class TestFileChannelTransferTo { public static void main(String[] args) { try ( FileChannel from = new FileInputStream(\"data.txt\").getChannel(); FileChannel to = new FileOutputStream(\"to.txt\").getChannel(); ) { // 效率高，底层会利用操作系统的零拷贝进行优化 long size = from.size(); // left 变量代表还剩余多少字节 for (long left = size; left > 0; ) { System.out.println(\"position:\" + (size - left) + \" left:\" + left); left -= from.transferTo((size - left), left, to); } } catch (IOException e) { e.printStackTrace(); } } } 实际传输一个超大文件 position:0 left:7769948160 position:2147483647 left:5622464513 position:4294967294 left:3474980866 position:6442450941 left:1327497219 3.3 Path jdk7 引入了 Path 和 Paths 类 Path 用来表示文件路径 Paths 是工具类，用来获取 Path 实例 Path source = Paths.get(\"1.txt\"); // 相对路径 使用 user.dir 环境变量来定位 1.txt Path source = Paths.get(\"d:\\\\1.txt\"); // 绝对路径 代表了 d:\\1.txt Path source = Paths.get(\"d:/1.txt\"); // 绝对路径 同样代表了 d:\\1.txt Path projects = Paths.get(\"d:\\\\data\", \"projects\"); // 代表了 d:\\data\\projects . 代表了当前路径 .. 代表了上一级路径 例如目录结构如下 d: |- data |- projects |- a |- b 代码 Path path = Paths.get(\"d:\\\\data\\\\projects\\\\a\\\\..\\\\b\"); System.out.println(path); System.out.println(path.normalize()); // 正常化路径 会输出 d:\\data\\projects\\a\\..\\b d:\\data\\projects\\b 3.4 Files 检查文件是否存在 Path path = Paths.get(\"helloword/data.txt\"); System.out.println(Files.exists(path)); 创建一级目录 Path path = Paths.get(\"helloword/d1\"); Files.createDirectory(path); 如果目录已存在，会抛异常 FileAlreadyExistsException 不能一次创建多级目录，否则会抛异常 NoSuchFileException 创建多级目录用 Path path = Paths.get(\"helloword/d1/d2\"); Files.createDirectories(path); 拷贝文件 Path source = Paths.get(\"helloword/data.txt\"); Path target = Paths.get(\"helloword/target.txt\"); Files.copy(source, target); 如果文件已存在，会抛异常 FileAlreadyExistsException 如果希望用 source 覆盖掉 target，需要用 StandardCopyOption 来控制 Files.copy(source, target, StandardCopyOption.REPLACE_EXISTING); 移动文件 Path source = Paths.get(\"helloword/data.txt\"); Path target = Paths.get(\"helloword/data.txt\"); Files.move(source, target, StandardCopyOption.ATOMIC_MOVE); StandardCopyOption.ATOMIC_MOVE 保证文件移动的原子性 删除文件 Path target = Paths.get(\"helloword/target.txt\"); Files.delete(target); 如果文件不存在，会抛异常 NoSuchFileException 删除目录 Path target = Paths.get(\"helloword/d1\"); Files.delete(target); 如果目录还有内容，会抛异常 DirectoryNotEmptyException 遍历目录文件 public static void main(String[] args) throws IOException { Path path = Paths.get(\"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_91\"); AtomicInteger dirCount = new AtomicInteger(); AtomicInteger fileCount = new AtomicInteger(); Files.walkFileTree(path, new SimpleFileVisitor(){ @Override public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException { System.out.println(dir); dirCount.incrementAndGet(); return super.preVisitDirectory(dir, attrs); } @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { System.out.println(file); fileCount.incrementAndGet(); return super.visitFile(file, attrs); } }); System.out.println(dirCount); // 133 System.out.println(fileCount); // 1479 } 统计 jar 的数目 Path path = Paths.get(\"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_91\"); AtomicInteger fileCount = new AtomicInteger(); Files.walkFileTree(path, new SimpleFileVisitor(){ @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { if (file.toFile().getName().endsWith(\".jar\")) { fileCount.incrementAndGet(); } return super.visitFile(file, attrs); } }); System.out.println(fileCount); // 724 删除多级目录 Path path = Paths.get(\"d:\\\\a\"); Files.walkFileTree(path, new SimpleFileVisitor(){ @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { Files.delete(file); return super.visitFile(file, attrs); } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException { Files.delete(dir); return super.postVisitDirectory(dir, exc); } }); ⚠️ 删除很危险 删除是危险操作，确保要递归删除的文件夹没有重要内容 拷贝多级目录 long start = System.currentTimeMillis(); String source = \"D:\\\\Snipaste-1.16.2-x64\"; String target = \"D:\\\\Snipaste-1.16.2-x64aaa\"; Files.walk(Paths.get(source)).forEach(path -> { try { String targetName = path.toString().replace(source, target); // 是目录 if (Files.isDirectory(path)) { Files.createDirectory(Paths.get(targetName)); } // 是普通文件 else if (Files.isRegularFile(path)) { Files.copy(path, Paths.get(targetName)); } } catch (IOException e) { e.printStackTrace(); } }); long end = System.currentTimeMillis(); System.out.println(end - start); 4. 网络编程 4.1 非阻塞 vs 阻塞 阻塞 阻塞模式下，相关方法都会导致线程暂停 ServerSocketChannel.accept 会在没有连接建立时让线程暂停 SocketChannel.read 会在没有数据可读时让线程暂停 阻塞的表现其实就是线程暂停了，暂停期间不会占用 cpu，但线程相当于闲置 单线程下，阻塞方法之间相互影响，几乎不能正常工作，需要多线程支持 但多线程下，有新的问题，体现在以下方面 32 位 jvm 一个线程 320k，64 位 jvm 一个线程 1024k，如果连接数过多，必然导致 OOM，并且线程太多，反而会因为频繁上下文切换导致性能降低 可以采用线程池技术来减少线程数和线程上下文切换，但治标不治本，如果有很多连接建立，但长时间 inactive，会阻塞线程池中所有线程，因此不适合长连接，只适合短连接 服务器端 // 使用 nio 来理解阻塞模式, 单线程 // 0. ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(16); // 1. 创建了服务器 ServerSocketChannel ssc = ServerSocketChannel.open(); // 2. 绑定监听端口 ssc.bind(new InetSocketAddress(8080)); // 3. 连接集合 List channels = new ArrayList<>(); while (true) { // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信 log.debug(\"connecting...\"); SocketChannel sc = ssc.accept(); // 阻塞方法，线程停止运行 log.debug(\"connected... {}\", sc); channels.add(sc); for (SocketChannel channel : channels) { // 5. 接收客户端发送的数据 log.debug(\"before read... {}\", channel); channel.read(buffer); // 阻塞方法，线程停止运行 buffer.flip(); debugRead(buffer); buffer.clear(); log.debug(\"after read...{}\", channel); } } 客户端 SocketChannel sc = SocketChannel.open(); sc.connect(new InetSocketAddress(\"localhost\", 8080)); System.out.println(\"waiting...\"); 非阻塞 非阻塞模式下，相关方法都会不会让线程暂停 在 ServerSocketChannel.accept 在没有连接建立时，会返回 null，继续运行 SocketChannel.read 在没有数据可读时，会返回 0，但线程不必阻塞，可以去执行其它 SocketChannel 的 read 或是去执行 ServerSocketChannel.accept 写数据时，线程只是等待数据写入 Channel 即可，无需等 Channel 通过网络把数据发送出去 但非阻塞模式下，即使没有连接建立，和可读数据，线程仍然在不断运行，白白浪费了 cpu 数据复制过程中，线程实际还是阻塞的（AIO 改进的地方） 服务器端，客户端代码不变 // 使用 nio 来理解非阻塞模式, 单线程 // 0. ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(16); // 1. 创建了服务器 ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.configureBlocking(false); // 非阻塞模式 // 2. 绑定监听端口 ssc.bind(new InetSocketAddress(8080)); // 3. 连接集合 List channels = new ArrayList<>(); while (true) { // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信 SocketChannel sc = ssc.accept(); // 非阻塞，线程还会继续运行，如果没有连接建立，但sc是null if (sc != null) { log.debug(\"connected... {}\", sc); sc.configureBlocking(false); // 非阻塞模式 channels.add(sc); } for (SocketChannel channel : channels) { // 5. 接收客户端发送的数据 int read = channel.read(buffer);// 非阻塞，线程仍然会继续运行，如果没有读到数据，read 返回 0 if (read > 0) { buffer.flip(); debugRead(buffer); buffer.clear(); log.debug(\"after read...{}\", channel); } } } 多路复用 单线程可以配合 Selector 完成对多个 Channel 可读写事件的监控，这称之为多路复用 多路复用仅针对网络 IO、普通文件 IO 没法利用多路复用 如果不用 Selector 的非阻塞模式，线程大部分时间都在做无用功，而 Selector 能够保证 有可连接事件时才去连接 有可读事件才去读取 有可写事件才去写入 限于网络传输能力，Channel 未必时时可写，一旦 Channel 可写，会触发 Selector 的可写事件 4.2 Selector graph TD subgraph selector 版 thread --> selector selector --> c1(channel) selector --> c2(channel) selector --> c3(channel) end 好处 一个线程配合 selector 就可以监控多个 channel 的事件，事件发生线程才去处理。避免非阻塞模式下所做无用功 让这个线程能够被充分利用 节约了线程的数量 减少了线程上下文切换 创建 Selector selector = Selector.open(); 绑定 Channel 事件 也称之为注册事件，绑定的事件 selector 才会关心 channel.configureBlocking(false); SelectionKey key = channel.register(selector, 绑定事件); channel 必须工作在非阻塞模式 FileChannel 没有非阻塞模式，因此不能配合 selector 一起使用 绑定的事件类型可以有 connect - 客户端连接成功时触发 accept - 服务器端成功接受连接时触发 read - 数据可读入时触发，有因为接收能力弱，数据暂不能读入的情况 write - 数据可写出时触发，有因为发送能力弱，数据暂不能写出的情况 监听 Channel 事件 可以通过下面三种方法来监听是否有事件发生，方法的返回值代表有多少 channel 发生了事件 方法1，阻塞直到绑定事件发生 int count = selector.select(); 方法2，阻塞直到绑定事件发生，或是超时（时间单位为 ms） int count = selector.select(long timeout); 方法3，不会阻塞，也就是不管有没有事件，立刻返回，自己根据返回值检查是否有事件 int count = selector.selectNow(); &#x1F4A1; select 何时不阻塞 事件发生时 客户端发起连接请求，会触发 accept 事件 客户端发送数据过来，客户端正常、异常关闭时，都会触发 read 事件，另外如果发送的数据大于 buffer 缓冲区，会触发多次读取事件 channel 可写，会触发 write 事件 在 linux 下 nio bug 发生时 调用 selector.wakeup() 调用 selector.close() selector 所在线程 interrupt 4.3 处理 accept 事件 客户端代码为 public class Client { public static void main(String[] args) { try (Socket socket = new Socket(\"localhost\", 8080)) { System.out.println(socket); socket.getOutputStream().write(\"world\".getBytes()); System.in.read(); } catch (IOException e) { e.printStackTrace(); } } } 服务器端代码为 @Slf4j public class ChannelDemo6 { public static void main(String[] args) { try (ServerSocketChannel channel = ServerSocketChannel.open()) { channel.bind(new InetSocketAddress(8080)); System.out.println(channel); Selector selector = Selector.open(); channel.configureBlocking(false); channel.register(selector, SelectionKey.OP_ACCEPT); while (true) { int count = selector.select(); // int count = selector.selectNow(); log.debug(\"select count: {}\", count); // if(count keys = selector.selectedKeys(); // 遍历所有事件，逐一处理 Iterator iter = keys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); // 判断事件类型 if (key.isAcceptable()) { ServerSocketChannel c = (ServerSocketChannel) key.channel(); // 必须处理 SocketChannel sc = c.accept(); log.debug(\"{}\", sc); } // 处理完毕，必须将事件移除 iter.remove(); } } } catch (IOException e) { e.printStackTrace(); } } } &#x1F4A1; 事件发生后能否不处理 事件发生后，要么处理，要么取消（cancel），不能什么都不做，否则下次该事件仍会触发，这是因为 nio 底层使用的是水平触发 4.4 处理 read 事件 @Slf4j public class ChannelDemo6 { public static void main(String[] args) { try (ServerSocketChannel channel = ServerSocketChannel.open()) { channel.bind(new InetSocketAddress(8080)); System.out.println(channel); Selector selector = Selector.open(); channel.configureBlocking(false); channel.register(selector, SelectionKey.OP_ACCEPT); while (true) { int count = selector.select(); // int count = selector.selectNow(); log.debug(\"select count: {}\", count); // if(count keys = selector.selectedKeys(); // 遍历所有事件，逐一处理 Iterator iter = keys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); // 判断事件类型 if (key.isAcceptable()) { ServerSocketChannel c = (ServerSocketChannel) key.channel(); // 必须处理 SocketChannel sc = c.accept(); sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ); log.debug(\"连接已建立: {}\", sc); } else if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(128); int read = sc.read(buffer); if(read == -1) { key.cancel(); sc.close(); } else { buffer.flip(); debug(buffer); } } // 处理完毕，必须将事件移除 iter.remove(); } } } catch (IOException e) { e.printStackTrace(); } } } 开启两个客户端，修改一下发送文字，输出 sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:8080] 21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - 连接已建立: java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:60367] 21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 65 6c 6c 6f |hello | +--------+-------------------------------------------------+----------------+ 21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - 连接已建立: java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:60378] 21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 77 6f 72 6c 64 |world | +--------+-------------------------------------------------+----------------+ &#x1F4A1; 为何要 iter.remove() 因为 select 在事件发生后，就会将相关的 key 放入 selectedKeys 集合，但不会在处理完后从 selectedKeys 集合中移除，需要我们自己编码删除。例如 第一次触发了 ssckey 上的 accept 事件，没有移除 ssckey 第二次触发了 sckey 上的 read 事件，但这时 selectedKeys 中还有上次的 ssckey ，在处理时因为没有真正的 serverSocket 连上了，就会导致空指针异常 &#x1F4A1; cancel 的作用 cancel 会取消注册在 selector 上的 channel，并从 keys 集合中删除 key 后续不会再监听事件 ⚠️ 不处理边界的问题 以前有同学写过这样的代码，思考注释中两个问题，以 bio 为例，其实 nio 道理是一样的 public class Server { public static void main(String[] args) throws IOException { ServerSocket ss=new ServerSocket(9000); while (true) { Socket s = ss.accept(); InputStream in = s.getInputStream(); // 这里这么写，有没有问题 byte[] arr = new byte[4]; while(true) { int read = in.read(arr); // 这里这么写，有没有问题 if(read == -1) { break; } System.out.println(new String(arr, 0, read)); } } } } 客户端 public class Client { public static void main(String[] args) throws IOException { Socket max = new Socket(\"localhost\", 9000); OutputStream out = max.getOutputStream(); out.write(\"hello\".getBytes()); out.write(\"world\".getBytes()); out.write(\"你好\".getBytes()); max.close(); } } 输出 hell owor ld� �好 为什么？ 处理消息的边界 一种思路是固定消息长度，数据包大小一样，服务器按预定长度读取，缺点是浪费带宽 另一种思路是按分隔符拆分，缺点是效率低 TLV 格式，即 Type 类型、Length 长度、Value 数据，类型和长度已知的情况下，就可以方便获取消息大小，分配合适的 buffer，缺点是 buffer 需要提前分配，如果内容过大，则影响 server 吞吐量 Http 1.1 是 TLV 格式 Http 2.0 是 LTV 格式 sequenceDiagram participant c1 as 客户端1 participant s as 服务器 participant b1 as ByteBuffer1 participant b2 as ByteBuffer2 c1 ->> s: 发送 01234567890abcdef3333\\r s ->> b1: 第一次 read 存入 01234567890abcdef s ->> b2: 扩容 b1 ->> b2: 拷贝 01234567890abcdef s ->> b2: 第二次 read 存入 3333\\r b2 ->> b2: 01234567890abcdef3333\\r 服务器端 private static void split(ByteBuffer source) { source.flip(); for (int i = 0; i iter = selector.selectedKeys().iterator(); // accept, read while (iter.hasNext()) { SelectionKey key = iter.next(); // 处理key 时，要从 selectedKeys 集合中删除，否则下次处理就会有问题 iter.remove(); log.debug(\"key: {}\", key); // 5. 区分事件类型 if (key.isAcceptable()) { // 如果是 accept ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); sc.configureBlocking(false); ByteBuffer buffer = ByteBuffer.allocate(16); // attachment // 将一个 byteBuffer 作为附件关联到 selectionKey 上 SelectionKey scKey = sc.register(selector, 0, buffer); scKey.interestOps(SelectionKey.OP_READ); log.debug(\"{}\", sc); log.debug(\"scKey:{}\", scKey); } else if (key.isReadable()) { // 如果是 read try { SocketChannel channel = (SocketChannel) key.channel(); // 拿到触发事件的channel // 获取 selectionKey 上关联的附件 ByteBuffer buffer = (ByteBuffer) key.attachment(); int read = channel.read(buffer); // 如果是正常断开，read 的方法的返回值是 -1 if(read == -1) { key.cancel(); } else { split(buffer); // 需要扩容 if (buffer.position() == buffer.limit()) { ByteBuffer newBuffer = ByteBuffer.allocate(buffer.capacity() * 2); buffer.flip(); newBuffer.put(buffer); // 0123456789abcdef3333\\n key.attach(newBuffer); } } } catch (IOException e) { e.printStackTrace(); key.cancel(); // 因为客户端断开了,因此需要将 key 取消（从 selector 的 keys 集合中真正删除 key） } } } } } 客户端 SocketChannel sc = SocketChannel.open(); sc.connect(new InetSocketAddress(\"localhost\", 8080)); SocketAddress address = sc.getLocalAddress(); // sc.write(Charset.defaultCharset().encode(\"hello\\nworld\\n\")); sc.write(Charset.defaultCharset().encode(\"0123\\n456789abcdef\")); sc.write(Charset.defaultCharset().encode(\"0123456789abcdef3333\\n\")); System.in.read(); ByteBuffer 大小分配 每个 channel 都需要记录可能被切分的消息，因为 ByteBuffer 不能被多个 channel 共同使用，因此需要为每个 channel 维护一个独立的 ByteBuffer ByteBuffer 不能太大，比如一个 ByteBuffer 1Mb 的话，要支持百万连接就要 1Tb 内存，因此需要设计大小可变的 ByteBuffer 一种思路是首先分配一个较小的 buffer，例如 4k，如果发现数据不够，再分配 8k 的 buffer，将 4k buffer 内容拷贝至 8k buffer，优点是消息连续容易处理，缺点是数据拷贝耗费性能，参考实现 http://tutorials.jenkov.com/java-performance/resizable-array.html 另一种思路是用多个数组组成 buffer，一个数组不够，把多出来的内容写入新的数组，与前面的区别是消息存储不连续解析复杂，优点是避免了拷贝引起的性能损耗 4.5 处理 write 事件 一次无法写完例子 非阻塞模式下，无法保证把 buffer 中所有数据都写入 channel，因此需要追踪 write 方法的返回值（代表实际写入字节数） 用 selector 监听所有 channel 的可写事件，每个 channel 都需要一个 key 来跟踪 buffer，但这样又会导致占用内存过多，就有两阶段策略 当消息处理器第一次写入消息时，才将 channel 注册到 selector 上 selector 检查 channel 上的可写事件，如果所有的数据写完了，就取消 channel 的注册 如果不取消，会每次可写均会触发 write 事件 public class WriteServer { public static void main(String[] args) throws IOException { ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.configureBlocking(false); ssc.bind(new InetSocketAddress(8080)); Selector selector = Selector.open(); ssc.register(selector, SelectionKey.OP_ACCEPT); while(true) { selector.select(); Iterator iter = selector.selectedKeys().iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); iter.remove(); if (key.isAcceptable()) { SocketChannel sc = ssc.accept(); sc.configureBlocking(false); SelectionKey sckey = sc.register(selector, SelectionKey.OP_READ); // 1. 向客户端发送内容 StringBuilder sb = new StringBuilder(); for (int i = 0; i 客户端 public class WriteClient { public static void main(String[] args) throws IOException { Selector selector = Selector.open(); SocketChannel sc = SocketChannel.open(); sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ); sc.connect(new InetSocketAddress(\"localhost\", 8080)); int count = 0; while (true) { selector.select(); Iterator iter = selector.selectedKeys().iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); iter.remove(); if (key.isConnectable()) { System.out.println(sc.finishConnect()); } else if (key.isReadable()) { ByteBuffer buffer = ByteBuffer.allocate(1024 * 1024); count += sc.read(buffer); buffer.clear(); System.out.println(count); } } } } } &#x1F4A1; write 为何要取消 只要向 channel 发送数据时，socket 缓冲可写，这个事件会频繁触发，因此应当只在 socket 缓冲区写不下时再关注可写事件，数据写完之后再取消关注 4.6 更进一步 &#x1F4A1; 利用多线程优化 现在都是多核 cpu，设计时要充分考虑别让 cpu 的力量被白白浪费 前面的代码只有一个选择器，没有充分利用多核 cpu，如何改进呢？ 分两组选择器 单线程配一个选择器，专门处理 accept 事件 创建 cpu 核心数的线程，每个线程配一个选择器，轮流处理 read 事件 public class ChannelDemo7 { public static void main(String[] args) throws IOException { new BossEventLoop().register(); } @Slf4j static class BossEventLoop implements Runnable { private Selector boss; private WorkerEventLoop[] workers; private volatile boolean start = false; AtomicInteger index = new AtomicInteger(); public void register() throws IOException { if (!start) { ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(8080)); ssc.configureBlocking(false); boss = Selector.open(); SelectionKey ssckey = ssc.register(boss, 0, null); ssckey.interestOps(SelectionKey.OP_ACCEPT); workers = initEventLoops(); new Thread(this, \"boss\").start(); log.debug(\"boss start...\"); start = true; } } public WorkerEventLoop[] initEventLoops() { // EventLoop[] eventLoops = new EventLoop[Runtime.getRuntime().availableProcessors()]; WorkerEventLoop[] workerEventLoops = new WorkerEventLoop[2]; for (int i = 0; i iter = boss.selectedKeys().iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); iter.remove(); if (key.isAcceptable()) { ServerSocketChannel c = (ServerSocketChannel) key.channel(); SocketChannel sc = c.accept(); sc.configureBlocking(false); log.debug(\"{} connected\", sc.getRemoteAddress()); workers[index.getAndIncrement() % workers.length].register(sc); } } } catch (IOException e) { e.printStackTrace(); } } } } @Slf4j static class WorkerEventLoop implements Runnable { private Selector worker; private volatile boolean start = false; private int index; private final ConcurrentLinkedQueue tasks = new ConcurrentLinkedQueue<>(); public WorkerEventLoop(int index) { this.index = index; } public void register(SocketChannel sc) throws IOException { if (!start) { worker = Selector.open(); new Thread(this, \"worker-\" + index).start(); start = true; } tasks.add(() -> { try { SelectionKey sckey = sc.register(worker, 0, null); sckey.interestOps(SelectionKey.OP_READ); worker.selectNow(); } catch (IOException e) { e.printStackTrace(); } }); worker.wakeup(); } @Override public void run() { while (true) { try { worker.select(); Runnable task = tasks.poll(); if (task != null) { task.run(); } Set keys = worker.selectedKeys(); Iterator iter = keys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(128); try { int read = sc.read(buffer); if (read == -1) { key.cancel(); sc.close(); } else { buffer.flip(); log.debug(\"{} message:\", sc.getRemoteAddress()); debugAll(buffer); } } catch (IOException e) { e.printStackTrace(); key.cancel(); sc.close(); } } iter.remove(); } } catch (IOException e) { e.printStackTrace(); } } } } } &#x1F4A1; 如何拿到 cpu 个数 Runtime.getRuntime().availableProcessors() 如果工作在 docker 容器下，因为容器不是物理隔离的，会拿到物理 cpu 个数，而不是容器申请时的个数 这个问题直到 jdk 10 才修复，使用 jvm 参数 UseContainerSupport 配置， 默认开启 4.7 UDP UDP 是无连接的，client 发送数据不会管 server 是否开启 server 这边的 receive 方法会将接收到的数据存入 byte buffer，但如果数据报文超过 buffer 大小，多出来的数据会被默默抛弃 首先启动服务器端 public class UdpServer { public static void main(String[] args) { try (DatagramChannel channel = DatagramChannel.open()) { channel.socket().bind(new InetSocketAddress(9999)); System.out.println(\"waiting...\"); ByteBuffer buffer = ByteBuffer.allocate(32); channel.receive(buffer); buffer.flip(); debug(buffer); } catch (IOException e) { e.printStackTrace(); } } } 输出 waiting... 运行客户端 public class UdpClient { public static void main(String[] args) { try (DatagramChannel channel = DatagramChannel.open()) { ByteBuffer buffer = StandardCharsets.UTF_8.encode(\"hello\"); InetSocketAddress address = new InetSocketAddress(\"localhost\", 9999); channel.send(buffer, address); } catch (Exception e) { e.printStackTrace(); } } } 接下来服务器端输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 65 6c 6c 6f |hello | +--------+-------------------------------------------------+----------------+ 5. NIO vs BIO 5.1 stream vs channel stream 不会自动缓冲数据，channel 会利用系统提供的发送缓冲区、接收缓冲区（更为底层） stream 仅支持阻塞 API，channel 同时支持阻塞、非阻塞 API，网络 channel 可配合 selector 实现多路复用 二者均为全双工，即读写可以同时进行 5.2 IO 模型 同步阻塞、同步非阻塞、同步多路复用、异步阻塞（没有此情况）、异步非阻塞 同步：线程自己去获取结果（一个线程） 异步：线程自己不去获取结果，而是由其它线程送结果（至少两个线程） 当调用一次 channel.read 或 stream.read 后，会切换至操作系统内核态来完成真正数据读取，而读取又分为两个阶段，分别为： 等待数据阶段 复制数据阶段 阻塞 IO 非阻塞 IO 多路复用 信号驱动 异步 IO 阻塞 IO vs 多路复用 &#x1F516; 参考 UNIX 网络编程 - 卷 I 5.3 零拷贝 传统 IO 问题 传统的 IO 将一个文件通过 socket 写出 File f = new File(\"helloword/data.txt\"); RandomAccessFile file = new RandomAccessFile(file, \"r\"); byte[] buf = new byte[(int)f.length()]; file.read(buf); Socket socket = ...; socket.getOutputStream().write(buf); 内部工作流程是这样的： java 本身并不具备 IO 读写能力，因此 read 方法调用后，要从 java 程序的用户态切换至内核态，去调用操作系统（Kernel）的读能力，将数据读入内核缓冲区。这期间用户线程阻塞，操作系统使用 DMA（Direct Memory Access）来实现文件读，其间也不会使用 cpu DMA 也可以理解为硬件单元，用来解放 cpu 完成文件 IO 从内核态切换回用户态，将数据从内核缓冲区读入用户缓冲区（即 byte[] buf），这期间 cpu 会参与拷贝，无法利用 DMA 调用 write 方法，这时将数据从用户缓冲区（byte[] buf）写入 socket 缓冲区，cpu 会参与拷贝 接下来要向网卡写数据，这项能力 java 又不具备，因此又得从用户态切换至内核态，调用操作系统的写能力，使用 DMA 将 socket 缓冲区的数据写入网卡，不会使用 cpu 可以看到中间环节较多，java 的 IO 实际不是物理设备级别的读写，而是缓存的复制，底层的真正读写是操作系统来完成的 用户态与内核态的切换发生了 3 次，这个操作比较重量级 数据拷贝了共 4 次 NIO 优化 通过 DirectByteBuf ByteBuffer.allocate(10) HeapByteBuffer 使用的还是 java 内存 ByteBuffer.allocateDirect(10) DirectByteBuffer 使用的是操作系统内存 大部分步骤与优化前相同，不再赘述。唯有一点：java 可以使用 DirectByteBuf 将堆外内存映射到 jvm 内存中来直接访问使用 这块内存不受 jvm 垃圾回收的影响，因此内存地址固定，有助于 IO 读写 java 中的 DirectByteBuf 对象仅维护了此内存的虚引用，内存回收分成两步 DirectByteBuf 对象被垃圾回收，将虚引用加入引用队列 通过专门线程访问引用队列，根据虚引用释放堆外内存 减少了一次数据拷贝，用户态与内核态的切换次数没有减少 进一步优化（底层采用了 linux 2.1 后提供的 sendFile 方法），java 中对应着两个 channel 调用 transferTo/transferFrom 方法拷贝数据 java 调用 transferTo 方法后，要从 java 程序的用户态切换至内核态，使用 DMA将数据读入内核缓冲区，不会使用 cpu 数据从内核缓冲区传输到 socket 缓冲区，cpu 会参与拷贝 最后使用 DMA 将 socket 缓冲区的数据写入网卡，不会使用 cpu 可以看到 只发生了一次用户态与内核态的切换 数据拷贝了 3 次 进一步优化（linux 2.4） java 调用 transferTo 方法后，要从 java 程序的用户态切换至内核态，使用 DMA将数据读入内核缓冲区，不会使用 cpu 只会将一些 offset 和 length 信息拷入 socket 缓冲区，几乎无消耗 使用 DMA 将 内核缓冲区的数据写入网卡，不会使用 cpu 整个过程仅只发生了一次用户态与内核态的切换，数据拷贝了 2 次。所谓的【零拷贝】，并不是真正无拷贝，而是在不会拷贝重复数据到 jvm 内存中，零拷贝的优点有 更少的用户态与内核态的切换 不利用 cpu 计算，减少 cpu 缓存伪共享 零拷贝适合小文件传输 5.3 AIO AIO 用来解决数据复制阶段的阻塞问题 同步意味着，在进行读写操作时，线程需要等待结果，还是相当于闲置 异步意味着，在进行读写操作时，线程不必等待结果，而是将来由操作系统来通过回调方式由另外的线程来获得结果 异步模型需要底层操作系统（Kernel）提供支持 Windows 系统通过 IOCP 实现了真正的异步 IO Linux 系统异步 IO 在 2.6 版本引入，但其底层实现还是用多路复用模拟了异步 IO，性能没有优势 文件 AIO 先来看看 AsynchronousFileChannel @Slf4j public class AioDemo1 { public static void main(String[] args) throws IOException { try{ AsynchronousFileChannel s = AsynchronousFileChannel.open( Paths.get(\"1.txt\"), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(2); log.debug(\"begin...\"); s.read(buffer, 0, null, new CompletionHandler() { @Override public void completed(Integer result, ByteBuffer attachment) { log.debug(\"read completed...{}\", result); buffer.flip(); debug(buffer); } @Override public void failed(Throwable exc, ByteBuffer attachment) { log.debug(\"read failed...\"); } }); } catch (IOException e) { e.printStackTrace(); } log.debug(\"do other things...\"); System.in.read(); } } 输出 13:44:56 [DEBUG] [main] c.i.aio.AioDemo1 - begin... 13:44:56 [DEBUG] [main] c.i.aio.AioDemo1 - do other things... 13:44:56 [DEBUG] [Thread-5] c.i.aio.AioDemo1 - read completed...2 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 0d |a. | +--------+-------------------------------------------------+----------------+ 可以看到 响应文件读取成功的是另一个线程 Thread-5 主线程并没有 IO 操作阻塞 &#x1F4A1; 守护线程 默认文件 AIO 使用的线程都是守护线程，所以最后要执行 System.in.read() 以避免守护线程意外结束 网络 AIO public class AioServer { public static void main(String[] args) throws IOException { AsynchronousServerSocketChannel ssc = AsynchronousServerSocketChannel.open(); ssc.bind(new InetSocketAddress(8080)); ssc.accept(null, new AcceptHandler(ssc)); System.in.read(); } private static void closeChannel(AsynchronousSocketChannel sc) { try { System.out.printf(\"[%s] %s close\\n\", Thread.currentThread().getName(), sc.getRemoteAddress()); sc.close(); } catch (IOException e) { e.printStackTrace(); } } private static class ReadHandler implements CompletionHandler { private final AsynchronousSocketChannel sc; public ReadHandler(AsynchronousSocketChannel sc) { this.sc = sc; } @Override public void completed(Integer result, ByteBuffer attachment) { try { if (result == -1) { closeChannel(sc); return; } System.out.printf(\"[%s] %s read\\n\", Thread.currentThread().getName(), sc.getRemoteAddress()); attachment.flip(); System.out.println(Charset.defaultCharset().decode(attachment)); attachment.clear(); // 处理完第一个 read 时，需要再次调用 read 方法来处理下一个 read 事件 sc.read(attachment, attachment, this); } catch (IOException e) { e.printStackTrace(); } } @Override public void failed(Throwable exc, ByteBuffer attachment) { closeChannel(sc); exc.printStackTrace(); } } private static class WriteHandler implements CompletionHandler { private final AsynchronousSocketChannel sc; private WriteHandler(AsynchronousSocketChannel sc) { this.sc = sc; } @Override public void completed(Integer result, ByteBuffer attachment) { // 如果作为附件的 buffer 还有内容，需要再次 write 写出剩余内容 if (attachment.hasRemaining()) { sc.write(attachment); } } @Override public void failed(Throwable exc, ByteBuffer attachment) { exc.printStackTrace(); closeChannel(sc); } } private static class AcceptHandler implements CompletionHandler { private final AsynchronousServerSocketChannel ssc; public AcceptHandler(AsynchronousServerSocketChannel ssc) { this.ssc = ssc; } @Override public void completed(AsynchronousSocketChannel sc, Object attachment) { try { System.out.printf(\"[%s] %s connected\\n\", Thread.currentThread().getName(), sc.getRemoteAddress()); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(16); // 读事件由 ReadHandler 处理 sc.read(buffer, buffer, new ReadHandler(sc)); // 写事件由 WriteHandler 处理 sc.write(Charset.defaultCharset().encode(\"server hello!\"), ByteBuffer.allocate(16), new WriteHandler(sc)); // 处理完第一个 accpet 时，需要再次调用 accept 方法来处理下一个 accept 事件 ssc.accept(null, this); } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 09:20:53 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/Netty01-nio.html":{"url":"distributed/netty/Netty01-nio.html","title":"Netty01-nio","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 三大组件 1.1 Channel & Buffer 1.2 Selector 多线程版设计 ⚠️ 多线程版缺点 线程池版设计 ⚠️ 线程池版缺点 selector 版设计 2. ByteBuffer 2.1 ByteBuffer 正确使用姿势 2.2 ByteBuffer 结构 &#x1F4A1; 调试工具类 2.3 ByteBuffer 常见方法 分配空间 向 buffer 写入数据 从 buffer 读取数据 mark 和 reset 字符串与 ByteBuffer 互转 ⚠️ Buffer 的线程安全 2.4 Scattering Reads 2.5 Gathering Writes 2.6 练习 3. 文件编程 3.1 FileChannel ⚠️ FileChannel 工作模式 获取 读取 写入 关闭 位置 大小 强制写入 3.2 两个 Channel 传输数据 3.3 Path 3.4 Files ⚠️ 删除很危险 4. 网络编程 4.1 非阻塞 vs 阻塞 阻塞 非阻塞 多路复用 4.2 Selector 创建 绑定 Channel 事件 监听 Channel 事件 &#x1F4A1; select 何时不阻塞 4.3 处理 accept 事件 &#x1F4A1; 事件发生后能否不处理 4.4 处理 read 事件 &#x1F4A1; 为何要 iter.remove() &#x1F4A1; cancel 的作用 ⚠️ 不处理边界的问题 处理消息的边界 ByteBuffer 大小分配 4.5 处理 write 事件 一次无法写完例子 &#x1F4A1; write 为何要取消 4.6 更进一步 &#x1F4A1; 利用多线程优化 &#x1F4A1; 如何拿到 cpu 个数 4.7 UDP 5. NIO vs BIO 5.1 stream vs channel 5.2 IO 模型 &#x1F516; 参考 5.3 零拷贝 传统 IO 问题 NIO 优化 5.3 AIO 文件 AIO &#x1F4A1; 守护线程 网络 AIO 一. NIO 基础 non-blocking io 非阻塞 IO 1. 三大组件 1.1 Channel & Buffer channel 有一点类似于 stream，它就是读写数据的双向通道，可以从 channel 将数据读入 buffer，也可以将 buffer 的数据写入 channel，而之前的 stream 要么是输入，要么是输出，channel 比 stream 更为底层 graph LR channel --> buffer buffer --> channel 常见的 Channel 有 FileChannel DatagramChannel SocketChannel ServerSocketChannel buffer 则用来缓冲读写数据，常见的 buffer 有 ByteBuffer MappedByteBuffer DirectByteBuffer HeapByteBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer CharBuffer 1.2 Selector selector 单从字面意思不好理解，需要结合服务器的设计演化来理解它的用途 多线程版设计 graph TD subgraph 多线程版 t1(thread) --> s1(socket1) t2(thread) --> s2(socket2) t3(thread) --> s3(socket3) end ⚠️ 多线程版缺点 内存占用高 线程上下文切换成本高 只适合连接数少的场景 线程池版设计 graph TD subgraph 线程池版 t4(thread) --> s4(socket1) t5(thread) --> s5(socket2) t4(thread) -.-> s6(socket3) t5(thread) -.-> s7(socket4) end ⚠️ 线程池版缺点 阻塞模式下，线程仅能处理一个 socket 连接 仅适合短连接场景 selector 版设计 selector 的作用就是配合一个线程来管理多个 channel，获取这些 channel 上发生的事件，这些 channel 工作在非阻塞模式下，不会让线程吊死在一个 channel 上。适合连接数特别多，但流量低的场景（low traffic） graph TD subgraph selector 版 thread --> selector selector --> c1(channel) selector --> c2(channel) selector --> c3(channel) end 调用 selector 的 select() 会阻塞直到 channel 发生了读写就绪事件，这些事件发生，select 方法就会返回这些事件交给 thread 来处理 2. ByteBuffer 有一普通文本文件 data.txt，内容为 1234567890abcd 使用 FileChannel 来读取文件内容 @Slf4j public class ChannelDemo1 { public static void main(String[] args) { try (RandomAccessFile file = new RandomAccessFile(\"helloword/data.txt\", \"rw\")) { FileChannel channel = file.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(10); do { // 向 buffer 写入 int len = channel.read(buffer); log.debug(\"读到字节数：{}\", len); if (len == -1) { break; } // 切换 buffer 读模式 buffer.flip(); while(buffer.hasRemaining()) { log.debug(\"{}\", (char)buffer.get()); } // 切换 buffer 写模式 buffer.clear(); } while (true); } catch (IOException e) { e.printStackTrace(); } } } 输出 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：10 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 1 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 2 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 3 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 4 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 5 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 6 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 7 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 8 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 9 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 0 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：4 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - a 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - b 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - c 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - d 10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：-1 2.1 ByteBuffer 正确使用姿势 向 buffer 写入数据，例如调用 channel.read(buffer) 调用 flip() 切换至读模式 从 buffer 读取数据，例如调用 buffer.get() 调用 clear() 或 compact() 切换至写模式 重复 1~4 步骤 2.2 ByteBuffer 结构 ByteBuffer 有以下重要属性 capacity position limit 一开始 写模式下，position 是写入位置，limit 等于容量，下图表示写入了 4 个字节后的状态 flip 动作发生后，position 切换为读取位置，limit 切换为读取限制 读取 4 个字节后，状态 clear 动作发生后，状态 compact 方法，是把未读完的部分向前压缩，然后切换至写模式 &#x1F4A1; 调试工具类 public class ByteBufferUtil { private static final char[] BYTE2CHAR = new char[256]; private static final char[] HEXDUMP_TABLE = new char[256 * 4]; private static final String[] HEXPADDING = new String[16]; private static final String[] HEXDUMP_ROWPREFIXES = new String[65536 >>> 4]; private static final String[] BYTE2HEX = new String[256]; private static final String[] BYTEPADDING = new String[16]; static { final char[] DIGITS = \"0123456789abcdef\".toCharArray(); for (int i = 0; i >> 4 & 0x0F]; HEXDUMP_TABLE[(i = 0x7f) { BYTE2CHAR[i] = '.'; } else { BYTE2CHAR[i] = (char) i; } } } /** * 打印所有内容 * @param buffer */ public static void debugAll(ByteBuffer buffer) { int oldlimit = buffer.limit(); buffer.limit(buffer.capacity()); StringBuilder origin = new StringBuilder(256); appendPrettyHexDump(origin, buffer, 0, buffer.capacity()); System.out.println(\"+--------+-------------------- all ------------------------+----------------+\"); System.out.printf(\"position: [%d], limit: [%d]\\n\", buffer.position(), oldlimit); System.out.println(origin); buffer.limit(oldlimit); } /** * 打印可读取内容 * @param buffer */ public static void debugRead(ByteBuffer buffer) { StringBuilder builder = new StringBuilder(256); appendPrettyHexDump(builder, buffer, buffer.position(), buffer.limit() - buffer.position()); System.out.println(\"+--------+-------------------- read -----------------------+----------------+\"); System.out.printf(\"position: [%d], limit: [%d]\\n\", buffer.position(), buffer.limit()); System.out.println(builder); } private static void appendPrettyHexDump(StringBuilder dump, ByteBuffer buf, int offset, int length) { if (isOutOfBounds(offset, length, buf.capacity())) { throw new IndexOutOfBoundsException( \"expected: \" + \"0 >> 4; final int remainder = length & 0xF; // Dump the rows which have 16 bytes. for (int row = 0; row 2.3 ByteBuffer 常见方法 分配空间 可以使用 allocate 方法为 ByteBuffer 分配空间，其它 buffer 类也有该方法 Bytebuffer buf = ByteBuffer.allocate(16); 向 buffer 写入数据 有两种办法 调用 channel 的 read 方法 调用 buffer 自己的 put 方法 int readBytes = channel.read(buf); 和 buf.put((byte)127); 从 buffer 读取数据 同样有两种办法 调用 channel 的 write 方法 调用 buffer 自己的 get 方法 int writeBytes = channel.write(buf); 和 byte b = buf.get(); get 方法会让 position 读指针向后走，如果想重复读取数据 可以调用 rewind 方法将 position 重新置为 0 或者调用 get(int i) 方法获取索引 i 的内容，它不会移动读指针 mark 和 reset mark 是在读取时，做一个标记，即使 position 改变，只要调用 reset 就能回到 mark 的位置 注意 rewind 和 flip 都会清除 mark 位置 字符串与 ByteBuffer 互转 ByteBuffer buffer1 = StandardCharsets.UTF_8.encode(\"你好\"); ByteBuffer buffer2 = Charset.forName(\"utf-8\").encode(\"你好\"); debug(buffer1); debug(buffer2); CharBuffer buffer3 = StandardCharsets.UTF_8.decode(buffer1); System.out.println(buffer3.getClass()); System.out.println(buffer3.toString()); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| e4 bd a0 e5 a5 bd |...... | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| e4 bd a0 e5 a5 bd |...... | +--------+-------------------------------------------------+----------------+ class java.nio.HeapCharBuffer 你好 ⚠️ Buffer 的线程安全 Buffer 是非线程安全的 2.4 Scattering Reads 分散读取，有一个文本文件 3parts.txt onetwothree 使用如下方式读取，可以将数据填充至多个 buffer try (RandomAccessFile file = new RandomAccessFile(\"helloword/3parts.txt\", \"rw\")) { FileChannel channel = file.getChannel(); ByteBuffer a = ByteBuffer.allocate(3); ByteBuffer b = ByteBuffer.allocate(3); ByteBuffer c = ByteBuffer.allocate(5); channel.read(new ByteBuffer[]{a, b, c}); a.flip(); b.flip(); c.flip(); debug(a); debug(b); debug(c); } catch (IOException e) { e.printStackTrace(); } 结果 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6f 6e 65 |one | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 74 77 6f |two | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 74 68 72 65 65 |three | +--------+-------------------------------------------------+----------------+ 2.5 Gathering Writes 使用如下方式写入，可以将多个 buffer 的数据填充至 channel try (RandomAccessFile file = new RandomAccessFile(\"helloword/3parts.txt\", \"rw\")) { FileChannel channel = file.getChannel(); ByteBuffer d = ByteBuffer.allocate(4); ByteBuffer e = ByteBuffer.allocate(4); channel.position(11); d.put(new byte[]{'f', 'o', 'u', 'r'}); e.put(new byte[]{'f', 'i', 'v', 'e'}); d.flip(); e.flip(); debug(d); debug(e); channel.write(new ByteBuffer[]{d, e}); } catch (IOException e) { e.printStackTrace(); } 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 6f 75 72 |four | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 69 76 65 |five | +--------+-------------------------------------------------+----------------+ 文件内容 onetwothreefourfive 2.6 练习 网络上有多条数据发送给服务端，数据之间使用 \\n 进行分隔 但由于某种原因这些数据在接收时，被进行了重新组合，例如原始数据有3条为 Hello,world\\n I'm zhangsan\\n How are you?\\n 变成了下面的两个 byteBuffer (黏包，半包) Hello,world\\nI'm zhangsan\\nHo w are you?\\n 现在要求你编写程序，将错乱的数据恢复成原始的按 \\n 分隔的数据 public static void main(String[] args) { ByteBuffer source = ByteBuffer.allocate(32); // 11 24 source.put(\"Hello,world\\nI'm zhangsan\\nHo\".getBytes()); split(source); source.put(\"w are you?\\nhaha!\\n\".getBytes()); split(source); } private static void split(ByteBuffer source) { source.flip(); int oldLimit = source.limit(); for (int i = 0; i 3. 文件编程 3.1 FileChannel ⚠️ FileChannel 工作模式 FileChannel 只能工作在阻塞模式下 获取 不能直接打开 FileChannel，必须通过 FileInputStream、FileOutputStream 或者 RandomAccessFile 来获取 FileChannel，它们都有 getChannel 方法 通过 FileInputStream 获取的 channel 只能读 通过 FileOutputStream 获取的 channel 只能写 通过 RandomAccessFile 是否能读写根据构造 RandomAccessFile 时的读写模式决定 读取 会从 channel 读取数据填充 ByteBuffer，返回值表示读到了多少字节，-1 表示到达了文件的末尾 int readBytes = channel.read(buffer); 写入 写入的正确姿势如下， SocketChannel ByteBuffer buffer = ...; buffer.put(...); // 存入数据 buffer.flip(); // 切换读模式 while(buffer.hasRemaining()) { channel.write(buffer); } 在 while 中调用 channel.write 是因为 write 方法并不能保证一次将 buffer 中的内容全部写入 channel 关闭 channel 必须关闭，不过调用了 FileInputStream、FileOutputStream 或者 RandomAccessFile 的 close 方法会间接地调用 channel 的 close 方法 位置 获取当前位置 long pos = channel.position(); 设置当前位置 long newPos = ...; channel.position(newPos); 设置当前位置时，如果设置为文件的末尾 这时读取会返回 -1 这时写入，会追加内容，但要注意如果 position 超过了文件末尾，再写入时在新内容和原末尾之间会有空洞（00） 大小 使用 size 方法获取文件的大小 强制写入 操作系统出于性能的考虑，会将数据缓存，不是立刻写入磁盘。可以调用 force(true) 方法将文件内容和元数据（文件的权限等信息）立刻写入磁盘 3.2 两个 Channel 传输数据 String FROM = \"helloword/data.txt\"; String TO = \"helloword/to.txt\"; long start = System.nanoTime(); try (FileChannel from = new FileInputStream(FROM).getChannel(); FileChannel to = new FileOutputStream(TO).getChannel(); ) { from.transferTo(0, from.size(), to); } catch (IOException e) { e.printStackTrace(); } long end = System.nanoTime(); System.out.println(\"transferTo 用时：\" + (end - start) / 1000_000.0); 输出 transferTo 用时：8.2011 超过 2g 大小的文件传输 public class TestFileChannelTransferTo { public static void main(String[] args) { try ( FileChannel from = new FileInputStream(\"data.txt\").getChannel(); FileChannel to = new FileOutputStream(\"to.txt\").getChannel(); ) { // 效率高，底层会利用操作系统的零拷贝进行优化 long size = from.size(); // left 变量代表还剩余多少字节 for (long left = size; left > 0; ) { System.out.println(\"position:\" + (size - left) + \" left:\" + left); left -= from.transferTo((size - left), left, to); } } catch (IOException e) { e.printStackTrace(); } } } 实际传输一个超大文件 position:0 left:7769948160 position:2147483647 left:5622464513 position:4294967294 left:3474980866 position:6442450941 left:1327497219 3.3 Path jdk7 引入了 Path 和 Paths 类 Path 用来表示文件路径 Paths 是工具类，用来获取 Path 实例 Path source = Paths.get(\"1.txt\"); // 相对路径 使用 user.dir 环境变量来定位 1.txt Path source = Paths.get(\"d:\\\\1.txt\"); // 绝对路径 代表了 d:\\1.txt Path source = Paths.get(\"d:/1.txt\"); // 绝对路径 同样代表了 d:\\1.txt Path projects = Paths.get(\"d:\\\\data\", \"projects\"); // 代表了 d:\\data\\projects . 代表了当前路径 .. 代表了上一级路径 例如目录结构如下 d: |- data |- projects |- a |- b 代码 Path path = Paths.get(\"d:\\\\data\\\\projects\\\\a\\\\..\\\\b\"); System.out.println(path); System.out.println(path.normalize()); // 正常化路径 会输出 d:\\data\\projects\\a\\..\\b d:\\data\\projects\\b 3.4 Files 检查文件是否存在 Path path = Paths.get(\"helloword/data.txt\"); System.out.println(Files.exists(path)); 创建一级目录 Path path = Paths.get(\"helloword/d1\"); Files.createDirectory(path); 如果目录已存在，会抛异常 FileAlreadyExistsException 不能一次创建多级目录，否则会抛异常 NoSuchFileException 创建多级目录用 Path path = Paths.get(\"helloword/d1/d2\"); Files.createDirectories(path); 拷贝文件 Path source = Paths.get(\"helloword/data.txt\"); Path target = Paths.get(\"helloword/target.txt\"); Files.copy(source, target); 如果文件已存在，会抛异常 FileAlreadyExistsException 如果希望用 source 覆盖掉 target，需要用 StandardCopyOption 来控制 Files.copy(source, target, StandardCopyOption.REPLACE_EXISTING); 移动文件 Path source = Paths.get(\"helloword/data.txt\"); Path target = Paths.get(\"helloword/data.txt\"); Files.move(source, target, StandardCopyOption.ATOMIC_MOVE); StandardCopyOption.ATOMIC_MOVE 保证文件移动的原子性 删除文件 Path target = Paths.get(\"helloword/target.txt\"); Files.delete(target); 如果文件不存在，会抛异常 NoSuchFileException 删除目录 Path target = Paths.get(\"helloword/d1\"); Files.delete(target); 如果目录还有内容，会抛异常 DirectoryNotEmptyException 遍历目录文件 public static void main(String[] args) throws IOException { Path path = Paths.get(\"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_91\"); AtomicInteger dirCount = new AtomicInteger(); AtomicInteger fileCount = new AtomicInteger(); Files.walkFileTree(path, new SimpleFileVisitor(){ @Override public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException { System.out.println(dir); dirCount.incrementAndGet(); return super.preVisitDirectory(dir, attrs); } @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { System.out.println(file); fileCount.incrementAndGet(); return super.visitFile(file, attrs); } }); System.out.println(dirCount); // 133 System.out.println(fileCount); // 1479 } 统计 jar 的数目 Path path = Paths.get(\"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_91\"); AtomicInteger fileCount = new AtomicInteger(); Files.walkFileTree(path, new SimpleFileVisitor(){ @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { if (file.toFile().getName().endsWith(\".jar\")) { fileCount.incrementAndGet(); } return super.visitFile(file, attrs); } }); System.out.println(fileCount); // 724 删除多级目录 Path path = Paths.get(\"d:\\\\a\"); Files.walkFileTree(path, new SimpleFileVisitor(){ @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { Files.delete(file); return super.visitFile(file, attrs); } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException { Files.delete(dir); return super.postVisitDirectory(dir, exc); } }); ⚠️ 删除很危险 删除是危险操作，确保要递归删除的文件夹没有重要内容 拷贝多级目录 long start = System.currentTimeMillis(); String source = \"D:\\\\Snipaste-1.16.2-x64\"; String target = \"D:\\\\Snipaste-1.16.2-x64aaa\"; Files.walk(Paths.get(source)).forEach(path -> { try { String targetName = path.toString().replace(source, target); // 是目录 if (Files.isDirectory(path)) { Files.createDirectory(Paths.get(targetName)); } // 是普通文件 else if (Files.isRegularFile(path)) { Files.copy(path, Paths.get(targetName)); } } catch (IOException e) { e.printStackTrace(); } }); long end = System.currentTimeMillis(); System.out.println(end - start); 4. 网络编程 4.1 非阻塞 vs 阻塞 阻塞 阻塞模式下，相关方法都会导致线程暂停 ServerSocketChannel.accept 会在没有连接建立时让线程暂停 SocketChannel.read 会在没有数据可读时让线程暂停 阻塞的表现其实就是线程暂停了，暂停期间不会占用 cpu，但线程相当于闲置 单线程下，阻塞方法之间相互影响，几乎不能正常工作，需要多线程支持 但多线程下，有新的问题，体现在以下方面 32 位 jvm 一个线程 320k，64 位 jvm 一个线程 1024k，如果连接数过多，必然导致 OOM，并且线程太多，反而会因为频繁上下文切换导致性能降低 可以采用线程池技术来减少线程数和线程上下文切换，但治标不治本，如果有很多连接建立，但长时间 inactive，会阻塞线程池中所有线程，因此不适合长连接，只适合短连接 服务器端 // 使用 nio 来理解阻塞模式, 单线程 // 0. ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(16); // 1. 创建了服务器 ServerSocketChannel ssc = ServerSocketChannel.open(); // 2. 绑定监听端口 ssc.bind(new InetSocketAddress(8080)); // 3. 连接集合 List channels = new ArrayList<>(); while (true) { // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信 log.debug(\"connecting...\"); SocketChannel sc = ssc.accept(); // 阻塞方法，线程停止运行 log.debug(\"connected... {}\", sc); channels.add(sc); for (SocketChannel channel : channels) { // 5. 接收客户端发送的数据 log.debug(\"before read... {}\", channel); channel.read(buffer); // 阻塞方法，线程停止运行 buffer.flip(); debugRead(buffer); buffer.clear(); log.debug(\"after read...{}\", channel); } } 客户端 SocketChannel sc = SocketChannel.open(); sc.connect(new InetSocketAddress(\"localhost\", 8080)); System.out.println(\"waiting...\"); 非阻塞 非阻塞模式下，相关方法都会不会让线程暂停 在 ServerSocketChannel.accept 在没有连接建立时，会返回 null，继续运行 SocketChannel.read 在没有数据可读时，会返回 0，但线程不必阻塞，可以去执行其它 SocketChannel 的 read 或是去执行 ServerSocketChannel.accept 写数据时，线程只是等待数据写入 Channel 即可，无需等 Channel 通过网络把数据发送出去 但非阻塞模式下，即使没有连接建立，和可读数据，线程仍然在不断运行，白白浪费了 cpu 数据复制过程中，线程实际还是阻塞的（AIO 改进的地方） 服务器端，客户端代码不变 // 使用 nio 来理解非阻塞模式, 单线程 // 0. ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(16); // 1. 创建了服务器 ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.configureBlocking(false); // 非阻塞模式 // 2. 绑定监听端口 ssc.bind(new InetSocketAddress(8080)); // 3. 连接集合 List channels = new ArrayList<>(); while (true) { // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信 SocketChannel sc = ssc.accept(); // 非阻塞，线程还会继续运行，如果没有连接建立，但sc是null if (sc != null) { log.debug(\"connected... {}\", sc); sc.configureBlocking(false); // 非阻塞模式 channels.add(sc); } for (SocketChannel channel : channels) { // 5. 接收客户端发送的数据 int read = channel.read(buffer);// 非阻塞，线程仍然会继续运行，如果没有读到数据，read 返回 0 if (read > 0) { buffer.flip(); debugRead(buffer); buffer.clear(); log.debug(\"after read...{}\", channel); } } } 多路复用 单线程可以配合 Selector 完成对多个 Channel 可读写事件的监控，这称之为多路复用 多路复用仅针对网络 IO、普通文件 IO 没法利用多路复用 如果不用 Selector 的非阻塞模式，线程大部分时间都在做无用功，而 Selector 能够保证 有可连接事件时才去连接 有可读事件才去读取 有可写事件才去写入 限于网络传输能力，Channel 未必时时可写，一旦 Channel 可写，会触发 Selector 的可写事件 4.2 Selector graph TD subgraph selector 版 thread --> selector selector --> c1(channel) selector --> c2(channel) selector --> c3(channel) end 好处 一个线程配合 selector 就可以监控多个 channel 的事件，事件发生线程才去处理。避免非阻塞模式下所做无用功 让这个线程能够被充分利用 节约了线程的数量 减少了线程上下文切换 创建 Selector selector = Selector.open(); 绑定 Channel 事件 也称之为注册事件，绑定的事件 selector 才会关心 channel.configureBlocking(false); SelectionKey key = channel.register(selector, 绑定事件); channel 必须工作在非阻塞模式 FileChannel 没有非阻塞模式，因此不能配合 selector 一起使用 绑定的事件类型可以有 connect - 客户端连接成功时触发 accept - 服务器端成功接受连接时触发 read - 数据可读入时触发，有因为接收能力弱，数据暂不能读入的情况 write - 数据可写出时触发，有因为发送能力弱，数据暂不能写出的情况 监听 Channel 事件 可以通过下面三种方法来监听是否有事件发生，方法的返回值代表有多少 channel 发生了事件 方法1，阻塞直到绑定事件发生 int count = selector.select(); 方法2，阻塞直到绑定事件发生，或是超时（时间单位为 ms） int count = selector.select(long timeout); 方法3，不会阻塞，也就是不管有没有事件，立刻返回，自己根据返回值检查是否有事件 int count = selector.selectNow(); &#x1F4A1; select 何时不阻塞 事件发生时 客户端发起连接请求，会触发 accept 事件 客户端发送数据过来，客户端正常、异常关闭时，都会触发 read 事件，另外如果发送的数据大于 buffer 缓冲区，会触发多次读取事件 channel 可写，会触发 write 事件 在 linux 下 nio bug 发生时 调用 selector.wakeup() 调用 selector.close() selector 所在线程 interrupt 4.3 处理 accept 事件 客户端代码为 public class Client { public static void main(String[] args) { try (Socket socket = new Socket(\"localhost\", 8080)) { System.out.println(socket); socket.getOutputStream().write(\"world\".getBytes()); System.in.read(); } catch (IOException e) { e.printStackTrace(); } } } 服务器端代码为 @Slf4j public class ChannelDemo6 { public static void main(String[] args) { try (ServerSocketChannel channel = ServerSocketChannel.open()) { channel.bind(new InetSocketAddress(8080)); System.out.println(channel); Selector selector = Selector.open(); channel.configureBlocking(false); channel.register(selector, SelectionKey.OP_ACCEPT); while (true) { int count = selector.select(); // int count = selector.selectNow(); log.debug(\"select count: {}\", count); // if(count keys = selector.selectedKeys(); // 遍历所有事件，逐一处理 Iterator iter = keys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); // 判断事件类型 if (key.isAcceptable()) { ServerSocketChannel c = (ServerSocketChannel) key.channel(); // 必须处理 SocketChannel sc = c.accept(); log.debug(\"{}\", sc); } // 处理完毕，必须将事件移除 iter.remove(); } } } catch (IOException e) { e.printStackTrace(); } } } &#x1F4A1; 事件发生后能否不处理 事件发生后，要么处理，要么取消（cancel），不能什么都不做，否则下次该事件仍会触发，这是因为 nio 底层使用的是水平触发 4.4 处理 read 事件 @Slf4j public class ChannelDemo6 { public static void main(String[] args) { try (ServerSocketChannel channel = ServerSocketChannel.open()) { channel.bind(new InetSocketAddress(8080)); System.out.println(channel); Selector selector = Selector.open(); channel.configureBlocking(false); channel.register(selector, SelectionKey.OP_ACCEPT); while (true) { int count = selector.select(); // int count = selector.selectNow(); log.debug(\"select count: {}\", count); // if(count keys = selector.selectedKeys(); // 遍历所有事件，逐一处理 Iterator iter = keys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); // 判断事件类型 if (key.isAcceptable()) { ServerSocketChannel c = (ServerSocketChannel) key.channel(); // 必须处理 SocketChannel sc = c.accept(); sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ); log.debug(\"连接已建立: {}\", sc); } else if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(128); int read = sc.read(buffer); if(read == -1) { key.cancel(); sc.close(); } else { buffer.flip(); debug(buffer); } } // 处理完毕，必须将事件移除 iter.remove(); } } } catch (IOException e) { e.printStackTrace(); } } } 开启两个客户端，修改一下发送文字，输出 sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:8080] 21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - 连接已建立: java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:60367] 21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 65 6c 6c 6f |hello | +--------+-------------------------------------------------+----------------+ 21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - 连接已建立: java.nio.channels.SocketChannel[connected local=/127.0.0.1:8080 remote=/127.0.0.1:60378] 21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 77 6f 72 6c 64 |world | +--------+-------------------------------------------------+----------------+ &#x1F4A1; 为何要 iter.remove() 因为 select 在事件发生后，就会将相关的 key 放入 selectedKeys 集合，但不会在处理完后从 selectedKeys 集合中移除，需要我们自己编码删除。例如 第一次触发了 ssckey 上的 accept 事件，没有移除 ssckey 第二次触发了 sckey 上的 read 事件，但这时 selectedKeys 中还有上次的 ssckey ，在处理时因为没有真正的 serverSocket 连上了，就会导致空指针异常 &#x1F4A1; cancel 的作用 cancel 会取消注册在 selector 上的 channel，并从 keys 集合中删除 key 后续不会再监听事件 ⚠️ 不处理边界的问题 以前有同学写过这样的代码，思考注释中两个问题，以 bio 为例，其实 nio 道理是一样的 public class Server { public static void main(String[] args) throws IOException { ServerSocket ss=new ServerSocket(9000); while (true) { Socket s = ss.accept(); InputStream in = s.getInputStream(); // 这里这么写，有没有问题 byte[] arr = new byte[4]; while(true) { int read = in.read(arr); // 这里这么写，有没有问题 if(read == -1) { break; } System.out.println(new String(arr, 0, read)); } } } } 客户端 public class Client { public static void main(String[] args) throws IOException { Socket max = new Socket(\"localhost\", 9000); OutputStream out = max.getOutputStream(); out.write(\"hello\".getBytes()); out.write(\"world\".getBytes()); out.write(\"你好\".getBytes()); max.close(); } } 输出 hell owor ld� �好 为什么？ 处理消息的边界 一种思路是固定消息长度，数据包大小一样，服务器按预定长度读取，缺点是浪费带宽 另一种思路是按分隔符拆分，缺点是效率低 TLV 格式，即 Type 类型、Length 长度、Value 数据，类型和长度已知的情况下，就可以方便获取消息大小，分配合适的 buffer，缺点是 buffer 需要提前分配，如果内容过大，则影响 server 吞吐量 Http 1.1 是 TLV 格式 Http 2.0 是 LTV 格式 sequenceDiagram participant c1 as 客户端1 participant s as 服务器 participant b1 as ByteBuffer1 participant b2 as ByteBuffer2 c1 ->> s: 发送 01234567890abcdef3333\\r s ->> b1: 第一次 read 存入 01234567890abcdef s ->> b2: 扩容 b1 ->> b2: 拷贝 01234567890abcdef s ->> b2: 第二次 read 存入 3333\\r b2 ->> b2: 01234567890abcdef3333\\r 服务器端 private static void split(ByteBuffer source) { source.flip(); for (int i = 0; i iter = selector.selectedKeys().iterator(); // accept, read while (iter.hasNext()) { SelectionKey key = iter.next(); // 处理key 时，要从 selectedKeys 集合中删除，否则下次处理就会有问题 iter.remove(); log.debug(\"key: {}\", key); // 5. 区分事件类型 if (key.isAcceptable()) { // 如果是 accept ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); sc.configureBlocking(false); ByteBuffer buffer = ByteBuffer.allocate(16); // attachment // 将一个 byteBuffer 作为附件关联到 selectionKey 上 SelectionKey scKey = sc.register(selector, 0, buffer); scKey.interestOps(SelectionKey.OP_READ); log.debug(\"{}\", sc); log.debug(\"scKey:{}\", scKey); } else if (key.isReadable()) { // 如果是 read try { SocketChannel channel = (SocketChannel) key.channel(); // 拿到触发事件的channel // 获取 selectionKey 上关联的附件 ByteBuffer buffer = (ByteBuffer) key.attachment(); int read = channel.read(buffer); // 如果是正常断开，read 的方法的返回值是 -1 if(read == -1) { key.cancel(); } else { split(buffer); // 需要扩容 if (buffer.position() == buffer.limit()) { ByteBuffer newBuffer = ByteBuffer.allocate(buffer.capacity() * 2); buffer.flip(); newBuffer.put(buffer); // 0123456789abcdef3333\\n key.attach(newBuffer); } } } catch (IOException e) { e.printStackTrace(); key.cancel(); // 因为客户端断开了,因此需要将 key 取消（从 selector 的 keys 集合中真正删除 key） } } } } } 客户端 SocketChannel sc = SocketChannel.open(); sc.connect(new InetSocketAddress(\"localhost\", 8080)); SocketAddress address = sc.getLocalAddress(); // sc.write(Charset.defaultCharset().encode(\"hello\\nworld\\n\")); sc.write(Charset.defaultCharset().encode(\"0123\\n456789abcdef\")); sc.write(Charset.defaultCharset().encode(\"0123456789abcdef3333\\n\")); System.in.read(); ByteBuffer 大小分配 每个 channel 都需要记录可能被切分的消息，因为 ByteBuffer 不能被多个 channel 共同使用，因此需要为每个 channel 维护一个独立的 ByteBuffer ByteBuffer 不能太大，比如一个 ByteBuffer 1Mb 的话，要支持百万连接就要 1Tb 内存，因此需要设计大小可变的 ByteBuffer 一种思路是首先分配一个较小的 buffer，例如 4k，如果发现数据不够，再分配 8k 的 buffer，将 4k buffer 内容拷贝至 8k buffer，优点是消息连续容易处理，缺点是数据拷贝耗费性能，参考实现 http://tutorials.jenkov.com/java-performance/resizable-array.html 另一种思路是用多个数组组成 buffer，一个数组不够，把多出来的内容写入新的数组，与前面的区别是消息存储不连续解析复杂，优点是避免了拷贝引起的性能损耗 4.5 处理 write 事件 一次无法写完例子 非阻塞模式下，无法保证把 buffer 中所有数据都写入 channel，因此需要追踪 write 方法的返回值（代表实际写入字节数） 用 selector 监听所有 channel 的可写事件，每个 channel 都需要一个 key 来跟踪 buffer，但这样又会导致占用内存过多，就有两阶段策略 当消息处理器第一次写入消息时，才将 channel 注册到 selector 上 selector 检查 channel 上的可写事件，如果所有的数据写完了，就取消 channel 的注册 如果不取消，会每次可写均会触发 write 事件 public class WriteServer { public static void main(String[] args) throws IOException { ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.configureBlocking(false); ssc.bind(new InetSocketAddress(8080)); Selector selector = Selector.open(); ssc.register(selector, SelectionKey.OP_ACCEPT); while(true) { selector.select(); Iterator iter = selector.selectedKeys().iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); iter.remove(); if (key.isAcceptable()) { SocketChannel sc = ssc.accept(); sc.configureBlocking(false); SelectionKey sckey = sc.register(selector, SelectionKey.OP_READ); // 1. 向客户端发送内容 StringBuilder sb = new StringBuilder(); for (int i = 0; i 客户端 public class WriteClient { public static void main(String[] args) throws IOException { Selector selector = Selector.open(); SocketChannel sc = SocketChannel.open(); sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ); sc.connect(new InetSocketAddress(\"localhost\", 8080)); int count = 0; while (true) { selector.select(); Iterator iter = selector.selectedKeys().iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); iter.remove(); if (key.isConnectable()) { System.out.println(sc.finishConnect()); } else if (key.isReadable()) { ByteBuffer buffer = ByteBuffer.allocate(1024 * 1024); count += sc.read(buffer); buffer.clear(); System.out.println(count); } } } } } &#x1F4A1; write 为何要取消 只要向 channel 发送数据时，socket 缓冲可写，这个事件会频繁触发，因此应当只在 socket 缓冲区写不下时再关注可写事件，数据写完之后再取消关注 4.6 更进一步 &#x1F4A1; 利用多线程优化 现在都是多核 cpu，设计时要充分考虑别让 cpu 的力量被白白浪费 前面的代码只有一个选择器，没有充分利用多核 cpu，如何改进呢？ 分两组选择器 单线程配一个选择器，专门处理 accept 事件 创建 cpu 核心数的线程，每个线程配一个选择器，轮流处理 read 事件 public class ChannelDemo7 { public static void main(String[] args) throws IOException { new BossEventLoop().register(); } @Slf4j static class BossEventLoop implements Runnable { private Selector boss; private WorkerEventLoop[] workers; private volatile boolean start = false; AtomicInteger index = new AtomicInteger(); public void register() throws IOException { if (!start) { ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(8080)); ssc.configureBlocking(false); boss = Selector.open(); SelectionKey ssckey = ssc.register(boss, 0, null); ssckey.interestOps(SelectionKey.OP_ACCEPT); workers = initEventLoops(); new Thread(this, \"boss\").start(); log.debug(\"boss start...\"); start = true; } } public WorkerEventLoop[] initEventLoops() { // EventLoop[] eventLoops = new EventLoop[Runtime.getRuntime().availableProcessors()]; WorkerEventLoop[] workerEventLoops = new WorkerEventLoop[2]; for (int i = 0; i iter = boss.selectedKeys().iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); iter.remove(); if (key.isAcceptable()) { ServerSocketChannel c = (ServerSocketChannel) key.channel(); SocketChannel sc = c.accept(); sc.configureBlocking(false); log.debug(\"{} connected\", sc.getRemoteAddress()); workers[index.getAndIncrement() % workers.length].register(sc); } } } catch (IOException e) { e.printStackTrace(); } } } } @Slf4j static class WorkerEventLoop implements Runnable { private Selector worker; private volatile boolean start = false; private int index; private final ConcurrentLinkedQueue tasks = new ConcurrentLinkedQueue<>(); public WorkerEventLoop(int index) { this.index = index; } public void register(SocketChannel sc) throws IOException { if (!start) { worker = Selector.open(); new Thread(this, \"worker-\" + index).start(); start = true; } tasks.add(() -> { try { SelectionKey sckey = sc.register(worker, 0, null); sckey.interestOps(SelectionKey.OP_READ); worker.selectNow(); } catch (IOException e) { e.printStackTrace(); } }); worker.wakeup(); } @Override public void run() { while (true) { try { worker.select(); Runnable task = tasks.poll(); if (task != null) { task.run(); } Set keys = worker.selectedKeys(); Iterator iter = keys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(128); try { int read = sc.read(buffer); if (read == -1) { key.cancel(); sc.close(); } else { buffer.flip(); log.debug(\"{} message:\", sc.getRemoteAddress()); debugAll(buffer); } } catch (IOException e) { e.printStackTrace(); key.cancel(); sc.close(); } } iter.remove(); } } catch (IOException e) { e.printStackTrace(); } } } } } &#x1F4A1; 如何拿到 cpu 个数 Runtime.getRuntime().availableProcessors() 如果工作在 docker 容器下，因为容器不是物理隔离的，会拿到物理 cpu 个数，而不是容器申请时的个数 这个问题直到 jdk 10 才修复，使用 jvm 参数 UseContainerSupport 配置， 默认开启 4.7 UDP UDP 是无连接的，client 发送数据不会管 server 是否开启 server 这边的 receive 方法会将接收到的数据存入 byte buffer，但如果数据报文超过 buffer 大小，多出来的数据会被默默抛弃 首先启动服务器端 public class UdpServer { public static void main(String[] args) { try (DatagramChannel channel = DatagramChannel.open()) { channel.socket().bind(new InetSocketAddress(9999)); System.out.println(\"waiting...\"); ByteBuffer buffer = ByteBuffer.allocate(32); channel.receive(buffer); buffer.flip(); debug(buffer); } catch (IOException e) { e.printStackTrace(); } } } 输出 waiting... 运行客户端 public class UdpClient { public static void main(String[] args) { try (DatagramChannel channel = DatagramChannel.open()) { ByteBuffer buffer = StandardCharsets.UTF_8.encode(\"hello\"); InetSocketAddress address = new InetSocketAddress(\"localhost\", 9999); channel.send(buffer, address); } catch (Exception e) { e.printStackTrace(); } } } 接下来服务器端输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 65 6c 6c 6f |hello | +--------+-------------------------------------------------+----------------+ 5. NIO vs BIO 5.1 stream vs channel stream 不会自动缓冲数据，channel 会利用系统提供的发送缓冲区、接收缓冲区（更为底层） stream 仅支持阻塞 API，channel 同时支持阻塞、非阻塞 API，网络 channel 可配合 selector 实现多路复用 二者均为全双工，即读写可以同时进行 5.2 IO 模型 同步阻塞、同步非阻塞、同步多路复用、异步阻塞（没有此情况）、异步非阻塞 同步：线程自己去获取结果（一个线程） 异步：线程自己不去获取结果，而是由其它线程送结果（至少两个线程） 当调用一次 channel.read 或 stream.read 后，会切换至操作系统内核态来完成真正数据读取，而读取又分为两个阶段，分别为： 等待数据阶段 复制数据阶段 阻塞 IO 非阻塞 IO 多路复用 信号驱动 异步 IO 阻塞 IO vs 多路复用 &#x1F516; 参考 UNIX 网络编程 - 卷 I 5.3 零拷贝 传统 IO 问题 传统的 IO 将一个文件通过 socket 写出 File f = new File(\"helloword/data.txt\"); RandomAccessFile file = new RandomAccessFile(file, \"r\"); byte[] buf = new byte[(int)f.length()]; file.read(buf); Socket socket = ...; socket.getOutputStream().write(buf); 内部工作流程是这样的： java 本身并不具备 IO 读写能力，因此 read 方法调用后，要从 java 程序的用户态切换至内核态，去调用操作系统（Kernel）的读能力，将数据读入内核缓冲区。这期间用户线程阻塞，操作系统使用 DMA（Direct Memory Access）来实现文件读，其间也不会使用 cpu DMA 也可以理解为硬件单元，用来解放 cpu 完成文件 IO 从内核态切换回用户态，将数据从内核缓冲区读入用户缓冲区（即 byte[] buf），这期间 cpu 会参与拷贝，无法利用 DMA 调用 write 方法，这时将数据从用户缓冲区（byte[] buf）写入 socket 缓冲区，cpu 会参与拷贝 接下来要向网卡写数据，这项能力 java 又不具备，因此又得从用户态切换至内核态，调用操作系统的写能力，使用 DMA 将 socket 缓冲区的数据写入网卡，不会使用 cpu 可以看到中间环节较多，java 的 IO 实际不是物理设备级别的读写，而是缓存的复制，底层的真正读写是操作系统来完成的 用户态与内核态的切换发生了 3 次，这个操作比较重量级 数据拷贝了共 4 次 NIO 优化 通过 DirectByteBuf ByteBuffer.allocate(10) HeapByteBuffer 使用的还是 java 内存 ByteBuffer.allocateDirect(10) DirectByteBuffer 使用的是操作系统内存 大部分步骤与优化前相同，不再赘述。唯有一点：java 可以使用 DirectByteBuf 将堆外内存映射到 jvm 内存中来直接访问使用 这块内存不受 jvm 垃圾回收的影响，因此内存地址固定，有助于 IO 读写 java 中的 DirectByteBuf 对象仅维护了此内存的虚引用，内存回收分成两步 DirectByteBuf 对象被垃圾回收，将虚引用加入引用队列 通过专门线程访问引用队列，根据虚引用释放堆外内存 减少了一次数据拷贝，用户态与内核态的切换次数没有减少 进一步优化（底层采用了 linux 2.1 后提供的 sendFile 方法），java 中对应着两个 channel 调用 transferTo/transferFrom 方法拷贝数据 java 调用 transferTo 方法后，要从 java 程序的用户态切换至内核态，使用 DMA将数据读入内核缓冲区，不会使用 cpu 数据从内核缓冲区传输到 socket 缓冲区，cpu 会参与拷贝 最后使用 DMA 将 socket 缓冲区的数据写入网卡，不会使用 cpu 可以看到 只发生了一次用户态与内核态的切换 数据拷贝了 3 次 进一步优化（linux 2.4） java 调用 transferTo 方法后，要从 java 程序的用户态切换至内核态，使用 DMA将数据读入内核缓冲区，不会使用 cpu 只会将一些 offset 和 length 信息拷入 socket 缓冲区，几乎无消耗 使用 DMA 将 内核缓冲区的数据写入网卡，不会使用 cpu 整个过程仅只发生了一次用户态与内核态的切换，数据拷贝了 2 次。所谓的【零拷贝】，并不是真正无拷贝，而是在不会拷贝重复数据到 jvm 内存中，零拷贝的优点有 更少的用户态与内核态的切换 不利用 cpu 计算，减少 cpu 缓存伪共享 零拷贝适合小文件传输 5.3 AIO AIO 用来解决数据复制阶段的阻塞问题 同步意味着，在进行读写操作时，线程需要等待结果，还是相当于闲置 异步意味着，在进行读写操作时，线程不必等待结果，而是将来由操作系统来通过回调方式由另外的线程来获得结果 异步模型需要底层操作系统（Kernel）提供支持 Windows 系统通过 IOCP 实现了真正的异步 IO Linux 系统异步 IO 在 2.6 版本引入，但其底层实现还是用多路复用模拟了异步 IO，性能没有优势 文件 AIO 先来看看 AsynchronousFileChannel @Slf4j public class AioDemo1 { public static void main(String[] args) throws IOException { try{ AsynchronousFileChannel s = AsynchronousFileChannel.open( Paths.get(\"1.txt\"), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(2); log.debug(\"begin...\"); s.read(buffer, 0, null, new CompletionHandler() { @Override public void completed(Integer result, ByteBuffer attachment) { log.debug(\"read completed...{}\", result); buffer.flip(); debug(buffer); } @Override public void failed(Throwable exc, ByteBuffer attachment) { log.debug(\"read failed...\"); } }); } catch (IOException e) { e.printStackTrace(); } log.debug(\"do other things...\"); System.in.read(); } } 输出 13:44:56 [DEBUG] [main] c.i.aio.AioDemo1 - begin... 13:44:56 [DEBUG] [main] c.i.aio.AioDemo1 - do other things... 13:44:56 [DEBUG] [Thread-5] c.i.aio.AioDemo1 - read completed...2 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 0d |a. | +--------+-------------------------------------------------+----------------+ 可以看到 响应文件读取成功的是另一个线程 Thread-5 主线程并没有 IO 操作阻塞 &#x1F4A1; 守护线程 默认文件 AIO 使用的线程都是守护线程，所以最后要执行 System.in.read() 以避免守护线程意外结束 网络 AIO public class AioServer { public static void main(String[] args) throws IOException { AsynchronousServerSocketChannel ssc = AsynchronousServerSocketChannel.open(); ssc.bind(new InetSocketAddress(8080)); ssc.accept(null, new AcceptHandler(ssc)); System.in.read(); } private static void closeChannel(AsynchronousSocketChannel sc) { try { System.out.printf(\"[%s] %s close\\n\", Thread.currentThread().getName(), sc.getRemoteAddress()); sc.close(); } catch (IOException e) { e.printStackTrace(); } } private static class ReadHandler implements CompletionHandler { private final AsynchronousSocketChannel sc; public ReadHandler(AsynchronousSocketChannel sc) { this.sc = sc; } @Override public void completed(Integer result, ByteBuffer attachment) { try { if (result == -1) { closeChannel(sc); return; } System.out.printf(\"[%s] %s read\\n\", Thread.currentThread().getName(), sc.getRemoteAddress()); attachment.flip(); System.out.println(Charset.defaultCharset().decode(attachment)); attachment.clear(); // 处理完第一个 read 时，需要再次调用 read 方法来处理下一个 read 事件 sc.read(attachment, attachment, this); } catch (IOException e) { e.printStackTrace(); } } @Override public void failed(Throwable exc, ByteBuffer attachment) { closeChannel(sc); exc.printStackTrace(); } } private static class WriteHandler implements CompletionHandler { private final AsynchronousSocketChannel sc; private WriteHandler(AsynchronousSocketChannel sc) { this.sc = sc; } @Override public void completed(Integer result, ByteBuffer attachment) { // 如果作为附件的 buffer 还有内容，需要再次 write 写出剩余内容 if (attachment.hasRemaining()) { sc.write(attachment); } } @Override public void failed(Throwable exc, ByteBuffer attachment) { exc.printStackTrace(); closeChannel(sc); } } private static class AcceptHandler implements CompletionHandler { private final AsynchronousServerSocketChannel ssc; public AcceptHandler(AsynchronousServerSocketChannel ssc) { this.ssc = ssc; } @Override public void completed(AsynchronousSocketChannel sc, Object attachment) { try { System.out.printf(\"[%s] %s connected\\n\", Thread.currentThread().getName(), sc.getRemoteAddress()); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(16); // 读事件由 ReadHandler 处理 sc.read(buffer, buffer, new ReadHandler(sc)); // 写事件由 WriteHandler 处理 sc.write(Charset.defaultCharset().encode(\"server hello!\"), ByteBuffer.allocate(16), new WriteHandler(sc)); // 处理完第一个 accpet 时，需要再次调用 accept 方法来处理下一个 accept 事件 ssc.accept(null, this); } @Override public void failed(Throwable exc, Object attachment) { exc.printStackTrace(); } } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 09:20:53 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/Netty02_use.html":{"url":"distributed/netty/Netty02_use.html","title":"Netty02-入门","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 概述 1.1 Netty 是什么？ 1.2 Netty 的作者 1.3 Netty 的地位 1.4 Netty 的优势 2. Hello World 2.1 目标 2.2 服务器端 2.3 客户端 2.4 流程梳理 &#x1F4A1; 提示 3. 组件 3.1 EventLoop &#x1F4A1; 优雅关闭 演示 NioEventLoop 处理 io 事件 &#x1F4A1; handler 执行中如何换人？ 演示 NioEventLoop 处理普通任务 演示 NioEventLoop 处理定时任务 3.2 Channel ChannelFuture CloseFuture &#x1F4A1; 异步提升的是什么 3.3 Future & Promise 例1 例2 例3 例4 例5 例6 3.4 Handler & Pipeline 3.5 ByteBuf 1）创建 2）直接内存 vs 堆内存 3）池化 vs 非池化 4）组成 5）写入 6）扩容 7）读取 8）retain & release 9）slice 10）duplicate 11）copy 12）CompositeByteBuf 13）Unpooled &#x1F4A1; ByteBuf 优势 4. 双向通信 4.1 练习 &#x1F4A1; 读和写的误解 二. Netty 入门 1. 概述 1.1 Netty 是什么？ Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients. Netty 是一个异步的、基于事件驱动的网络应用框架，用于快速开发可维护、高性能的网络服务器和客户端 1.2 Netty 的作者 他还是另一个著名网络应用框架 Mina 的重要贡献者 1.3 Netty 的地位 Netty 在 Java 网络应用框架中的地位就好比：Spring 框架在 JavaEE 开发中的地位 以下的框架都使用了 Netty，因为它们有网络通信需求！ Cassandra - nosql 数据库 Spark - 大数据分布式计算框架 Hadoop - 大数据分布式存储框架 RocketMQ - ali 开源的消息队列 ElasticSearch - 搜索引擎 gRPC - rpc 框架 Dubbo - rpc 框架 Spring 5.x - flux api 完全抛弃了 tomcat ，使用 netty 作为服务器端 Zookeeper - 分布式协调框架 1.4 Netty 的优势 Netty vs NIO，工作量大，bug 多 需要自己构建协议 解决 TCP 传输问题，如粘包、半包 epoll 空轮询导致 CPU 100% 对 API 进行增强，使之更易用，如 FastThreadLocal => ThreadLocal，ByteBuf => ByteBuffer Netty vs 其它网络应用框架 Mina 由 apache 维护，将来 3.x 版本可能会有较大重构，破坏 API 向下兼容性，Netty 的开发迭代更迅速，API 更简洁、文档更优秀 久经考验，16年，Netty 版本 2.x 2004 3.x 2008 4.x 2013 5.x 已废弃（没有明显的性能提升，维护成本高） 2. Hello World 2.1 目标 开发一个简单的服务器端和客户端 客户端向服务器端发送 hello, world 服务器仅接收，不返回 加入依赖 io.netty netty-all 4.1.39.Final 2.2 服务器端 new ServerBootstrap() .group(new NioEventLoopGroup()) // 1 .channel(NioServerSocketChannel.class) // 2 .childHandler(new ChannelInitializer() { // 3 protected void initChannel(NioSocketChannel ch) { ch.pipeline().addLast(new StringDecoder()); // 5 ch.pipeline().addLast(new SimpleChannelInboundHandler() { // 6 @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) { System.out.println(msg); } }); } }) .bind(8080); // 4 代码解读 1 处，创建 NioEventLoopGroup，可以简单理解为 线程池 + Selector 后面会详细展开 2 处，选择服务 Scoket 实现类，其中 NioServerSocketChannel 表示基于 NIO 的服务器端实现，其它实现还有 3 处，为啥方法叫 childHandler，是接下来添加的处理器都是给 SocketChannel 用的，而不是给 ServerSocketChannel。ChannelInitializer 处理器（仅执行一次），它的作用是待客户端 SocketChannel 建立连接后，执行 initChannel 以便添加更多的处理器 4 处，ServerSocketChannel 绑定的监听端口 5 处，SocketChannel 的处理器，解码 ByteBuf => String 6 处，SocketChannel 的业务处理器，使用上一个处理器的处理结果 2.3 客户端 new Bootstrap() .group(new NioEventLoopGroup()) // 1 .channel(NioSocketChannel.class) // 2 .handler(new ChannelInitializer() { // 3 @Override protected void initChannel(Channel ch) { ch.pipeline().addLast(new StringEncoder()); // 8 } }) .connect(\"127.0.0.1\", 8080) // 4 .sync() // 5 .channel() // 6 .writeAndFlush(new Date() + \": hello world!\"); // 7 代码解读 1 处，创建 NioEventLoopGroup，同 Server 2 处，选择客户 Socket 实现类，NioSocketChannel 表示基于 NIO 的客户端实现，其它实现还有 3 处，添加 SocketChannel 的处理器，ChannelInitializer 处理器（仅执行一次），它的作用是待客户端 SocketChannel 建立连接后，执行 initChannel 以便添加更多的处理器 4 处，指定要连接的服务器和端口 5 处，Netty 中很多方法都是异步的，如 connect，这时需要使用 sync 方法等待 connect 建立连接完毕 6 处，获取 channel 对象，它即为通道抽象，可以进行数据读写操作 7 处，写入消息并清空缓冲区 8 处，消息会经过通道 handler 处理，这里是将 String => ByteBuf 发出 数据经过网络传输，到达服务器端，服务器端 5 和 6 处的 handler 先后被触发，走完一个流程 2.4 流程梳理 &#x1F4A1; 提示 一开始需要树立正确的观念 把 channel 理解为数据的通道 把 msg 理解为流动的数据，最开始输入是 ByteBuf，但经过 pipeline 的加工，会变成其它类型对象，最后输出又变成 ByteBuf 把 handler 理解为数据的处理工序 工序有多道，合在一起就是 pipeline，pipeline 负责发布事件（读、读取完成...）传播给每个 handler， handler 对自己感兴趣的事件进行处理（重写了相应事件处理方法） handler 分 Inbound 和 Outbound 两类 把 eventLoop 理解为处理数据的工人 工人可以管理多个 channel 的 io 操作，并且一旦工人负责了某个 channel，就要负责到底（绑定） 工人既可以执行 io 操作，也可以进行任务处理，每位工人有任务队列，队列里可以堆放多个 channel 的待处理任务，任务分为普通任务、定时任务 工人按照 pipeline 顺序，依次按照 handler 的规划（代码）处理数据，可以为每道工序指定不同的工人 3. 组件 3.1 EventLoop 事件循环对象 EventLoop 本质是一个单线程执行器（同时维护了一个 Selector），里面有 run 方法处理 Channel 上源源不断的 io 事件。 它的继承关系比较复杂 一条线是继承自 j.u.c.ScheduledExecutorService 因此包含了线程池中所有的方法 另一条线是继承自 netty 自己的 OrderedEventExecutor， 提供了 boolean inEventLoop(Thread thread) 方法判断一个线程是否属于此 EventLoop 提供了 parent 方法来看看自己属于哪个 EventLoopGroup 事件循环组 EventLoopGroup 是一组 EventLoop，Channel 一般会调用 EventLoopGroup 的 register 方法来绑定其中一个 EventLoop，后续这个 Channel 上的 io 事件都由此 EventLoop 来处理（保证了 io 事件处理时的线程安全） 继承自 netty 自己的 EventExecutorGroup 实现了 Iterable 接口提供遍历 EventLoop 的能力 另有 next 方法获取集合中下一个 EventLoop 以一个简单的实现为例： // 内部创建了两个 EventLoop, 每个 EventLoop 维护一个线程 DefaultEventLoopGroup group = new DefaultEventLoopGroup(2); System.out.println(group.next()); System.out.println(group.next()); System.out.println(group.next()); 输出 io.netty.channel.DefaultEventLoop@60f82f98 io.netty.channel.DefaultEventLoop@35f983a6 io.netty.channel.DefaultEventLoop@60f82f98 也可以使用 for 循环 DefaultEventLoopGroup group = new DefaultEventLoopGroup(2); for (EventExecutor eventLoop : group) { System.out.println(eventLoop); } 输出 io.netty.channel.DefaultEventLoop@60f82f98 io.netty.channel.DefaultEventLoop@35f983a6 &#x1F4A1; 优雅关闭 优雅关闭 shutdownGracefully 方法。该方法会首先切换 EventLoopGroup 到关闭状态从而拒绝新的任务的加入，然后在任务队列的任务都处理完成后，停止线程的运行。从而确保整体应用是在正常有序的状态下退出的 演示 NioEventLoop 处理 io 事件 服务器端两个 nio worker 工人 new ServerBootstrap() .group(new NioEventLoopGroup(1), new NioEventLoopGroup(2)) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() { @Override protected void initChannel(NioSocketChannel ch) { ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { ByteBuf byteBuf = msg instanceof ByteBuf ? ((ByteBuf) msg) : null; if (byteBuf != null) { byte[] buf = new byte[16]; ByteBuf len = byteBuf.readBytes(buf, 0, byteBuf.readableBytes()); log.debug(new String(buf)); } } }); } }).bind(8080).sync(); 客户端，启动三次，分别修改发送字符串为 zhangsan（第一次），lisi（第二次），wangwu（第三次） public static void main(String[] args) throws InterruptedException { Channel channel = new Bootstrap() .group(new NioEventLoopGroup(1)) .handler(new ChannelInitializer() { @Override protected void initChannel(NioSocketChannel ch) throws Exception { System.out.println(\"init...\"); ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); } }) .channel(NioSocketChannel.class).connect(\"localhost\", 8080) .sync() .channel(); channel.writeAndFlush(ByteBufAllocator.DEFAULT.buffer().writeBytes(\"wangwu\".getBytes())); Thread.sleep(2000); channel.writeAndFlush(ByteBufAllocator.DEFAULT.buffer().writeBytes(\"wangwu\".getBytes())); 最后输出 22:03:34 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - zhangsan 22:03:36 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - zhangsan 22:05:36 [DEBUG] [nioEventLoopGroup-3-2] c.i.o.EventLoopTest - lisi 22:05:38 [DEBUG] [nioEventLoopGroup-3-2] c.i.o.EventLoopTest - lisi 22:06:09 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - wangwu 22:06:11 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - wangwu 可以看到两个工人轮流处理 channel，但工人与 channel 之间进行了绑定 再增加两个非 nio 工人 DefaultEventLoopGroup normalWorkers = new DefaultEventLoopGroup(2); new ServerBootstrap() .group(new NioEventLoopGroup(1), new NioEventLoopGroup(2)) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() { @Override protected void initChannel(NioSocketChannel ch) { ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(normalWorkers,\"myhandler\", new ChannelInboundHandlerAdapter() { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { ByteBuf byteBuf = msg instanceof ByteBuf ? ((ByteBuf) msg) : null; if (byteBuf != null) { byte[] buf = new byte[16]; ByteBuf len = byteBuf.readBytes(buf, 0, byteBuf.readableBytes()); log.debug(new String(buf)); } } }); } }).bind(8080).sync(); 客户端代码不变，启动三次，分别修改发送字符串为 zhangsan（第一次），lisi（第二次），wangwu（第三次） 输出 22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:/127.0.0.1:8080 - R:/127.0.0.1:52588] REGISTERED 22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:/127.0.0.1:8080 - R:/127.0.0.1:52588] ACTIVE 22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:/127.0.0.1:8080 - R:/127.0.0.1:52588] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 7a 68 61 6e 67 73 61 6e |zhangsan | +--------+-------------------------------------------------+----------------+ 22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:/127.0.0.1:8080 - R:/127.0.0.1:52588] READ COMPLETE 22:19:48 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - zhangsan 22:19:50 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:/127.0.0.1:8080 - R:/127.0.0.1:52588] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 7a 68 61 6e 67 73 61 6e |zhangsan | +--------+-------------------------------------------------+----------------+ 22:19:50 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:/127.0.0.1:8080 - R:/127.0.0.1:52588] READ COMPLETE 22:19:50 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - zhangsan 22:20:24 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:/127.0.0.1:8080 - R:/127.0.0.1:52612] REGISTERED 22:20:24 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:/127.0.0.1:8080 - R:/127.0.0.1:52612] ACTIVE 22:20:25 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:/127.0.0.1:8080 - R:/127.0.0.1:52612] READ: 4B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6c 69 73 69 |lisi | +--------+-------------------------------------------------+----------------+ 22:20:25 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:/127.0.0.1:8080 - R:/127.0.0.1:52612] READ COMPLETE 22:20:25 [DEBUG] [defaultEventLoopGroup-2-2] c.i.o.EventLoopTest - lisi 22:20:27 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:/127.0.0.1:8080 - R:/127.0.0.1:52612] READ: 4B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6c 69 73 69 |lisi | +--------+-------------------------------------------------+----------------+ 22:20:27 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:/127.0.0.1:8080 - R:/127.0.0.1:52612] READ COMPLETE 22:20:27 [DEBUG] [defaultEventLoopGroup-2-2] c.i.o.EventLoopTest - lisi 22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:/127.0.0.1:8080 - R:/127.0.0.1:52625] REGISTERED 22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:/127.0.0.1:8080 - R:/127.0.0.1:52625] ACTIVE 22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:/127.0.0.1:8080 - R:/127.0.0.1:52625] READ: 6B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 77 61 6e 67 77 75 |wangwu | +--------+-------------------------------------------------+----------------+ 22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:/127.0.0.1:8080 - R:/127.0.0.1:52625] READ COMPLETE 22:20:38 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - wangwu 22:20:40 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:/127.0.0.1:8080 - R:/127.0.0.1:52625] READ: 6B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 77 61 6e 67 77 75 |wangwu | +--------+-------------------------------------------------+----------------+ 22:20:40 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:/127.0.0.1:8080 - R:/127.0.0.1:52625] READ COMPLETE 22:20:40 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - wangwu 可以看到，nio 工人和 非 nio 工人也分别绑定了 channel（LoggingHandler 由 nio 工人执行，而我们自己的 handler 由非 nio 工人执行） &#x1F4A1; handler 执行中如何换人？ 关键代码 io.netty.channel.AbstractChannelHandlerContext#invokeChannelRead() static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) { final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, \"msg\"), next); // 下一个 handler 的事件循环是否与当前的事件循环是同一个线程 EventExecutor executor = next.executor(); // 是，直接调用 if (executor.inEventLoop()) { next.invokeChannelRead(m); } // 不是，将要执行的代码作为任务提交给下一个事件循环处理（换人） else { executor.execute(new Runnable() { @Override public void run() { next.invokeChannelRead(m); } }); } } 如果两个 handler 绑定的是同一个线程，那么就直接调用 否则，把要调用的代码封装为一个任务对象，由下一个 handler 的线程来调用 演示 NioEventLoop 处理普通任务 NioEventLoop 除了可以处理 io 事件，同样可以向它提交普通任务 NioEventLoopGroup nioWorkers = new NioEventLoopGroup(2); log.debug(\"server start...\"); Thread.sleep(2000); nioWorkers.execute(()->{ log.debug(\"normal task...\"); }); 输出 22:30:36 [DEBUG] [main] c.i.o.EventLoopTest2 - server start... 22:30:38 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - normal task... 可以用来执行耗时较长的任务 演示 NioEventLoop 处理定时任务 NioEventLoopGroup nioWorkers = new NioEventLoopGroup(2); log.debug(\"server start...\"); Thread.sleep(2000); nioWorkers.scheduleAtFixedRate(() -> { log.debug(\"running...\"); }, 0, 1, TimeUnit.SECONDS); 输出 22:35:15 [DEBUG] [main] c.i.o.EventLoopTest2 - server start... 22:35:17 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running... 22:35:18 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running... 22:35:19 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running... 22:35:20 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running... ... 可以用来执行定时任务 3.2 Channel channel 的主要作用 close() 可以用来关闭 channel closeFuture() 用来处理 channel 的关闭 sync 方法作用是同步等待 channel 关闭 而 addListener 方法是异步等待 channel 关闭 pipeline() 方法添加处理器 write() 方法将数据写入 writeAndFlush() 方法将数据写入并刷出 ChannelFuture 这时刚才的客户端代码 new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override protected void initChannel(Channel ch) { ch.pipeline().addLast(new StringEncoder()); } }) .connect(\"127.0.0.1\", 8080) .sync() .channel() .writeAndFlush(new Date() + \": hello world!\"); 现在把它拆开来看 ChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override protected void initChannel(Channel ch) { ch.pipeline().addLast(new StringEncoder()); } }) .connect(\"127.0.0.1\", 8080); // 1 channelFuture.sync().channel().writeAndFlush(new Date() + \": hello world!\"); 1 处返回的是 ChannelFuture 对象，它的作用是利用 channel() 方法来获取 Channel 对象 注意 connect 方法是异步的，意味着不等连接建立，方法执行就返回了。因此 channelFuture 对象中不能【立刻】获得到正确的 Channel 对象 实验如下： ChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override protected void initChannel(Channel ch) { ch.pipeline().addLast(new StringEncoder()); } }) .connect(\"127.0.0.1\", 8080); System.out.println(channelFuture.channel()); // 1 channelFuture.sync(); // 2 System.out.println(channelFuture.channel()); // 3 执行到 1 时，连接未建立，打印 [id: 0x2e1884dd] 执行到 2 时，sync 方法是同步等待连接建立完成 执行到 3 时，连接肯定建立了，打印 [id: 0x2e1884dd, L:/127.0.0.1:57191 - R:/127.0.0.1:8080] 除了用 sync 方法可以让异步操作同步以外，还可以使用回调的方式： ChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override protected void initChannel(Channel ch) { ch.pipeline().addLast(new StringEncoder()); } }) .connect(\"127.0.0.1\", 8080); System.out.println(channelFuture.channel()); // 1 channelFuture.addListener((ChannelFutureListener) future -> { System.out.println(future.channel()); // 2 }); 执行到 1 时，连接未建立，打印 [id: 0x749124ba] ChannelFutureListener 会在连接建立时被调用（其中 operationComplete 方法），因此执行到 2 时，连接肯定建立了，打印 [id: 0x749124ba, L:/127.0.0.1:57351 - R:/127.0.0.1:8080] CloseFuture @Slf4j public class CloseFutureClient { public static void main(String[] args) throws InterruptedException { NioEventLoopGroup group new NioEventLoopGroup(); ChannelFuture channelFuture = new Bootstrap() .group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override // 在连接建立后被调用 protected void initChannel(NioSocketChannel ch) throws Exception { ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(\"localhost\", 8080)); Channel channel = channelFuture.sync().channel(); log.debug(\"{}\", channel); new Thread(()->{ Scanner scanner = new Scanner(System.in); while (true) { String line = scanner.nextLine(); if (\"q\".equals(line)) { channel.close(); // close 异步操作 1s 之后 // log.debug(\"处理关闭之后的操作\"); // 不能在这里善后 break; } channel.writeAndFlush(line); } }, \"input\").start(); // 获取 CloseFuture 对象， 1) 同步处理关闭， 2) 异步处理关闭 ChannelFuture closeFuture = channel.closeFuture(); /*log.debug(\"waiting close...\"); closeFuture.sync(); log.debug(\"处理关闭之后的操作\");*/ closeFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { log.debug(\"处理关闭之后的操作\"); group.shutdownGracefully(); } }); } } &#x1F4A1; 异步提升的是什么 有些同学看到这里会有疑问：为什么不在一个线程中去执行建立连接、去执行关闭 channel，那样不是也可以吗？非要用这么复杂的异步方式：比如一个线程发起建立连接，另一个线程去真正建立连接 还有同学会笼统地回答，因为 netty 异步方式用了多线程、多线程就效率高。其实这些认识都比较片面，多线程和异步所提升的效率并不是所认为的 思考下面的场景，4 个医生给人看病，每个病人花费 20 分钟，而且医生看病的过程中是以病人为单位的，一个病人看完了，才能看下一个病人。假设病人源源不断地来，可以计算一下 4 个医生一天工作 8 小时，处理的病人总数是：4 * 8 * 3 = 96 经研究发现，看病可以细分为四个步骤，经拆分后每个步骤需要 5 分钟，如下 因此可以做如下优化，只有一开始，医生 2、3、4 分别要等待 5、10、15 分钟才能执行工作，但只要后续病人源源不断地来，他们就能够满负荷工作，并且处理病人的能力提高到了 4 * 8 * 12 效率几乎是原来的四倍 要点 单线程没法异步提高效率，必须配合多线程、多核 cpu 才能发挥异步的优势 异步并没有缩短响应时间，反而有所增加 合理进行任务拆分，也是利用异步的关键 3.3 Future & Promise 在异步处理时，经常用到这两个接口 首先要说明 netty 中的 Future 与 jdk 中的 Future 同名，但是是两个接口，netty 的 Future 继承自 jdk 的 Future，而 Promise 又对 netty Future 进行了扩展 jdk Future 只能同步等待任务结束（或成功、或失败）才能得到结果 netty Future 可以同步等待任务结束得到结果，也可以异步方式得到结果，但都是要等任务结束 netty Promise 不仅有 netty Future 的功能，而且脱离了任务独立存在，只作为两个线程间传递结果的容器 功能/名称 jdk Future netty Future Promise cancel 取消任务 - - isCanceled 任务是否取消 - - isDone 任务是否完成，不能区分成功失败 - - get 获取任务结果，阻塞等待 - - getNow - 获取任务结果，非阻塞，还未产生结果时返回 null - await - 等待任务结束，如果任务失败，不会抛异常，而是通过 isSuccess 判断 - sync - 等待任务结束，如果任务失败，抛出异常 - isSuccess - 判断任务是否成功 - cause - 获取失败信息，非阻塞，如果没有失败，返回null - addLinstener - 添加回调，异步接收结果 - setSuccess - - 设置成功结果 setFailure - - 设置失败结果 例1 同步处理任务成功 DefaultEventLoop eventExecutors = new DefaultEventLoop(); DefaultPromise promise = new DefaultPromise<>(eventExecutors); eventExecutors.execute(()->{ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } log.debug(\"set success, {}\",10); promise.setSuccess(10); }); log.debug(\"start...\"); log.debug(\"{}\",promise.getNow()); // 还没有结果 log.debug(\"{}\",promise.get()); 输出 11:51:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start... 11:51:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - null 11:51:54 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set success, 10 11:51:54 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - 10 例2 异步处理任务成功 DefaultEventLoop eventExecutors = new DefaultEventLoop(); DefaultPromise promise = new DefaultPromise<>(eventExecutors); // 设置回调，异步接收结果 promise.addListener(future -> { // 这里的 future 就是上面的 promise log.debug(\"{}\",future.getNow()); }); // 等待 1000 后设置成功结果 eventExecutors.execute(()->{ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } log.debug(\"set success, {}\",10); promise.setSuccess(10); }); log.debug(\"start...\"); 输出 11:49:30 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start... 11:49:31 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set success, 10 11:49:31 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - 10 例3 同步处理任务失败 - sync & get DefaultEventLoop eventExecutors = new DefaultEventLoop(); DefaultPromise promise = new DefaultPromise<>(eventExecutors); eventExecutors.execute(() -> { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } RuntimeException e = new RuntimeException(\"error...\"); log.debug(\"set failure, {}\", e.toString()); promise.setFailure(e); }); log.debug(\"start...\"); log.debug(\"{}\", promise.getNow()); promise.get(); // sync() 也会出现异常，只是 get 会再用 ExecutionException 包一层异常 输出 12:11:07 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start... 12:11:07 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - null 12:11:08 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set failure, java.lang.RuntimeException: error... Exception in thread \"main\" java.util.concurrent.ExecutionException: java.lang.RuntimeException: error... at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:41) at com.itcast.oio.DefaultPromiseTest2.main(DefaultPromiseTest2.java:34) Caused by: java.lang.RuntimeException: error... at com.itcast.oio.DefaultPromiseTest2.lambda$main$0(DefaultPromiseTest2.java:27) at io.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:745) 例4 同步处理任务失败 - await DefaultEventLoop eventExecutors = new DefaultEventLoop(); DefaultPromise promise = new DefaultPromise<>(eventExecutors); eventExecutors.execute(() -> { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } RuntimeException e = new RuntimeException(\"error...\"); log.debug(\"set failure, {}\", e.toString()); promise.setFailure(e); }); log.debug(\"start...\"); log.debug(\"{}\", promise.getNow()); promise.await(); // 与 sync 和 get 区别在于，不会抛异常 log.debug(\"result {}\", (promise.isSuccess() ? promise.getNow() : promise.cause()).toString()); 输出 12:18:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start... 12:18:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - null 12:18:54 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set failure, java.lang.RuntimeException: error... 12:18:54 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - result java.lang.RuntimeException: error... 例5 异步处理任务失败 DefaultEventLoop eventExecutors = new DefaultEventLoop(); DefaultPromise promise = new DefaultPromise<>(eventExecutors); promise.addListener(future -> { log.debug(\"result {}\", (promise.isSuccess() ? promise.getNow() : promise.cause()).toString()); }); eventExecutors.execute(() -> { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } RuntimeException e = new RuntimeException(\"error...\"); log.debug(\"set failure, {}\", e.toString()); promise.setFailure(e); }); log.debug(\"start...\"); 输出 12:04:57 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start... 12:04:58 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set failure, java.lang.RuntimeException: error... 12:04:58 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - result java.lang.RuntimeException: error... 例6 await 死锁检查 DefaultEventLoop eventExecutors = new DefaultEventLoop(); DefaultPromise promise = new DefaultPromise<>(eventExecutors); eventExecutors.submit(()->{ System.out.println(\"1\"); try { promise.await(); // 注意不能仅捕获 InterruptedException 异常 // 否则 死锁检查抛出的 BlockingOperationException 会继续向上传播 // 而提交的任务会被包装为 PromiseTask，它的 run 方法中会 catch 所有异常然后设置为 Promise 的失败结果而不会抛出 } catch (Exception e) { e.printStackTrace(); } System.out.println(\"2\"); }); eventExecutors.submit(()->{ System.out.println(\"3\"); try { promise.await(); } catch (Exception e) { e.printStackTrace(); } System.out.println(\"4\"); }); 输出 1 2 3 4 io.netty.util.concurrent.BlockingOperationException: DefaultPromise@47499c2a(incomplete) at io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:384) at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:212) at com.itcast.oio.DefaultPromiseTest.lambda$main$0(DefaultPromiseTest.java:27) at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38) at io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:73) at io.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:745) io.netty.util.concurrent.BlockingOperationException: DefaultPromise@47499c2a(incomplete) at io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:384) at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:212) at com.itcast.oio.DefaultPromiseTest.lambda$main$1(DefaultPromiseTest.java:36) at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38) at io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:73) at io.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:745) 3.4 Handler & Pipeline ChannelHandler 用来处理 Channel 上的各种事件，分为入站、出站两种。所有 ChannelHandler 被连成一串，就是 Pipeline 入站处理器通常是 ChannelInboundHandlerAdapter 的子类，主要用来读取客户端数据，写回结果 出站处理器通常是 ChannelOutboundHandlerAdapter 的子类，主要对写回结果进行加工 打个比喻，每个 Channel 是一个产品的加工车间，Pipeline 是车间中的流水线，ChannelHandler 就是流水线上的各道工序，而后面要讲的 ByteBuf 是原材料，经过很多工序的加工：先经过一道道入站工序，再经过一道道出站工序最终变成产品 先搞清楚顺序，服务端 new ServerBootstrap() .group(new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() { protected void initChannel(NioSocketChannel ch) { ch.pipeline().addLast(new ChannelInboundHandlerAdapter(){ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { System.out.println(1); ctx.fireChannelRead(msg); // 1 } }); ch.pipeline().addLast(new ChannelInboundHandlerAdapter(){ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { System.out.println(2); ctx.fireChannelRead(msg); // 2 } }); ch.pipeline().addLast(new ChannelInboundHandlerAdapter(){ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { System.out.println(3); ctx.channel().write(msg); // 3 } }); ch.pipeline().addLast(new ChannelOutboundHandlerAdapter(){ @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { System.out.println(4); ctx.write(msg, promise); // 4 } }); ch.pipeline().addLast(new ChannelOutboundHandlerAdapter(){ @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { System.out.println(5); ctx.write(msg, promise); // 5 } }); ch.pipeline().addLast(new ChannelOutboundHandlerAdapter(){ @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { System.out.println(6); ctx.write(msg, promise); // 6 } }); } }) .bind(8080); 客户端 new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override protected void initChannel(Channel ch) { ch.pipeline().addLast(new StringEncoder()); } }) .connect(\"127.0.0.1\", 8080) .addListener((ChannelFutureListener) future -> { future.channel().writeAndFlush(\"hello,world\"); }); 服务器端打印： 1 2 3 6 5 4 可以看到，ChannelInboundHandlerAdapter 是按照 addLast 的顺序执行的，而 ChannelOutboundHandlerAdapter 是按照 addLast 的逆序执行的。ChannelPipeline 的实现是一个 ChannelHandlerContext（包装了 ChannelHandler） 组成的双向链表 入站处理器中，ctx.fireChannelRead(msg) 是 调用下一个入站处理器 如果注释掉 1 处代码，则仅会打印 1 如果注释掉 2 处代码，则仅会打印 1 2 3 处的 ctx.channel().write(msg) 会 从尾部开始触发 后续出站处理器的执行 如果注释掉 3 处代码，则仅会打印 1 2 3 类似的，出站处理器中，ctx.write(msg, promise) 的调用也会 触发上一个出站处理器 如果注释掉 6 处代码，则仅会打印 1 2 3 6 ctx.channel().write(msg) vs ctx.write(msg) 都是触发出站处理器的执行 ctx.channel().write(msg) 从尾部开始查找出站处理器 ctx.write(msg) 是从当前节点找上一个出站处理器 3 处的 ctx.channel().write(msg) 如果改为 ctx.write(msg) 仅会打印 1 2 3，因为节点3 之前没有其它出站处理器了 6 处的 ctx.write(msg, promise) 如果改为 ctx.channel().write(msg) 会打印 1 2 3 6 6 6... 因为 ctx.channel().write() 是从尾部开始查找，结果又是节点6 自己 图1 - 服务端 pipeline 触发的原始流程，图中数字代表了处理步骤的先后次序 3.5 ByteBuf 是对字节数据的封装 1）创建 ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(10); log(buffer); 上面代码创建了一个默认的 ByteBuf（池化基于直接内存的 ByteBuf），初始容量是 10 输出 read index:0 write index:0 capacity:10 其中 log 方法参考如下 private static void log(ByteBuf buffer) { int length = buffer.readableBytes(); int rows = length / 16 + (length % 15 == 0 ? 0 : 1) + 4; StringBuilder buf = new StringBuilder(rows * 80 * 2) .append(\"read index:\").append(buffer.readerIndex()) .append(\" write index:\").append(buffer.writerIndex()) .append(\" capacity:\").append(buffer.capacity()) .append(NEWLINE); appendPrettyHexDump(buf, buffer); System.out.println(buf.toString()); } 2）直接内存 vs 堆内存 可以使用下面的代码来创建池化基于堆的 ByteBuf ByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer(10); 也可以使用下面的代码来创建池化基于直接内存的 ByteBuf ByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer(10); 直接内存创建和销毁的代价昂贵，但读写性能高（少一次内存复制），适合配合池化功能一起用 直接内存对 GC 压力小，因为这部分内存不受 JVM 垃圾回收的管理，但也要注意及时主动释放 3）池化 vs 非池化 池化的最大意义在于可以重用 ByteBuf，优点有 没有池化，则每次都得创建新的 ByteBuf 实例，这个操作对直接内存代价昂贵，就算是堆内存，也会增加 GC 压力 有了池化，则可以重用池中 ByteBuf 实例，并且采用了与 jemalloc 类似的内存分配算法提升分配效率 高并发时，池化功能更节约内存，减少内存溢出的可能 池化功能是否开启，可以通过下面的系统环境变量来设置 -Dio.netty.allocator.type={unpooled|pooled} 4.1 以后，非 Android 平台默认启用池化实现，Android 平台启用非池化实现 4.1 之前，池化功能还不成熟，默认是非池化实现 4）组成 ByteBuf 由四部分组成 最开始读写指针都在 0 位置 5）写入 方法列表，省略一些不重要的方法 方法签名 含义 备注 writeBoolean(boolean value) 写入 boolean 值 用一字节 01\\ 00 代表 true\\ false writeByte(int value) 写入 byte 值 writeShort(int value) 写入 short 值 writeInt(int value) 写入 int 值 Big Endian，即 0x250，写入后 00 00 02 50 writeIntLE(int value) 写入 int 值 Little Endian，即 0x250，写入后 50 02 00 00 writeLong(long value) 写入 long 值 writeChar(int value) 写入 char 值 writeFloat(float value) 写入 float 值 writeDouble(double value) 写入 double 值 writeBytes(ByteBuf src) 写入 netty 的 ByteBuf writeBytes(byte[] src) 写入 byte[] writeBytes(ByteBuffer src) 写入 nio 的 ByteBuffer int writeCharSequence(CharSequence sequence, Charset charset) 写入字符串 注意 这些方法的未指明返回值的，其返回值都是 ByteBuf，意味着可以链式调用 网络传输，默认习惯是 Big Endian 先写入 4 个字节 buffer.writeBytes(new byte[]{1, 2, 3, 4}); log(buffer); 结果是 read index:0 write index:4 capacity:10 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 |.... | +--------+-------------------------------------------------+----------------+ 再写入一个 int 整数，也是 4 个字节 buffer.writeInt(5); log(buffer); 结果是 read index:0 write index:8 capacity:10 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 00 00 00 05 |........ | +--------+-------------------------------------------------+----------------+ 还有一类方法是 set 开头的一系列方法，也可以写入数据，但不会改变写指针位置 6）扩容 再写入一个 int 整数时，容量不够了（初始容量是 10），这时会引发扩容 buffer.writeInt(6); log(buffer); 扩容规则是 如何写入后数据大小未超过 512，则选择下一个 16 的整数倍，例如写入后大小为 12 ，则扩容后 capacity 是 16 如果写入后数据大小超过 512，则选择下一个 2^n，例如写入后大小为 513，则扩容后 capacity 是 2^10=1024（2^9=512 已经不够了） 扩容不能超过 max capacity 会报错 结果是 read index:0 write index:12 capacity:16 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 00 00 00 05 00 00 00 06 |............ | +--------+-------------------------------------------------+----------------+ 7）读取 例如读了 4 次，每次一个字节 System.out.println(buffer.readByte()); System.out.println(buffer.readByte()); System.out.println(buffer.readByte()); System.out.println(buffer.readByte()); log(buffer); 读过的内容，就属于废弃部分了，再读只能读那些尚未读取的部分 1 2 3 4 read index:4 write index:12 capacity:16 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 00 00 00 05 00 00 00 06 |........ | +--------+-------------------------------------------------+----------------+ 如果需要重复读取 int 整数 5，怎么办？ 可以在 read 前先做个标记 mark buffer.markReaderIndex(); System.out.println(buffer.readInt()); log(buffer); 结果 5 read index:8 write index:12 capacity:16 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 00 00 00 06 |.... | +--------+-------------------------------------------------+----------------+ 这时要重复读取的话，重置到标记位置 reset buffer.resetReaderIndex(); log(buffer); 这时 read index:4 write index:12 capacity:16 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 00 00 00 05 00 00 00 06 |........ | +--------+-------------------------------------------------+----------------+ 还有种办法是采用 get 开头的一系列方法，这些方法不会改变 read index 8）retain & release 由于 Netty 中有堆外内存的 ByteBuf 实现，堆外内存最好是手动来释放，而不是等 GC 垃圾回收。 UnpooledHeapByteBuf 使用的是 JVM 内存，只需等 GC 回收内存即可 UnpooledDirectByteBuf 使用的就是直接内存了，需要特殊的方法来回收内存 PooledByteBuf 和它的子类使用了池化机制，需要更复杂的规则来回收内存 回收内存的源码实现，请关注下面方法的不同实现 protected abstract void deallocate() Netty 这里采用了引用计数法来控制回收内存，每个 ByteBuf 都实现了 ReferenceCounted 接口 每个 ByteBuf 对象的初始计数为 1 调用 release 方法计数减 1，如果计数为 0，ByteBuf 内存被回收 调用 retain 方法计数加 1，表示调用者没用完之前，其它 handler 即使调用了 release 也不会造成回收 当计数为 0 时，底层内存会被回收，这时即使 ByteBuf 对象还在，其各个方法均无法正常使用 谁来负责 release 呢？ 不是我们想象的（一般情况下） ByteBuf buf = ... try { ... } finally { buf.release(); } 请思考，因为 pipeline 的存在，一般需要将 ByteBuf 传递给下一个 ChannelHandler，如果在 finally 中 release 了，就失去了传递性（当然，如果在这个 ChannelHandler 内这个 ByteBuf 已完成了它的使命，那么便无须再传递） 基本规则是，谁是最后使用者，谁负责 release，详细分析如下 起点，对于 NIO 实现来讲，在 io.netty.channel.nio.AbstractNioByteChannel.NioByteUnsafe#read 方法中首次创建 ByteBuf 放入 pipeline（line 163 pipeline.fireChannelRead(byteBuf)） 入站 ByteBuf 处理原则 对原始 ByteBuf 不做处理，调用 ctx.fireChannelRead(msg) 向后传递，这时无须 release 将原始 ByteBuf 转换为其它类型的 Java 对象，这时 ByteBuf 就没用了，必须 release 如果不调用 ctx.fireChannelRead(msg) 向后传递，那么也必须 release 注意各种异常，如果 ByteBuf 没有成功传递到下一个 ChannelHandler，必须 release 假设消息一直向后传，那么 TailContext 会负责释放未处理消息（原始的 ByteBuf） 出站 ByteBuf 处理原则 出站消息最终都会转为 ByteBuf 输出，一直向前传，由 HeadContext flush 后 release 异常处理原则 有时候不清楚 ByteBuf 被引用了多少次，但又必须彻底释放，可以循环调用 release 直到返回 true TailContext 释放未处理消息逻辑 // io.netty.channel.DefaultChannelPipeline#onUnhandledInboundMessage(java.lang.Object) protected void onUnhandledInboundMessage(Object msg) { try { logger.debug( \"Discarded inbound message {} that reached at the tail of the pipeline. \" + \"Please check your pipeline configuration.\", msg); } finally { ReferenceCountUtil.release(msg); } } 具体代码 // io.netty.util.ReferenceCountUtil#release(java.lang.Object) public static boolean release(Object msg) { if (msg instanceof ReferenceCounted) { return ((ReferenceCounted) msg).release(); } return false; } 9）slice 【零拷贝】的体现之一，对原始 ByteBuf 进行切片成多个 ByteBuf，切片后的 ByteBuf 并没有发生内存复制，还是使用原始 ByteBuf 的内存，切片后的 ByteBuf 维护独立的 read，write 指针 例，原始 ByteBuf 进行一些初始操作 ByteBuf origin = ByteBufAllocator.DEFAULT.buffer(10); origin.writeBytes(new byte[]{1, 2, 3, 4}); origin.readByte(); System.out.println(ByteBufUtil.prettyHexDump(origin)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 02 03 04 |... | +--------+-------------------------------------------------+----------------+ 这时调用 slice 进行切片，无参 slice 是从原始 ByteBuf 的 read index 到 write index 之间的内容进行切片，切片后的 max capacity 被固定为这个区间的大小，因此不能追加 write ByteBuf slice = origin.slice(); System.out.println(ByteBufUtil.prettyHexDump(slice)); // slice.writeByte(5); 如果执行，会报 IndexOutOfBoundsException 异常 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 02 03 04 |... | +--------+-------------------------------------------------+----------------+ 如果原始 ByteBuf 再次读操作（又读了一个字节） origin.readByte(); System.out.println(ByteBufUtil.prettyHexDump(origin)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 03 04 |.. | +--------+-------------------------------------------------+----------------+ 这时的 slice 不受影响，因为它有独立的读写指针 System.out.println(ByteBufUtil.prettyHexDump(slice)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 02 03 04 |... | +--------+-------------------------------------------------+----------------+ 如果 slice 的内容发生了更改 slice.setByte(2, 5); System.out.println(ByteBufUtil.prettyHexDump(slice)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 02 03 05 |... | +--------+-------------------------------------------------+----------------+ 这时，原始 ByteBuf 也会受影响，因为底层都是同一块内存 System.out.println(ByteBufUtil.prettyHexDump(origin)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 03 05 |.. | +--------+-------------------------------------------------+----------------+ 10）duplicate 【零拷贝】的体现之一，就好比截取了原始 ByteBuf 所有内容，并且没有 max capacity 的限制，也是与原始 ByteBuf 使用同一块底层内存，只是读写指针是独立的 11）copy 会将底层内存数据进行深拷贝，因此无论读写，都与原始 ByteBuf 无关 12）CompositeByteBuf 【零拷贝】的体现之一，可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免拷贝 有两个 ByteBuf 如下 ByteBuf buf1 = ByteBufAllocator.DEFAULT.buffer(5); buf1.writeBytes(new byte[]{1, 2, 3, 4, 5}); ByteBuf buf2 = ByteBufAllocator.DEFAULT.buffer(5); buf2.writeBytes(new byte[]{6, 7, 8, 9, 10}); System.out.println(ByteBufUtil.prettyHexDump(buf1)); System.out.println(ByteBufUtil.prettyHexDump(buf2)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 05 |..... | +--------+-------------------------------------------------+----------------+ +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 06 07 08 09 0a |..... | +--------+-------------------------------------------------+----------------+ 现在需要一个新的 ByteBuf，内容来自于刚才的 buf1 和 buf2，如何实现？ 方法1： ByteBuf buf3 = ByteBufAllocator.DEFAULT .buffer(buf1.readableBytes()+buf2.readableBytes()); buf3.writeBytes(buf1); buf3.writeBytes(buf2); System.out.println(ByteBufUtil.prettyHexDump(buf3)); 结果 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 05 06 07 08 09 0a |.......... | +--------+-------------------------------------------------+----------------+ 这种方法好不好？回答是不太好，因为进行了数据的内存复制操作 方法2： CompositeByteBuf buf3 = ByteBufAllocator.DEFAULT.compositeBuffer(); // true 表示增加新的 ByteBuf 自动递增 write index, 否则 write index 会始终为 0 buf3.addComponents(true, buf1, buf2); 结果是一样的 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 05 06 07 08 09 0a |.......... | +--------+-------------------------------------------------+----------------+ CompositeByteBuf 是一个组合的 ByteBuf，它内部维护了一个 Component 数组，每个 Component 管理一个 ByteBuf，记录了这个 ByteBuf 相对于整体偏移量等信息，代表着整体中某一段的数据。 优点，对外是一个虚拟视图，组合这些 ByteBuf 不会产生内存复制 缺点，复杂了很多，多次操作会带来性能的损耗 13）Unpooled Unpooled 是一个工具类，类如其名，提供了非池化的 ByteBuf 创建、组合、复制等操作 这里仅介绍其跟【零拷贝】相关的 wrappedBuffer 方法，可以用来包装 ByteBuf ByteBuf buf1 = ByteBufAllocator.DEFAULT.buffer(5); buf1.writeBytes(new byte[]{1, 2, 3, 4, 5}); ByteBuf buf2 = ByteBufAllocator.DEFAULT.buffer(5); buf2.writeBytes(new byte[]{6, 7, 8, 9, 10}); // 当包装 ByteBuf 个数超过一个时, 底层使用了 CompositeByteBuf ByteBuf buf3 = Unpooled.wrappedBuffer(buf1, buf2); System.out.println(ByteBufUtil.prettyHexDump(buf3)); 输出 +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 05 06 07 08 09 0a |.......... | +--------+-------------------------------------------------+----------------+ 也可以用来包装普通字节数组，底层也不会有拷贝操作 ByteBuf buf4 = Unpooled.wrappedBuffer(new byte[]{1, 2, 3}, new byte[]{4, 5, 6}); System.out.println(buf4.getClass()); System.out.println(ByteBufUtil.prettyHexDump(buf4)); 输出 class io.netty.buffer.CompositeByteBuf +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 01 02 03 04 05 06 |...... | +--------+-------------------------------------------------+----------------+ &#x1F4A1; ByteBuf 优势 池化 - 可以重用池中 ByteBuf 实例，更节约内存，减少内存溢出的可能 读写指针分离，不需要像 ByteBuffer 一样切换读写模式 可以自动扩容 支持链式调用，使用更流畅 很多地方体现零拷贝，例如 slice、duplicate、CompositeByteBuf 4. 双向通信 4.1 练习 实现一个 echo server 编写 server new ServerBootstrap() .group(new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() { @Override protected void initChannel(NioSocketChannel ch) { ch.pipeline().addLast(new ChannelInboundHandlerAdapter(){ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { ByteBuf buffer = (ByteBuf) msg; System.out.println(buffer.toString(Charset.defaultCharset())); // 建议使用 ctx.alloc() 创建 ByteBuf ByteBuf response = ctx.alloc().buffer(); response.writeBytes(buffer); ctx.writeAndFlush(response); // 思考：需要释放 buffer 吗 // 思考：需要释放 response 吗 } }); } }).bind(8080); 编写 client NioEventLoopGroup group = new NioEventLoopGroup(); Channel channel = new Bootstrap() .group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() { @Override protected void initChannel(NioSocketChannel ch) throws Exception { ch.pipeline().addLast(new StringEncoder()); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { ByteBuf buffer = (ByteBuf) msg; System.out.println(buffer.toString(Charset.defaultCharset())); // 思考：需要释放 buffer 吗 } }); } }).connect(\"127.0.0.1\", 8080).sync().channel(); channel.closeFuture().addListener(future -> { group.shutdownGracefully(); }); new Thread(() -> { Scanner scanner = new Scanner(System.in); while (true) { String line = scanner.nextLine(); if (\"q\".equals(line)) { channel.close(); break; } channel.writeAndFlush(line); } }).start(); &#x1F4A1; 读和写的误解 我最初在认识上有这样的误区，认为只有在 netty，nio 这样的多路复用 IO 模型时，读写才不会相互阻塞，才可以实现高效的双向通信，但实际上，Java Socket 是全双工的：在任意时刻，线路上存在A 到 B 和 B 到 A 的双向信号传输。即使是阻塞 IO，读和写是可以同时进行的，只要分别采用读线程和写线程即可，读不会阻塞写、写也不会阻塞读 例如 public class TestServer { public static void main(String[] args) throws IOException { ServerSocket ss = new ServerSocket(8888); Socket s = ss.accept(); new Thread(() -> { try { BufferedReader reader = new BufferedReader(new InputStreamReader(s.getInputStream())); while (true) { System.out.println(reader.readLine()); } } catch (IOException e) { e.printStackTrace(); } }).start(); new Thread(() -> { try { BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(s.getOutputStream())); // 例如在这个位置加入 thread 级别断点，可以发现即使不写入数据，也不妨碍前面线程读取客户端数据 for (int i = 0; i 客户端 public class TestClient { public static void main(String[] args) throws IOException { Socket s = new Socket(\"localhost\", 8888); new Thread(() -> { try { BufferedReader reader = new BufferedReader(new InputStreamReader(s.getInputStream())); while (true) { System.out.println(reader.readLine()); } } catch (IOException e) { e.printStackTrace(); } }).start(); new Thread(() -> { try { BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(s.getOutputStream())); for (int i = 0; i Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 09:20:53 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/Netty03-high.html":{"url":"distributed/netty/Netty03-high.html","title":"Netty03-进阶","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 粘包与半包 1.1 粘包现象 1.2 半包现象 1.3 现象分析 1.4 解决方案 方法1，短链接 方法2，固定长度 方法3，固定分隔符 方法4，预设长度 2. 协议设计与解析 2.1 为什么需要协议？ 2.2 redis 协议举例 2.3 http 协议举例 2.4 自定义协议要素 编解码器 &#x1F4A1; 什么时候可以加 @Sharable 3. 聊天室案例 3.1 聊天室业务介绍 3.2 聊天室业务-登录 3.3 聊天室业务-单聊 3.4 聊天室业务-群聊 3.5 聊天室业务-退出 3.6 聊天室业务-空闲检测 连接假死 三. Netty 进阶 1. 粘包与半包 1.1 粘包现象 服务端代码 public class HelloWorldServer { static final Logger log = LoggerFactory.getLogger(HelloWorldServer.class); void start() { NioEventLoopGroup boss = new NioEventLoopGroup(1); NioEventLoopGroup worker = new NioEventLoopGroup(); try { ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.channel(NioServerSocketChannel.class); serverBootstrap.group(boss, worker); serverBootstrap.childHandler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { log.debug(\"connected {}\", ctx.channel()); super.channelActive(ctx); } @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { log.debug(\"disconnect {}\", ctx.channel()); super.channelInactive(ctx); } }); } }); ChannelFuture channelFuture = serverBootstrap.bind(8080); log.debug(\"{} binding...\", channelFuture.channel()); channelFuture.sync(); log.debug(\"{} bound...\", channelFuture.channel()); channelFuture.channel().closeFuture().sync(); } catch (InterruptedException e) { log.error(\"server error\", e); } finally { boss.shutdownGracefully(); worker.shutdownGracefully(); log.debug(\"stoped\"); } } public static void main(String[] args) { new HelloWorldServer().start(); } } 客户端代码希望发送 10 个消息，每个消息是 16 字节 public class HelloWorldClient { static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class); public static void main(String[] args) { NioEventLoopGroup worker = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(worker); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { log.debug(\"connetted...\"); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { log.debug(\"sending...\"); Random r = new Random(); char c = 'a'; for (int i = 0; i 服务器端的某次输出，可以看到一次就接收了 160 个字节，而非分 10 次接收 08:24:46 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x81e0fda5] binding... 08:24:46 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x81e0fda5, L:/0:0:0:0:0:0:0:0:8080] bound... 08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:/127.0.0.1:8080 - R:/127.0.0.1:58177] REGISTERED 08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:/127.0.0.1:8080 - R:/127.0.0.1:58177] ACTIVE 08:24:55 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0x94132411, L:/127.0.0.1:8080 - R:/127.0.0.1:58177] 08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:/127.0.0.1:8080 - R:/127.0.0.1:58177] READ: 160B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000010| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000020| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000030| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000040| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000050| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000060| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000070| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000080| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000090| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| +--------+-------------------------------------------------+----------------+ 08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:/127.0.0.1:8080 - R:/127.0.0.1:58177] READ COMPLETE 1.2 半包现象 客户端代码希望发送 1 个消息，这个消息是 160 字节，代码改为 ByteBuf buffer = ctx.alloc().buffer(); for (int i = 0; i 为现象明显，服务端修改一下接收缓冲区，其它代码不变 serverBootstrap.option(ChannelOption.SO_RCVBUF, 10); 服务器端的某次输出，可以看到接收的消息被分为两节，第一次 20 字节，第二次 140 字节 08:43:49 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x4d6c6a84] binding... 08:43:49 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x4d6c6a84, L:/0:0:0:0:0:0:0:0:8080] bound... 08:44:23 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] REGISTERED 08:44:23 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] ACTIVE 08:44:23 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] 08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] READ: 20B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................| |00000010| 00 01 02 03 |.... | +--------+-------------------------------------------------+----------------+ 08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] READ COMPLETE 08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] READ: 140B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000010| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000020| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000030| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000040| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000050| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000060| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000070| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................| |00000080| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |............ | +--------+-------------------------------------------------+----------------+ 08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:/127.0.0.1:8080 - R:/127.0.0.1:59221] READ COMPLETE 注意 serverBootstrap.option(ChannelOption.SO_RCVBUF, 10) 影响的底层接收缓冲区（即滑动窗口）大小，仅决定了 netty 读取的最小单位，netty 实际每次读取的一般是它的整数倍 1.3 现象分析 粘包 现象，发送 abc def，接收 abcdef 原因 应用层：接收方 ByteBuf 设置太大（Netty 默认 1024） 滑动窗口：假设发送方 256 bytes 表示一个完整报文，但由于接收方处理不及时且窗口大小足够大，这 256 bytes 字节就会缓冲在接收方的滑动窗口中，当滑动窗口中缓冲了多个报文就会粘包 Nagle 算法：会造成粘包 半包 现象，发送 abcdef，接收 abc def 原因 应用层：接收方 ByteBuf 小于实际发送数据量 滑动窗口：假设接收方的窗口只剩了 128 bytes，发送方的报文大小是 256 bytes，这时放不下了，只能先发送前 128 bytes，等待 ack 后才能发送剩余部分，这就造成了半包 MSS 限制：当发送的数据超过 MSS 限制后，会将数据切分发送，就会造成半包 本质是因为 TCP 是流式协议，消息无边界 滑动窗口 TCP 以一个段（segment）为单位，每发送一个段就需要进行一次确认应答（ack）处理，但如果这么做，缺点是包的往返时间越长性能就越差 为了解决此问题，引入了窗口概念，窗口大小即决定了无需等待应答而可以继续发送的数据最大值 窗口实际就起到一个缓冲区的作用，同时也能起到流量控制的作用 图中深色的部分即要发送的数据，高亮的部分即窗口 窗口内的数据才允许被发送，当应答未到达前，窗口必须停止滑动 如果 1001~2000 这个段的数据 ack 回来了，窗口就可以向前滑动 接收方也会维护一个窗口，只有落在窗口内的数据才能允许接收 MSS 限制 链路层对一次能够发送的最大数据有限制，这个限制称之为 MTU（maximum transmission unit），不同的链路设备的 MTU 值也有所不同，例如 以太网的 MTU 是 1500 FDDI（光纤分布式数据接口）的 MTU 是 4352 本地回环地址的 MTU 是 65535 - 本地测试不走网卡 MSS 是最大段长度（maximum segment size），它是 MTU 刨去 tcp 头和 ip 头后剩余能够作为数据传输的字节数 ipv4 tcp 头占用 20 bytes，ip 头占用 20 bytes，因此以太网 MSS 的值为 1500 - 40 = 1460 TCP 在传递大量数据时，会按照 MSS 大小将数据进行分割发送 MSS 的值在三次握手时通知对方自己 MSS 的值，然后在两者之间选择一个小值作为 MSS Nagle 算法 即使发送一个字节，也需要加入 tcp 头和 ip 头，也就是总字节数会使用 41 bytes，非常不经济。因此为了提高网络利用率，tcp 希望尽可能发送足够大的数据，这就是 Nagle 算法产生的缘由 该算法是指发送端即使还有应该发送的数据，但如果这部分数据很少的话，则进行延迟发送 如果 SO_SNDBUF 的数据达到 MSS，则需要发送 如果 SO_SNDBUF 中含有 FIN（表示需要连接关闭）这时将剩余数据发送，再关闭 如果 TCP_NODELAY = true，则需要发送 已发送的数据都收到 ack 时，则需要发送 上述条件不满足，但发生超时（一般为 200ms）则需要发送 除上述情况，延迟发送 1.4 解决方案 短链接，发一个包建立一次连接，这样连接建立到连接断开之间就是消息的边界，缺点效率太低 每一条消息采用固定长度，缺点浪费空间 每一条消息采用分隔符，例如 \\n，缺点需要转义 每一条消息分为 head 和 body，head 中包含 body 的长度 方法1，短链接 以解决粘包为例 public class HelloWorldClient { static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class); public static void main(String[] args) { // 分 10 次发送 for (int i = 0; i () { @Override protected void initChannel(SocketChannel ch) throws Exception { log.debug(\"conneted...\"); ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { log.debug(\"sending...\"); ByteBuf buffer = ctx.alloc().buffer(); buffer.writeBytes(new byte[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}); ctx.writeAndFlush(buffer); // 发完即关 ctx.close(); } }); } }); ChannelFuture channelFuture = bootstrap.connect(\"localhost\", 8080).sync(); channelFuture.channel().closeFuture().sync(); } catch (InterruptedException e) { log.error(\"client error\", e); } finally { worker.shutdownGracefully(); } } } 输出，略 半包用这种办法还是不好解决，因为接收方的缓冲区大小是有限的 方法2，固定长度 让所有数据包长度固定（假设长度为 8 字节），服务器端加入 ch.pipeline().addLast(new FixedLengthFrameDecoder(8)); 客户端测试代码，注意, 采用这种方法后，客户端什么时候 flush 都可以 public class HelloWorldClient { static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class); public static void main(String[] args) { NioEventLoopGroup worker = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(worker); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { log.debug(\"connetted...\"); ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { log.debug(\"sending...\"); // 发送内容随机的数据包 Random r = new Random(); char c = 'a'; ByteBuf buffer = ctx.alloc().buffer(); for (int i = 0; i 客户端输出 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - connetted... 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2] REGISTERED 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2] CONNECT: /192.168.0.103:9090 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2, L:/192.168.0.103:53155 - R:/192.168.0.103:9090] ACTIVE 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - sending... 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2, L:/192.168.0.103:53155 - R:/192.168.0.103:9090] WRITE: 80B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 61 61 61 00 00 00 00 62 00 00 00 00 00 00 00 |aaaa....b.......| |00000010| 63 63 00 00 00 00 00 00 64 00 00 00 00 00 00 00 |cc......d.......| |00000020| 00 00 00 00 00 00 00 00 66 66 66 66 00 00 00 00 |........ffff....| |00000030| 67 67 67 00 00 00 00 00 68 00 00 00 00 00 00 00 |ggg.....h.......| |00000040| 69 69 69 69 69 00 00 00 6a 6a 6a 6a 00 00 00 00 |iiiii...jjjj....| +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2, L:/192.168.0.103:53155 - R:/192.168.0.103:9090] FLUSH 服务端输出 12:06:51 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xe3d9713f] binding... 12:06:51 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xe3d9713f, L:/192.168.0.103:9090] bound... 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] REGISTERED 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] ACTIVE 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 61 61 61 00 00 00 00 |aaaa.... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 62 00 00 00 00 00 00 00 |b....... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 63 63 00 00 00 00 00 00 |cc...... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 64 00 00 00 00 00 00 00 |d....... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 00 00 00 00 00 00 00 00 |........ | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 66 66 66 00 00 00 00 |ffff.... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 67 67 67 00 00 00 00 00 |ggg..... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 00 00 00 00 00 00 00 |h....... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 69 69 69 69 69 00 00 00 |iiiii... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6a 6a 6a 6a 00 00 00 00 |jjjj.... | +--------+-------------------------------------------------+----------------+ 12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:/192.168.0.103:9090 - R:/192.168.0.103:53155] READ COMPLETE 缺点是，数据包的大小不好把握 长度定的太大，浪费 长度定的太小，对某些数据包又显得不够 方法3，固定分隔符 服务端加入，默认以 \\n 或 \\r\\n 作为分隔符，如果超出指定长度仍未出现分隔符，则抛出异常 ch.pipeline().addLast(new LineBasedFrameDecoder(1024)); 客户端在每条消息之后，加入 \\n 分隔符 public class HelloWorldClient { static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class); public static void main(String[] args) { NioEventLoopGroup worker = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(worker); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { log.debug(\"connetted...\"); ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { log.debug(\"sending...\"); Random r = new Random(); char c = 'a'; ByteBuf buffer = ctx.alloc().buffer(); for (int i = 0; i 客户端输出 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - connetted... 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755] REGISTERED 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755] CONNECT: /192.168.0.103:9090 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755, L:/192.168.0.103:63641 - R:/192.168.0.103:9090] ACTIVE 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - sending... 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755, L:/192.168.0.103:63641 - R:/192.168.0.103:9090] WRITE: 60B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 0a 62 62 62 0a 63 63 63 0a 64 64 0a 65 65 65 |a.bbb.ccc.dd.eee| |00000010| 65 65 65 65 65 65 65 0a 66 66 0a 67 67 67 67 67 |eeeeeee.ff.ggggg| |00000020| 67 67 0a 68 68 68 68 0a 69 69 69 69 69 69 69 0a |gg.hhhh.iiiiiii.| |00000030| 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a 0a |jjjjjjjjjjj. | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755, L:/192.168.0.103:63641 - R:/192.168.0.103:9090] FLUSH 服务端输出 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] c.i.n.HelloWorldServer - connected [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 1B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 |a | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 3B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 62 62 62 |bbb | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 3B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 63 63 63 |ccc | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 2B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 64 64 |dd | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 10B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 65 65 65 65 65 65 65 65 65 65 |eeeeeeeeee | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 2B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 66 |ff | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 7B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 67 67 67 67 67 67 67 |ggggggg | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 4B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 68 68 68 |hhhh | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 7B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 69 69 69 69 69 69 69 |iiiiiii | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a |jjjjjjjjjjj | +--------+-------------------------------------------------+----------------+ 14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:/192.168.0.103:9090 - R:/192.168.0.103:63641] READ COMPLETE 缺点，处理字符数据比较合适，但如果内容本身包含了分隔符（字节数据常常会有此情况），那么就会解析错误 方法4，预设长度 在发送消息前，先约定用定长字节表示接下来数据的长度 // 最大长度，长度偏移，长度占用字节，长度调整，剥离字节数 ch.pipeline().addLast(new LengthFieldBasedFrameDecoder(1024, 0, 1, 0, 1)); 客户端代码 public class HelloWorldClient { static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class); public static void main(String[] args) { NioEventLoopGroup worker = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(worker); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { log.debug(\"connetted...\"); ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { log.debug(\"sending...\"); Random r = new Random(); char c = 'a'; ByteBuf buffer = ctx.alloc().buffer(); for (int i = 0; i 客户端输出 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - connetted... 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8] REGISTERED 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8] CONNECT: /192.168.0.103:9090 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8, L:/192.168.0.103:49979 - R:/192.168.0.103:9090] ACTIVE 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - sending... 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8, L:/192.168.0.103:49979 - R:/192.168.0.103:9090] WRITE: 97B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 09 61 61 61 61 61 61 61 61 61 09 62 62 62 62 62 |.aaaaaaaaa.bbbbb| |00000010| 62 62 62 62 06 63 63 63 63 63 63 08 64 64 64 64 |bbbb.cccccc.dddd| |00000020| 64 64 64 64 0f 65 65 65 65 65 65 65 65 65 65 65 |dddd.eeeeeeeeeee| |00000030| 65 65 65 65 0d 66 66 66 66 66 66 66 66 66 66 66 |eeee.fffffffffff| |00000040| 66 66 02 67 67 02 68 68 0e 69 69 69 69 69 69 69 |ff.gg.hh.iiiiiii| |00000050| 69 69 69 69 69 69 69 09 6a 6a 6a 6a 6a 6a 6a 6a |iiiiiii.jjjjjjjj| |00000060| 6a |j | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8, L:/192.168.0.103:49979 - R:/192.168.0.103:9090] FLUSH 服务端输出 14:36:50 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xdff439d3] binding... 14:36:51 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xdff439d3, L:/192.168.0.103:9090] bound... 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] REGISTERED 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] ACTIVE 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 9B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 61 61 61 61 61 61 61 61 61 |aaaaaaaaa | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 9B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 62 62 62 62 62 62 62 62 62 |bbbbbbbbb | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 6B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 63 63 63 63 63 63 |cccccc | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 8B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 64 64 64 64 64 64 64 64 |dddddddd | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 15B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 |eeeeeeeeeeeeeee | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 13B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 66 66 66 66 66 66 66 66 66 66 66 66 66 |fffffffffffff | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 2B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 67 67 |gg | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 2B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 68 68 |hh | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 14B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 69 69 69 69 69 69 69 69 69 69 69 69 69 69 |iiiiiiiiiiiiii | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ: 9B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 6a 6a 6a 6a 6a 6a 6a 6a 6a |jjjjjjjjj | +--------+-------------------------------------------------+----------------+ 14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:/192.168.0.103:9090 - R:/192.168.0.103:49979] READ COMPLETE 2. 协议设计与解析 2.1 为什么需要协议？ TCP/IP 中消息传输基于流的方式，没有边界。 协议的目的就是划定消息的边界，制定通信双方要共同遵守的通信规则 例如：在网络上传输 下雨天留客天留我不留 是中文一句著名的无标点符号句子，在没有标点符号情况下，这句话有数种拆解方式，而意思却是完全不同，所以常被用作讲述标点符号的重要性 一种解读 下雨天留客，天留，我不留 另一种解读 下雨天，留客天，留我不？留 如何设计协议呢？其实就是给网络传输的信息加上“标点符号”。但通过分隔符来断句不是很好，因为分隔符本身如果用于传输，那么必须加以区分。因此，下面一种协议较为常用 定长字节表示内容长度 + 实际内容 例如，假设一个中文字符长度为 3，按照上述协议的规则，发送信息方式如下，就不会被接收方弄错意思了 0f下雨天留客06天留09我不留 小故事 很久很久以前，一位私塾先生到一家任教。双方签订了一纸协议：“无鸡鸭亦可无鱼肉亦可白菜豆腐不可少不得束修金”。此后，私塾先生虽然认真教课，但主人家则总是给私塾先生以白菜豆腐为菜，丝毫未见鸡鸭鱼肉的款待。私塾先生先是很不解，可是后来也就想通了：主人把鸡鸭鱼肉的钱都会换为束修金的，也罢。至此双方相安无事。 年关将至，一个学年段亦告结束。私塾先生临行时，也不见主人家为他交付束修金，遂与主家理论。然主家亦振振有词：“有协议为证——无鸡鸭亦可，无鱼肉亦可，白菜豆腐不可少，不得束修金。这白纸黑字明摆着的，你有什么要说的呢？” 私塾先生据理力争：“协议是这样的——无鸡，鸭亦可；无鱼，肉亦可；白菜豆腐不可，少不得束修金。” 双方唇枪舌战，你来我往，真个是不亦乐乎！ 这里的束修金，也作“束脩”，应当是泛指教师应当得到的报酬 2.2 redis 协议举例 NioEventLoopGroup worker = new NioEventLoopGroup(); byte[] LINE = {13, 10}; try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(worker); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) { ch.pipeline().addLast(new LoggingHandler()); ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { // 会在连接 channel 建立成功后，会触发 active 事件 @Override public void channelActive(ChannelHandlerContext ctx) { set(ctx); get(ctx); } private void get(ChannelHandlerContext ctx) { ByteBuf buf = ctx.alloc().buffer(); buf.writeBytes(\"*2\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"$3\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"get\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"$3\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"aaa\".getBytes()); buf.writeBytes(LINE); ctx.writeAndFlush(buf); } private void set(ChannelHandlerContext ctx) { ByteBuf buf = ctx.alloc().buffer(); buf.writeBytes(\"*3\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"$3\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"set\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"$3\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"aaa\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"$3\".getBytes()); buf.writeBytes(LINE); buf.writeBytes(\"bbb\".getBytes()); buf.writeBytes(LINE); ctx.writeAndFlush(buf); } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg; System.out.println(buf.toString(Charset.defaultCharset())); } }); } }); ChannelFuture channelFuture = bootstrap.connect(\"localhost\", 6379).sync(); channelFuture.channel().closeFuture().sync(); } catch (InterruptedException e) { log.error(\"client error\", e); } finally { worker.shutdownGracefully(); } 2.3 http 协议举例 NioEventLoopGroup boss = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); try { ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.channel(NioServerSocketChannel.class); serverBootstrap.group(boss, worker); serverBootstrap.childHandler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); ch.pipeline().addLast(new HttpServerCodec()); ch.pipeline().addLast(new SimpleChannelInboundHandler() { @Override protected void channelRead0(ChannelHandlerContext ctx, HttpRequest msg) throws Exception { // 获取请求 log.debug(msg.uri()); // 返回响应 DefaultFullHttpResponse response = new DefaultFullHttpResponse(msg.protocolVersion(), HttpResponseStatus.OK); byte[] bytes = \"Hello, world!\".getBytes(); response.headers().setInt(CONTENT_LENGTH, bytes.length); response.content().writeBytes(bytes); // 写回响应 ctx.writeAndFlush(response); } }); /*ch.pipeline().addLast(new ChannelInboundHandlerAdapter() { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { log.debug(\"{}\", msg.getClass()); if (msg instanceof HttpRequest) { // 请求行，请求头 } else if (msg instanceof HttpContent) { //请求体 } } });*/ } }); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } catch (InterruptedException e) { log.error(\"server error\", e); } finally { boss.shutdownGracefully(); worker.shutdownGracefully(); } 2.4 自定义协议要素 魔数，用来在第一时间判定是否是无效数据包 版本号，可以支持协议的升级 序列化算法，消息正文到底采用哪种序列化反序列化方式，可以由此扩展，例如：json、protobuf、hessian、jdk 指令类型，是登录、注册、单聊、群聊... 跟业务相关 请求序号，为了双工通信，提供异步能力 正文长度 消息正文 编解码器 根据上面的要素，设计一个登录请求消息和登录响应消息，并使用 Netty 完成收发 @Slf4j public class MessageCodec extends ByteToMessageCodec { @Override protected void encode(ChannelHandlerContext ctx, Message msg, ByteBuf out) throws Exception { // 1. 4 字节的魔数 out.writeBytes(new byte[]{1, 2, 3, 4}); // 2. 1 字节的版本, out.writeByte(1); // 3. 1 字节的序列化方式 jdk 0 , json 1 out.writeByte(0); // 4. 1 字节的指令类型 out.writeByte(msg.getMessageType()); // 5. 4 个字节 out.writeInt(msg.getSequenceId()); // 无意义，对齐填充 out.writeByte(0xff); // 6. 获取内容的字节数组 ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(msg); byte[] bytes = bos.toByteArray(); // 7. 长度 out.writeInt(bytes.length); // 8. 写入内容 out.writeBytes(bytes); } @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List out) throws Exception { int magicNum = in.readInt(); byte version = in.readByte(); byte serializerType = in.readByte(); byte messageType = in.readByte(); int sequenceId = in.readInt(); in.readByte(); int length = in.readInt(); byte[] bytes = new byte[length]; in.readBytes(bytes, 0, length); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bytes)); Message message = (Message) ois.readObject(); log.debug(\"{}, {}, {}, {}, {}, {}\", magicNum, version, serializerType, messageType, sequenceId, length); log.debug(\"{}\", message); out.add(message); } } 测试 EmbeddedChannel channel = new EmbeddedChannel( new LoggingHandler(), new LengthFieldBasedFrameDecoder( 1024, 12, 4, 0, 0), new MessageCodec() ); // encode LoginRequestMessage message = new LoginRequestMessage(\"zhangsan\", \"123\", \"张三\"); // channel.writeOutbound(message); // decode ByteBuf buf = ByteBufAllocator.DEFAULT.buffer(); new MessageCodec().encode(null, message, buf); ByteBuf s1 = buf.slice(0, 100); ByteBuf s2 = buf.slice(100, buf.readableBytes() - 100); s1.retain(); // 引用计数 2 channel.writeInbound(s1); // release 1 channel.writeInbound(s2); 解读 &#x1F4A1; 什么时候可以加 @Sharable 当 handler 不保存状态时，就可以安全地在多线程下被共享 但要注意对于编解码器类，不能继承 ByteToMessageCodec 或 CombinedChannelDuplexHandler 父类，他们的构造方法对 @Sharable 有限制 如果能确保编解码器不会保存状态，可以继承 MessageToMessageCodec 父类 @Slf4j @ChannelHandler.Sharable /** * 必须和 LengthFieldBasedFrameDecoder 一起使用，确保接到的 ByteBuf 消息是完整的 */ public class MessageCodecSharable extends MessageToMessageCodec { @Override protected void encode(ChannelHandlerContext ctx, Message msg, List outList) throws Exception { ByteBuf out = ctx.alloc().buffer(); // 1. 4 字节的魔数 out.writeBytes(new byte[]{1, 2, 3, 4}); // 2. 1 字节的版本, out.writeByte(1); // 3. 1 字节的序列化方式 jdk 0 , json 1 out.writeByte(0); // 4. 1 字节的指令类型 out.writeByte(msg.getMessageType()); // 5. 4 个字节 out.writeInt(msg.getSequenceId()); // 无意义，对齐填充 out.writeByte(0xff); // 6. 获取内容的字节数组 ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(msg); byte[] bytes = bos.toByteArray(); // 7. 长度 out.writeInt(bytes.length); // 8. 写入内容 out.writeBytes(bytes); outList.add(out); } @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List out) throws Exception { int magicNum = in.readInt(); byte version = in.readByte(); byte serializerType = in.readByte(); byte messageType = in.readByte(); int sequenceId = in.readInt(); in.readByte(); int length = in.readInt(); byte[] bytes = new byte[length]; in.readBytes(bytes, 0, length); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bytes)); Message message = (Message) ois.readObject(); log.debug(\"{}, {}, {}, {}, {}, {}\", magicNum, version, serializerType, messageType, sequenceId, length); log.debug(\"{}\", message); out.add(message); } } 3. 聊天室案例 3.1 聊天室业务介绍 /** * 用户管理接口 */ public interface UserService { /** * 登录 * @param username 用户名 * @param password 密码 * @return 登录成功返回 true, 否则返回 false */ boolean login(String username, String password); } /** * 会话管理接口 */ public interface Session { /** * 绑定会话 * @param channel 哪个 channel 要绑定会话 * @param username 会话绑定用户 */ void bind(Channel channel, String username); /** * 解绑会话 * @param channel 哪个 channel 要解绑会话 */ void unbind(Channel channel); /** * 获取属性 * @param channel 哪个 channel * @param name 属性名 * @return 属性值 */ Object getAttribute(Channel channel, String name); /** * 设置属性 * @param channel 哪个 channel * @param name 属性名 * @param value 属性值 */ void setAttribute(Channel channel, String name, Object value); /** * 根据用户名获取 channel * @param username 用户名 * @return channel */ Channel getChannel(String username); } /** * 聊天组会话管理接口 */ public interface GroupSession { /** * 创建一个聊天组, 如果不存在才能创建成功, 否则返回 null * @param name 组名 * @param members 成员 * @return 成功时返回组对象, 失败返回 null */ Group createGroup(String name, Set members); /** * 加入聊天组 * @param name 组名 * @param member 成员名 * @return 如果组不存在返回 null, 否则返回组对象 */ Group joinMember(String name, String member); /** * 移除组成员 * @param name 组名 * @param member 成员名 * @return 如果组不存在返回 null, 否则返回组对象 */ Group removeMember(String name, String member); /** * 移除聊天组 * @param name 组名 * @return 如果组不存在返回 null, 否则返回组对象 */ Group removeGroup(String name); /** * 获取组成员 * @param name 组名 * @return 成员集合, 没有成员会返回 empty set */ Set getMembers(String name); /** * 获取组成员的 channel 集合, 只有在线的 channel 才会返回 * @param name 组名 * @return 成员 channel 集合 */ List getMembersChannel(String name); } 3.2 聊天室业务-登录 @Slf4j public class ChatServer { public static void main(String[] args) { NioEventLoopGroup boss = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); LoggingHandler LOGGING_HANDLER = new LoggingHandler(LogLevel.DEBUG); MessageCodecSharable MESSAGE_CODEC = new MessageCodecSharable(); try { ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.channel(NioServerSocketChannel.class); serverBootstrap.group(boss, worker); serverBootstrap.childHandler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new ProcotolFrameDecoder()); ch.pipeline().addLast(LOGGING_HANDLER); ch.pipeline().addLast(MESSAGE_CODEC); ch.pipeline().addLast(new SimpleChannelInboundHandler() { @Override protected void channelRead0(ChannelHandlerContext ctx, LoginRequestMessage msg) throws Exception { String username = msg.getUsername(); String password = msg.getPassword(); boolean login = UserServiceFactory.getUserService().login(username, password); LoginResponseMessage message; if(login) { message = new LoginResponseMessage(true, \"登录成功\"); } else { message = new LoginResponseMessage(false, \"用户名或密码不正确\"); } ctx.writeAndFlush(message); } }); } }); Channel channel = serverBootstrap.bind(8080).sync().channel(); channel.closeFuture().sync(); } catch (InterruptedException e) { log.error(\"server error\", e); } finally { boss.shutdownGracefully(); worker.shutdownGracefully(); } } } @Slf4j public class ChatClient { public static void main(String[] args) { NioEventLoopGroup group = new NioEventLoopGroup(); LoggingHandler LOGGING_HANDLER = new LoggingHandler(LogLevel.DEBUG); MessageCodecSharable MESSAGE_CODEC = new MessageCodecSharable(); CountDownLatch WAIT_FOR_LOGIN = new CountDownLatch(1); AtomicBoolean LOGIN = new AtomicBoolean(false); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(group); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new ProcotolFrameDecoder()); // ch.pipeline().addLast(LOGGING_HANDLER); ch.pipeline().addLast(MESSAGE_CODEC); ch.pipeline().addLast(\"client handler\", new ChannelInboundHandlerAdapter() { // 接收响应消息 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { log.debug(\"msg: {}\", msg); if ((msg instanceof LoginResponseMessage)) { LoginResponseMessage response = (LoginResponseMessage) msg; if (response.isSuccess()) { // 如果登录成功 LOGIN.set(true); } // 唤醒 system in 线程 WAIT_FOR_LOGIN.countDown(); } } // 在连接建立后触发 active 事件 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { // 负责接收用户在控制台的输入，负责向服务器发送各种消息 new Thread(() -> { Scanner scanner = new Scanner(System.in); System.out.println(\"请输入用户名:\"); String username = scanner.nextLine(); System.out.println(\"请输入密码:\"); String password = scanner.nextLine(); // 构造消息对象 LoginRequestMessage message = new LoginRequestMessage(username, password); // 发送消息 ctx.writeAndFlush(message); System.out.println(\"等待后续操作...\"); try { WAIT_FOR_LOGIN.await(); } catch (InterruptedException e) { e.printStackTrace(); } // 如果登录失败 if (!LOGIN.get()) { ctx.channel().close(); return; } while (true) { System.out.println(\"==================================\"); System.out.println(\"send [username] [content]\"); System.out.println(\"gsend [group name] [content]\"); System.out.println(\"gcreate [group name] [m1,m2,m3...]\"); System.out.println(\"gmembers [group name]\"); System.out.println(\"gjoin [group name]\"); System.out.println(\"gquit [group name]\"); System.out.println(\"quit\"); System.out.println(\"==================================\"); String command = scanner.nextLine(); String[] s = command.split(\" \"); switch (s[0]){ case \"send\": ctx.writeAndFlush(new ChatRequestMessage(username, s[1], s[2])); break; case \"gsend\": ctx.writeAndFlush(new GroupChatRequestMessage(username, s[1], s[2])); break; case \"gcreate\": Set set = new HashSet<>(Arrays.asList(s[2].split(\",\"))); set.add(username); // 加入自己 ctx.writeAndFlush(new GroupCreateRequestMessage(s[1], set)); break; case \"gmembers\": ctx.writeAndFlush(new GroupMembersRequestMessage(s[1])); break; case \"gjoin\": ctx.writeAndFlush(new GroupJoinRequestMessage(username, s[1])); break; case \"gquit\": ctx.writeAndFlush(new GroupQuitRequestMessage(username, s[1])); break; case \"quit\": ctx.channel().close(); return; } } }, \"system in\").start(); } }); } }); Channel channel = bootstrap.connect(\"localhost\", 8080).sync().channel(); channel.closeFuture().sync(); } catch (Exception e) { log.error(\"client error\", e); } finally { group.shutdownGracefully(); } } } 3.3 聊天室业务-单聊 服务器端将 handler 独立出来 登录 handler @ChannelHandler.Sharable public class LoginRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, LoginRequestMessage msg) throws Exception { String username = msg.getUsername(); String password = msg.getPassword(); boolean login = UserServiceFactory.getUserService().login(username, password); LoginResponseMessage message; if(login) { SessionFactory.getSession().bind(ctx.channel(), username); message = new LoginResponseMessage(true, \"登录成功\"); } else { message = new LoginResponseMessage(false, \"用户名或密码不正确\"); } ctx.writeAndFlush(message); } } 单聊 handler @ChannelHandler.Sharable public class ChatRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, ChatRequestMessage msg) throws Exception { String to = msg.getTo(); Channel channel = SessionFactory.getSession().getChannel(to); // 在线 if(channel != null) { channel.writeAndFlush(new ChatResponseMessage(msg.getFrom(), msg.getContent())); } // 不在线 else { ctx.writeAndFlush(new ChatResponseMessage(false, \"对方用户不存在或者不在线\")); } } } 3.4 聊天室业务-群聊 创建群聊 @ChannelHandler.Sharable public class GroupCreateRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, GroupCreateRequestMessage msg) throws Exception { String groupName = msg.getGroupName(); Set members = msg.getMembers(); // 群管理器 GroupSession groupSession = GroupSessionFactory.getGroupSession(); Group group = groupSession.createGroup(groupName, members); if (group == null) { // 发生成功消息 ctx.writeAndFlush(new GroupCreateResponseMessage(true, groupName + \"创建成功\")); // 发送拉群消息 List channels = groupSession.getMembersChannel(groupName); for (Channel channel : channels) { channel.writeAndFlush(new GroupCreateResponseMessage(true, \"您已被拉入\" + groupName)); } } else { ctx.writeAndFlush(new GroupCreateResponseMessage(false, groupName + \"已经存在\")); } } } 群聊 @ChannelHandler.Sharable public class GroupChatRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, GroupChatRequestMessage msg) throws Exception { List channels = GroupSessionFactory.getGroupSession() .getMembersChannel(msg.getGroupName()); for (Channel channel : channels) { channel.writeAndFlush(new GroupChatResponseMessage(msg.getFrom(), msg.getContent())); } } } 加入群聊 @ChannelHandler.Sharable public class GroupJoinRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, GroupJoinRequestMessage msg) throws Exception { Group group = GroupSessionFactory.getGroupSession().joinMember(msg.getGroupName(), msg.getUsername()); if (group != null) { ctx.writeAndFlush(new GroupJoinResponseMessage(true, msg.getGroupName() + \"群加入成功\")); } else { ctx.writeAndFlush(new GroupJoinResponseMessage(true, msg.getGroupName() + \"群不存在\")); } } } 退出群聊 @ChannelHandler.Sharable public class GroupQuitRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, GroupQuitRequestMessage msg) throws Exception { Group group = GroupSessionFactory.getGroupSession().removeMember(msg.getGroupName(), msg.getUsername()); if (group != null) { ctx.writeAndFlush(new GroupJoinResponseMessage(true, \"已退出群\" + msg.getGroupName())); } else { ctx.writeAndFlush(new GroupJoinResponseMessage(true, msg.getGroupName() + \"群不存在\")); } } } 查看成员 @ChannelHandler.Sharable public class GroupMembersRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, GroupMembersRequestMessage msg) throws Exception { Set members = GroupSessionFactory.getGroupSession() .getMembers(msg.getGroupName()); ctx.writeAndFlush(new GroupMembersResponseMessage(members)); } } 3.5 聊天室业务-退出 @Slf4j @ChannelHandler.Sharable public class QuitHandler extends ChannelInboundHandlerAdapter { // 当连接断开时触发 inactive 事件 @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { SessionFactory.getSession().unbind(ctx.channel()); log.debug(\"{} 已经断开\", ctx.channel()); } // 当出现异常时触发 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { SessionFactory.getSession().unbind(ctx.channel()); log.debug(\"{} 已经异常断开 异常是{}\", ctx.channel(), cause.getMessage()); } } 3.6 聊天室业务-空闲检测 连接假死 原因 网络设备出现故障，例如网卡，机房等，底层的 TCP 连接已经断开了，但应用程序没有感知到，仍然占用着资源。 公网网络不稳定，出现丢包。如果连续出现丢包，这时现象就是客户端数据发不出去，服务端也一直收不到数据，就这么一直耗着 应用程序线程阻塞，无法进行数据读写 问题 假死的连接占用的资源不能自动释放 向假死的连接发送数据，得到的反馈是发送超时 服务器端解决 怎么判断客户端连接是否假死呢？如果能收到客户端数据，说明没有假死。因此策略就可以定为，每隔一段时间就检查这段时间内是否接收到客户端数据，没有就可以判定为连接假死 // 用来判断是不是 读空闲时间过长，或 写空闲时间过长 // 5s 内如果没有收到 channel 的数据，会触发一个 IdleState#READER_IDLE 事件 ch.pipeline().addLast(new IdleStateHandler(5, 0, 0)); // ChannelDuplexHandler 可以同时作为入站和出站处理器 ch.pipeline().addLast(new ChannelDuplexHandler() { // 用来触发特殊事件 @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception{ IdleStateEvent event = (IdleStateEvent) evt; // 触发了读空闲事件 if (event.state() == IdleState.READER_IDLE) { log.debug(\"已经 5s 没有读到数据了\"); ctx.channel().close(); } } }); 客户端定时心跳 客户端可以定时向服务器端发送数据，只要这个时间间隔小于服务器定义的空闲检测的时间间隔，那么就能防止前面提到的误判，客户端可以定义如下心跳处理器 // 用来判断是不是 读空闲时间过长，或 写空闲时间过长 // 3s 内如果没有向服务器写数据，会触发一个 IdleState#WRITER_IDLE 事件 ch.pipeline().addLast(new IdleStateHandler(0, 3, 0)); // ChannelDuplexHandler 可以同时作为入站和出站处理器 ch.pipeline().addLast(new ChannelDuplexHandler() { // 用来触发特殊事件 @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception{ IdleStateEvent event = (IdleStateEvent) evt; // 触发了写空闲事件 if (event.state() == IdleState.WRITER_IDLE) { // log.debug(\"3s 没有写数据了，发送一个心跳包\"); ctx.writeAndFlush(new PingMessage()); } } }); Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 09:20:53 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/Netty04-source.html":{"url":"distributed/netty/Netty04-source.html","title":"Netty04-优化与源码","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 优化 1.1 扩展序列化算法 1.2 参数调优 1）CONNECT_TIMEOUT_MILLIS 2）SO_BACKLOG 3）ulimit -n 4）TCP_NODELAY 5）SO_SNDBUF & SO_RCVBUF 6）ALLOCATOR 7）RCVBUF_ALLOCATOR 1.3 RPC 框架 1）准备工作 2）服务器 handler 3）客户端代码第一版 4）客户端 handler 第一版 5）客户端代码 第二版 6）客户端 handler 第二版 2. 源码分析 2.1 启动剖析 2.2 NioEventLoop 剖析 ⚠️ 注意 2.3 accept 剖析 2.4 read 剖析 四. 优化与源码 1. 优化 1.1 扩展序列化算法 序列化，反序列化主要用在消息正文的转换上 序列化时，需要将 Java 对象变为要传输的数据（可以是 byte[]，或 json 等，最终都需要变成 byte[]） 反序列化时，需要将传入的正文数据还原成 Java 对象，便于处理 目前的代码仅支持 Java 自带的序列化，反序列化机制，核心代码如下 // 反序列化 byte[] body = new byte[bodyLength]; byteByf.readBytes(body); ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(body)); Message message = (Message) in.readObject(); message.setSequenceId(sequenceId); // 序列化 ByteArrayOutputStream out = new ByteArrayOutputStream(); new ObjectOutputStream(out).writeObject(message); byte[] bytes = out.toByteArray(); 为了支持更多序列化算法，抽象一个 Serializer 接口 public interface Serializer { // 反序列化方法 T deserialize(Class clazz, byte[] bytes); // 序列化方法 byte[] serialize(T object); } 提供两个实现，我这里直接将实现加入了枚举类 Serializer.Algorithm 中 enum SerializerAlgorithm implements Serializer { // Java 实现 Java { @Override public T deserialize(Class clazz, byte[] bytes) { try { ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(bytes)); Object object = in.readObject(); return (T) object; } catch (IOException | ClassNotFoundException e) { throw new RuntimeException(\"SerializerAlgorithm.Java 反序列化错误\", e); } } @Override public byte[] serialize(T object) { try { ByteArrayOutputStream out = new ByteArrayOutputStream(); new ObjectOutputStream(out).writeObject(object); return out.toByteArray(); } catch (IOException e) { throw new RuntimeException(\"SerializerAlgorithm.Java 序列化错误\", e); } } }, // Json 实现(引入了 Gson 依赖) Json { @Override public T deserialize(Class clazz, byte[] bytes) { return new Gson().fromJson(new String(bytes, StandardCharsets.UTF_8), clazz); } @Override public byte[] serialize(T object) { return new Gson().toJson(object).getBytes(StandardCharsets.UTF_8); } }; // 需要从协议的字节中得到是哪种序列化算法 public static SerializerAlgorithm getByInt(int type) { SerializerAlgorithm[] array = SerializerAlgorithm.values(); if (type array.length - 1) { throw new IllegalArgumentException(\"超过 SerializerAlgorithm 范围\"); } return array[type]; } } 增加配置类和配置文件 public abstract class Config { static Properties properties; static { try (InputStream in = Config.class.getResourceAsStream(\"/application.properties\")) { properties = new Properties(); properties.load(in); } catch (IOException e) { throw new ExceptionInInitializerError(e); } } public static int getServerPort() { String value = properties.getProperty(\"server.port\"); if(value == null) { return 8080; } else { return Integer.parseInt(value); } } public static Serializer.Algorithm getSerializerAlgorithm() { String value = properties.getProperty(\"serializer.algorithm\"); if(value == null) { return Serializer.Algorithm.Java; } else { return Serializer.Algorithm.valueOf(value); } } } 配置文件 serializer.algorithm=Json 修改编解码器 /** * 必须和 LengthFieldBasedFrameDecoder 一起使用，确保接到的 ByteBuf 消息是完整的 */ public class MessageCodecSharable extends MessageToMessageCodec { @Override public void encode(ChannelHandlerContext ctx, Message msg, List outList) throws Exception { ByteBuf out = ctx.alloc().buffer(); // 1. 4 字节的魔数 out.writeBytes(new byte[]{1, 2, 3, 4}); // 2. 1 字节的版本, out.writeByte(1); // 3. 1 字节的序列化方式 jdk 0 , json 1 out.writeByte(Config.getSerializerAlgorithm().ordinal()); // 4. 1 字节的指令类型 out.writeByte(msg.getMessageType()); // 5. 4 个字节 out.writeInt(msg.getSequenceId()); // 无意义，对齐填充 out.writeByte(0xff); // 6. 获取内容的字节数组 byte[] bytes = Config.getSerializerAlgorithm().serialize(msg); // 7. 长度 out.writeInt(bytes.length); // 8. 写入内容 out.writeBytes(bytes); outList.add(out); } @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List out) throws Exception { int magicNum = in.readInt(); byte version = in.readByte(); byte serializerAlgorithm = in.readByte(); // 0 或 1 byte messageType = in.readByte(); // 0,1,2... int sequenceId = in.readInt(); in.readByte(); int length = in.readInt(); byte[] bytes = new byte[length]; in.readBytes(bytes, 0, length); // 找到反序列化算法 Serializer.Algorithm algorithm = Serializer.Algorithm.values()[serializerAlgorithm]; // 确定具体消息类型 Class messageClass = Message.getMessageClass(messageType); Message message = algorithm.deserialize(messageClass, bytes); // log.debug(\"{}, {}, {}, {}, {}, {}\", magicNum, version, serializerType, messageType, sequenceId, length); // log.debug(\"{}\", message); out.add(message); } } 其中确定具体消息类型，可以根据 消息类型字节 获取到对应的 消息 class @Data public abstract class Message implements Serializable { /** * 根据消息类型字节，获得对应的消息 class * @param messageType 消息类型字节 * @return 消息 class */ public static Class getMessageClass(int messageType) { return messageClasses.get(messageType); } private int sequenceId; private int messageType; public abstract int getMessageType(); public static final int LoginRequestMessage = 0; public static final int LoginResponseMessage = 1; public static final int ChatRequestMessage = 2; public static final int ChatResponseMessage = 3; public static final int GroupCreateRequestMessage = 4; public static final int GroupCreateResponseMessage = 5; public static final int GroupJoinRequestMessage = 6; public static final int GroupJoinResponseMessage = 7; public static final int GroupQuitRequestMessage = 8; public static final int GroupQuitResponseMessage = 9; public static final int GroupChatRequestMessage = 10; public static final int GroupChatResponseMessage = 11; public static final int GroupMembersRequestMessage = 12; public static final int GroupMembersResponseMessage = 13; public static final int PingMessage = 14; public static final int PongMessage = 15; private static final Map> messageClasses = new HashMap<>(); static { messageClasses.put(LoginRequestMessage, LoginRequestMessage.class); messageClasses.put(LoginResponseMessage, LoginResponseMessage.class); messageClasses.put(ChatRequestMessage, ChatRequestMessage.class); messageClasses.put(ChatResponseMessage, ChatResponseMessage.class); messageClasses.put(GroupCreateRequestMessage, GroupCreateRequestMessage.class); messageClasses.put(GroupCreateResponseMessage, GroupCreateResponseMessage.class); messageClasses.put(GroupJoinRequestMessage, GroupJoinRequestMessage.class); messageClasses.put(GroupJoinResponseMessage, GroupJoinResponseMessage.class); messageClasses.put(GroupQuitRequestMessage, GroupQuitRequestMessage.class); messageClasses.put(GroupQuitResponseMessage, GroupQuitResponseMessage.class); messageClasses.put(GroupChatRequestMessage, GroupChatRequestMessage.class); messageClasses.put(GroupChatResponseMessage, GroupChatResponseMessage.class); messageClasses.put(GroupMembersRequestMessage, GroupMembersRequestMessage.class); messageClasses.put(GroupMembersResponseMessage, GroupMembersResponseMessage.class); } } 1.2 参数调优 1）CONNECT_TIMEOUT_MILLIS 属于 SocketChannal 参数 用在客户端建立连接时，如果在指定毫秒内无法连接，会抛出 timeout 异常 SO_TIMEOUT 主要用在阻塞 IO，阻塞 IO 中 accept，read 等都是无限等待的，如果不希望永远阻塞，使用它调整超时时间 @Slf4j public class TestConnectionTimeout { public static void main(String[] args) { NioEventLoopGroup group = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap() .group(group) .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 300) .channel(NioSocketChannel.class) .handler(new LoggingHandler()); ChannelFuture future = bootstrap.connect(\"127.0.0.1\", 8080); future.sync().channel().closeFuture().sync(); // 断点1 } catch (Exception e) { e.printStackTrace(); log.debug(\"timeout\"); } finally { group.shutdownGracefully(); } } } 另外源码部分 io.netty.channel.nio.AbstractNioChannel.AbstractNioUnsafe#connect @Override public final void connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) { // ... // Schedule connect timeout. int connectTimeoutMillis = config().getConnectTimeoutMillis(); if (connectTimeoutMillis > 0) { connectTimeoutFuture = eventLoop().schedule(new Runnable() { @Override public void run() { ChannelPromise connectPromise = AbstractNioChannel.this.connectPromise; ConnectTimeoutException cause = new ConnectTimeoutException(\"connection timed out: \" + remoteAddress); // 断点2 if (connectPromise != null && connectPromise.tryFailure(cause)) { close(voidPromise()); } } }, connectTimeoutMillis, TimeUnit.MILLISECONDS); } // ... } 2）SO_BACKLOG 属于 ServerSocketChannal 参数 sequenceDiagram participant c as client participant s as server participant sq as syns queue participant aq as accept queue s ->> s : bind() s ->> s : listen() c ->> c : connect() c ->> s : 1. SYN Note left of c : SYN_SEND s ->> sq : put Note right of s : SYN_RCVD s ->> c : 2. SYN + ACK Note left of c : ESTABLISHED c ->> s : 3. ACK sq ->> aq : put Note right of s : ESTABLISHED aq -->> s : s ->> s : accept() 第一次握手，client 发送 SYN 到 server，状态修改为 SYN_SEND，server 收到，状态改变为 SYN_REVD，并将该请求放入 sync queue 队列 第二次握手，server 回复 SYN + ACK 给 client，client 收到，状态改变为 ESTABLISHED，并发送 ACK 给 server 第三次握手，server 收到 ACK，状态改变为 ESTABLISHED，将该请求从 sync queue 放入 accept queue 其中 在 linux 2.2 之前，backlog 大小包括了两个队列的大小，在 2.2 之后，分别用下面两个参数来控制 sync queue - 半连接队列 大小通过 /proc/sys/net/ipv4/tcp_max_syn_backlog 指定，在 syncookies 启用的情况下，逻辑上没有最大值限制，这个设置便被忽略 accept queue - 全连接队列 其大小通过 /proc/sys/net/core/somaxconn 指定，在使用 listen 函数时，内核会根据传入的 backlog 参数与系统参数，取二者的较小值 如果 accpet queue 队列满了，server 将发送一个拒绝连接的错误信息到 client netty 中 可以通过 option(ChannelOption.SO_BACKLOG, 值) 来设置大小 可以通过下面源码查看默认大小 public class DefaultServerSocketChannelConfig extends DefaultChannelConfig implements ServerSocketChannelConfig { private volatile int backlog = NetUtil.SOMAXCONN; // ... } 课堂调试关键断点为：io.netty.channel.nio.NioEventLoop#processSelectedKey oio 中更容易说明，不用 debug 模式 public class Server { public static void main(String[] args) throws IOException { ServerSocket ss = new ServerSocket(8888, 2); Socket accept = ss.accept(); System.out.println(accept); System.in.read(); } } 客户端启动 4 个 public class Client { public static void main(String[] args) throws IOException { try { Socket s = new Socket(); System.out.println(new Date()+\" connecting...\"); s.connect(new InetSocketAddress(\"localhost\", 8888),1000); System.out.println(new Date()+\" connected...\"); s.getOutputStream().write(1); System.in.read(); } catch (IOException e) { System.out.println(new Date()+\" connecting timeout...\"); e.printStackTrace(); } } } 第 1，2，3 个客户端都打印，但除了第一个处于 accpet 外，其它两个都处于 accept queue 中 Tue Apr 21 20:30:28 CST 2020 connecting... Tue Apr 21 20:30:28 CST 2020 connected... 第 4 个客户端连接时 Tue Apr 21 20:53:58 CST 2020 connecting... Tue Apr 21 20:53:59 CST 2020 connecting timeout... java.net.SocketTimeoutException: connect timed out 3）ulimit -n 属于操作系统参数 4）TCP_NODELAY 属于 SocketChannal 参数 5）SO_SNDBUF & SO_RCVBUF SO_SNDBUF 属于 SocketChannal 参数 SO_RCVBUF 既可用于 SocketChannal 参数，也可以用于 ServerSocketChannal 参数（建议设置到 ServerSocketChannal 上） 6）ALLOCATOR 属于 SocketChannal 参数 用来分配 ByteBuf， ctx.alloc() 7）RCVBUF_ALLOCATOR 属于 SocketChannal 参数 控制 netty 接收缓冲区大小 负责入站数据的分配，决定入站缓冲区的大小（并可动态调整），统一采用 direct 直接内存，具体池化还是非池化由 allocator 决定 1.3 RPC 框架 1）准备工作 这些代码可以认为是现成的，无需从头编写练习 为了简化起见，在原来聊天项目的基础上新增 Rpc 请求和响应消息 @Data public abstract class Message implements Serializable { // 省略旧的代码 public static final int RPC_MESSAGE_TYPE_REQUEST = 101; public static final int RPC_MESSAGE_TYPE_RESPONSE = 102; static { // ... messageClasses.put(RPC_MESSAGE_TYPE_REQUEST, RpcRequestMessage.class); messageClasses.put(RPC_MESSAGE_TYPE_RESPONSE, RpcResponseMessage.class); } } 请求消息 @Getter @ToString(callSuper = true) public class RpcRequestMessage extends Message { /** * 调用的接口全限定名，服务端根据它找到实现 */ private String interfaceName; /** * 调用接口中的方法名 */ private String methodName; /** * 方法返回类型 */ private Class returnType; /** * 方法参数类型数组 */ private Class[] parameterTypes; /** * 方法参数值数组 */ private Object[] parameterValue; public RpcRequestMessage(int sequenceId, String interfaceName, String methodName, Class returnType, Class[] parameterTypes, Object[] parameterValue) { super.setSequenceId(sequenceId); this.interfaceName = interfaceName; this.methodName = methodName; this.returnType = returnType; this.parameterTypes = parameterTypes; this.parameterValue = parameterValue; } @Override public int getMessageType() { return RPC_MESSAGE_TYPE_REQUEST; } } 响应消息 @Data @ToString(callSuper = true) public class RpcResponseMessage extends Message { /** * 返回值 */ private Object returnValue; /** * 异常值 */ private Exception exceptionValue; @Override public int getMessageType() { return RPC_MESSAGE_TYPE_RESPONSE; } } 服务器架子 @Slf4j public class RpcServer { public static void main(String[] args) { NioEventLoopGroup boss = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); LoggingHandler LOGGING_HANDLER = new LoggingHandler(LogLevel.DEBUG); MessageCodecSharable MESSAGE_CODEC = new MessageCodecSharable(); // rpc 请求消息处理器，待实现 RpcRequestMessageHandler RPC_HANDLER = new RpcRequestMessageHandler(); try { ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.channel(NioServerSocketChannel.class); serverBootstrap.group(boss, worker); serverBootstrap.childHandler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new ProcotolFrameDecoder()); ch.pipeline().addLast(LOGGING_HANDLER); ch.pipeline().addLast(MESSAGE_CODEC); ch.pipeline().addLast(RPC_HANDLER); } }); Channel channel = serverBootstrap.bind(8080).sync().channel(); channel.closeFuture().sync(); } catch (InterruptedException e) { log.error(\"server error\", e); } finally { boss.shutdownGracefully(); worker.shutdownGracefully(); } } } 客户端架子 public class RpcClient { public static void main(String[] args) { NioEventLoopGroup group = new NioEventLoopGroup(); LoggingHandler LOGGING_HANDLER = new LoggingHandler(LogLevel.DEBUG); MessageCodecSharable MESSAGE_CODEC = new MessageCodecSharable(); // rpc 响应消息处理器，待实现 RpcResponseMessageHandler RPC_HANDLER = new RpcResponseMessageHandler(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(group); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new ProcotolFrameDecoder()); ch.pipeline().addLast(LOGGING_HANDLER); ch.pipeline().addLast(MESSAGE_CODEC); ch.pipeline().addLast(RPC_HANDLER); } }); Channel channel = bootstrap.connect(\"localhost\", 8080).sync().channel(); channel.closeFuture().sync(); } catch (Exception e) { log.error(\"client error\", e); } finally { group.shutdownGracefully(); } } } 服务器端的 service 获取 public class ServicesFactory { static Properties properties; static Map, Object> map = new ConcurrentHashMap<>(); static { try (InputStream in = Config.class.getResourceAsStream(\"/application.properties\")) { properties = new Properties(); properties.load(in); Set names = properties.stringPropertyNames(); for (String name : names) { if (name.endsWith(\"Service\")) { Class interfaceClass = Class.forName(name); Class instanceClass = Class.forName(properties.getProperty(name)); map.put(interfaceClass, instanceClass.newInstance()); } } } catch (IOException | ClassNotFoundException | InstantiationException | IllegalAccessException e) { throw new ExceptionInInitializerError(e); } } public static T getService(Class interfaceClass) { return (T) map.get(interfaceClass); } } 相关配置 application.properties serializer.algorithm=Json cn.itcast.server.service.HelloService=cn.itcast.server.service.HelloServiceImpl 2）服务器 handler @Slf4j @ChannelHandler.Sharable public class RpcRequestMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, RpcRequestMessage message) { RpcResponseMessage response = new RpcResponseMessage(); response.setSequenceId(message.getSequenceId()); try { // 获取真正的实现对象 HelloService service = (HelloService) ServicesFactory.getService(Class.forName(message.getInterfaceName())); // 获取要调用的方法 Method method = service.getClass().getMethod(message.getMethodName(), message.getParameterTypes()); // 调用方法 Object invoke = method.invoke(service, message.getParameterValue()); // 调用成功 response.setReturnValue(invoke); } catch (Exception e) { e.printStackTrace(); // 调用异常 response.setExceptionValue(e); } // 返回结果 ctx.writeAndFlush(response); } } 3）客户端代码第一版 只发消息 @Slf4j public class RpcClient { public static void main(String[] args) { NioEventLoopGroup group = new NioEventLoopGroup(); LoggingHandler LOGGING_HANDLER = new LoggingHandler(LogLevel.DEBUG); MessageCodecSharable MESSAGE_CODEC = new MessageCodecSharable(); RpcResponseMessageHandler RPC_HANDLER = new RpcResponseMessageHandler(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(group); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new ProcotolFrameDecoder()); ch.pipeline().addLast(LOGGING_HANDLER); ch.pipeline().addLast(MESSAGE_CODEC); ch.pipeline().addLast(RPC_HANDLER); } }); Channel channel = bootstrap.connect(\"localhost\", 8080).sync().channel(); ChannelFuture future = channel.writeAndFlush(new RpcRequestMessage( 1, \"cn.itcast.server.service.HelloService\", \"sayHello\", String.class, new Class[]{String.class}, new Object[]{\"张三\"} )).addListener(promise -> { if (!promise.isSuccess()) { Throwable cause = promise.cause(); log.error(\"error\", cause); } }); channel.closeFuture().sync(); } catch (Exception e) { log.error(\"client error\", e); } finally { group.shutdownGracefully(); } } } 4）客户端 handler 第一版 @Slf4j @ChannelHandler.Sharable public class RpcResponseMessageHandler extends SimpleChannelInboundHandler { @Override protected void channelRead0(ChannelHandlerContext ctx, RpcResponseMessage msg) throws Exception { log.debug(\"{}\", msg); } } 5）客户端代码 第二版 包括 channel 管理，代理，接收结果 @Slf4j public class RpcClientManager { public static void main(String[] args) { HelloService service = getProxyService(HelloService.class); System.out.println(service.sayHello(\"zhangsan\")); // System.out.println(service.sayHello(\"lisi\")); // System.out.println(service.sayHello(\"wangwu\")); } // 创建代理类 public static T getProxyService(Class serviceClass) { ClassLoader loader = serviceClass.getClassLoader(); Class[] interfaces = new Class[]{serviceClass}; // sayHello \"张三\" Object o = Proxy.newProxyInstance(loader, interfaces, (proxy, method, args) -> { // 1. 将方法调用转换为 消息对象 int sequenceId = SequenceIdGenerator.nextId(); RpcRequestMessage msg = new RpcRequestMessage( sequenceId, serviceClass.getName(), method.getName(), method.getReturnType(), method.getParameterTypes(), args ); // 2. 将消息对象发送出去 getChannel().writeAndFlush(msg); // 3. 准备一个空 Promise 对象，来接收结果 指定 promise 对象异步接收结果线程 DefaultPromise promise = new DefaultPromise<>(getChannel().eventLoop()); RpcResponseMessageHandler.PROMISES.put(sequenceId, promise); // promise.addListener(future -> { // // 线程 // }); // 4. 等待 promise 结果 promise.await(); if(promise.isSuccess()) { // 调用正常 return promise.getNow(); } else { // 调用失败 throw new RuntimeException(promise.cause()); } }); return (T) o; } private static Channel channel = null; private static final Object LOCK = new Object(); // 获取唯一的 channel 对象 public static Channel getChannel() { if (channel != null) { return channel; } synchronized (LOCK) { // t2 if (channel != null) { // t1 return channel; } initChannel(); return channel; } } // 初始化 channel 方法 private static void initChannel() { NioEventLoopGroup group = new NioEventLoopGroup(); LoggingHandler LOGGING_HANDLER = new LoggingHandler(LogLevel.DEBUG); MessageCodecSharable MESSAGE_CODEC = new MessageCodecSharable(); RpcResponseMessageHandler RPC_HANDLER = new RpcResponseMessageHandler(); Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.group(group); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new ProcotolFrameDecoder()); ch.pipeline().addLast(LOGGING_HANDLER); ch.pipeline().addLast(MESSAGE_CODEC); ch.pipeline().addLast(RPC_HANDLER); } }); try { channel = bootstrap.connect(\"localhost\", 8080).sync().channel(); channel.closeFuture().addListener(future -> { group.shutdownGracefully(); }); } catch (Exception e) { log.error(\"client error\", e); } } } 6）客户端 handler 第二版 @Slf4j @ChannelHandler.Sharable public class RpcResponseMessageHandler extends SimpleChannelInboundHandler { // 序号 用来接收结果的 promise 对象 public static final Map> PROMISES = new ConcurrentHashMap<>(); @Override protected void channelRead0(ChannelHandlerContext ctx, RpcResponseMessage msg) throws Exception { log.debug(\"{}\", msg); // 拿到空的 promise Promise promise = PROMISES.remove(msg.getSequenceId()); if (promise != null) { Object returnValue = msg.getReturnValue(); Exception exceptionValue = msg.getExceptionValue(); if(exceptionValue != null) { promise.setFailure(exceptionValue); } else { promise.setSuccess(returnValue); } } } } 2. 源码分析 2.1 启动剖析 我们就来看看 netty 中对下面的代码是怎样进行处理的 //1 netty 中使用 NioEventLoopGroup （简称 nio boss 线程）来封装线程和 selector Selector selector = Selector.open(); //2 创建 NioServerSocketChannel，同时会初始化它关联的 handler，以及为原生 ssc 存储 config NioServerSocketChannel attachment = new NioServerSocketChannel(); //3 创建 NioServerSocketChannel 时，创建了 java 原生的 ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); //4 启动 nio boss 线程执行接下来的操作 //5 注册（仅关联 selector 和 NioServerSocketChannel），未关注事件 SelectionKey selectionKey = serverSocketChannel.register(selector, 0, attachment); //6 head -> 初始化器 -> ServerBootstrapAcceptor -> tail，初始化器是一次性的，只为添加 acceptor //7 绑定端口 serverSocketChannel.bind(new InetSocketAddress(8080)); //8 触发 channel active 事件，在 head 中关注 op_accept 事件 selectionKey.interestOps(SelectionKey.OP_ACCEPT); 入口 io.netty.bootstrap.ServerBootstrap#bind 关键代码 io.netty.bootstrap.AbstractBootstrap#doBind private ChannelFuture doBind(final SocketAddress localAddress) { // 1. 执行初始化和注册 regFuture 会由 initAndRegister 设置其是否完成，从而回调 3.2 处代码 final ChannelFuture regFuture = initAndRegister(); final Channel channel = regFuture.channel(); if (regFuture.cause() != null) { return regFuture; } // 2. 因为是 initAndRegister 异步执行，需要分两种情况来看，调试时也需要通过 suspend 断点类型加以区分 // 2.1 如果已经完成 if (regFuture.isDone()) { ChannelPromise promise = channel.newPromise(); // 3.1 立刻调用 doBind0 doBind0(regFuture, channel, localAddress, promise); return promise; } // 2.2 还没有完成 else { final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); // 3.2 回调 doBind0 regFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { Throwable cause = future.cause(); if (cause != null) { // 处理异常... promise.setFailure(cause); } else { promise.registered(); // 3. 由注册线程去执行 doBind0 doBind0(regFuture, channel, localAddress, promise); } } }); return promise; } } 关键代码 io.netty.bootstrap.AbstractBootstrap#initAndRegister final ChannelFuture initAndRegister() { Channel channel = null; try { channel = channelFactory.newChannel(); // 1.1 初始化 - 做的事就是添加一个初始化器 ChannelInitializer init(channel); } catch (Throwable t) { // 处理异常... return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } // 1.2 注册 - 做的事就是将原生 channel 注册到 selector 上 ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) { // 处理异常... } return regFuture; } 关键代码 io.netty.bootstrap.ServerBootstrap#init // 这里 channel 实际上是 NioServerSocketChannel void init(Channel channel) throws Exception { final Map, Object> options = options0(); synchronized (options) { setChannelOptions(channel, options, logger); } final Map, Object> attrs = attrs0(); synchronized (attrs) { for (Entry, Object> e: attrs.entrySet()) { @SuppressWarnings(\"unchecked\") AttributeKey key = (AttributeKey) e.getKey(); channel.attr(key).set(e.getValue()); } } ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry, Object>[] currentChildOptions; final Entry, Object>[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } // 为 NioServerSocketChannel 添加初始化器 p.addLast(new ChannelInitializer() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } // 初始化器的职责是将 ServerBootstrapAcceptor 加入至 NioServerSocketChannel ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } 关键代码 io.netty.channel.AbstractChannel.AbstractUnsafe#register public final void register(EventLoop eventLoop, final ChannelPromise promise) { // 一些检查，略... AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { try { // 首次执行 execute 方法时，会启动 nio 线程，之后注册等操作在 nio 线程上执行 // 因为只有一个 NioServerSocketChannel 因此，也只会有一个 boss nio 线程 // 这行代码完成的事实是 main -> nio boss 线程的切换 eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { // 日志记录... closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } } io.netty.channel.AbstractChannel.AbstractUnsafe#register0 private void register0(ChannelPromise promise) { try { if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; // 1.2.1 原生的 nio channel 绑定到 selector 上，注意此时没有注册 selector 关注事件，附件为 NioServerSocketChannel doRegister(); neverRegistered = false; registered = true; // 1.2.2 执行 NioServerSocketChannel 初始化器的 initChannel pipeline.invokeHandlerAddedIfNeeded(); // 回调 3.2 io.netty.bootstrap.AbstractBootstrap#doBind0 safeSetSuccess(promise); pipeline.fireChannelRegistered(); // 对应 server socket channel 还未绑定，isActive 为 false if (isActive()) { if (firstRegistration) { pipeline.fireChannelActive(); } else if (config().isAutoRead()) { beginRead(); } } } catch (Throwable t) { // Close the channel directly to avoid FD leak. closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } 关键代码 io.netty.channel.ChannelInitializer#initChannel private boolean initChannel(ChannelHandlerContext ctx) throws Exception { if (initMap.add(ctx)) { // Guard against re-entrance. try { // 1.2.2.1 执行初始化 initChannel((C) ctx.channel()); } catch (Throwable cause) { exceptionCaught(ctx, cause); } finally { // 1.2.2.2 移除初始化器 ChannelPipeline pipeline = ctx.pipeline(); if (pipeline.context(this) != null) { pipeline.remove(this); } } return true; } return false; } 关键代码 io.netty.bootstrap.AbstractBootstrap#doBind0 // 3.1 或 3.2 执行 doBind0 private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } 关键代码 io.netty.channel.AbstractChannel.AbstractUnsafe#bind public final void bind(final SocketAddress localAddress, final ChannelPromise promise) { assertEventLoop(); if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } if (Boolean.TRUE.equals(config().getOption(ChannelOption.SO_BROADCAST)) && localAddress instanceof InetSocketAddress && !((InetSocketAddress) localAddress).getAddress().isAnyLocalAddress() && !PlatformDependent.isWindows() && !PlatformDependent.maybeSuperUser()) { // 记录日志... } boolean wasActive = isActive(); try { // 3.3 执行端口绑定 doBind(localAddress); } catch (Throwable t) { safeSetFailure(promise, t); closeIfClosed(); return; } if (!wasActive && isActive()) { invokeLater(new Runnable() { @Override public void run() { // 3.4 触发 active 事件 pipeline.fireChannelActive(); } }); } safeSetSuccess(promise); } 3.3 关键代码 io.netty.channel.socket.nio.NioServerSocketChannel#doBind protected void doBind(SocketAddress localAddress) throws Exception { if (PlatformDependent.javaVersion() >= 7) { javaChannel().bind(localAddress, config.getBacklog()); } else { javaChannel().socket().bind(localAddress, config.getBacklog()); } } 3.4 关键代码 io.netty.channel.DefaultChannelPipeline.HeadContext#channelActive public void channelActive(ChannelHandlerContext ctx) { ctx.fireChannelActive(); // 触发 read (NioServerSocketChannel 上的 read 不是读取数据，只是为了触发 channel 的事件注册) readIfIsAutoRead(); } 关键代码 io.netty.channel.nio.AbstractNioChannel#doBeginRead protected void doBeginRead() throws Exception { // Channel.read() or ChannelHandlerContext.read() was called final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) { return; } readPending = true; final int interestOps = selectionKey.interestOps(); // readInterestOp 取值是 16，在 NioServerSocketChannel 创建时初始化好，代表关注 accept 事件 if ((interestOps & readInterestOp) == 0) { selectionKey.interestOps(interestOps | readInterestOp); } } 2.2 NioEventLoop 剖析 NioEventLoop 线程不仅要处理 IO 事件，还要处理 Task（包括普通任务和定时任务）， 提交任务代码 io.netty.util.concurrent.SingleThreadEventExecutor#execute public void execute(Runnable task) { if (task == null) { throw new NullPointerException(\"task\"); } boolean inEventLoop = inEventLoop(); // 添加任务，其中队列使用了 jctools 提供的 mpsc 无锁队列 addTask(task); if (!inEventLoop) { // inEventLoop 如果为 false 表示由其它线程来调用 execute，即首次调用，这时需要向 eventLoop 提交首个任务，启动死循环，会执行到下面的 doStartThread startThread(); if (isShutdown()) { // 如果已经 shutdown，做拒绝逻辑，代码略... } } if (!addTaskWakesUp && wakesUpForTask(task)) { // 如果线程由于 IO select 阻塞了，添加的任务的线程需要负责唤醒 NioEventLoop 线程 wakeup(inEventLoop); } } 唤醒 select 阻塞线程io.netty.channel.nio.NioEventLoop#wakeup @Override protected void wakeup(boolean inEventLoop) { if (!inEventLoop && wakenUp.compareAndSet(false, true)) { selector.wakeup(); } } 启动 EventLoop 主循环 io.netty.util.concurrent.SingleThreadEventExecutor#doStartThread private void doStartThread() { assert thread == null; executor.execute(new Runnable() { @Override public void run() { // 将线程池的当前线程保存在成员变量中，以便后续使用 thread = Thread.currentThread(); if (interrupted) { thread.interrupt(); } boolean success = false; updateLastExecutionTime(); try { // 调用外部类 SingleThreadEventExecutor 的 run 方法，进入死循环，run 方法见下 SingleThreadEventExecutor.this.run(); success = true; } catch (Throwable t) { logger.warn(\"Unexpected exception from an event executor: \", t); } finally { // 清理工作，代码略... } } }); } io.netty.channel.nio.NioEventLoop#run 主要任务是执行死循环，不断看有没有新任务，有没有 IO 事件 protected void run() { for (;;) { try { try { // calculateStrategy 的逻辑如下： // 有任务，会执行一次 selectNow，清除上一次的 wakeup 结果，无论有没有 IO 事件，都会跳过 switch // 没有任务，会匹配 SelectStrategy.SELECT，看是否应当阻塞 switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) { case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: case SelectStrategy.SELECT: // 因为 IO 线程和提交任务线程都有可能执行 wakeup，而 wakeup 属于比较昂贵的操作，因此使用了一个原子布尔对象 wakenUp，它取值为 true 时，表示该由当前线程唤醒 // 进行 select 阻塞，并设置唤醒状态为 false boolean oldWakenUp = wakenUp.getAndSet(false); // 如果在这个位置，非 EventLoop 线程抢先将 wakenUp 置为 true，并 wakeup // 下面的 select 方法不会阻塞 // 等 runAllTasks 处理完成后，到再循环进来这个阶段新增的任务会不会及时执行呢? // 因为 oldWakenUp 为 true，因此下面的 select 方法就会阻塞，直到超时 // 才能执行，让 select 方法无谓阻塞 select(oldWakenUp); if (wakenUp.get()) { selector.wakeup(); } default: } } catch (IOException e) { rebuildSelector0(); handleLoopException(e); continue; } cancelledKeys = 0; needsToSelectAgain = false; // ioRatio 默认是 50 final int ioRatio = this.ioRatio; if (ioRatio == 100) { try { processSelectedKeys(); } finally { // ioRatio 为 100 时，总是运行完所有非 IO 任务 runAllTasks(); } } else { final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // 记录 io 事件处理耗时 final long ioTime = System.nanoTime() - ioStartTime; // 运行非 IO 任务，一旦超时会退出 runAllTasks runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } } catch (Throwable t) { handleLoopException(t); } try { if (isShuttingDown()) { closeAll(); if (confirmShutdown()) { return; } } } catch (Throwable t) { handleLoopException(t); } } } ⚠️ 注意 这里有个费解的地方就是 wakeup，它既可以由提交任务的线程来调用（比较好理解），也可以由 EventLoop 线程来调用（比较费解），这里要知道 wakeup 方法的效果： 由非 EventLoop 线程调用，会唤醒当前在执行 select 阻塞的 EventLoop 线程 由 EventLoop 自己调用，会本次的 wakeup 会取消下一次的 select 操作 参考下图 io.netty.channel.nio.NioEventLoop#select private void select(boolean oldWakenUp) throws IOException { Selector selector = this.selector; try { int selectCnt = 0; long currentTimeNanos = System.nanoTime(); // 计算等待时间 // * 没有 scheduledTask，超时时间为 1s // * 有 scheduledTask，超时时间为 `下一个定时任务执行时间 - 当前时间` long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos); for (;;) { long timeoutMillis = (selectDeadLineNanos - currentTimeNanos + 500000L) / 1000000L; // 如果超时，退出循环 if (timeoutMillis = currentTimeNanos) { // 如果超时，计数重置为 1，下次循环就会 break selectCnt = 1; } // 计数超过阈值，由 io.netty.selectorAutoRebuildThreshold 指定，默认 512 // 这是为了解决 nio 空轮询 bug else if (SELECTOR_AUTO_REBUILD_THRESHOLD > 0 && selectCnt >= SELECTOR_AUTO_REBUILD_THRESHOLD) { // 重建 selector selector = selectRebuildSelector(selectCnt); selectCnt = 1; break; } currentTimeNanos = time; } if (selectCnt > MIN_PREMATURE_SELECTOR_RETURNS) { // 记录日志 } } catch (CancelledKeyException e) { // 记录日志 } } 处理 keys io.netty.channel.nio.NioEventLoop#processSelectedKeys private void processSelectedKeys() { if (selectedKeys != null) { // 通过反射将 Selector 实现类中的就绪事件集合替换为 SelectedSelectionKeySet // SelectedSelectionKeySet 底层为数组实现，可以提高遍历性能（原本为 HashSet） processSelectedKeysOptimized(); } else { processSelectedKeysPlain(selector.selectedKeys()); } } io.netty.channel.nio.NioEventLoop#processSelectedKey private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) { final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); // 当 key 取消或关闭时会导致这个 key 无效 if (!k.isValid()) { // 无效时处理... return; } try { int readyOps = k.readyOps(); // 连接事件 if ((readyOps & SelectionKey.OP_CONNECT) != 0) { int ops = k.interestOps(); ops &= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); } // 可写事件 if ((readyOps & SelectionKey.OP_WRITE) != 0) { ch.unsafe().forceFlush(); } // 可读或可接入事件 if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { // 如果是可接入 io.netty.channel.nio.AbstractNioMessageChannel.NioMessageUnsafe#read // 如果是可读 io.netty.channel.nio.AbstractNioByteChannel.NioByteUnsafe#read unsafe.read(); } } catch (CancelledKeyException ignored) { unsafe.close(unsafe.voidPromise()); } } 2.3 accept 剖析 nio 中如下代码，在 netty 中的流程 //1 阻塞直到事件发生 selector.select(); Iterator iter = selector.selectedKeys().iterator(); while (iter.hasNext()) { //2 拿到一个事件 SelectionKey key = iter.next(); //3 如果是 accept 事件 if (key.isAcceptable()) { //4 执行 accept SocketChannel channel = serverSocketChannel.accept(); channel.configureBlocking(false); //5 关注 read 事件 channel.register(selector, SelectionKey.OP_READ); } // ... } 先来看可接入事件处理（accept） io.netty.channel.nio.AbstractNioMessageChannel.NioMessageUnsafe#read public void read() { assert eventLoop().inEventLoop(); final ChannelConfig config = config(); final ChannelPipeline pipeline = pipeline(); final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.reset(config); boolean closed = false; Throwable exception = null; try { try { do { // doReadMessages 中执行了 accept 并创建 NioSocketChannel 作为消息放入 readBuf // readBuf 是一个 ArrayList 用来缓存消息 int localRead = doReadMessages(readBuf); if (localRead == 0) { break; } if (localRead 关键代码 io.netty.bootstrap.ServerBootstrap.ServerBootstrapAcceptor#channelRead public void channelRead(ChannelHandlerContext ctx, Object msg) { // 这时的 msg 是 NioSocketChannel final Channel child = (Channel) msg; // NioSocketChannel 添加 childHandler 即初始化器 child.pipeline().addLast(childHandler); // 设置选项 setChannelOptions(child, childOptions, logger); for (Entry, Object> e: childAttrs) { child.attr((AttributeKey) e.getKey()).set(e.getValue()); } try { // 注册 NioSocketChannel 到 nio worker 线程，接下来的处理也移交至 nio worker 线程 childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } 又回到了熟悉的 io.netty.channel.AbstractChannel.AbstractUnsafe#register 方法 public final void register(EventLoop eventLoop, final ChannelPromise promise) { // 一些检查，略... AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { try { // 这行代码完成的事实是 nio boss -> nio worker 线程的切换 eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { // 日志记录... closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } } io.netty.channel.AbstractChannel.AbstractUnsafe#register0 private void register0(ChannelPromise promise) { try { if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; // 执行初始化器，执行前 pipeline 中只有 head -> 初始化器 -> tail pipeline.invokeHandlerAddedIfNeeded(); // 执行后就是 head -> logging handler -> my handler -> tail safeSetSuccess(promise); pipeline.fireChannelRegistered(); if (isActive()) { if (firstRegistration) { // 触发 pipeline 上 active 事件 pipeline.fireChannelActive(); } else if (config().isAutoRead()) { beginRead(); } } } catch (Throwable t) { closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } 回到了熟悉的代码 io.netty.channel.DefaultChannelPipeline.HeadContext#channelActive public void channelActive(ChannelHandlerContext ctx) { ctx.fireChannelActive(); // 触发 read (NioSocketChannel 这里 read，只是为了触发 channel 的事件注册，还未涉及数据读取) readIfIsAutoRead(); } io.netty.channel.nio.AbstractNioChannel#doBeginRead protected void doBeginRead() throws Exception { // Channel.read() or ChannelHandlerContext.read() was called final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) { return; } readPending = true; // 这时候 interestOps 是 0 final int interestOps = selectionKey.interestOps(); if ((interestOps & readInterestOp) == 0) { // 关注 read 事件 selectionKey.interestOps(interestOps | readInterestOp); } } 2.4 read 剖析 再来看可读事件 io.netty.channel.nio.AbstractNioByteChannel.NioByteUnsafe#read，注意发送的数据未必能够一次读完，因此会触发多次 nio read 事件，一次事件内会触发多次 pipeline read，一次事件会触发一次 pipeline read complete public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); // io.netty.allocator.type 决定 allocator 的实现 final ByteBufAllocator allocator = config.getAllocator(); // 用来分配 byteBuf，确定单次读取大小 final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { byteBuf = allocHandle.allocate(allocator); // 读取 allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator.MaxMessageHandle#continueReading(io.netty.util.UncheckedBooleanSupplier) public boolean continueReading(UncheckedBooleanSupplier maybeMoreDataSupplier) { return // 一般为 true config.isAutoRead() && // respectMaybeMoreData 默认为 true // maybeMoreDataSupplier 的逻辑是如果预期读取字节与实际读取字节相等，返回 true (!respectMaybeMoreData || maybeMoreDataSupplier.get()) && // 小于最大次数，maxMessagePerRead 默认 16 totalMessages 0; } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 09:20:53 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/":{"url":"distributed/dubbo/","title":"dubbo远程调用","keywords":"","body":"dubbo远程调用 1.从0到1整体认知分布式系统 2.快速掌握Dubbo常规应用 3.Dubbo企业级应用进阶 2.Dubb调用模块详解 3.Dubbo协议模块源码剖析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:51:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-introduce.html":{"url":"distributed/dubbo/dubbo-introduce.html","title":"1.从0到1整体认知分布式系统","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、分布式架构的发展历史与背景 架构的发展历史： 分布式架构所带来的成本与风险: 二、如何选型分布式架构 基于反向代理的集中式分布式架构 嵌入应用内部的去中心化架构 基于独立代理进程的架构(Service Mesh) 三种架构的比较 **三、Dubbo 架构与设计说明 ** dubbo架构简要讲解 Dubbo 整体设计 Dubbo 中的SPI机制 概要： 分布式架构的发展历史与背景 如何着手架构一套分布示式系统 Dubbo 架构与设计说明 一、分布式架构的发展历史与背景 场景一： 一家做政务OA系统的公司老板发现跟竞争对手比发现自己的系统的架构不是分布示的，找到技术负责人问，把系统架构升级成分布示架构要多长时间？技术负责人网上查了查 dubbo官网看了看 Demo 这不很简单吗，拍着胸脯一个月能升级好。 现在我的问题是：这位技术理在改造过程中可能会遇到什么风险和问题？ 新功能和旧BUG的问题 业务完整性的问题 团队协作方式转变 开发人员技能提升 系统交付方式转变 这些问题解决涉及业务部门及整个技术部门（开发、测试、运维）协商与工作标准的制定。业务相关问题暂不做讨论,技术架构上应该要清楚自己的职责是，如何通过技术手段把业务波动降至最低、开发成本最低、实施风险最低？ 架构的发展历史： 单体式架构： 垂直架构: 分布示架构： 分布式架构所带来的成本与风险: 分布式事物： 分布式事物是指一个操作，分成几个小操作在多个服务器上执行，要么多成功，要么多失败这些分布事物要做的 不允许服务有状态（**stateless service**） 无状态服务是指对单次请求的处理，不依赖其他请求，也就是说，处理一次请求所需的全部信息，要么都包含在这个请求里，要么可以从外部获取到（比如说数据库），服务器本身不存储任何信息。 服务依懒关系复杂 服务 A --> B--> C 那和服务C 的修改 就可能会影响 B 和C，事实上当服务越来 越多的时候，C的变动将会越来越困难。 部署运维成本增加 不用说了，相比之前几个节点，运维成本的增加必须的。 源码管理成本增加： 原本一套或几套源码现在拆分成几十个源码库，其中分支、tag都要进行相应管理。 如何保证系统的伸缩性： 伸缩性是指,当前服务器硬件升级后或新增服务器处理能力就能相对应的提升。 分布式会话： 此仅针对应用层服务，不能将Session 存储在一个服务器上。 分布式JOB 通常定时任务只需要在一台机器上触发执行，分布式的情况下在哪台执行呢？ 最后通过一张图直观感受一下 单体到分布式的区别： 二、如何选型分布式架构 提问：实现一个分布示框架最核心功能是什么? RPC远程调用技术： 大家知道的 有哪些远程调用的 方式？拿几个大家比较熟悉的来举例：RMI 、Web Service、Http | 协议 | 描述 | 优点 | 缺点 | |:----|:----|:----|:----| | RMI | JAVA 远程方法调用、使用原生二进制方式进行序列化 | 简单易用、SDK支持，提高开发效率 | 不支持跨语言 | | Web Service | 比较早系统调用解决方案 ，跨语言, 其基于WSDL 生成 SOAP 进行消息的传递。 | SDK支持、跨语言 | 实现较重，发布繁琐 | | Http | 采用htpp +json 实现 | 简单、轻量、跨语言 | 不支持SDK | 基于比较上述比较，大家会选择哪个方案，综合考虑 RMI是比较合适的方案，基本没有学习成本。而跨语言问题基本可以勿略。 如果服务端不是单个的话，这个方案差点我就用了。实际上服务端是多个的 ，好了新的问题又来了。 负载均衡：这么多个机器调用哪一台? 服务发现：样发现新的服务地址呢？ 健康检测：服务关宕机或恢复后怎么办？ 容错：如果调用其中一台调用出错了怎么办？ 这些功能怎么解决呢？一个一个的去编码实现么？。有没有现成的方案可以直接借鉴呢？ 分布式架构的三种解决方案： 基于反向代理的中心化架构 嵌入应用内部的去中心化架构 基于独立代理进程的Service Mesh架构基于反向代理的集中式分布式架构 这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。 Http+Nginx 方案总结： 优点：简单快速、几乎没有学习成本 适用场景：轻量级分布式系统、局部分布式架构。 瓶颈：Nginx中心负载、Http传输、JSON序列化、开发效率、运维效率。 嵌入应用内部的去中心化架构 这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。我们所熟悉的 duboo 和spring cloud Eureka +Ribbon/'rɪbən/ 都是这种方式实现。 相比第一代架构它有以下特点几点： 去中心化，客户端直连服务端 动态注册和发现服务 高效稳定的网络传输 高效可容错的序列化基于独立代理进程的架构(Service Mesh) 这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同第二代架构。 三种架构的比较 模式 优点 缺点 适应场景 案例 集中式负载架构 简单 集中式治理 与语言无关 配置维护成本高 多了一层IO 单点问题 大部分公司都适用，对运维有要求 亿贝、携程、早期互联网公司 客户端嵌入式架构 无单点 性能更好 客户端复杂 语言栈要求 中大规模公司、语言栈统一 Dubbo 、 Twitter finagle、 Spring Cloud Ribbon 独立进程代理架构 无单点 性能更好 与语言无关 运维部署复杂 开发联调复杂 中大规模公司 对运维有要求 Smart Stack Service Mesh 三、Dubbo 架构与设计说明 dubbo架构简要讲解 架构图 流程说明： Provider(提供者)绑定指定端口并启动服务 指供者连接注册中心，并发本机IP、端口、应用信息和提供服务信息发送至注册中心存储 Consumer(消费者），连接注册中心 ，并发送应用信息、所求服务信息至注册中心 注册中心根据 消费 者所求服务信息匹配对应的提供者列表发送至Consumer 应用缓存。 Consumer 在发起远程调用时基于缓存的消费者列表择其一发起调用。 Provider 状态变更会实时通知注册中心、在由注册中心实时推送至Consumer 这么设计的意义： Consumer 与Provider 解偶，双方都可以横向增减节点数。 注册中心对本身可做对等集群，可动态增减节点，并且任意一台宕掉后，将自动切换到另一台 去中心化，双方不直接依懒注册中心，即使注册中心全部宕机短时间内也不会影响服务的调用 服务提供者无状态，任意一台宕掉后，不影响使用 Dubbo 整体设计 config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成动态代理 扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 其协作流程如下： Dubbo 中的SPI机制 在了解Dubbo的spi之前 先来了解一下 JAVA自带的SPI java spi的具体约定为:当服务的提供者，提供了服务接口的一种实现之后，在jar包的META-INF/services/目录里同时创建一个以服务接口命名的文件。该文件里就是实现该服务接口的具体实现类。而当外部程序装配这个模块的时候，就能通过该jar包META-INF/services/里的配置文件找到具体的实现类名，并装载实例化，完成模块的注入。 基于这样一个约定就能很好的找到服务接口的实现类，而不需要再代码里制定。jdk提供服务实现查找的一个工具类java.util.ServiceLoader 演示JAVA SPI机制 [ ] 编写接口 [ ] 编写实现类 [ ] 编辑META-INF/services/xxx 文件 [ ] 演示spi 实现 spi 目录文件： META-INF/services/tuling.dubbo.server.UserService 中的值： tuling.dubbo.server.impl.UserServiceImpl2 装载获取SPI实现类： public static void main(String[] args) { Iterator services = ServiceLoader.load(UserService.class).iterator(); UserService service = null; while (services.hasNext()) { service = services.next(); } System.out.println(service.getUser(111)); } Dubbo的SPI机制： dubbo spi 在JAVA自带的SPI基础上加入了扩展点的功能，即每个实现类都会对应至一个扩展点名称，其目的是 应用可基于此名称进行相应的装配。 演示Dubbo SPI机制： [ ] 编写Filter 过滤器 [ ] 编写 dubbo spi 配置文件 [ ] 装配自定义Filter dubbo spi 目录文件 dubbo spi 文件内容： luban=tuling.dubbo.server.LubanFilter 装配自定义Filter Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-base-use.html":{"url":"distributed/dubbo/dubbo-base-use.html","title":"2.快速掌握Dubbo常规应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Dubbo 快速入门 Dubbo核心功能解释 快速演示Dubbo的远程调用 基于Dubbo实现服务集群： 二、Dubbo常规配置说明 Dubbo配置的整体说明： dubbo 配置的一些套路: 一般建议配置示例： Copyright &copy ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 概要： Dubbo 快速入门 Dubbo 常规配置说明一、Dubbo 快速入门 Dubbo核心功能解释 dubbo 阿里开源的一个SOA服务治理框架，从目前来看把它称作是一个RPC远程调用框架更为贴切。单从RPC框架来说，功能较完善，支持多种传输和序列化方案。所以想必大家已经知道他的核心功能了：就是远程调用。 快速演示Dubbo的远程调用 实现步骤 [ ] 创建服务端项目 [ ] 引入dubbo 依赖 [ ] 编写服务端代码 [ ] 创建客户端项目 [ ] 引入dubbo 依赖 [ ] 编写客户端调用代码 dubbo 引入： com.alibaba dubbo 2.6.2 dubbo 默认依懒： 客户端代码： static String remoteUrl = \"dubbo://127.0.0.1:12345/tuling.dubbo.server.UserService\"; // 构建远程服务对象 public UserService buildRemoteService(String remoteUrl) { ApplicationConfig application = new ApplicationConfig(); application.setName(\"young-app\"); ReferenceConfig referenceConfig = new ReferenceConfig<>(); referenceConfig.setApplication(application); referenceConfig.setInterface(UserService.class); referenceConfig.setUrl(remoteUrl); UserService userService = referenceConfig.get(); return userService; } 服务端代码： public void openServer(int port) { ApplicationConfig config = new ApplicationConfig(); config.setName(\"simple-app\"); ProtocolConfig protocolConfig=new ProtocolConfig(); protocolConfig.setName(\"dubbo\"); protocolConfig.setPort(port); protocolConfig.setThreads(20); ServiceConfig serviceConfig=new ServiceConfig(); serviceConfig.setApplication(config); serviceConfig.setProtocol(protocolConfig); serviceConfig.setRegistry(new RegistryConfig(RegistryConfig.NO_AVAILABLE)); serviceConfig.setInterface(UserService.class); serviceConfig.setRef(new UserServiceImpl()); serviceConfig.export(); } 基于Dubbo实现服务集群： 在上一个例子中如多个服务的集群？即当有多个服务同时提供的时候，客户端该调用哪个？以什么方式进行调用以实现负载均衡？ 一个简单的办法是将多个服务的URL同时设置到客户端并初始化对应的服务实例，然后以轮询的方式进行调用。 但如果访问增大，需要扩容服务器数量，那么就必须增加配置重启客户端实例。显然这不是我们愿意看到的。Dubbo引入了服务注册中的概念，可以解决动态扩容的问题。 演示基于注册中心实现服集群： [ ] 修改服务端代码，添加multicast 注册中心。 [ ] 修改客户端代码，添加multicast 注册中心。 [ ] 观察 多个服务时，客户端如何调用。 [ ] 观察 动态增减服务，客户端的调用。 # 服务端连接注册中心 serviceConfig.setRegistry(new RegistryConfig(\"multicast://224.1.1.1:2222\")); # 客户端连接注册中心 referenceConfig.setRegistry(new RegistryConfig(\"multicast://224.1.1.1:2222\")); #查看 基于UDP 占用的2222 端口 netstat -ano|findstr 2222 基于spring IOC维护Dubbo 实例 在前面两个例子中 出现了,ApplicationConfig、ReferenceConfig、RegistryConfig、com.alibaba.dubbo.config.ServiceConfig等实例 ，很显然不需要每次调用的时候都去创建该实例那就需要一个IOC 容器去管理这些实例，spring 是一个很好的选择。 提供者配置---------------------------------- 提供者服务暴露代码： ApplicationContext context = new ClassPathXmlApplicationContext(\"/spring-provide.xml\"); ((ClassPathXmlApplicationContext) context).start(); System.in.read(); 消费者配置--------------------------------------- 消费者调用代码： ApplicationContext context = new ClassPathXmlApplicationContext(\"/spring-consumer.xml\"); UserService userService = context.getBean(UserService.class); UserVo u = userService.getUser(1111); System.out.println(u); 二、Dubbo常规配置说明 Dubbo配置的整体说明： 标签 用途 解释 公共 用于配置当前应用信息，不管该应用是提供者还是消费者 公共 用于配置连接注册中心相关信息 服务 用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受 服务 用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心 服务 当 ProtocolConfig 和 ServiceConfig 某属性没有配置时，采用此缺省值，可选 引用 当 ReferenceConfig 某属性没有配置时，采用此缺省值，可选 引用 用于创建一个远程服务代理，一个引用可以指向多个注册中心 公共 用于 ServiceConfig 和 ReferenceConfig 指定方法级的配置信息 公共 用于指定方法参数配置 配置关系图： 配置分类 所有配置项分为三大类。 服务发现：表示该配置项用于服务的注册与发现，目的是让消费方找到提供方。 服务治理：表示该配置项用于治理服务间的关系，或为开发测试提供便利条件。 性能调优：表示该配置项用于调优性能，不同的选项对性能会产生影响。dubbo 配置的一些套路: 先来看一个简单配置 通过字面了解 timeout即服务的执行超时时间。但当服务执行真正超时的时候 报的错跟timeout并没有半毛钱的关系，其异常堆栈如下： 可以看到错误表达的意思是 因为Channel 关闭导致 无法返回 Response 消息。 出现这情况的原因在于 虽然timeout 配置在服务端去是用在客户端，其表示的是客户端调用超时间，而非服务端方法的执行超时。当我们去看客户端的日志时候就能看到timeout异常了 类似这种配在服务端用在客户端的配置还有很多，如retries/riː'traɪ/(重试次数)、async/əˈsɪŋk/（是否异步）、loadbalance(负载均衡)。。。等。 套路一：*服务端配置客户端来使用*。 注：其参数传递机制是 服务端所有配置都会封装到URL参数，在通过注册中心传递到客户端 如果需要暴露多个服务的时候，每个服务都要设置其超时时间，貌似有点繁琐。Dubbo中可以通过 来实现服务端缺省配置。它可以同时为 和 两个标签提供缺省配置。如： #相当于每个服务提供者设置了超时时间 和重试次数 同样客户端也有缺省配置标签：，这些缺省设置可以配置多个 通过 ,如果没指定就用第一个。 、 套路二：与 ，与傻傻分不清楚 在服务端配置timeout 之后 所有客户端都会采用该方超时时间，其客户端可以自定义超时时间吗？通过 可以设定或者在 也可以设定 甚至可以设定到方法级别 。加上服务端的配置，超时总共有6处可以配置。如果6处都配置了不同的值，最后肯定只会有一个超时值生效，其优先级如下： 小提示：通过DefaultFuture的get 方法就可观测到实际的超时设置。 com.alibaba.dubbo.remoting.exchange.support.DefaultFuture 套路三：同一属性到处配置，优先级要小心。 一般建议配置示例： 提供端：--------------------------- 消费端示例：-------------------- Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-high-use.html":{"url":"distributed/dubbo/dubbo-high-use.html","title":"3.Dubbo企业级应用进阶","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、分布式项目开发与联调 接口暴露与引用 自动化构建与协作 接口平滑升级： 开发联调： 二、Dubbo控制管理后台使用 Dubbo 控制后台版本说明： Dubbo 控制后台的安装： 三、Dubbo注册中心详解 注册中心的作用 Dubbo所支持的注册中心 Redis 注册中心 Zookeeper 注册中心 课程概要： 分布式项目开发与联调 控制管理后台使用 Dubbo注册中心详解 一、分布式项目开发与联调 接口暴露与引用 在一个RPC场景中 ，调用方是通过接口来调用服务端，传入参数并获得返回结果。这样服务端的接口和模型必须暴露给调用方项目。服务端如何暴露呢？客户端如何引用呢？ 接口信息 、模型信息 、异常 暴露接口的通常做法是 接口与实现分离，服务端将 接口、模型、异常 等统一放置于一个模块，实现置于另一个模块。调用方通过Maven进行引用。 自动化构建与协作 当项目越来越多，服务依懒关系越发复杂的时候，为了提高协作效率，必须采用自动化工具 完成 接口从编写到构建成JAR包，最后到引用的整个过程。 流程描述： 服务提供者项目发人员编写Client 接口 push 至远程仓库 jenkins 构建指定版本 jenkins Deploye 至私服仓库 nexus 服务消费者项目开发人员基于maven 从私服务仓库下载接口平滑升级： 在项目迭代过程当中， 经常会有多个项目依懒同一个接口，如下图 项目B、C都依懒了项目A当中的接口1，此时项目B业务需要，需要接口1多增加一个参数，升级完成后。项目B能正确构建上线，项目C却不行。 解决办法与原则： 接口要做到向下兼容：接口参数尽量以对象形式进行封装。Model属性只增不删，如果需要作废，可以添加@Deprecated 标识。 如果出现了不可兼容的变更，则必须通知调用方整改，并制定上线计划。 开发联调： 在项目开发过程当中，一个开发或测试环境的注册中心很有可能会同时承载着多个服务，如果两组服务正在联调，如何保证调用的是目标服务呢？ 1、基于临时分组联调 group 分组 在reference 和server 当中采用相同的临时组 ,通过group 进行设置 2、直连提供者： 在reference 中指定提供者的url即可做到直连 3、只注册： 一个项目有可能同是为即是服务提供者又消费者，在测试时需要调用某一服务同时又不希望正在开发的服务影响到其它订阅者如何实现？ 通过修改 register=false 即可实现 二、Dubbo控制管理后台使用 Dubbo 控制后台版本说明： dubbo 在2.6.0 以前 使用dubbo-admin 作为管理后台，2.6 以后已经去掉dubbo-admin 并采用 incubator-dubbo-ops 作为新的管理后台，目前该后台还在开发中还没有发布正式的版本 ，所以本节课还是采用的旧版的dubbo-admin 来演示。 Dubbo 控制后台的安装： #从github 中下载dubbo 项目 git clone https://github.com/apache/incubator-dubbo.git #更新项目 git fetch #临时切换至 dubbo-2.5.8 版本 git checkout dubbo-2.5.8 #进入 dubbo-admin 目录 cd dubbo-admin #mvn 构建admin war 包 mvn clean pakcage -DskipTests #得到 dubbo-admin-2.5.8.war 即可直接部署至Tomcat #修改 dubbo.properties 配置文件 dubbo.registry.address=zookeeper://127.0.0.1:2181 注：如果实在懒的构建 可直接下载已构建好的： 链接：https://pan.baidu.com/s/1zJFNPgwNVgZZ-xobAfi5eQ 提取码：gjtv 控制后台基本功能介绍 ： 服务查找： 服务关系查看: 服务权重调配： 服务路由： 服务禁用 三、Dubbo注册中心详解 注册中心的作用 为了到达服务集群动态扩容的目的，注册中心存储了服务的地址信息与可用状态信息，并实时推送给订阅了相关服务的客户端。 一个完整的注册中心需要实现以下功能： 接收服务端的注册与客户端的引用，即将引用与消费建立关联，并支持多对多。 当服务非正常关闭时能即时清除其状态 当注册中心重启时，能自动恢复注册数据，以及订阅请求 注册中心本身的集群 Dubbo所支持的注册中心 Multicast 注册中心 基于组网广播技术，只能用在局域网内，一般用于简单的测试服务 Zookeeper 注册中心(**推荐**) Zookeeper 是 Apacahe Hadoop 的子项目，是一个树型的目录服务，支持变更推送，适合作为 Dubbo 服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 Redis 注册中心 基于Redis的注册中心 Simple 注册中心 基于本身的Dubbo服务实现（SimpleRegistryService），不支持集群可作为自定义注册中心的参考，但不适合直接用于生产环境。 Redis 注册中心 关于Redis注册中心我们需要了解两点， 如何存储服务的注册与订阅关系 是当服务状态改变时如何即时更新 演示使用Redis 做为注册中心的使用。 [ ] 启动Redis服务 [ ] 服务端配置注册中心 [ ] 启动两个服务端 [ ] 通过RedisClient 客户端观察Redis中的数据 redis 注册中心配置： 当我们启动两个服务端后发现，Reids中增加了一个Hash 类型的记录，其key为/dubbo/tuling.dubbo.server.UserService/providers。Value中分别存储了两个服务提供者的URL和有效期。 同样消费者也是类似其整体结构如下： //服务提供者注册信息 /dubbbo/com.tuling.teach.service.DemoService/providers dubbo://192.168.246.1:20880/XXX.DemoService=1542619052964 dubbo://192.168.246.2:20880/XXX.DemoService=1542619052964 //服务消费订阅信息 /dubbbo/com.tuling.teach.service.DemoService/consumers dubbo://192.168.246.1:20880/XXX.DemoService=1542619788641 主 Key 为服务名和类型 Map 中的 Key 为 URL 地址 Map 中的 Value 为过期时间，用于判断脏数据，脏数据由监控中心删除 接下来回答第二个问题 当提供者突然 宕机状态能即里变更吗？ 这里Dubbo采用的是定时心跳的机制 来维护服务URL的有效期，默认每30秒更新一次有效期。即URL对应的毫秒值。具体代码参见：com.alibaba.dubbo.registry.redis.RedisRegistry#expireExecutor com.alibaba.dubbo.registry.redis.RedisRegistry#deferExpired com.alibaba.dubbo.registry.integration.RegistryDirectory com.alibaba.dubbo.registry.support.ProviderConsumerRegTable Zookeeper 注册中心 关于Zookeeper 注册中心同样需要了解其存储结构和更新机制。 Zookeper是一个树型的目录服务，本身支持变更推送相比redis的实现Publish/Subscribe功能更稳定。 结构： 失败重连 com.alibaba.dubbo.registry.support.FailbackRegistry 提供者突然断开： 基于Zookeeper 临时节点机制实现，在客户端会话超时后 Zookeeper会自动删除所有临时节点，默认为40秒。 // 创建临时节点 com.alibaba.dubbo.remoting.zookeeper.curator.CuratorZookeeperClient#createEphemeral 提问： 在zookeeper 断开的40秒内 如果 有客户端加入 会调用 已失效的提供者连接吗？ 答：不会，提供者宕机后 ，其与客户端的链接也随即断开，客户端在调用前会检测长连接状态。 // 检测连接是否有效 com.alibaba.dubbo.rpc.protocol.dubbo.DubboInvoker#isAvailable 创建 configurators与routers 会创建持久节点 // 创建持久节点 com.alibaba.dubbo.remoting.zookeeper.curator.CuratorZookeeperClient#createPersistent 服务订阅机制实现： // 注册目录 com.alibaba.dubbo.registry.integration.RegistryDirectory 源码解析： com.alibaba.dubbo.registry.integration.RegistryDirectory Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-module-detail.html":{"url":"distributed/dubbo/dubbo-module-detail.html","title":"2.Dubb调用模块详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Dubbo 调用模块基本组成 Dubbo调用模块概述： 透明代理： 负载均衡 TODO 一至性hash 演示 容错 异步调用 过滤器 TODO 演示添加日志访问过滤: 二 、Dubbo 调用非典型使用场景 泛化提供&引用 TODO 示例演示 隐示传参 令牌验证 三、调用通信内部实现源码分析 网络传输的实现组成 Dubbo 长连接实现与配置 dubbo传输uml类图: Dubbo 传输协作线程 概要： 一、Dubbo 调用模块基本组成 二 、Dubbo 调用非典型使用场景 三、调用通信内部实现源码分析 一、Dubbo 调用模块基本组成 Dubbo调用模块概述： dubbo调用模块核心功能是发起一个远程方法的调用并顺利拿到返回结果，其体系组成如下： 透明代理：通过动态代理技术，屏蔽远程调用细节以提高编程友好性。 负载均衡：当有多个提供者是，如何选择哪个进行调用的负载算法。 容错机制：当服务调用失败时采取的策略 调用方式：支持同步调用、异步调用 透明代理： 参见源码： com.alibaba.dubbo.config.ReferenceConfig#createProxy com.alibaba.dubbo.common.bytecode.ClassGenerator com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory 负载均衡 Dubbo 目前官方支持以下负载均衡策略： 随机(random)：按权重设置随机概率。此为默认算法. 轮循 (roundrobin):按公约后的权重设置轮循比率。 最少活跃调用数(leastactive):相同活跃数的随机，活跃数指调用前后计数差。 一致性Hash(consistenthash ):相同的参数总是发到同一台机器 设置方式支持如下四种方式设置，优先级由低至高 TODO 一至性hash 演示 [ ] 配置loadbalance [ ] 配置需要hash 的参数与虚拟节点数 [ ] 发起远程调用 一至性hash 算法详解： 容错 Dubbo 官方目前支持以下容错策略： 失败自动切换：调用失败后基于retries=“2” 属性重试其它服务器 快速失败：快速失败，只发起一次调用，失败立即报错。 勿略失败：失败后勿略，不抛出异常给客户端。 失败重试：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作 并行调用: 只要一个成功即返回，并行调用指定数量机器，可通过 forks=\"2\" 来设置最大并行数。 广播调用：广播调用所有提供者，逐个调用，任意一台报错则报错 设置方式支持如下两种方式设置，优先级由低至高 注：容错机制 在基于 API设置时无效 如 referenceConfig.setCluster(\"failback\"); 经测试不启作用 异步调用 异步调用是指发起远程调用之后获取结果的方式。 同步等待结果返回（默认） 异步等待结果返回 不需要返回结果 Dubbo 中关于异步等待结果返回的实现流程如下图： 异步调用配置: 注：在进行异步调用时 容错机制不能为 cluster=\"forking\" 或 cluster=\"broadcast\" 异步获取结果演示： [ ] 编写异步调用代码 [ ] 编写同步调用代码 [ ] 分别演示同步调用与异步调用耗时 异步调用结果获取Demo demoService.sayHello1(\"han\"); Future future1 = RpcContext.getContext().getFuture(); demoService.sayHello2(\"han2\"); Future future2 = RpcContext.getContext().getFuture(); Object r1 = null, r2 = null; // wait 直到拿到结果 获超时 r1 = future1.get(); // wait 直到拿到结果 获超时 r2 = future2.get(); 过滤器 类似于 WEB 中的Filter ，Dubbo本身提供了Filter 功能用于拦截远程方法的调用。其支持自定义过滤器与官方的过滤器使用： TODO 演示添加日志访问过滤: 以上配置 就是 为 服务提供者 添加 日志记录过滤器， 所有访问日志将会集中打印至 accesslog 当中 二 、Dubbo 调用非典型使用场景 泛化提供&引用 泛化提供 是指不通过接口的方式直接将服务暴露出去。通常用于Mock框架或服务降级框架实现。 TODO 示例演示 public static void doExportGenericService() { ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName(\"demo-provider\"); // 注册中心 RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setProtocol(\"zookeeper\"); registryConfig.setAddress(\"192.168.0.147:2181\"); ProtocolConfig protocol=new ProtocolConfig(); protocol.setPort(-1); protocol.setName(\"dubbo\"); GenericService demoService = new MyGenericService(); ServiceConfig service = new ServiceConfig(); // 弱类型接口名 service.setInterface(\"com.tuling.teach.service.DemoService\"); // 指向一个通用服务实现 service.setRef(demoService); service.setApplication(applicationConfig); service.setRegistry(registryConfig); service.setProtocol(protocol); // 暴露及注册服务 service.export(); } 泛化引用 是指不通过常规接口的方式去引用服务，通常用于测试框架。 ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName(\"demo-provider\"); // 注册中心 RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setProtocol(\"zookeeper\"); registryConfig.setAddress(\"192.168.0.147:2181\"); // 引用远程服务 ReferenceConfig reference = new ReferenceConfig(); // 弱类型接口名 reference.setInterface(\"com.tuling.teach.service.DemoService\"); // 声明为泛化接口 reference.setGeneric(true); reference.setApplication(applicationConfig); reference.setRegistry(registryConfig); // 用com.alibaba.dubbo.rpc.service.GenericService可以替代所有接口引用 GenericService genericService = reference.get(); Object result = genericService.$invoke(\"sayHello\", new String[]{\"java.lang.String\"}, new Object[]{\"world\"}); 隐示传参 是指通过非常方法参数传递参数，类似于http 调用当中添加cookie值。通常用于分布式追踪框架的实现。使用方式如下 ： //客户端隐示设置值 RpcContext.getContext().setAttachment(\"index\", \"1\"); // 隐式传参，后面的远程调用都会隐 //服务端隐示获取值 String index = RpcContext.getContext().getAttachment(\"index\"); 令牌验证 通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者，另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者 使用： 三、调用通信内部实现源码分析 网络传输的实现组成 IO模型： BIO 同步阻塞 NIO 同步非阻塞 AIO 异步非阻塞 连接模型： 长连接 短连接 线程分类： IO线程 服务端业务线程 客户端调度线程 客户端结果exchange线程。 保活心跳线程 重连线程 线程池模型： 固定数量线程池 缓存线程池 有限线程池Dubbo 长连接实现与配置 初始连接： 引用服务增加提供者==>获取连接===》是否获取共享连接==>创建连接客户端==》开启心跳检测状态检查定时任务===》开启连接状态检测 源码见：com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol#getClients 心跳发送： 在创建一个连接客户端同时也会创建一个心跳客户端，客户端默认基于60秒发送一次心跳来保持连接的存活，可通过 heartbeat 设置。 源码见：com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeClient#startHeatbeatTimer 断线重连： 每创建一个客户端连接都会启动一个定时任务每两秒中检测一次当前连接状态，如果断线则自动重连。 源码见：com.alibaba.dubbo.remoting.transport.AbstractClient#initConnectStatusCheckCommand 连接销毁: 基于注册中心通知，服务端断开后销毁 源码见：com.alibaba.dubbo.remoting.transport.AbstractClient#close() dubbo传输uml类图: Dubbo 传输协作线程 客户端调度线程：用于发起远程方法调用的线程。 客户端结果**Exchange**线程：当远程方法返回response后由该线程填充至指定ResponseFuture，并叫醒等待的调度线程。 客户端IO线程：由传输框架实现，用于request 消息流发送、response 消息流读取与解码等操作。 服务端IO线程：由传输框架实现，用于request消息流读取与解码 与Response发送。 业务执行线程：服务端具体执行业务方法的线程 客户端线程协作流程： 调度线程 调用远程方法 对request 进行协议编码 发送request 消息至IO线程 等待结果的获取 IO线程 读取response流 response 解码 提交Exchange 任务 Exchange线程 填写response值 至 ResponseFuture 唤醒调度线程，通知其获取结果 调用调试： 客户端的执行线程: 1、业务线程 1) DubboInvoker#doInvoke(隐示传公共参数、获取客户端、异步、单向、同步（等待返回结果）) 2)AbstractPeer#send// netty Client客户端发送消息 写入管道 3)DubboCodec#encodeRequestData // Request 协议编码 2、IO线程 DubboCodec#decodeBody //Response解码 AllChannelHandler#received //// 派发消息处理线程 3、调度线程 DefaultFuture#doReceived // 设置返回结果 服务端线程协作： IO线程： request 流读取 request 解码 提交业务处理任务 业务线程： 业务方法执行 response 编码 回写结果至channel 线程池 fixed：固定线程池,此线程池启动时即创建固定大小的线程数，不做任何伸缩， cached：缓存线程池,此线程池可伸缩，线程空闲一分钟后回收，新请求重新创建线程 Limited：有限线程池,此线程池一直增长，直到上限，增长后不收缩。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-proto-detail.html":{"url":"distributed/dubbo/dubbo-proto-detail.html","title":"3.Dubbo协议模块源码剖析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 RPC协议基本组成 RPC 协议名词解释 协议基本组成： Dubbo中所支持RPC协议使用 协议的使用与配置: TODO 演示采用其它协议来配置Dubbo Hessian 序列化： 三 、RPC协议报文编码与实现详解 RPC 传输实现： 拆包与粘包产生的原因： 拆包与粘包解决办法： Dubbo 协议报文编码： Dubbo协议的编解码过程： 主讲：鲁班 时间：2018/12/2 8:10 地址：腾讯课堂图灵学院 课程概要： RPC协议基本组成 RPC协议报文编码与实现详解 Dubbo中所支持RPC协议与使用 RPC协议基本组成 RPC 协议名词解释 在一个典型RPC的使用场景中，包含了服务发现、负载、容错、网络传输、序列化等组件，其中RPC协议就指明了程序如何进行网络传输和序列化 。也就是说一个RPC协议的实现就等于一个非透明的远程调用实现，如何做到的的呢？ 协议基本组成： 地址：服务提供者地址 端口：协议指定开放的端口 报文编码：协议报文编码 ，分为请求头和请求体两部分。 序列化方式：将请求体序列化成对象 Hessian2Serialization、 DubboSerialization、 JavaSerialization JsonSerialization 运行服务: 网络传输实现 netty mina RMI 服务 servlet 容器（jetty、Tomcat、Jboss） Dubbo中所支持RPC协议使用 dubbo 支持的RPC协议列表 | 名称 | 实现描述 | 连接描述 | 适用场景 | |:----|:----|:----|:----| | dubbo | 传输服务: mina, netty(默认), grizzy序列化: hessian2(默认), java, fastjson自定义报文 | 单个长连接NIO异步传输 | 1、常规RPC调用2、传输数据量小3、提供者少于消费者 | | rmi | 传输：java rmi 服务序列化：java原生二进制序列化 | 多个短连接BIO同步传输 | 1、常规RPC调用2、与原RMI客户端集成3、可传少量文件4、不支持防火墙穿透 | | hessian | 传输服务：servlet容器序列化：hessian二进制序列化 | 基于Http 协议传输，依懒servlet容器配置 | 1、提供者多于消费者2、可传大字段和文件3、跨语言调用 | | http | 传输服务：servlet容器序列化：java原生二进制序列化 | 依懒servlet容器配置 | 1、数据包大小混合 | | thrift | 与thrift RPC 实现集成，并在其基础上修改了报文头 | 长连接、NIO异步传输 | | 关于RMI不支持防火墙穿透的补充说明： 原因在于RMI 底层实现中会有两个端口，一个是固定的用于服务发现的注册端口，另外会生成一个随机端口用于网络传输。因为这个随机端口就不能在防火墙中提前设置开放开。所以存在防火墙穿透问题 协议的使用与配置: Dubbo框架配置协议非常方便，用户只需要在 provider 应用中 配置 元素即可。 TODO 演示采用其它协议来配置Dubbo [ ] dubbo 协议采用 json 进行序列化 (源码参见：com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol) [ ] 采用RMI协议 (源码参见：com.alibaba.dubbo.rpc.protocol.rmi.RmiProtocol) [ ] 采用Http协议 (源码参见：com.alibaba.dubbo.rpc.protocol.http.HttpProtocol.InternalHandler) [ ] 采用Heason协议 (源码参见:com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol.HessianHandler) netstat -aon|findstr \"17732\" 序列化： | | 特点 | |:----|:----| | fastjson | 文本型：体积较大，性能慢、跨语言、可读性高 | | fst | 二进制型：体积小、兼容 JDK 原生的序列化。要求 JDK 1.7 支持。 | | hessian2 | 二进制型：跨语言、容错性高、体积小 | | java | 二进制型：在JAVA原生的基础上 可以写入Null | | compactedjava | 二进制型：与java 类似，内容做了压缩 | | nativejava | 二进制型：原生的JAVA 序列化 | | kryo | 二进制型：体积比hessian2 还要小，但容错性 没有hessian2 好 | Hessian 序列化： 参数及返回值需实现 Serializable 接口 参数及返回值不能自定义实现 List, Map, Number, Date, Calendar 等接口，只能用 JDK 自带的实现，因为 hessian 会做特殊处理，自定义实现类中的属性值都会丢失。 Hessian 序列化，只传成员属性值和值的类型，不传方法或静态变量，兼容情况 [1][2]： | 数据通讯 | 情况 | 结果 | |:----|:----|:----| | A->B | 类A多一种 属性（或者说类B少一种 属性） | 不抛异常，A多的那 个属性的值，B没有， 其他正常 | | A->B | 枚举A多一种 枚举（或者说B少一种 枚举），A使用多 出来的枚举进行传输 | 抛异常 | | A->B | 枚举A多一种 枚举（或者说B少一种 枚举），A不使用 多出来的枚举进行传输 | 不抛异常，B正常接 收数据 | | A->B | A和B的属性 名相同，但类型不相同 | 抛异常 | | A->B | serialId 不相同 | 正常传输 | 接口增加方法，对客户端无影响，如果该方法不是客户端需要的，客户端不需要重新部署。输入参数和结果集中增加属性，对客户端无影响，如果客户端并不需要新属性，不用重新部署。 输入参数和结果集属性名变化，对客户端序列化无影响，但是如果客户端不重新部署，不管输入还是输出，属性名变化的属性值是获取不到的。 总结：服务器端和客户端对领域对象并不需要完全一致，而是按照最大匹配原则。 [ ] 演示Hession2 序列化的容错性 三 、RPC协议报文编码与实现详解 RPC 传输实现： RPC的协议的传输是基于 TCP/IP 做为基础使用Socket 或Netty、mina等网络编程组件实现。但有个问题是TCP是面向字节流的无边边界协议，其只管负责数据传输并不会区分每次请求所对应的消息，这样就会出现TCP协义传输当中的拆包与粘包问题 拆包与粘包产生的原因： 我们知道tcp是以流动的方式传输数据，传输的最小单位为一个报文段（segment）。tcp Header中有个Options标识位，常见的标识为mss(Maximum Segment Size)指的是，连接层每次传输的数据有个最大限制MTU(Maximum Transmission Unit)，一般是1500比特，超过这个量要分成多个报文段，mss则是这个最大限制减去TCP的header，光是要传输的数据的大小，一般为1460比特。换算成字节，也就是180多字节。 tcp为提高性能，发送端会将需要发送的数据发送到缓冲区，等待缓冲区满了之后，再将缓冲中的数据发送到接收方。同理，接收方也有缓冲区这样的机制，来接收数据。这时就会出现以下情况： 应用程序写入的数据大于MSS大小，这将会发生拆包。 应用程序写入数据小于MSS大小，这将会发生粘包。 接收方法不及时读取套接字缓冲区数据，这将发生粘包。拆包与粘包解决办法： 设置定长消息，服务端每次读取既定长度的内容作为一条完整消息。 {\"type\":\"message\",\"content\":\"hello\"}\\n 使用带消息头的协议、消息头存储消息开始标识及消息长度信息，服务端获取消息头的时候解析出消息长度，然后向后读取该长度的内容。 比如：Http协议 heade 中的 Content-Length 就表示消息体的大小。 (注①：http 报文编码) Dubbo 协议报文编码： 注②Dubbo 协议报文编码： | | 0-7 | 8-15 | 16-20 | 21 | 22 | 23 | 24-31 | | |:----|:----|:----|:----|:----|:----|:----|:----|:----| | | | 1 | 1 | | | | | | | 32-95 | | | | | | | | | | 96-127 | | | | | | | | | magic：类似java字节码文件里的魔数，用来判断是不是dubbo协议的数据包。魔数是常量0xdabb,用于判断报文的开始。 flag：标志位, 一共8个地址位。低四位用来表示消息体数据用的序列化工具的类型（默认hessian），高四位中，第一位为1表示是request请求，第二位为1表示双向传输（即有返回response），第三位为1表示是心跳ping事件。 status：状态位, 设置请求响应状态，dubbo定义了一些响应的类型。具体类型见 com.alibaba.dubbo.remoting.exchange.Response invoke id：消息id, long 类型。每一个请求的唯一识别id（由于采用异步通讯的方式，用来把请求request和返回的response对应上） body length：消息体 body 长度, int 类型，即记录Body Content有多少个字节。 （注：相关源码参见 com.alibaba.dubbo.rpc.protocol.dubbo.DubboCodec**） Dubbo协议的编解码过程： Dubbo 协议编解码实现过程 (源码来源于**dubbo2.5.8 ) 1、DubboCodec.encodeRequestData() 116L // 编码request 2、DecodeableRpcInvocation.decode() 89L // 解码request 3、DubboCodec.encodeResponseData() 184L // 编码response 4、DecodeableRpcResult.decode() 73L // 解码response Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rocketmq/":{"url":"distributed/rocketmq/","title":"rocketmq消息中间件","keywords":"","body":"rocketmq消息中间件 1.初识消息中间件 2.特性详解&场景介绍 3.源码分析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 15:48:15 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rocketmq/初识消息中间件.html":{"url":"distributed/rocketmq/初识消息中间件.html","title":"1.初识消息中间件","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 初识消息中间件&部署与快速开始特性 1.什么是消息中间件 2.为什么要用消息中间件？ 3.为什么是RocketMq 4.RocketMq部署搭建 4.1、环境 4.2、部署 4.2.1下载 4.2.2解压： 4.2.3配置host(给nameServer用)： 4.2.4配置集群参数： 4.2.5创建存储&日志文件 4.2.6修改日志配置文件 4.2.7改参数 4.2.8复制 3、启动服务 3.1启动Nameserver服务 3.2启动BrokerServer 3.3是否启动成功 监控源码下载地址： 测试： 服务停止： 常见错误 初识消息中间件&部署与快速开始特性 1.什么是消息中间件 消息+中间件 消息：消息即为数据，数据就会有规划，有长度，有大小。 中间件：为我们提供发送消息的程序或者服务. 消息： java提供了一套标准JMS（java message server） JMS 消息主体（Body） ​ JMS提供五种消息主体的形式，每种形式通过消息接口定义： StreamMessage 消息整体主体包含流式Java原生值，它是连续地被填充和读取的。 MapMessage 消息整体主体包含键值对集合，其中键为字符串，值为Java原生类型。条目访问可被计算器连续地或者名称随机地访问，它的顺序并不一定。 TextMessage ​ 消息整体主体包含一个Java String 对象。 ObjectMessage 消息整体主体包含一个Serializable 对象，如果需要使用集合对象，确保JDK 1.2或更高。 BytesMessage 中间件： 为我们提供发送消息的程序或者服务，目前主流的有 rocketMq 、kafka、rabbitMq、activemq等。 2.为什么要用消息中间件？ 3.为什么是RocketMq 官网：http://rocketmq.apache.org/ 常见的消息中间件：Kafka、ActiveMq、Rocketmq、RabbitMQ 稳定无单点HA 集群功能完善 经历过双十一 Java语言实现 架构轻、源码可读性好（面向过程） 生态圈完善，配套好 开源社区活跃 普通消息、顺序消息、分布式事务消息、定时消息 4.RocketMq部署搭建 架构图 消息中间件和RPC最大区别： Broker Cluster存储 首先来提问下： 1、我们为什么要用消息中间件？ 2、什么是消息中间件，有哪些中间件了？ 3、为什么是rocketmq 大家可以先尝试部署下rocketmq环境，并且我会在课上把这些问题告诉大家。 4.1、环境 192.168.0.31 192.168.0.32 部署模式：2M-2S-SYNC(两主两从同步写)（机器不够用可以不用部署这么多2M-nosalve） 相关安装包存储路径：/root/svr/rocketmq 注意：部署的时候jdk最好用JDK8。 通过本地打包部署的话，要注意了windows和linux系统的转义符不一样，需要对脚本进行更改。 4.2、部署 4.2.1下载 Apache: https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.2.0/rocketmq-all-4.2.0-bin-release.zip Github： https://github.com/apache/rocketmq/ 4.2.2解压： unzip rocketmq-all-4.2.0-bin-release.zip -d /root/svr/rocketmq 4.2.3配置host(给nameServer用)： 命令：vim /etc/hosts 192.168.0.12 rocketmq1 192.168.0.13 rocketmq2 配置环境变量：vim /etc/profile export ROCKETMQ_HOME=/root/svr/rocketmq export PATH=$PATH::$ROCKETMQ_HOME/bin 执行：source /etc/profile 4.2.4配置集群参数： 命令（master）：vim /root/svr/rocketmq/conf/2m-2s-sync/broker-a.properties brokerClusterName=tl-rocketmq-cluster brokerName=broker-a brokerId=0 namesrvAddr=rocketmq1:9876;rocketmq2:9876 defaultTopicQueueNums=4 autoCreateTopicEnable=true autoCreateSubscriptionGroup=true listenPort=10911 deleteWhen=04 fileReservedTime=120 mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 destroyMapedFileIntervalForcibly=120000 redeleteHangedFileInterval=120000 diskMaxUsedSpaceRatio=88 storePathRootDir=/root/svr/rocketmq/data/store storePathCommitLog=/root/svr/rocketmq/data/store/commitlog maxMessageSize=65536 flushCommitLogLeastPages=4 flushConsumeQueueLeastPages=2 flushCommitLogThoroughInterval=10000 flushConsumeQueueThoroughInterval=60000 checkTransactionMessageEnable=false sendMessageThreadPoolNums=128 pullMessageThreadPoolNums=128 brokerRole=SYNC_MASTER flushDiskType=ASYNC_FLUSH 命令（slave）：vim /root/svr/rocketmq/conf/2m-2s-sync/broker-b-s.properties brokerClusterName=tl-rocketmq-cluster brokerName=broker-a #0 表示 Master，>0 表示 Slave brokerId=1 namesrvAddr=rocketmq1:9876;rocketmq2:9876 defaultTopicQueueNums=4 autoCreateTopicEnable=true autoCreateSubscriptionGroup=true listenPort=10911 deleteWhen=04 fileReservedTime=120 mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 destroyMapedFileIntervalForcibly=120000 redeleteHangedFileInterval=120000 diskMaxUsedSpaceRatio=88 storePathRootDir=/root/svr/rocketmq/data/store storePathCommitLog=/root/svr/rocketmq/data/store/commitlog maxMessageSize=65536 flushCommitLogLeastPages=4 flushConsumeQueueLeastPages=2 flushCommitLogThoroughInterval=10000 flushConsumeQueueThoroughInterval=60000 checkTransactionMessageEnable=false sendMessageThreadPoolNums=128 pullMessageThreadPoolNums=128 #角色 brokerRole=SLAVE flushDiskType=ASYNC_FLUSH ​ 自动切换角色 zk redis 切换 不自动切换 4.2.5创建存储&日志文件 mkdir /root/svr/rocketmq/data (也要创建store文件夹) mkdir /root/svr/rocketmq/data/store/commitlog mkdir /root/svr/rocketmq/data/store/consumequeue mkdir /root/svr/rocketmq/data/store/index 注意：如果一台虚拟机部署多个需要用文件进行区分 4.2.6修改日志配置文件 mkdir -p /root/svr/rocketmq/logs cd /root/svr/rocketmq/conf && sed -i 's#${user.home}#/root/svr/rocketmq#g' *.xml 注意logback.*.xml配置文件中${user.home}需要替换自己指定的目录 4.2.7改参数 runbroker.sh,runserver.sh启动参数默认对jvm的堆内存设置比较大(不改启动不起来)，如果是虚拟机非线上环境需要改下参数，大小可以根据自己机器来决定 默认大小 -Xms8g -Xmx8g -Xmn4g 改为： JAVA_OPT=\"${JAVA_OPT} -server -Xms1g -Xmx1g -Xmn512m\" 4.2.8复制 scp其他服务器 scp -r rocketmq/ root@192.168.0.13:/root/svr/ 3、启动服务 3.1启动Nameserver服务 在cd /root/svr/rocketmq/bin目录下执行命令： nohup sh mqnamesrv & 3.2启动BrokerServer 在cd /root/svr/rocketmq/bin目录下执行命令： nohup sh mqbroker -c /root/svr/rocketmq/conf/2m-2s-sync/broker-a.properties& 3.3是否启动成功 输入命令jps或者查看rocketmq/logs下日志是否输出正常。 安装成功。 查看集群监控状态： sh mqadmin clusterlist -n 192.168.0.12:9876 监控源码下载地址： https://github.com/apache/rocketmq-externals.git 配置namesrv rocketmq.config.namesrvAddr=192.168.0.31:9876 测试： export NAMESRV_ADDR=rocketmq1:9876;rocketmq2:9876 测试发送端 > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 测试消费端 > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 服务停止： 在cd /root/svr/rocketmq/bin目录下执行命令或者jps查看进程号 kill -9 pid sh mqshutdown broker sh mqshutdown namesrv 常见错误 Jvm参数没有配置启动错误： # There is insufficient memory for the Java Runtime Environment to continue. # Native memory allocation (malloc) failed to allocate 8589934592 bytes for committing reserved memory. # An error report file with more information is saved as: # /root/svr/rocketmq/bin/hs_err_pid30288.log [root@localhost bin]# ll 启动日志报:Please set the JAVA_HOME variable in your environment, We need java(x64)! !! 需要设置vim /etc/profile环境变量 export JAVA_HOME=/root/svr/jdk source /etc/profile 。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 16:07:55 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rocketmq/特性详解&场景介绍.html":{"url":"distributed/rocketmq/特性详解&场景介绍.html","title":"2.特性详解&场景介绍","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 整体认识rocketmq 架构： 概念： Rocketmq模块划分： Rocketmq高可用： 特性： Producer端： 发送方式： 发送结果： 普通消息： 定时消息 顺序消息： 事务消息 Consumer端： 消费模型： 消费选择： 消息重复幂等： Rocketmq特性详解&场景介绍 整体认识rocketmq 架构： 整体认识：远程通讯，发送消息，存储消息。 概念： Producer：消息生产者，负责产生消息，一般由业务系统负责产生消息 Consumer：消息消费者，负责消费消息，一般是后台系统负责异步消费 Topic**：**消息主题，负责标记一类消息，生产者将消息发送到Topic，消费者从该Topic消费消息 Broker**：**消息中转角色，负责存储消息，转发消息，一般也称为 Server，在 JMS 规范中称为 Provider NameServer**：**服务发现Server，用于生产者和消费者获取Broker的服务； Rocketmq模块划分： 名称 作用 broker broker模块：c和p端消息存储逻辑 client 客户端api：produce、consumer端 接受与发送api common 公共组件：常量、基类、数据结构 tools 运维tools：命令行工具模块 store 存储模块：消息、索引、commitlog存储 namesrv 服务管理模块：服务注册topic等信息存储 remoting 远程通讯模块：netty+fastjson logappender 日志适配模块 example Demo列子 filtersrv 消息过滤器模块 srvutil 辅助模块 filter 过滤模块：消息过滤模块 distribution 部署、运维相关zip包中的代码 openmessaging 兼容openmessaging分布式消息模块 Rocketmq高可用： 特性： Producer端： 发送方式： Sync**：**同步的发送方式，会等待发送结果后才返回 Async**：**异步的发送方式，发送完后，立刻返回。Client 在拿到 Broker 的响应结果后，会回调指定的 callback. 这个 API 也可以指定 Timeout，不指定也是默认的 3000ms. Oneway**：**比较简单，发出去后，什么都不管直接返回。Ps:日志 发送结果： org.apache.rocketmq.client.producer.SendStatus SEND_OK,：消息发送成功 FLUSH_DISK_TIMEOUT,消息发送成功，但是服务器刷盘超时，消息已经进入服务器队列，只有此时服务器宕机，消息才会丢失 FLUSH_SLAVE_TIMEOUT,消息发送成功，但是服务器同步到 Slave 时超时，消息已经进入服务器队列，只有此时服务器宕机，消息才会丢失 SLAVE_NOT_AVAILABLE, 消息发送成功，但是此时 slave 不可用，消息已经进入服务器队列，只有此时服务器宕机，消息才会丢 普通消息： org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendDefaultImpl 1、准备工作 mesasge、网络相关、线程相关 2、从namesrv获取topic路由（缓存机制） 3、组装数据，broker需要的序列化数据（json） 4、Netty发送（源码） 定时消息 定时消息是指消息发到 Broker 后，不能立刻被 Consumer 消费，要到特定的时间点或者等待特定的时间后才能被消费。（第三方 job 步长） 固定精度： 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h org.apache.rocketmq.store.config.MessageStoreConfig#messageDelayLevel 顺序消息： org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendSelectImpl 场景：订单》下单》支付》配送》签收 底层原理：4个队列，一个订单下面不同状态的消息是顺序的只需要发到一个队列中 org.apache.rocketmq.client.producer.MessageQueueSelector 如何选择一个队列 事务消息 http://rocketmq.apache.org/rocketmq/the-design-of-transactional-message/ Consumer端： 消费模型： org.apache.rocketmq.common.protocol.heartbeat.MessageModel#BROADCASTING org.apache.rocketmq.common.protocol.heartbeat.MessageModel#CLUSTERING 消费选择： org.apache.rocketmq.common.consumer.ConsumeFromWhere#CONSUME_FROM_LAST_OFFSET 第一次启动从队列最后位置消费，后续再启动接着上次消费的进度开始消费 org.apache.rocketmq.common.consumer.ConsumeFromWhere#CONSUME_FROM_FIRST_OFFSET 第一次启动从队列初始位置消费，后续再启动接着上次消费的进度开始消费 org.apache.rocketmq.common.consumer.ConsumeFromWhere#CONSUME_FROM_TIMESTAMP 第一次启动从指定时间点位置消费，后续再启动接着上次消费的进度开始消费 以上所说的第一次启动是指从来没有消费过的消费者，如果该消费者消费过，那么会在broker端记录该消费者的消费位置，如果该消费者挂了再启动，那么自动从上次消费的进度开始 消息重复幂等： RocketMQ无法避免消息重复，所以如果业务对消费重复非常敏感，务必要在业务层面去重 Ps：见开发文档 为什么要有组group：? sh mqshutdown broker sh mqshutdown namesrv Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 16:18:12 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rocketmq/源码分析.html":{"url":"distributed/rocketmq/源码分析.html","title":"3.源码分析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 消费模型： 消费选择： 消息重复幂等： 消息过滤： Namesrv端： Broker端： 存储&索引： 存储文件结构: 序列化 Consumer端： RocketMQ提供了两种消费模式：PUSH（pull进行监听）和PULL（长轮训） \\1. Push 方式：rocketmq 已经提供了很全面的实现， consumer 通过长轮询拉取消息后回调 MessageListener 接口实现完成消费， 应用系统只要 MessageListener 完成业务逻辑即可 \\2. Pull 方式：完全由业务系统去控制，定时拉取消息，指定队列消费等等， 当然这里需要业务系统 根据自己的业务需求去实现。 这两种模式分别对应的是DefaultMQPushConsumer类和DefaultMQPullConsumer类 设计：简化程序员使用rocketmq消息中间件，pull 调用broker org.apache.rocketmq.client.impl.consumer.PullMessageService#run 消费模型： org.apache.rocketmq.common.protocol.heartbeat.MessageModel#BROADCASTING org.apache.rocketmq.common.protocol.heartbeat.MessageModel#CLUSTERING 消费选择： org.apache.rocketmq.common.consumer.ConsumeFromWhere#CONSUME_FROM_LAST_OFFSET 第一次启动从队列最后位置消费，后续再启动接着上次消费的进度开始消费 org.apache.rocketmq.common.consumer.ConsumeFromWhere#CONSUME_FROM_FIRST_OFFSET 第一次启动从队列初始位置消费，后续再启动接着上次消费的进度开始消费 org.apache.rocketmq.common.consumer.ConsumeFromWhere#CONSUME_FROM_TIMESTAMP 第一次启动从指定时间点位置消费，后续再启动接着上次消费的进度开始消费 以上所说的第一次启动是指从来没有消费过的消费者，如果该消费者消费过，那么会在broker端记录该消费者的消费位置，如果该消费者挂了再启动，那么自动从上次消费的进度开始 消息重复幂等： RocketMQ无法避免消息重复，所以如果业务对消费重复非常敏感，务必要在业务层面去重 Ps：见开发文档 接口幂等性处理 redis incr 消息过滤： enablePropertyFilter=true Status=1 消费 status=2不需要 为什么要有组group：? sh mqshutdown broker sh mqshutdown namesrv Namesrv端： Namesrv入口：org.apache.rocketmq.namesrv.NamesrvStartup Namesrv 名称服务，是没有状态可集群横向扩展。可以理解为一个注册中心, 整个Namesrv的代码非常简单，主要包含两块功能： 1、管理一些 KV 的配置 2、管理一些 Topic、Broker的注册信息 大致提供服务为： \\1. 每个 broker 启动的时候会向 namesrv 注册 \\2. Producer 发送消息的时候根据 topic 获取路由到 broker 的信息 \\3. Consumer 根据 topic 到 namesrv 获取 topic 的路由到 broker 的信息 Broker端： 程序入口：org.apache.rocketmq.broker.BrokerStartup 存储&索引： 存储文件结构: org.apache.rocketmq.store.index.IndexFile 索引文件由索引文件头（IndexHeader）+（ 槽位 Slot ）+（消息的索引内容）三部分构成。 beginTimestamp 8 位 long 类型，索引文件构建第一个索引的消息落在 broker 的时间 endTimestamp 8 位 long 类型，索引文件构建最后一个索引消息落 broker 时间 beginPhyOffset 8 位 long 类型，索引文件构建第一个索引的消息 commitLog 偏移量endPhyOffset 8 位 long 类型，索引文件构建最后一个索引消息 commitLog 偏移量hashSlotCount 4 位 int 类型，构建索引占用的槽位数(这个值貌似没有具体作用) indexCount 4 位 int 类型，索引文件中构建的索引个数 槽位 slot 默认每个文件配置的 slot 个数为 500 万个， 每个 slot 是 4 位的 int 类型数据计算消息的对应的 slotPos=Math.abs(keyHash)%hashSlotNum 消息在 IndexFile 中的偏移量，Slot 存储的值为消息个数索引 消息的索引内容是 20 位定长内容的数据 4 位 int 值， 存储的是 key 的 hash 值 8 位 long 值 存储的是消息在 commitlog 的物理偏移量 phyOffset 4 位 int 值 存储了当前消息跟索引文件中第一个消息在 broker 落地的时间差 4 位 int 值 如果存在 hash 冲突，存储的是上一个消息的索引地址 序列化 在RocketMQ中，RemotingCommand这个类在消息传输过程中对所有数据内容的封装，不但包含了所有的数据结构，还包含了编码解码操作 org.apache.rocketmq.remoting.protocol.RemotingCommand 本地启动集群： 1、持久化 producer发送数据了 写磁盘 2、集群完好 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 16:21:38 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rabbitmq/":{"url":"distributed/rabbitmq/","title":"rabbitmq消息中间件","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/kafka/":{"url":"distributed/kafka/","title":"kafka消息中间件","keywords":"","body":"kafka消息中间件 1.kafka消息中间件 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 16:30:34 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/kafka/kafka.html":{"url":"distributed/kafka/kafka.html","title":"1.kafka消息中间件","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.1 简介 1.2 特点 1.3 场景 1.4 架构 2.kafka 环境搭建 1）解压安装包 2）在kafka_2.12-2.6.0 目录下创建 logs 文件夹 3）修改配置文件 4）分发安装包 5）分别在 hadoop102 和 hadoop103 上修改配置文件 config/server.properties 中的 broker.id=1、broker.id=2 8）启动集群 9）关闭集群 3.Kafka shell 使用 1）查看当前服务器中的所有 topic 2）创建 topic 3）删除 topic 4）发送消息 5）消费消息 6）查看某个 Topic 的详情 4. Kafka API 实战 4.1 环境准备 4.2 Kafka 生产者 Java API 4.2.1 创建生产者（过时的 API） 4.2.2 创建生产者（新 API） 4.2.3 创建生产者带回调函数（新 API） 4.2.4 自定义分区生产者 4.3 Kafka 消费者 Java API 4.3.1 高级 API 4.3.2 低级 API 5. Kafka producer 拦截器(interceptor) 5.1 拦截器原理 6. Kafka 工作流程分析 3.1 Kafka 生产过程分析 3.1.1 写入方式 3.1.2 分区（Partition） 3.1.3 副本（Replication） 3.1.4 写入流程 3.2 Broker 保存消息 3.2.1 存储方式 3.2.2 存储策略 3.2.3 Zookeeper 存储结构 3.3 Kafka 消费过程分析 3.3.1 高级 API 3.3.2 低级 API 3.3.3 消费者组 3.3.4 消费方式 3.3.5 消费者组案例 7. 扩展 7.1 Kafka 与 Flume 比较 7.2 Flume 与 kafka 集成 7.3 Kafka 配置信息 7.3.1 Broker 配置信息 7.3.2 Producer 配置信息 7.3.3 Consumer 配置信息 1.概述 1.1 简介 Apache Kafka 是一个分布式消息系统，由Scala写成。是由 Apache 软件基金会开发的一个开源消息系统项目 1.2 特点 高吞吐量：Kafka 每秒可以生产约 25 万消息（50 MB），每秒处理 55 万消息（110 MB） 持久化数据存储：可进行持久化操作。将消息持久化到磁盘，因此可用于批量消费，例如 ETL，以及实时应用程序。通过将数据持久化到硬盘以及 replication 防止数据丢失。 分布式系统易于扩展：所有的 producer、broker 和 consumer 都会有多个，均为分布式的。无需停机即可扩展机器。 客户端状态维护：消息被处理的状态是在 consumer 端维护，而不是由 server 端维护。当失败时能自动平衡。 1.3 场景 大数据日志收集缓存 1.4 架构 2.kafka 环境搭建 1）解压安装包 tar -zxvf kafka_2.12-2.6.0.tgz -C /usr/local/software 2）在kafka_2.12-2.6.0 目录下创建 logs 文件夹 cd kafka_2.12-2.6.0 && mkdir logs 3）修改配置文件 vim server.properties 输入以下内容： #broker 的全局唯一编号，不能重复 broker.id=0 #删除 topic 功能使能 delete.topic.enable=true #处理网络请求的线程数量 num.network.threads=3 #用来处理磁盘 IO 的现成数量 num.io.threads=8 #发送套接字的缓冲区大小 socket.send.buffer.bytes=102400 #接收套接字的缓冲区大小 socket.receive.buffer.bytes=102400 #请求套接字的缓冲区大小 socket.request.max.bytes=104857600 #kafka 运行日志存放的路径 log.dirs=/usr/local/software/kafka_2.12-2.6.0/logs #topic 在当前 broker 上的分区个数 num.partitions=1 #用来恢复和清理 data 下数据的线程数量 num.recovery.threads.per.data.dir=1 #segment 文件保留的最长时间，超时将被删除 log.retention.hours=168 #配置连接 Zookeeper 集群地址 zookeeper.connect=hadoop101:2181,hadoop102:2181,hadoop103:2181 broker.id=0 log.dirs=/usr/local/software/kafka_2.12-2.6.0/logs zookeeper.connect=hadoop101:2181,hadoop102:2181,hadoop103:2181 4）分发安装包 xsync kafka_2.12-2.6.0 注意：分发之后记得配置其他机器的环境变量 5）分别在 hadoop102 和 hadoop103 上修改配置文件 config/server.properties 中的 broker.id=1、broker.id=2 注：broker.id 不得重复 8）启动集群 依次在 hadoop101、hadoop102、hadoop103 节点上启动 kafka bin/kafka-server-start.sh config/server.properties & 9）关闭集群 依次在 hadoop101、hadoop102、hadoop103 节点上关闭 kafka bin/kafka-server-stop.sh stop 3.Kafka shell 使用 1）查看当前服务器中的所有 topic bin/kafka-topics.sh --zookeeper hadoop101:2181 --list 2）创建 topic bin/kafka-topics.sh --zookeeper hadoop101:2181 --create --replication-factor 1 --partitions 1 --topic first 选项说明： --topic 定义 topic 名 --replication-factor 定义副本数 --partitions 定义分区数 bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server hadoop101:9092 3）删除 topic bin/kafka-topics.sh --zookeeper hadoop101:2181 --delete --topic first 需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启。 4）发送消息 bin/kafka-console-producer.sh --broker-list hadoop101:9092 --topic first hello world bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server hadoop101:9092 5）消费消息 bin/kafka-console-consumer.sh --zookeeper hadoop101:2181 --from-beginning --topic first --from-beginning：会把 first 主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。 bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server hadoop101:9092 6）查看某个 Topic 的详情 bin/kafka-topics.sh --zookeeper hadoop101:2181 --describe --topic first bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server hadoop101:9092 4. Kafka API 实战 4.1 环境准备 1）启动 zk 和 kafka 集群，在 kafka 集群中打开一个消费者 bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server hadoop101:9092 2）导入 pom 依赖 org.apache.kafka kafka-clients 0.11.0.0 org.apache.kafka kafka_2.12 0.11.0.0 4.2 Kafka 生产者 Java API 4.2.1 创建生产者（过时的 API） import java.util.Properties; import kafka.javaapi.producer.Producer; import kafka.producer.KeyedMessage; import kafka.producer.ProducerConfig; public class OldProducer { public static void main(String[] args) { Properties properties = new Properties(); properties.put(\"metadata.broker.list\", \"hadoop102:9092\"); properties.put(\"request.required.acks\", \"1\"); properties.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); Producer producer = new Producer(new ProducerConfig(properties)); KeyedMessage message = new KeyedMessage(\"first\", \"hello world\"); producer.send(message ); } } 4.2.2 创建生产者（新 API） import java.util.Properties; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; public class NewProducer { public static void main(String[] args) { Properties props = new Properties(); // Kafka 服务端的主机名和端口号 props.put(\"bootstrap.servers\", \"hadoop103:9092\"); // 等待所有副本节点的应答 props.put(\"acks\", \"all\"); // 消息发送最大尝试次数 props.put(\"retries\", 0); // 一批消息处理大小 props.put(\"batch.size\", 16384); // 请求延时 props.put(\"linger.ms\", 1); // 发送缓存区内存大小 props.put(\"buffer.memory\", 33554432); // key 序列化 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // value 序列化 props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer producer = new KafkaProducer<>(props); for (int i = 0; i (\"first\", Integer.toString(i), \"hello world-\" + i)); } producer.close(); } } 4.2.3 创建生产者带回调函数（新 API） import java.util.Properties; import org.apache.kafka.clients.producer.Callback; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class CallBackProducer { public static void main(String[] args) { Properties props = new Properties(); // Kafka 服务端的主机名和端口号 props.put(\"bootstrap.servers\", \"hadoop103:9092\"); // 等待所有副本节点的应答 props.put(\"acks\", \"all\"); // 消息发送最大尝试次数 props.put(\"retries\", 0); // 一批消息处理大小 props.put(\"batch.size\", 16384); // 增加服务端请求延时 props.put(\"linger.ms\", 1); // 发送缓存区内存大小 props.put(\"buffer.memory\", 33554432); // key 序列化 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // value 序列化 props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer kafkaProducer = new KafkaProducer<>(props); for (int i = 0; i (\"first\", \"hello\" + i), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (metadata != null) { System.err.println(metadata.partition() + \"---\" + metadata.offset()); } } }); } kafkaProducer.close(); } } 4.2.4 自定义分区生产者 0）需求：将所有数据存储到 topic 的第 0 号分区上 1）定义一个类实现 Partitioner 接口，重写里面的方法（过时 API） import java.util.Map; import kafka.producer.Partitioner; public class CustomPartitioner implements Partitioner { public CustomPartitioner() { super(); } @Override public int partition(Object key, int numPartitions) { // 控制分区 return 0; } } 2）自定义分区（新 API） import java.util.Map; import org.apache.kafka.clients.producer.Partitioner; import org.apache.kafka.common.Cluster; public class CustomPartitioner implements Partitioner { @Override public void configure(Map configs) { } @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { // 控制分区 return 0; } @Override public void close() { } } 3）在代码中调用 import java.util.Properties; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; public class PartitionerProducer { public static void main(String[] args) { Properties props = new Properties(); // Kafka 服务端的主机名和端口号 props.put(\"bootstrap.servers\", \"hadoop103:9092\"); // 等待所有副本节点的应答 props.put(\"acks\", \"all\"); // 消息发送最大尝试次数 props.put(\"retries\", 0); // 一批消息处理大小 props.put(\"batch.size\", 16384); // 增加服务端请求延时 props.put(\"linger.ms\", 1); // 发送缓存区内存大小 props.put(\"buffer.memory\", 33554432); // key 序列化 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // value 序列化 props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 自定义分区 props.put(\"partitioner.class\", \"com.haha.kafka.CustomPartitioner\"); Producer producer = new KafkaProducer<>(props); producer.send(new ProducerRecord(\"first\", \"1\", \"haha\")); producer.close(); } } 4）测试 （1）在 hadoop102 上监控/opt/module/kafka/logs/目录下 first 主题 3 个分区的 log 日志动态变化情况 tail -f 00000000000000000000.log （2）发现数据都存储到指定的分区了。 4.3 Kafka 消费者 Java API 4.3.1 高级 API 0）在控制台创建发送者 bin/kafka-console-producer.sh \\ --broker-list hadoop102:9092 --topic first >hello world 1）创建消费者（过时 API） import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.Properties; import kafka.consumer.Consumer; import kafka.consumer.ConsumerConfig; import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; public class CustomConsumer { public static void main(String[] args) { Properties properties = new Properties(); properties.put(\"zookeeper.connect\", \"hadoop102:2181\"); properties.put(\"group.id\", \"g1\"); properties.put(\"zookeeper.session.timeout.ms\", \"500\"); properties.put(\"zookeeper.sync.time.ms\", \"250\"); properties.put(\"auto.commit.interval.ms\", \"1000\"); // 创建消费者连接器 ConsumerConnector consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(properties)); HashMap topicCount = new HashMap<>(); topicCount.put(\"first\", 1); Map>> consumerMap = consumer.createMessageStreams(topicCount); KafkaStream stream = consumerMap.get(\"first\").get(0); ConsumerIterator it = stream.iterator(); while (it.hasNext()) { System.out.println(new String(it.next().message())); } } } 2）官方提供案例（自动维护消费情况）（新 API） import java.util.Arrays; import java.util.Properties; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; public class CustomNewConsumer { public static void main(String[] args) { Properties props = new Properties(); // 定义 kakfa 服务的地址，不需要将所有 broker 指定上 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); // 制定 consumer group props.put(\"group.id\", \"test\"); // 是否自动确认 offset props.put(\"enable.auto.commit\", \"true\"); // 自动确认 offset 的时间间隔 props.put(\"auto.commit.interval.ms\", \"1000\"); // key 的序列化类 props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); // value 的序列化类 props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); // 定义 consumer KafkaConsumer consumer = new KafkaConsumer<>(props); // 消费者订阅的 topic, 可同时订阅多个 consumer.subscribe(Arrays.asList(\"first\", \"second\",\"third\")); while (true) { // 读取数据，读取超时时间为 100ms ConsumerRecords records = consumer.poll(100); for (ConsumerRecord record : records) System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); } } } 4.3.2 低级 API 实现使用低级 API 读取指定 topic，指定 partition,指定 offset 的数据。 1）消费者使用低级 API 的主要步骤： 步骤 主要工作 1 根据指定的分区从主题元数据中找到主副本 2 获取分区最新的消费进度 3 从主副本拉取分区的消息 4 识别主副本的变化，重试 2）方法描述： findLeader() 客户端向种子节点发送主题元数据，将副本集加入备用节点 getLastOffset() 消费者客户端发送偏移量请求，获取分区最近的偏移量 run() 消费者低级 AP I 拉取消息的主要方法 findNewLeader() 当分区的主副本节点发生故障，客户将要找出新的主副本 3）代码： import java.nio.ByteBuffer; import java.util.ArrayList; import java.util.Collections; import java.util.HashMap; import java.util.List; import java.util.Map; import kafka.api.FetchRequest; import kafka.api.FetchRequestBuilder; import kafka.api.PartitionOffsetRequestInfo; import kafka.cluster.BrokerEndPoint; import kafka.common.ErrorMapping; import kafka.common.TopicAndPartition; import kafka.javaapi.FetchResponse; import kafka.javaapi.OffsetResponse; import kafka.javaapi.PartitionMetadata; import kafka.javaapi.TopicMetadata; import kafka.javaapi.TopicMetadataRequest; import kafka.javaapi.consumer.SimpleConsumer; import kafka.message.MessageAndOffset; public class SimpleExample { private List m_replicaBrokers = new ArrayList<>(); public SimpleExample() { m_replicaBrokers = new ArrayList<>(); } public static void main(String args[]) { SimpleExample example = new SimpleExample(); // 最大读取消息数量 long maxReads = Long.parseLong(\"3\"); // 要订阅的 topic String topic = \"test1\"; // 要查找的分区 int partition = Integer.parseInt(\"0\"); // broker 节点的 ip List seeds = new ArrayList<>(); seeds.add(\"192.168.9.102\"); seeds.add(\"192.168.9.103\"); seeds.add(\"192.168.9.104\"); // 端口 int port = Integer.parseInt(\"9092\"); try { example.run(maxReads, topic, partition, seeds, port); } catch (Exception e) { System.out.println(\"Oops:\" + e); e.printStackTrace(); } } public void run(long a_maxReads, String a_topic, int a_partition, List a_seedBrokers, int a_port) throws Exception { // 获取指定 Topic partition 的元数据 PartitionMetadata metadata = findLeader(a_seedBrokers, a_port, a_topic, a_partition); if (metadata == null) { System.out.println(\"Can't find metadata for Topic and Partition. Exiting\"); return; } if (metadata.leader() == null) { System.out.println(\"Can't find Leader for Topic and Partition. Exiting\"); return; } String leadBroker = metadata.leader().host(); String clientName = \"Client\" + a_topic + \"\" + a_partition; SimpleConsumer consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName); long readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.EarliestTime(), clientName); int numErrors = 0; while (a_maxReads > 0) { if (consumer == null) { consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName); } FetchRequest req = new FetchRequestBuilder().clientId(clientName).addFetch(a_topic, a_partition, readOffset, 100000).build(); FetchResponse fetchResponse = consumer.fetch(req); if (fetchResponse.hasError()) { numErrors++; // Something went wrong! short code = fetchResponse.errorCode(a_topic, a_partition); System.out.println(\"Error fetching data from the Broker:\" + leadBroker + \" Reason: \" + code); if (numErrors > 5) break; if (code == ErrorMapping.OffsetOutOfRangeCode()) { // We asked for an invalid offset. For simple case ask for // the last element to reset readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.LatestTime(), clientName); continue; } consumer.close(); consumer = null; leadBroker = findNewLeader(leadBroker, a_topic, a_partition, a_port); continue; } numErrors = 0; long numRead = 0; for (MessageAndOffset messageAndOffset : fetchResponse.messageSet(a_topic, a_partition)) { long currentOffset = messageAndOffset.offset(); if (currentOffset System.out.println(\"Found an old offset: \" + currentOffset + \" Expecting: \" + readOffset); continue; } readOffset = messageAndOffset.nextOffset(); ByteBuffer payload = messageAndOffset.message().payload(); byte[] bytes = new byte[payload.limit()]; payload.get(bytes); System.out.println(String.valueOf(messageAndOffset.offset()) + \": \" + new String(bytes, \"UTF-8\")); numRead++; a_maxReads--; } if (numRead == 0) { try { Thread.sleep(1000); } catch (InterruptedException ie) { } } } if (consumer != null) consumer.close(); } public static long getLastOffset(SimpleConsumer consumer, String topic, int partition, long whichTime, String clientName) { TopicAndPartition topicAndPartition = new TopicAndPartition(topic, partition); Map requestInfo = new HashMap(); requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(whichTime, 1)); kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(requestInfo, kafka.api.OffsetRequest.CurrentVersion(), clientName); OffsetResponse response = consumer.getOffsetsBefore(request); if (response.hasError()) { System.out.println(\"Error fetching data Offset Data the Broker. Reason: \" + response.errorCode(topic, partition)); return 0; } long[] offsets = response.offsets(topic, partition); return offsets[0]; } private String findNewLeader(String a_oldLeader, String a_topic, int a_partition, int a_port) throws Exception { for (int i = 0; i boolean goToSleep = false; PartitionMetadata metadata = findLeader(m_replicaBrokers, a_port, a_topic, a_partition); if (metadata == null) { goToSleep = true; } else if (metadata.leader() == null) { goToSleep = true; } else if (a_oldLeader.equalsIgnoreCase(metadata.leader().host()) && i == 0) { // first time through if the leader hasn't changed give // ZooKeeper a second to recover // second time, assume the broker did recover before failover, // or it was a non-Broker issue // goToSleep = true; } else { return metadata.leader().host(); } if (goToSleep) { Thread.sleep(1000); } } System.out.println(\"Unable to find new leader after Broker failure. Exiting\"); throw new Exception(\"Unable to find new leader after Broker failure. Exiting\"); } private PartitionMetadata findLeader(List a_seedBrokers, int a_port, String a_topic, int a_partition) { PartitionMetadata returnMetaData = null; loop: for (String seed : a_seedBrokers) { SimpleConsumer consumer = null; try { consumer = new SimpleConsumer(seed, a_port, 100000, 64 * 1024, \"leaderLookup\"); List topics = Collections.singletonList(a_topic); TopicMetadataRequest req = new TopicMetadataRequest(topics); kafka.javaapi.TopicMetadataResponse resp = consumer.send(req); List metaData = resp.topicsMetadata(); for (TopicMetadata item : metaData) { for (PartitionMetadata part : item.partitionsMetadata()) { if (part.partitionId() == a_partition) { returnMetaData = part; break loop; } } } } catch (Exception e) { System.out.println(\"Error communicating with Broker [\" + seed + \"] to find Leader for [\" + a_topic + \", \" + a_partition + \"] Reason: \" + e); } finally { if (consumer != null) consumer.close(); } } if (returnMetaData != null) { m_replicaBrokers.clear(); for (BrokerEndPoint replica : returnMetaData.replicas()) { m_replicaBrokers.add(replica.host()); } } return returnMetaData; } } 5. Kafka producer 拦截器(interceptor) 5.1 拦截器原理 Producer 拦截器(interceptor)是在 Kafka 0.10 版本被引入的，主要用于实现 clients 端的定制化控制逻辑。 对于 producer 而言，interceptor 使得用户在消息发送前以及 producer 回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer 允许用户指定多个 interceptor 按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor 的实现接口是 org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括： （1）configure(configs) 获取配置信息和初始化数据时调用。 （2）onSend(ProducerRecord)： 该方法封装进 KafkaProducer.send 方法中，即它运行在用户主线程中。Producer 确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的 topic 和分区，否则会影响目标分区的计算 （3）onAcknowledgement(RecordMetadata, Exception)： 该方法会在消息被应答或消息发送失败时调用，并且通常都是在 producer 回调逻辑触发之前。onAcknowledgement 运行在 producer 的 IO 线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢 producer 的消息发送效率 （4）close： 关闭 interceptor，主要用于执行一些资源清理工作 如前所述，interceptor 可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个 interceptor，则 producer 将按照指定顺序调用它们，并仅仅是捕获每个 interceptor 可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 5.2 拦截器案例 1）需求： 实现一个简单的双 interceptor 组成的拦截链。第一个 interceptor 会在消息发送前将时间戳信息加到消息 value 的最前部；第二个 interceptor 会在消息发送后更新成功发送消息数或失败发送消息数。 2）案例实操 （1）增加时间戳拦截器 import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class TimeInterceptor implements ProducerInterceptor { @Override public void configure(Map configs) { } @Override public ProducerRecord onSend(ProducerRecord record) { // 创建一个新的 record，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + \",\" + record.value().toString()); } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { } @Override public void close() { } } （2）统计发送消息成功和发送失败消息数，并在 producer 关闭时打印这两个计数器 import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class CounterInterceptor implements ProducerInterceptor{ private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map configs) { } @Override public ProducerRecord onSend(ProducerRecord record) { return record; } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { // 统计成功和失败的次数 if (exception == null) { successCounter++; } else { errorCounter++; } } @Override public void close() { // 保存结果 System.out.println(\"Successful sent: \" + successCounter); System.out.println(\"Failed sent: \" + errorCounter); } } （3）producer 主程序 import java.util.ArrayList; import java.util.List; import java.util.Properties; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; public class InterceptorProducer { public static void main(String[] args) throws Exception { // 1 设置配置信息 Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 0); props.put(\"batch.size\", 16384); props.put(\"linger.ms\", 1); props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 2 构建拦截链 List interceptors = new ArrayList<>(); interceptors.add(\"com.atguigu.kafka.interceptor.TimeInterceptor\"); interceptors.add(\"com.atguigu.kafka.interceptor.CounterInterceptor\"); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = \"first\"; Producer producer = new KafkaProducer<>(props); // 3 发送消息 for (int i = 0; i record = new ProducerRecord<>(topic, \"message\" + i); producer.send(record); } // 4 一定要关闭 producer，这样才会调用 interceptor 的 close 方法 producer.close(); } } 3）测试 （1）在 kafka 上启动消费者，然后运行客户端 java 程序。 bin/kafka-console-consumer.sh \\ --zookeeper hadoop102:2181 --from-beginning --topic first 1501904047034,message0 1501904047225,message1 1501904047230,message2 1501904047234,message3 1501904047236,message4 1501904047240,message5 1501904047243,message6 1501904047246,message7 1501904047249,message8 1501904047252,message9 （2）观察 java 平台控制台输出数据如下： Successful sent: 10 Failed sent: 0 6. Kafka 工作流程分析 3.1 Kafka 生产过程分析 3.1.1 写入方式 producer 采用推（push）模式将消息发布到 broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。 3.1.2 分区（Partition） 消息发送时都被发送到一个 topic，其本质就是一个目录，而 topic 是由一些 Partition Logs(分区日志)组成，其组织结构如下图所示： 我们可以看到，每个 Partition 中的消息都是有序的，生产的消息被不断追加到 Partition log 上，其中的每一个消息都被赋予了一个唯一的 offset 值。 1）分区的原因 （1）方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了； （2）可以提高并发，因为可以以 Partition 为单位读写了。 2）分区的原则 （1）指定了 patition，则直接使用； （2）未指定 patition 但指定 key，通过对 key 的 value 进行 hash 出一个 patition； （3）patition 和 key 都未指定，使用轮询选出一个 patition。 DefaultPartitioner 类 public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { List partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) { int nextValue = nextValue(topic); List availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() > 0) { int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); } else { // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; } } else { // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; } } 3.1.3 副本（Replication） 同一个 partition 可能会有多个 replication（对应 server.properties 配置中的 default.replication.factor=N）。没有 replication 的情况下，一旦 broker 宕机，其上所有 patition 的数据都不可被消费，同时 producer 也不能再将数据存于其上的 patition。引入 replication 之后，同一个 partition 可能会有多个 replication，而这时需要在这些 replication 之间选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replication 作为 follower 从 leader 中复制数据。 3.1.4 写入流程 producer 写入消息流程如下： 1）producer 先从 zookeeper 的 \"/brokers/.../state\"节点找到该 partition 的 leader 2）producer 将消息发送给该 leader 3）leader 将消息写入本地 log 4）followers 从 leader pull 消息，写入本地 log 后向 leader 发送 ACK 5）leader 收到所有 ISR 中的 replication 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset）并向 producer 发送 ACK 3.2 Broker 保存消息 3.2.1 存储方式 物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下： ll drwxrwxr-x. 2 atguigu atguigu 4096 8 月 6 14:37 first-0 drwxrwxr-x. 2 atguigu atguigu 4096 8 月 6 14:35 first-1 drwxrwxr-x. 2 atguigu atguigu 4096 8 月 6 14:37 first-2 cd first-0 ll -rw-rw-r--. 1 atguigu atguigu 10485760 8 月 6 14:33 00000000000000000000.index -rw-rw-r--. 1 atguigu atguigu 219 8 月 6 15:07 00000000000000000000.log -rw-rw-r--. 1 atguigu atguigu 10485756 8 月 6 14:33 00000000000000000000.timeindex -rw-rw-r--. 1 atguigu atguigu 8 8 月 6 14:37 leader-epoch-checkpoint 3.2.2 存储策略 无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据： 1）基于时间：log.retention.hours=168 2）基于大小：log.retention.bytes=1073741824 需要注意的是，因为 Kafka 读取特定消息的时间复杂度为 O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 3.2.3 Zookeeper 存储结构 注意：producer 不在 zk 中注册，消费者在 zk 中注册。 3.3 Kafka 消费过程分析 kafka 提供了两套 consumer API：高级 Consumer API 和低级 Consumer API。 3.3.1 高级 API 1）高级 API 优点 高级 API 写起来简单 不需要自行去管理 offset，系统通过 zookeeper 自行管理。 不需要管理分区，副本等情况，.系统自动管理。 消费者断线会自动根据上一次记录在 zookeeper 中的 offset 去接着获取数据（默认设置 1 分钟更新一下 zookeeper 中存的 offset） 可以使用 group 来区分对同一个 topic 的不同程序访问分离开来（不同的 group 记录不同的 offset，这样不同程序读取同一个 topic 才不会因为 offset 互相影响） 2）高级 API 缺点 不能自行控制 offset（对于某些特殊需求来说） 不能细化控制如分区、副本、zk 等 3.3.2 低级 API 1）低级 API 优点 能够让开发者自己控制 offset，想从哪里读取就从哪里读取。 自行控制连接分区，对分区自定义进行负载均衡 对 zookeeper 的依赖性降低（如：offset 不一定非要靠 zk 存储，自行存储 offset 即可，比如存在文件或者内存中） 2）低级 API 缺点 太过复杂，需要自行控制 offset，连接哪个分区，找到分区 leader 等。 3.3.3 消费者组 消费者是以 consumer group 消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个 topic。每个分区在同一时间只能由 group 中的一个消费者读取，但是多个 group 可以同时消费这个 partition。在图中，有一个由三个消费者组成的 group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。 在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的 group 成员会自动负载均衡读取之前失败的消费者读取的分区。 3.3.4 消费方式 consumer 采用 pull（拉）模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。 对于 Kafka 而言，pull 模式更合适，它可简化 broker 的设计，consumer 可自主控制消费消息的速率，同时 consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。 3.3.5 消费者组案例 1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。 2）案例实操 （1）在 hadoop102、hadoop103 上修改/opt/module/kafka/config/consumer.properties 配置文件中的 group.id 属性为任意组名。 vi consumer.properties group.id=atguigu （2）在 hadoop102、hadoop103 上分别启动消费者 bin/kafka-console-consumer.sh \\ --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties （3）在 hadoop104 上启动生产者 bin/kafka-console-producer.sh \\ --broker-list hadoop102:9092 --topic first >hello world （4）查看 hadoop102 和 hadoop103 的接收者。 同一时刻只有一个消费者接收到消息。 7. 扩展 7.1 Kafka 与 Flume 比较 在企业中必须要清楚流式数据采集框架 flume 和 kafka 的定位是什么： flume：cloudera 公司研发: 适合多个生产者； 适合下游数据消费者不多的情况； 适合数据安全性要求不高的操作； 适合与 Hadoop 生态圈对接的操作。 kafka：linkedin 公司研发: 适合数据下游消费众多的情况； 适合数据安全性要求较高的操作，支持 replication。 因此我们常用的一种模型是： 线上数据 --> flume --> kafka --> flume(根据情景增删该流程) --> HDFS 7.2 Flume 与 kafka 集成 1）配置 flume(flume-kafka.conf) # define a1.sources = r1 a1.sinks = k1 a1.channels = c1 # source a1.sources.r1.type = exec a1.sources.r1.command = tail -F -c +0 /opt/module/datas/flume.log a1.sources.r1.shell = /bin/bash -c # sink a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092 a1.sinks.k1.kafka.topic = first a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.sinks.k1.kafka.producer.linger.ms = 1 # channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # bind a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2） 启动 kafkaIDEA 消费者 3） 进入 flume 根目录下，启动 flume bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf 4） 向 /opt/module/datas/flume.log 里追加数据，查看 kafka 消费者消费情况 $ echo hello > /opt/module/datas 7.3 Kafka 配置信息 7.3.1 Broker 配置信息 属性 默认值 描述 broker.id 必填参数，broker 的唯一标识 log.dirs /tmp/kafka-logs Kafka 数据存放的目录。可以指定多个目录，中间用逗号分隔，当新 partition 被创建的时会被存放到当前存放 partition 最少的目录。 port 9092 BrokerServer 接受客户端连接的端口号 zookeeper.connect null Zookeeper 的连接串，格式为：hostname1:port1,hostname2:port2,hostname3:port3。可以填一个或多个，为了提高可靠性，建议都填上。注意，此配置允许我们指定一个 zookeeper 路径来存放此 kafka 集群的所有数据，为了与其他应用集群区分开，建议在此配置中指定本集群存放目录，格式为：hostname1:port1,hostname2:port2,hostname3:port3/chroot/path。需要注意的是，消费者的参数要和此参数一致。 message.max.bytes 1000000 服务器可以接收到的最大的消息大小。注意此参数要和 consumer 的 maximum.message.size 大小一致，否则会因为生产者生产的消息太大导致消费者无法消费。 num.io.threads 8 服务器用来执行读写请求的 IO 线程数，此参数的数量至少要等于服务器上磁盘的数量。 queued.max.requests 500 I/O 线程可以处理请求的队列大小，若实际请求数超过此大小，网络线程将停止接收新的请求。 socket.send.buffer.bytes 100 * 1024 The SO_SNDBUFF buffer the server prefers for socket connections. socket.receive.buffer.bytes 100 * 1024 The SO_RCVBUFF buffer the server prefers for socket connections. socket.request.max.bytes 100 1024 1024 服务器允许请求的最大值， 用来防止内存溢出，其值应该小于 Java heap size. num.partitions 1 默认 partition 数量，如果 topic 在创建时没有指定 partition 数量，默认使用此值，建议改为 5 log.segment.bytes 1024 1024 1024 Segment 文件的大小，超过此值将会自动新建一个 segment，此值可以被 topic 级别的参数覆盖。 log.roll.{ms,hours} 24 * 7 hours 新建 segment 文件的时间，此值可以被 topic 级别的参数覆盖。 log.retention.{ms,minutes,hours} 7 days Kafka segment log 的保存周期，保存周期超过此时间日志就会被删除。此参数可以被 topic 级别参数覆盖。数据量大时，建议减小此值。 log.retention.bytes -1 每个 partition 的最大容量，若数据量超过此值，partition 数据将会被删除。注意这个参数控制的是每个 partition 而不是 topic。此参数可以被 log 级别参数覆盖。 log.retention.check.interval.ms 5 minutes 删除策略的检查周期 auto.create.topics.enable true 自动创建 topic 参数，建议此值设置为 false，严格控制 topic 管理，防止生产者错写 topic。 default.replication.factor 1 默认副本数量，建议改为 2。 replica.lag.time.max.ms 10000 在此窗口时间内没有收到 follower 的 fetch 请求，leader 会将其从 ISR(in-sync replicas)中移除。 replica.lag.max.messages 4000 如果 replica 节点落后 leader 节点此值大小的消息数量，leader 节点就会将其从 ISR 中移除。 replica.socket.timeout.ms 30 * 1000 replica 向 leader 发送请求的超时时间。 replica.socket.receive.buffer.bytes 64 * 1024 The socket receive buffer for network requests to the leader for replicating data. replica.fetch.max.bytes 1024 * 1024 The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader. replica.fetch.wait.max.ms 500 The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader. num.replica.fetchers 1 Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker. fetch.purgatory.purge.interval.requests 1000 The purge interval (in number of requests) of the fetch request purgatory. zookeeper.session.timeout.ms 6000 ZooKeeper session 超时时间。如果在此时间内 server 没有向 zookeeper 发送心跳，zookeeper 就会认为此节点已挂掉。 此值太低导致节点容易被标记死亡；若太高，.会导致太迟发现节点死亡。 zookeeper.connection.timeout.ms 6000 客户端连接 zookeeper 的超时时间。 zookeeper.sync.time.ms 2000 H ZK follower 落后 ZK leader 的时间。 controlled.shutdown.enable true 允许 broker shutdown。如果启用，broker 在关闭自己之前会把它上面的所有 leaders 转移到其它 brokers 上，建议启用，增加集群稳定性。 auto.leader.rebalance.enable true If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the “preferred” replica for each partition if it is available. leader.imbalance.per.broker.percentage 10 The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker. leader.imbalance.check.interval.seconds 300 The frequency with which to check for leader imbalance. offset.metadata.max.bytes 4096 The maximum amount of metadata to allow clients to save with their offsets. connections.max.idle.ms 600000 Idle connections timeout: the server socket processor threads close the connections that idle more than this. num.recovery.threads.per.data.dir 1 The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. unclean.leader.election.enable true Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss. delete.topic.enable false 启用 deletetopic 参数，建议设置为 true。 offsets.topic.num.partitions 50 The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, we recommend using a higher setting for production (e.g., 100-200). offsets.topic.retention.minutes 1440 Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic. offsets.retention.check.interval.ms 600000 The frequency at which the offset manager checks for stale offsets. offsets.topic.replication.factor 3 The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas. offsets.topic.segment.bytes 104857600 Segment size for the offsets topic. Since it uses a compacted topic, this should be kept relatively low in order to facilitate faster log compaction and loads. offsets.load.buffer.size 5242880 An offset load occurs when a broker becomes the offset manager for a set of consumer groups (i.e., when it becomes a leader for an offsets topic partition). This setting corresponds to the batch size (in bytes) to use when reading from the offsets segments when loading offsets into the offset manager’s cache. offsets.commit.required.acks -1 The number of acknowledgements that are required before the offset commit can be accepted. This is similar to the producer’s acknowledgement setting. In general, the default should not be overridden. offsets.commit.timeout.ms 5000 The offset commit will be delayed until this timeout or the required number of replicas have received the offset commit. This is similar to the producer request timeout. 7.3.2 Producer 配置信息 属性 默认值 描述 metadata.broker.list 启动时 producer 查询 brokers 的列表，可以是集群中所有 brokers 的一个子集。注意，这个参数只是用来获取 topic 的元信息用，producer 会从元信息中挑选合适的 broker 并与之建立 socket 连接。格式是：host1:port1,host2:port2。 request.required.acks 0 参见 3.2 节介绍 request.timeout.ms 10000 Broker 等待 ack 的超时时间，若等待时间超过此值，会返回客户端错误信息。 producer.type sync 同步异步模式。async 表示异步，sync 表示同步。如果设置成异步模式，可以允许生产者以 batch 的形式 push 数据，这样会极大的提高 broker 性能，推荐设置为异步。 serializer.class kafka.serializer.DefaultEncoder 序列号类，.默认序列化成 byte[] 。 key.serializer.class Key 的序列化类，默认同上。 partitioner.class kafka.producer.DefaultPartitioner Partition 类，默认对 key 进行 hash。 compression.codec none 指定 producer 消息的压缩格式，可选参数为： “none”, “gzip” and “snappy”。关于压缩参见 4.1 节 compressed.topics null 启用压缩的 topic 名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的 topic 有效，若本参数为空，则对所有 topic 有效。 message.send.max.retries 3 Producer 发送失败时重试次数。若网络出现问题，可能会导致不断重试。 retry.backoff.ms 100 Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata. topic.metadata.refresh.interval.ms 600 * 1000 The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). If you set this to a negative value, metadata will only get refreshed on failure. If you set this to zero, the metadata will get refreshed after each message sent (not recommended). Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed queue.buffering.max.ms 5000 启用异步模式时，producer 缓存消息的时间。比如我们设置成 1000 时，它会缓存 1 秒的数据再一次发送出去，这样可以极大的增加 broker 吞吐量，但也会造成时效性的降低。 queue.buffering.max.messages 10000 采用异步模式时 producer buffer 队列里最大缓存的消息数量，如果超过这个数值，producer 就会阻塞或者丢掉消息。 queue.enqueue.timeout.ms -1 当达到上面参数值时 producer 阻塞等待的时间。如果值设置为 0，buffer 队列满时 producer 不会阻塞，消息直接被丢掉。若值设置为-1，producer 会被阻塞，不会丢消息。 batch.num.messages 200 采用异步模式时，一个 batch 缓存的消息数量。达到这个数量值时 producer 才会发送消息。 send.buffer.bytes 100 * 1024 Socket write buffer size client.id “” The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request. 7.3.3 Consumer 配置信息 属性 默认值 描述 group.id Consumer 的组 ID，相同 goup.id 的 consumer 属于同一个组。 zookeeper.connect Consumer 的 zookeeper 连接串，要和 broker 的配置一致。 consumer.id null 如果不设置会自动生成。 socket.timeout.ms 30 * 1000 网络请求的 socket 超时时间。实际超时时间由 max.fetch.wait + socket.timeout.ms 确定。 socket.receive.buffer.bytes 64 * 1024 The socket receive buffer for network requests. fetch.message.max.bytes 1024 * 1024 查询 topic-partition 时允许的最大消息大小。consumer 会为每个 partition 缓存此大小的消息到内存，因此，这个参数可以控制 consumer 的内存使用量。这个值应该至少比 server 允许的最大消息大小大，以免 producer 发送的消息大于 consumer 允许的消息。 num.consumer.fetchers 1 The number fetcher threads used to fetch data. auto.commit.enable true 如果此值设置为 true，consumer 会周期性的把当前消费的 offset 值保存到 zookeeper。当 consumer 失败重启之后将会使用此值作为新开始消费的值。 auto.commit.interval.ms 60 * 1000 Consumer 提交 offset 值到 zookeeper 的周期。 queued.max.message.chunks 2 用来被 consumer 消费的 message chunks 数量， 每个 chunk 可以缓存 fetch.message.max.bytes 大小的数据量。 auto.commit.interval.ms 60 * 1000 Consumer 提交 offset 值到 zookeeper 的周期。 queued.max.message.chunks 2 用来被 consumer 消费的 message chunks 数量， 每个 chunk 可以缓存 fetch.message.max.bytes 大小的数据量。 fetch.min.bytes 1 The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. fetch.wait.max.ms 100 The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy fetch.min.bytes. rebalance.backoff.ms 2000 Backoff time between retries during rebalance. refresh.leader.backoff.ms 200 Backoff time to wait before trying to determine the leader of a partition that has just lost its leader. auto.offset.reset largest What to do when there is no initial offset in ZooKeeper or if an offset is out of range ;smallest : automatically reset the offset to the smallest offset; largest : automatically reset the offset to the largest offset;anything else: throw exception to the consumer consumer.timeout.ms -1 若在指定时间内没有消息消费，consumer 将会抛出异常。 exclude.internal.topics true Whether messages from internal topics (such as offsets) should be exposed to the consumer. zookeeper.session.timeout.ms 6000 ZooKeeper session timeout. If the consumer fails to heartbeat to ZooKeeper for this period of time it is considered dead and a rebalance will occur. zookeeper.connection.timeout.ms 6000 The max time that the client waits while establishing a connection to zookeeper. zookeeper.sync.time.ms 2000 How far a ZK follower can be behind a ZK leader Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 16:31:26 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/":{"url":"distributed/mongo/","title":"mongo文档数据库","keywords":"","body":"mongo文档数据库 1.准备数据 2.快速上手 3.企业应用 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:51:46 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/mongoDb-data.html":{"url":"distributed/mongo/mongoDb-data.html","title":"1.准备数据","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.职工信息 2. 学生信息 3. 学生科目 4. 课程项目 1.职工信息 db.emp.insert([ {_id:1101,name:'鲁班' ,job:'讲师' ,dep:'讲师部',salary:10000}, {_id:1102,name:'悟空' ,job:'讲师' ,dep:'讲师部',salary:10000}, {_id:1103,name:'诸葛' ,job:'讲师' ,dep:'讲师部',salary:10000}, {_id:1105,name:'赵云' ,job:'讲师' ,dep:'讲师部',salary:8000}, {_id:1106,name:'韩信',job:'校长' ,dep:'校办',salary:20000}, {_id:1107,name:'貂蝉' ,job:'班主任' ,dep:'客服部',salary:8000}, {_id:1108,name:'安其' ,job:'班主任' ,dep:'客服部',salary:8000}, {_id:1109,name:'李白' ,job:'教务' ,dep:'教务处',salary:8000}, {_id:1110,name:'默子' ,job:'教务',dep:'教务处',salary:8000}, {_id:1111,name:'大乔',job:'助教' ,dep:'客服部',salary:5000}, {_id:1112,name:'小乔' ,job:'助教' ,dep:'客服部',salary:3000}, ]); 2. 学生信息 db.student.insertMany([ {_id:\"001\",name:\"陈霸天\",age:5,grade:{redis:87,zookeper:85,dubbo:90}}, {_id:\"002\",name:\"张明明\",age:3,grade:{redis:86,zookeper:82,dubbo:59}}, {_id:\"003\",name:\"肖炎炎\",age:2,grade:{redis:81,zookeper:94,dubbo:88}}, {_id:\"004\",name:\"李鬼才\",age:6,grade:{redis:48,zookeper:87,dubbo:48}} ]) 3. 学生科目 db.subject.insertMany([ {_id:\"001\",name:\"陈霸天\",subjects:[\"redis\",\"zookeper\",\"dubbo\"]}, {_id:\"002\",name:\"张明明\",subjects:[\"redis\",\"Java\",\"mySql\"]}, {_id:\"003\",name:\"肖炎炎\",subjects:[\"mySql\",\"zookeper\",\"bootstrap\"]}, {_id:\"004\",name:\"李鬼才\",subjects:[\"Java\",\"dubbo\",\"Java\"]}, ]) db.subject2.insertMany([ {_id:\"001\",name:\"陈霸天\",subjects:[{name:\"redis\",hour:12},{name:\"dubbo\",hour:120},{name:\"zookeper\",hour:56}]}, {_id:\"002\",name:\"张明明\",subjects:[{name:\"java\",hour:120},{name:\"mysql\",hour:10},{name:\"oracle\",hour:30}]}, {_id:\"003\",name:\"肖炎炎\",subjects:[{name:\"mysql\",hour:12},{name:\"html5\",hour:120},{name:\"netty\",hour:56}]}, {_id:\"004\",name:\"李鬼才\",subjects:[{name:\"redis\",hour:12},{name:\"dubbo\",hour:120},{name:\"netty\",hour:56}]} ]) 4. 课程项目 db.project.insert([ { _id: 1, name: \"Java Script\", description: \"name is js and jquery\" }, { _id: 2, name: \"Git\", description: \"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency\" }, { _id: 3, name: \"Apache dubbo\", description: \"Apache Dubbo is a high-performance, java based open source RPC framework.阿里 开源 项目\" }, { _id: 4, name: \"Redis\", description: \"Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures\" }, { _id: 5, name: \"Apache ZooKeeper\", description: \"Apache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination\" } ]) Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/mongoDb-quick-start.html":{"url":"distributed/mongo/mongoDb-quick-start.html","title":"2.快速上手","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、MongoDb的体系结构 1、NoSql的概念 2、NoSql的应用场景 3、MongoDb的逻辑组成 二、MongoDb安装配置与基础命令 1.下载地址 2.mongoDb启动参数说明 3.客户端Shell 的使用及参数说明 4.数据库与集合的基础操作 三、MongoDB CRUD与全文索引 2、数据的查询 排序与分页： 修改 3、数据的修改与删除 4、全文索引 大纲： 1、MongoDb的体系结构 2、MongoDb安装配置与基础命令 3、MongoDB CRUD与全文索引 一、MongoDb的体系结构 概要： NoSql的概念 NoSql的应用场景 MongoDb的逻辑组成 1、NoSql的概念 NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是 [SQL](https://baike.baidu.com/item/SQL) ”，互联网的早期我们的数据大多以关系型数据库来存储的。其特点是规范的数据结构（预定义模式）、强一至性、表与表之间通过外键进行关联，这些特征使我们对数据的管理更加清晰和严谨，但随着互联网的发展数据成爆炸式的增长我们对数据库需要更好的灵活性和更快的速度。这就是NoSql可以做到的。它不需要预先定义模式，没有主外键关联、支持分片、支持复本。 NoSql的分类： 键值(Key-Value)存储数据库 这一类数据库主要会使用到一个哈希表，这个表中有一个特定的键和一个指针指向特定的数据。Key/value模型对于IT系统来说的优势在于简单、易部署。但是如果DBA只对部分值进行查询或更新的时候，Key/value就显得效率低下了。举例如：Tokyo Cabinet/Tyrant, Redis, Voldemort, Oracle BDB. 列存储数据库。 这部分数据库通常是用来应对分布式存储的海量数据。键仍然存在，但是它们的特点是指向了多个列。这些列是由列家族来安排的。如：Cassandra, HBase, Riak. 文档型数据库 文档型数据库的灵感是来自于Lotus Notes办公软件的，而且它同第一种键值存储相类似。该类型的数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可 以看作是键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高。如：CouchDB, MongoDb. 国内也有文档型数据库SequoiaDB，已经开源。 图形(Graph)数据库 图形结构的数据库同其他行列以及刚性结构的SQL数据库不同，它是使用灵活的图形模型，并且能够扩展到多个服务器上。NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型。许多NoSQL数据库都有REST式的数据接口或者查询API。如：Neo4J, InfoGrid, Infinite Graph. 2、NoSql的应用场景 NoSQL数据库在以下的这几种情况下比较适用： 1、数据模型比较简单； 2、需要灵活性更强的IT系统； 3、对数据库性能要求较高； 4、不需要高度的数据一致性； [ ] 基于豆瓣电影举例说明NoSQL的应用场景 [ ] 电影基本信息分析 [ ] 电影与明星关系存储 3、MongoDb的逻辑组成 体系结构： 逻辑结构与关系数据库的对比： | 关系型数据库 | MongoDb | |:----|:----| | database(数据库) | database（数据库） | | table （表） | collection（ 集合） | | row（ 行） | document（ BSON 文档） | | column （列） | field （字段） | | index（唯一索引、主键索引） | index （全文索引） | | join （主外键关联） | embedded Document (嵌套文档) | | primary key(指定1至N个列做主键) | primary key (指定_id field做为主键) | | aggreation(groupy) | aggreation (pipeline mapReduce) | 二、MongoDb安装配置与基础命令 概要： mongoDb版本说明 mongoDb启动参数说明 客户端Shell 的使用及参数说明 数据库与集合的基础操作 mongoDb社区版说明 1.下载地址 https://www.mongodb.com/download-center/community #下载 wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.5.tgz # 解压 tar -zxvf mongodb-linux-x86_64-4.0.5.tgz 2.mongoDb启动参数说明 mongoDb 由C++编写，下载下来的包可以直接启动 #创建数据库目录 mkdir -p /data/mongo # 启动mongo ./bin/mongod --dbpath=/data/mongo/ 常规参数 | 参数 | 说明 | |:----|:----| | dbpath | 数据库目录，默认/data/db | | bind_ip | 监听IP地址，默认全部可以访问 | | port | 监听的端口，默认27017 | | logpath | 日志路径 | | logappend | 是否追加日志 | | auth | 是开启用户密码登陆 | | fork | 是否已后台启动的方式登陆 | | config | 指定配置文件 | 配置文件示例 vim mongo.conf 内容： dbpath=/data/mongo/ port=27017 bind_ip=0.0.0.0 fork=true logpath = /data/mongo/mongodb.log logappend = true auth=false 已配置文件方式启动 ./bin/mongod -f mongo.conf 3.客户端Shell 的使用及参数说明 #启动客户端 连接 本机的地的默认端口 ./bin/mongo # 指定IP和端口 ./bin/mongo --host=127.0.0.1 --port=27017 mongo shell 是一个js 控台，可以执行js 相关运算如: > 1+1 2 > var i=123; > print(i) 123 > 4.数据库与集合的基础操作 #查看数据库 show dbs; #切换数据库 use luban; #创建数据库与集合，在插入数据时会自动 创建数据库与集和 db.friend.insertOne({name:\"wukong\"，sex:\"man\"}); #查看集合 show tables; show collections; #删除集合 db.friend.drop(); #删除数据库 db.dropDatabase(); 三、MongoDB CRUD与全文索引 概要： 数据的新增的方式 数据的查询 数据的修改删除 全文索引查询 数据的新增的方式 关于Mongodb数据插入的说明 数据库的新增不需要序先设计模型结构，插入数据时会自动创建。 同一个集合中不同数据字段结构可以不一样 插入相关方法： //插入单条 db.friend.insertOne({name:\"wukong\"，sex:\"man\"}); // 插入多条 db.friend.insertMany([ {name:\"wukong\",sex:\"man\"},{name:\"diaocan\",sex:\"woman\",age:18,birthday:new Date(\"1995-11-02\")},{name:\"zixiao\",sex:\"woman\"} ]); // 指定ID db.friend.insert([ {_id:1,name:\"wokong\",sex:\"man\",age:1}, {_id:2,name:\"diaocan\",sex:\"women\",birthday:new Date(\"1988-11- 11\")} ]) 2、数据的查询 概要： 基于条件的基础查询 $and、$or、$in、$gt、$gte、$lt、$lte 运算符 基于 sort skip limit 方法实现排序与分页 嵌套查询 数组查询 数组嵌套查询 基础查询： #基于ID查找 db.emp.find({_id:1101}) #基于属性查找 db.emp.find({\"name\":\"鲁班\"}) # && 运算 与大于 运算 db.emp.find({\"job\":\"讲师\",\"salary\":{$gt:8000}}) # in 运算 db.emp.find({\"job\":{$in:[\"讲师\",\"客服部\"]}}) # or 运算 db.emp.find({$or:[{job:\"讲师\" },{job:\"客服部\"}] }) 排序与分页： // sort skip limit db.emp.find().sort({dep:1,salary:-1}).skip(5).limit(2) 嵌套查询： # 错误示例：无结果 db.student.find({grade:{redis:87,dubbo:90 }); #错误示例：无结果 db.student.find({grade:{redis:87,dubbo:90,zookeper:85} }) # 基于复合属性查找 时必须包含其所有的值 并且顺序一至 db.student.find({grade:{redis:87,zookeper:85,dubbo:90} }) #基于复合属性当中的指定值 查找。注：名称必须用双引号 db.student.find({\"grade.redis\":87}); db.student.find({\"grade.redis\":{\"$gt\":80}}); 数组查询： db.subject.insertMany([ {_id:\"001\",name:\"陈霸天\",subjects:[\"redis\",\"zookeper\",\"dubbo\"]}, {_id:\"002\",name:\"张明明\",subjects:[\"redis\",\"Java\",\"mySql\"]}, {_id:\"003\",name:\"肖炎炎\",subjects:[\"mySql\",\"zookeper\",\"bootstrap\"]}, {_id:\"004\",name:\"李鬼才\",subjects:[\"Java\",\"dubbo\",\"Java\"]}, ]) #无结果 db.subject.find({subjects:[\"redis\",\"zookeper\"]}) #无结果 db.subject.find({subjects:[\"zookeper\",\"redis\",\"dubbo\"]}) # 与嵌套查询一样，必须是所有的值 并且顺序一至 db.subject.find({subjects:[\"redis\",\"zookeper\",\"dubbo\"]}) # $all 匹配数组中包含该两项的值。注：顺序不作要求 db.subject.find({subjects:{\"$all\": [\"redis\",\"zookeper\"]}}) 注： # 简化数组查询 db.subject.find({subjects:\"redis\"}) # 简化数组查询 ，匹配数组中存在任意一值。与$all相对应 db.subject.find({subjects:{$in: [\"redis\",\"zookeper\"]}}) 数组嵌套查询： #基础查询 ，必须查询全部，且顺序一至 db.subject2.find({subjects:{name:\"redis\",hour:12} }) #指定查询第一个数组 课时大于12 db.subject2.find({\"subjects.0.hour\":{$gt:12}}) #查询任科目 课时大于12 db.subject2.find({\"subjects.hour\":{$gt:12}}) # $elemMatch 元素匹配，指定属性满足，且不要求顺序一至 db.subject2.find({subjects:{$elemMatch:{name:\"redis\",hour:12}}}) # 数组中任意元素匹配 不限定在同一个对象当中 db.subject2.find({\"subjects.name\":\"mysql\",\"subjects.hour\":120}) 修改 #设置值 db.emp.update({_id:1101} ,{ $set:{salary:10300} }) #自增 db.emp.update({_id:1101} ,{ $inc:{salary:200}}) #基于条件 更新多条数据 # 只会更新第一条 db.emp.update({\"dep\":\"客服部\"},{$inc:{salary:100}}) # 更新所有 匹配的条件 db.emp.updateMany({\"dep\":\"客服部\"},{$inc:{salary:100}}) 3、数据的修改与删除 修改 #设置值 db.emp.update({_id:1101} ,{ $set:{salary:10300} }) #自增 db.emp.update({_id:1101} ,{ $inc:{salary:200}}) #基于条件 更新多条数据 # 只会更新第一条 db.emp.update({\"dep\":\"客服部\"},{$inc:{salary:100}}) # 更新所有 匹配的条件 db.emp.updateMany({\"dep\":\"客服部\"},{$inc:{salary:100}}) 删除： // 基于查找删除 db.emp.deleteOne({_id:1101}) // 删除整个集合 db.project.drop() // 删除库 db.dropDatabase() 4、全文索引 索引的创建 db.project.createIndex({name:\"text\",description:\"text\"}) 基于索引分词进行查询 db.project.find({$text:{$search:\"java jquery\"}}) 基于索引 短语 db.project.find({$text:{$search:\"\\\"Apache ZooKeeper\\\"\"}}) 过滤指定单词 db.project.find({$text:{$search:\"java apache -阿里\"}}) 查看执行计划 db.project.find({$text:{$search:\"java -阿里\"}}).explain(\"executionStats\") Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 09:13:11 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/mongoDb-enterprise.html":{"url":"distributed/mongo/mongoDb-enterprise.html","title":"3.企业应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、 mongoDB的聚合操作 1.pipeline 聚合 2.mapRedurce 聚合 3.在聚合中使用索引 二、mongodb 的主从复制机制 1.复制集群的架构 2.复制集群搭建基础示例 子节点配置2 3.复制集群选举操作 三、mongodb 中的分片机制 1.为什么需要分片？ 1.mongodb 中的分片架构 2.分片示例流程： 2.1节点1 config1-37017.conf 2.2节点2 config2-37018.conf 2.3配置 shard 节点集群 2.4节点 route-27017.conf 四、用户管理与数据集验证 概要： mongoDB的聚合操作 mongodb 集群：复制 mongodb 集群：分片 一、 mongoDB的聚合操作 知识点： pipeline 聚合 mapRedurce 聚合 在聚合中使用索引 1.pipeline 聚合 pipeline相关运算符： $match ：匹配过滤聚合的数据 $project：返回需要聚合的字段 $group：统计聚合数据 示例： # $match 与 $project使用 db.emp.aggregate( {$match:{\"dep\":{$eq:\"客服部\"}}}, {$project:{name:1,dep:1,salary:1}} ); # $group 与 $sum 使用 db.emp.aggregate( {$project:{dep:1,salary:1}}, {$group:{\"_id\":\"$dep\",total:{$sum:\"$salary\"}}} ); # 低于4000 忽略 db.emp.aggregate( {$match:{salary:{$gt:4000}}}, {$project:{dep:1,salary:1}}, {$group:{\"_id\":\"$dep\",total:{$sum:\"$salary\"}}} ); # 基于多个字段 进行组合group 部门+职位进行统计 db.emp.aggregate( {$project:{dep:1,job:1,salary:1}}, {$group:{\"_id\":{\"dep\":\"$dep\",\"job\":\"$job\"},total:{$sum:\"$salary\"}}} ); 二次过滤 db.emp.aggregate( {$project:{dep:1,job:1,salary:1}}, {$group:{\"_id\":{\"dep\":\"$dep\",\"job\":\"$job\"},total:{$sum:\"$salary\"}}}， {$match:{\"$total\":{$gt:10000}}} ); 2.mapRedurce 聚合 mapRedurce 说明： 为什么需要 MapReduce？ (1) 海量数据在单机上处理因为硬件资源限制，无法胜任 (2) 而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 (3) 引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理 mongodb中mapRedurce的使用流程 创建Map函数， 创建Redurce函数 将map、Redurce 函数添加至集合中，并返回新的结果集 查询新的结果集 示例操作 // 创建map 对象 var map1=function (){ emit(this.job,1); } // 创建reduce 对象 var reduce1=function(job,count){ return Array.sum(count); } // 执行mapReduce 任务 并将结果放到新的集合 result 当中 db.emp.mapReduce(map1,reduce1,{out:\"result\"}) // 查询新的集合 db.result.find() # 使用复合对象作为key var map2=function (){ emit({\"job\":this.job,\"dep\":this.dep},1); } var reduce2=function(key,values){ return values.length; } db.emp.mapReduce(map2,reduce2,{out:\"result2\"}).find() mapRedurce的原理 在map函数中使用emit函数添加指定的 key 与Value ，相同的key 将会发给Redurce进行聚合操作，所以Redurce函数中第二个参数 就是 所有集的数组。return 的显示就是聚合要显示的值。 3.在聚合中使用索引 通过$Math内 可以包合对$text 的运算 示例： db.project.aggregate( {$match:{$text:{$search:\"apache\"}}}, {$project:{\"name\":1,\"price\":1}}, {$group:{_id:\"$name\",price:{$sum:\"$price\"}}} ) 关于索引 除了全文索引之外，还有单键索引。即整个字段的值作为索引。单键索引用值1和-1表示，分别代表正序和降序索引。 示例： de 创建单键索引 db.emp.createIndex({\"dep\":1}) 查看基于索引的执行计划 db.emp.find({\"dep\":\"客服部\"}).explain() 除了单键索引外还可以创建联合索引如下： db.emp.createIndex({\"dep\":1,\"job\":-1}) 查看 复合索引的执行计划 db.emp.find({\"dep\":\"ddd\"}).explain() 查看索引在排序当中的使用 db.emp.find().sort({\"job\":-1,\"dep\":1}).explain() 二、mongodb 的主从复制机制 知识点： 复制集群的架构 复制集群搭建 复制集群的选举配置 1.复制集群的架构 2.复制集群搭建基础示例 主节点配置 dbpath=/data/mongo/master port=27017 fork=true logpath=master.log replSet=tulingCluster 从节点配置 dbpath=/data/mongo/slave port=27018 fork=true logpath=slave.log replSet=tulingCluster 子节点配置2 dbpath=/data/mongo/slave2 port=27019 fork=true logpath=slave2.log replSet=tulingCluster [ ] 分别启动三个节点 [ ] 进入其中一个节点 集群复制配置管理 #查看复制集群的帮助方法 rs.help() 添加配置 // 声明配置变量 var cfg ={\"_id\":\"tulingCluster\", \"members\":[ {\"_id\":0,\"host\":\"127.0.0.1:27017\"}, {\"_id\":1,\"host\":\"127.0.0.1:27018\"} ] } // 初始化配置 rs.initiate(cfg) // 查看集群状态 rs.status() 变更节点示例： // 插入新的复制节点 rs.add(\"127.0.0.1:27019\") // 删除slave 节点 rs.remove(\"127.0.0.1:27019\") [ ] 演示复制状态 [ ] 进入主节点客户端 [ ] 插入数据 [ ] 进入从节点查看数据 [ ] 尝试在从节点下插入数据 注：默认节点下从节点不能读取数据。调用 rs.slaveOk() 解决。 3.复制集群选举操作 为了保证高可用，在集群当中如果主节点挂掉后，会自动 在从节点中选举一个 重新做为主节点。 [ ] 演示节点的切换操作 [ ] kill 主节点 [ ] 进入从节点查看集群状态 。rs.status() 选举的原理： 在mongodb 中通过在 集群配置中的 rs.属性值大小来决定选举谁做为主节点，通时也可以设置arbiterOnly 为true 表示 做为裁判节点用于执行选举操作，该配置下的节点 永远不会被选举为主节点和从节点。 示例： 重新配置节点 var cfg ={\"_id\":\"tulingCluster\", \"protocolVersion\" : 1, \"members\":[ {\"_id\":0,\"host\":\"127.0.0.1:27017\",\"priority\":10}, {\"_id\":1,\"host\":\"127.0.0.1:27018\",\"priority\":2}, {\"_id\":2,\"host\":\"127.0.0.1:27019\",\"arbiterOnly\":true} ] } // 重新装载配置，并重新生成集群节点。 rs.reconfig(cfg) //重新查看集群状态 rs.status() 节点说明： PRIMARY 节点： 可以查询和新增数据 SECONDARY 节点：只能查询 不能新增 基于priority 权重可以被选为主节点 RBITER 节点： 不能查询数据 和新增数据 ，不能变成主节点 三、mongodb 中的分片机制 知识点： 分片的概念 mongodb 中的分片架构 分片示例 1.为什么需要分片？ 随着数据的增长，单机实例的瓶颈是很明显的。可以通过复制的机制应对压力，但mongodb中单个集群的 节点数量限制到了12个以内，所以需要通过分片进一步横向扩展。此外分片也可节约磁盘的存储。 1.mongodb 中的分片架构 分片中的节点说明： 路由节点(mongos)：用于分发用户的请求，起到反向代理的作用。 配置节点(config)：用于存储分片的元数据信息，路由节基于元数据信息 决定把请求发给哪个分片。（3.4版本之后，该节点，必须使用复制集。） 分片节点(shard):用于实际存储的节点，其每个数据块默认为64M，满了之后就会产生新的数据库。 2.分片示例流程： 配置 并启动config 节点集群 配置集群信息 配置并启动2个shard 节点 配置并启动路由节点 添加shard 节点 添加shard 数据库 添加shard 集合 插入测试数据 检查数据的分布 插入大批量数据查看shard 分布 设置shard 数据块为一M 插入10万条数据 配置 并启动config 节点集群 2.1节点1 config1-37017.conf dbpath=/data/mongo/config1 port=37017 fork=true logpath=logs/config1.log replSet=configCluster configsvr=true 2.2节点2 config2-37018.conf dbpath=/data/mongo/config2 port=37018 fork=true logpath=logs/config2.log replSet=configCluster configsvr=true 进入shell 并添加 config 集群配置： var cfg ={\"_id\":\"configCluster\", \"protocolVersion\" : 1, \"members\":[ {\"_id\":0,\"host\":\"127.0.0.1:37017\"}, {\"_id\":1,\"host\":\"127.0.0.1:37018\"} ] } // 重新装载配置，并重新生成集群。 rs.initiate(cfg) 2.3配置 shard 节点集群 # 节点1 shard1-47017.conf dbpath=/data/mongo/shard1 port=47017 fork=true logpath=logs/shard1.log shardsvr=true # 节点2 shard2-47018.conf dbpath=/data/mongo/shard2 port=47018 fork=true logpath=logs/shard2.log shardsvr=true 配置 路由节点 mongos ============== 2.4节点 route-27017.conf port=27017 bind_ip=0.0.0.0 fork=true logpath=logs/route.log configdb=configCluster/127.0.0.1:37017,127.0.0.1:37018 // 添加分片节点 sh.status() sh.addShard(\"127.0.0.1:47017\"); sh.addShard(\"127.0.0.1:47018\"); 为数据库开启分片功能 sh.enableSharding(\"tuling\") 为指定集合开启分片功能 sh.shardCollection(\"tuling.emp\",{\"_id\":1}) 修改分片大小 use config db.settings.find() db.settings.save({_id:\"chunksize\",value:1}) 尝试插入1万条数据： for(var i=1;i 四、用户管理与数据集验证 // 创建管理员用户 use admin; db.createUser({\"user\":\"admin\",\"pwd\":\"123456\",\"roles\":[\"root\"]}) #验证用户信息 db.auth(\"admin\",\"123456\") #查看用户信息 db.getUsers() # 修改密码 db.changeUserPassword(\"admin\",\"123456\") 以auth 方式启动mongod，需要添加auth=true 参数 ，mongdb 的权限体系才会起作用： #以auth 方向启动mongod （也可以在mongo.conf 中添加auth=true 参数） ./bin/mongod -f conf/mongo.conf --auth # 验证用户 use admin; db.auth(\"admin\",\"123456\") 创建只读用户 db.createUser({\"user\":\"dev\",\"pwd\":\"123456\",\"roles\":[\"read\"]}) 重新登陆 验证用户权限 use luban ; db.auth(\"dev\",\"123456\") [ ] 演示查看数据 [ ] 演示插入数据 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 15:21:54 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/":{"url":"distributed/redis/","title":"redis缓存","keywords":"","body":"redis缓存 1.特性介绍 2.集群部署 3.原理分析 4.源码分析 5.Redis6.0 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-20 17:24:22 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/特性介绍.html":{"url":"distributed/redis/特性介绍.html","title":"1.特性介绍","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 基础数据结构 string (字符串) list (列表) hash (字典) set (集合) zset (有序集合) 核心原理 Redis的单线程和高性能 持久化 RDB快照（snapshot） AOF（append-only file） RDB 和 AOF ，我应该用哪一个？ Redis 4.0 混合持久化 缓存淘汰策略 Redis基础数据结构与核心原理 基础数据结构 Redis 有 5 种基础数据结构，分别为：string (字符串)、list (列表)、set (集合)、hash (哈希) 和 zset (有序集合)。 string (字符串) 字符串 string 是 Redis 最简单的数据结构。Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。字符串结构使用非常广泛，一个常见的用途就是缓存用户信息。我们将用户信息结构体使用 JSON 序列化成字符串，然后将序列化后的字符串塞进 Redis 来缓存。同样，取用户信息会经过一次反序列化的过程。 原子计数：如果 value 值是一个整数，还可以对它进行自增操作。自增是有范围的，它的范围是 signed long 的最大最小值，超过了这个值，Redis 会报错 list (列表) Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)，这点让人非常意外。 当列表弹出了最后一个元素之后，该数据结构自动被删除，内存被回收。 Redis 的列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。 右边进左边出：队列 右边进右边出：栈 hash (字典) Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。 hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对 用户结构中的每个字段单独存储。这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取，这样就会比较浪费网络流量。 hash 也有缺点，hash 结构的存储消耗要高于单个字符串，到底该使用 hash 还是字符串，需要根据实际情况再三权衡。 set (集合) Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值NULL。 当集合中最后一个元素移除之后，数据结构自动删除，内存被回收。 zset (有序集合) zset 似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。 zset 可以用来存粉丝列表，value 值是粉丝的用户 ID，score 是关注时间。我们可以对粉丝列表按关注时间进行排序。 zset 还可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。 其他高级命令 keys：全量遍历键，用来列出所有满足特定正则字符串规则的key，当redis数据量比较大时，性能比较差，要避免使用 scan：渐进式遍历键，scan 参数提供了三个参数，第一个是 cursor 整数值，第二个是 key 的正则模式，第三个是遍历的 limit hint。第一次遍历时，cursor 值为 0，然后将返回结果中第一个整数值作为下一次遍历的 cursor。一直遍历到返回的 cursor 值为 0 时结束。 Redis存储键值对实际使用的是hashtable的数据结构 Info：查看redis服务运行信息，分为 9 大块，每个块都有非常多的参数，这 9 个块分别是: Server 服务器运行的环境参数 Clients 客户端相关信息 Memory 服务器运行内存统计数据 Persistence 持久化信息 Stats 通用统计数据 Replication 主从复制相关信息 CPU CPU 使用情况 Cluster 集群信息 KeySpace 键值对统计数量信息 核心原理 Redis的单线程和高性能 Redis 单线程为什么还能这么快？ 因为它所有的数据都在内存中，所有的运算都是内存级别的运算，而且单线程避免了多线程的切换性能损耗问题。正因为 Redis 是单线程，所以要小心使用 Redis 指令，对于那些耗时的指令(比如keys)，一定要谨慎使用，一不小心就可能会导致 Redis 卡顿。 Redis 单线程如何处理那么多的并发客户端连接？ Redis的IO多路复用：redis利用epoll来实现IO多路复用，将连接信息和事件放到队列中，依次放到文件事件分派器，事件分派器将事件分发给事件处理器。 Nginx也是采用IO多路复用原理解决C10K问题 持久化 RDB快照（snapshot） 在默认情况下， Redis 将内存数据库快照保存在名字为 dump.rdb 的二进制文件中。 你可以对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。 比如说， 以下设置会让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集： # save 60 1000 AOF（append-only file） 快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化，将修改的每一条指令记录进文件 你可以通过修改配置文件来打开 AOF 功能： # appendonly yes 从现在开始， 每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。 这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 你可以配置 Redis 多久才将数据 fsync 到磁盘一次。 有三个选项： l 每次有新命令追加到 AOF 文件时就执行一次 fsync ：非常慢，也非常安全。 l 每秒 fsync 一次：足够快（和使用 RDB 持久化差不多），并且在故障时只会丢失 1 秒钟的数据。 l 从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。 RDB 和 AOF ，我应该用哪一个？ 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。 Redis 4.0 混合持久化 重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。 Redis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。AOF在重写(aof文件里可能有太多没用指令，所以aof会定期根据内存的最新数据生成aof文件)时将重写这一刻之前的内存rdb快照文件的内容和增量的 AOF修改内存数据的命令日志文件存在一起，都写入新的aof文件，新的文件一开始不叫appendonly.aof，等到重写完新的AOF文件才会进行改名，原子的覆盖原有的AOF文件，完成新旧两个AOF文件的替换； AOF根据配置规则在后台自动重写，也可以人为执行命令bgrewriteaof重写AOF。 于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。 开启混合持久化： # aof-use-rdb-preamble yes 混合持久化aof文件结构 缓存淘汰策略 当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)。交换会让 Redis 的性能急剧下降，对于访问量比较频繁的 Redis 来说，这样龟速的存取效率基本上等于不可用。 在生产环境中我们是不允许 Redis 出现交换行为的，为了限制最大使用内存，Redis 提供了配置参数 maxmemory 来限制内存超出期望大小。 当实际内存超出 maxmemory 时，Redis 提供了几种可选策略 (maxmemory-policy) 来让用户自己决定该如何腾出新的空间以继续提供读写服务。 noeviction 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru 尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random 跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random 跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-20 17:32:15 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/集群部署.html":{"url":"distributed/redis/集群部署.html","title":"2.集群部署","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 集群部署 1、Redis集群方案比较 1.1哨兵模式 1.2高可用集群模式 2、Redis高可用集群搭建 2.1redis安装 2.2redis集群搭建 2.3Java操作redis集群 3、Redis集群原理分析 3.1槽位定位算法 3.2跳转重定位 3.3网络抖动 集群部署 1、Redis集群方案比较 1.1哨兵模式 在redis3.0以前的版本要实现集群一般是借助哨兵sentinel工具来监控master节点的状态，如果master节点异常，则会做主从切换，将某一台slave作为master，哨兵的配置略微复杂，并且性能和高可用性等各方面表现一般，特别是在主从切换的瞬间存在访问瞬断的情况，而且哨兵模式只有一个主节点对外提供服务，没法支持很高的并发，且单个主节点内存也不宜设置得过大，否则会导致持久化文件过大，影响数据恢复或主从同步的效率 1.2高可用集群模式 redis集群是一个由多个主从节点群组成的分布式服务器群，它具有复制、高可用和分片特性。Redis集群不需要sentinel哨兵也能完成节点移除和故障转移的功能。需要将每个节点设置成集群模式，这种集群模式没有中心节点，可水平扩展，据官方文档称可以线性扩展到上万个节点(官方推荐不超过1000个节点)。redis集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单 2、Redis高可用集群搭建 2.1redis安装 下载地址：http://redis.io/download 安装步骤： 安装gcc yum install gcc 把下载好的redis-5.0.2.tar.gz放在/usr/local文件夹下，并解压 wget http://download.redis.io/releases/redis-5.0.2.tar.gz tar xzf redis-5.0.2.tar.gz cd redis-5.0.2 进入到解压好的redis-5.0.2目录下， 进行编译与安装 make & make install 启动并指定配置文件 src/redis-server redis.conf（注意要使用后台启动，所以修改redis.conf里的daemonize改为yes) 验证启动是否成功 ps -ef | grep redis 进入redis客户端 /usr/local/redis/bin/redis-cli # 退出客户端 quit 退出redis服务： （1）pkill redis-server （2）kill 进程号 （3）src/redis-cli shutdown 2.2redis集群搭建 redis集群需要至少要三个master节点，我们这里搭建三个master节点，并且给每个master再搭建一个slave节点，总共6个redis节点，这里用三台机器部署6个redis实例，每台机器一主一从，搭建集群的步骤如下： 第一步：在第一台机器的/usr/local 下创建文件夹 redis-cluster，然后在其下面分别创建 2 个文件夾如下: （1）mkdir -p /usr/local/redis-cluster （2）mkdir 8001、mkdir 8004 第二步：把之前的 redis.conf 配置文件 copy 到 8001 下，修改如下内容： （1）daemonize yes （2）port 8001（分别对每个机器的端口号进行设置） （3）dir /usr/local/redis-cluster/8001/（指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据） （4）cluster-enabled yes（启动集群模式） （5）cluster-config-file nodes-8001.conf（集群节点信息文件，这里 800x 最好和 port 对应上） （6）cluster-node-timeout 5000 (7) # bind 127.0.0.1（去掉 bind 绑定访问 ip 信息） (8) protected-mode no （关闭保护模式） （9）appendonly yes 如果要设置密码需要增加如下配置： （10）requirepass zhuge (设置 redis 访问密码) （11）masterauth zhuge (设置集群节点间访问密码，跟上面一致) 第三步：把修改后的配置文件，copy 到 8002，修改第 2、3、5 项里的端口号，可以用批量替换： :%s/源字符串/目的字符串/g 第四步：另外两台机器也需要做上面几步操作，第二台机器用 8002 和 8005，第三台机器用 8003 和 8006 第五步：分别启动 6 个 redis 实例，然后检查是否启动成功 （1）/usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/800*/redis.conf （2）ps -ef | grep redis 查看是否启动成功 第六步：用 redis-cli 创建整个 redis 集群(redis5 以前的版本集群是依靠 ruby 脚本 redis-trib.rb 实现)（1）/usr/local/redis-5.0.2/src/redis-cli -a zhuge --cluster create --cluster-replicas 1 192.168.0.61:8001 192.168.0.62:8002 192.168.0.63:8003 192.168.0.61:8004 192.168.0.62:8005 192.168.0.63:8006 代表为每个创建的主服务器节点创建一个从服务器节点第七步：验证集群： （1）连接任意一个客户端即可：./redis-cli -c -h -p (-a 访问服务端密码，-c 表示集群模式，指定 ip 地址和端口号）如：/usr/local/redis-5.0.2/src/redis-cli -a zhuge -c -h 192.168.0.61 -p 800* （2）进行验证：cluster info（查看集群信息）、cluster nodes（查看节点列表） （3）进行数据操作验证 （4）关闭集群则需要逐个进行关闭，使用命令： /usr/local/redis/bin/redis-cli -a zhuge -c -h 192.168.0.60 -p 800* shutdown 2.3Java操作redis集群 借助redis的java客户端jedis可以操作以上集群，引用jedis版本的maven坐标如下： redis.clients jedis 2.9.0 Java编写访问redis集群的代码非常简单，如下所示： import java.io.IOException; import java.util.HashSet; import java.util.Set; import redis.clients.jedis.HostAndPort; import redis.clients.jedis.JedisCluster; import redis.clients.jedis.JedisPoolConfig; /** * 访问redis集群 * */ public class RedisCluster { public static void main(String[] args) throws IOException { Set jedisClusterNode = new HashSet<>(); jedisClusterNode.add(new HostAndPort(\"192.168.0.61\", 8001)); jedisClusterNode.add(new HostAndPort(\"192.168.0.62\", 8002)); jedisClusterNode.add(new HostAndPort(\"192.168.0.63\", 8003)); jedisClusterNode.add(new HostAndPort(\"192.168.0.61\", 8004)); jedisClusterNode.add(new HostAndPort(\"192.168.0.62\", 8005)); jedisClusterNode.add(new HostAndPort(\"192.168.0.63\", 8006)); JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(100); config.setMaxIdle(10); config.setTestOnBorrow(true); //connectionTimeout：指的是连接一个url的连接等待时间 //soTimeout：指的是连接上一个url，获取response的返回等待时间 JedisCluster jedisCluster = new JedisCluster(jedisClusterNode, 6000, 5000, 10, \"test\", config); System.out.println(jedisCluster.set(\"student\", \"test\")); System.out.println(jedisCluster.set(\"age\", \"19\")); System.out.println(jedisCluster.get(\"student\")); System.out.println(jedisCluster.get(\"age\")); jedisCluster.close(); } } 运行效果如下： OK OK aaron 18 3、Redis集群原理分析 Redis Cluster 将所有数据划分为 16384 的 slots(槽位)，每个节点负责其中一部分槽位。槽位的信息存储于每个节点中，。 当 Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息并将其缓存在客户端本地。这样当客户端要查找某个 key 时，可以直接定位到目标节点。同时因为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实现槽位信息的校验调整。 3.1槽位定位算法 Cluster 默认会对 key 值使用 crc16 算法进行 hash 得到一个整数值，然后用这个整数值对 16384 进行取模来得到具体槽位。 HASH_SLOT = CRC16(key) mod 16384 3.2跳转重定位 当客户端向一个错误的节点发出了指令，该节点会发现指令的 key 所在的槽位并不归自己管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告诉客户端去连这个节点去获取数据。客户端收到指令后除了跳转到正确的节点上去操作，还会同步更新纠正本地的槽位映射表缓存，后续所有 key 将使用新的槽位映射表。 3.3网络抖动 真实世界的机房网络往往并不是风平浪静的，它们经常会发生各种各样的小问题。比如网络抖动就是非常常见的一种现象，突然之间部分连接变得不可访问，然后很快又恢复正常。 为解决这种问题，Redis Cluster 提供了一种选项cluster-node-timeout，表示当某个节点持续 timeout 的时间失联时，才可以认定该节点出现故障，需要进行主从切换。如果没有这个选项，网络抖动会导致主从频繁切换 (数据的重新复制)。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-20 17:41:41 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/原理分析.html":{"url":"distributed/redis/原理分析.html","title":"3.原理分析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1、启动集群 2、集群操作 2.1增加redis实例 2.2查看redis集群的命令帮助 2.3配置8007为集群主节点 2.4配置8008为8007的从节点 2.5删除8008从节点 2.6删除8007主节点 Redis集群选举原理分析 Redis高可用集群之水平扩展 Redis3.0以后的版本虽然有了集群功能，提供了比之前版本的哨兵模式更高的性能与可用性，但是集群的水平扩展却比较麻烦，今天就来带大家看看redis高可用集群如何做水平扩展，原始集群(见下图)由6个节点组成，6个节点分布在三台机器上，采用三主三从的模式 1、启动集群 # 启动整个集群 /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8001/redis.conf /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8002/redis.conf /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8003/redis.conf /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8004/redis.conf /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8005/redis.conf /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8006/redis.conf # 客户端连接8001端口的redis实例 /usr/local/redis-5.0.2/src/redis-cli -a test -c -h 192.168.0.61 -p 8001 查看集群状态 192.168.0.61:8001> cluster nodes 从上图可以看出，整个集群运行正常，三个master节点和三个slave节点，8001端口的实例节点存储0-5460这些hash槽，8002端口的实例节点存储5461-10922这些hash槽，8003端口的实例节点存储10923-16383这些hash槽，这三个master节点存储的所有hash槽组成redis集群的存储槽位，slave点是每个主节点的备份从节点，不显示存储槽位 2、集群操作 我们在原始集群基础上再增加一主(8007)一从(8008)，增加节点后的集群参见下图，新增节点用虚线框表示 2.1增加redis实例 # 在/usr/local/redis-cluster下创建8007和8008文件夹，并拷贝8001文件夹下的redis.conf文件到8007和8008这两个文件夹下 mkdir 8007 mkdir 8008 cd 8001 cp redis.conf /usr/local/redis-cluster/8007/ cp redis.conf /usr/local/redis-cluster/8008/ # 修改8007文件夹下的redis.conf配置文件 vim /usr/local/redis-cluster/8007/redis.conf # 修改如下内容： port:8007 dir /usr/local/redis-cluster/8007/ cluster-config-file nodes-8007.conf # 修改8008文件夹下的redis.conf配置文件 vim /usr/local/redis-cluster/8008/redis.conf 修改内容如下： port:8008 dir /usr/local/redis-cluster/8008/ cluster-config-file nodes8008.conf # 启动8007和8008俩个服务并查看服务状态 /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8007/redis.conf /usr/local/redis-5.0.2/src/redis-server /usr/local/redis-cluster/8008/redis.conf ps -el | grep redis 2.2查看redis集群的命令帮助 cd /usr/local/redis-5.0.2 src/redis-cli --cluster help 1.create：创建一个集群环境host1:port1 ... hostN:portN 2.call：可以执行redis命令 3.add-node：将一个节点添加到集群里，第一个参数为新节点的ip:port，第二个参数为集群中任意一个已经存在的节点的ip:port 4.del-node：移除一个节点 5.reshard：重新分片 6.check：检查集群状态 2.3配置8007为集群主节点 # 使用add-node命令新增一个主节点8007(master)，绿色为新增节点，红色为已知存在节点，看到日志最后有\"[OK] New node added correctly\"提示代表新节点加入成功 /usr/local/redis-5.0.2/src/redis-cli --cluster add-node 192.168.0.64:8007 192.168.0.61:8001 # 查看集群状态 注意：当添加节点成功以后，新增的节点不会有任何数据，因为它还没有分配任何的slot(hash槽)，我们需要为新节点手工分配hash槽 # 使用redis-cli命令为8007分配hash槽，找到集群中的任意一个主节点(红色位置表示集群中的任意一个主节点)，对其进行重新分片工作。 /usr/local/redis-5.0.2/src/redis-cli --cluster reshard 192.168.0.61:8001 输出如下： ... ... How many slots do you want to move (from 1 to 16384)? 600 (ps:需要多少个槽移动到新的节点上，自己设置，比如600个hash槽) What is the receiving node ID? eb57a5700ee6f9ff099b3ce0d03b1a50ff247c3c (ps:把这600个hash槽移动到哪个节点上去，需要指定节点id) Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node 1:all (ps:输入all为从所有主节点(8001,8002,8003)中分别抽取相应的槽数指定到新节点中，抽取的总槽数为600个) ... ... Do you want to proceed with the proposed reshard plan (yes/no)? yes (ps:输入yes确认开始执行分片任务) ... ... # 查看下最新的集群状态 如上图所示，现在我们的8007已经有hash槽了，也就是说可以在8007上进行读写数据啦！到此为止我们的8007已经加入到集群中，并且是主节点(Master) 2.4配置8008为8007的从节点 # 添加从节点8008到集群中去并查看集群状态 /usr/local/redis-5.0.2/src/redis-cli --cluster add-node 192.168.0.64:8008 192.168.0.61:8001 如图所示，还是一个master节点，没有被分配任何的hash槽。 # 我们需要执行replicate命令来指定当前节点(从节点)的主节点id为哪个,首先需要连接新加的8008节点的客户端，然后使用集群命令进行操作，把当前的8008(slave)节点指定到一个主节点下(这里使用之前创建的8007主节点，红色表示节点id) /usr/local/redis-5.0.2/src/redis-cli -c -h 192.168.0.64 -p 8008 192.168.0.61:8008> cluster replicate eb57a5700ee6f9ff099b3ce0d03b1a50ff247c3c # 查看集群状态，8008节点已成功添加为8007节点的从节点 2.5删除8008从节点 # 用del-node删除从节点8008，指定删除节点ip和端口，以及节点id(红色为8008节点id) /usr/local/redis-5.0.2/src/redis-cli --cluster del-node 192.168.0.64:8008 1805b6339d91b0e051f46845eebacb9bc43baefe # 再次查看集群状态，如下图所示，8008这个slave节点已经移除，并且该节点的redis服务也已被停止 2.6删除8007主节点 最后，我们尝试删除之前加入的主节点8007，这个步骤相对比较麻烦一些，因为主节点的里面是有分配了hash槽的，所以我们这里必须先把8007里的hash槽放入到其他的可用主节点中去，然后再进行移除节点操作，不然会出现数据丢失问题(目前只能把master的数据迁移到一个节点上，暂时做不了平均分配功能)，执行命令如下： /usr/local/redis-5.0.2/src/redis-cli --cluster reshard 192.168.0.64:8007 输出如下： ... ... How many slots do you want to move (from 1 to 16384)? 600 What is the receiving node ID? deedad3c34e8437baa6ff013fd3d1461a0c2e761 (ps:这里是需要把数据移动到哪？8001的主节点id) Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node 1:eb57a5700ee6f9ff099b3ce0d03b1a50ff247c3c (ps:这里是需要数据源，也就是我们的8007节点id) Source node 2:done (ps:这里直接输入done 开始生成迁移计划) ... ... Do you want to proceed with the proposed reshard plan (yes/no)? Yes (ps:这里输入yes开始迁移) 至此，我们已经成功的把8007主节点的数据迁移到8001上去了，我们可以看一下现在的集群状态如下图，你会发现8007下面已经没有任何hash槽了，证明迁移成功！ # 最后我们直接使用del-node命令删除8007主节点即可（红色表示8007的节点id）。 /usr/local/redis-5.0.2/src/redis-cli --cluster del-node 192.168.0.64:8007 eb57a5700ee6f9ff099b3ce0d03b1a50ff247c3c # 查看集群状态，一切还原为最初始状态啦！大功告成！ Redis集群选举原理分析 当slave发现自己的master变为FAIL状态时，便尝试进行Failover，以期成为新的master。由于挂掉的master可能会有多个slave，从而存在多个slave竞争成为master节点的过程， 其过程如下： 1.slave发现自己的master变为FAIL 2.将自己记录的集群currentEpoch加1，并广播FAILOVER_AUTH_REQUEST 信息 3.其他节点收到该信息，只有master响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每一个epoch只发送一次ack 4.尝试failover的slave收集FAILOVER_AUTH_ACK 5.超过半数后变成新Master 6.广播Pong通知其他集群节点。 从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一定延迟，一定的延迟确保我们等待FAIL状态在集群中传播，slave如果立即尝试选举，其它masters或许尚未意识到FAIL状态，可能会拒绝投票 •延迟计算公式： DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms •SLAVE_RANK表示此slave已经从master复制数据的总量的rank。Rank越小代表已复制的数据越新。这种方式下，持有最新数据的slave将会首先发起选举（理论上）。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-20 17:42:44 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/源码分析.html":{"url":"distributed/redis/源码分析.html","title":"4.源码分析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.客户端&快速入门 2.API介绍 3.源码分析 Redis客户端详解与源码分析 1.客户端&快速入门 Pom依赖 redis.clients jedis 2.10.0 jar Jedis jedis = new Jedis(\"localhost\");``jedis.set(\"foo\", \"bar\");String value = jedis.get(\"foo\"); 2.API介绍 Redis api 使用过程 多练手，自己看了 Put get 原理 3.源码分析 套路： 宏观：操作redis api 网络请求 微观 ：debug一行行的看代码 画图 RESP:redis序列化协议 · For Simple Strings the first byte of the reply is \"+\" · For Errors the first byte of the reply is \"-\" · For Integers the first byte of the reply is \":\" · For Bulk Strings the first byte of the reply is \"$\" · For Arrays the first byte of the reply is \"*\" Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-20 16:28:37 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/redis6.html":{"url":"distributed/redis/redis6.html","title":"5.Redis6.0","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.NoSQL数据库简介 1.1技术发展 1.1.1Web1.0时代 1.1.2Web2.0时代 1.1.3解决CPU及内存压力 1.1.4解决IO压力 1.2NoSQL数据库 1.2.1NoSQL数据库概述 1.2.2NoSQL适用场景 1.2.3NoSQL不适用场景 1.2.4Memcache 1.2.5Redis 1.2.6MongoDB 1.3行式存储数据库（大数据时代） 1.3.1行式数据库 1.3.2列式数据库 1.4图关系型数据库 1.5DB-Engines 数据库排名 2.Redis概述安装 2.1应用场景 2.2Redis安装 2.2.1安装版本 2.2.2安装步骤 2.2.3安装目录：/usr/local/bin 2.2.4前台启动（不推荐） 2.2.5后台启动（推荐） 3.常用五大数据类型 3.1Redis键(key) 3.2Redis字符串(String) 3.3Redis列表(List) 3.4Redis集合(Set) 3.5Redis哈希(Hash) 3.6Redis有序集合Zset(sorted set) 4.Redis配置文件介绍 Units单位### INCLUDES包含### 网络相关配置 ### GENERAL通用### SECURITY安全### 5.Redis的发布和订阅 6.Redis新数据类型 7.Redis_Jedis_测试 8.Redis_Jedis_实例 9.Redis与Spring Boot整合 10.Redis_事务锁机制秒杀 11.Redis_事务_秒杀案例 12.Redis持久化之RDB SNAPSHOTTING快照### 13.Redis持久化之AOF 14.Redis_主从复制 15.Redis集群 16.Redis应用问题解决 16.1缓存穿透 17.2缓存击穿 17.3缓存雪崩 17.4分布式锁 17.Redis6.0新功能 17.1ACL 17.2IO多线程 框架高级课程系列之Redis6 尚硅谷JavaEE教研组 1.NoSQL数据库简介 1.1技术发展 技术的分类 1、解决功能性的问题：Java、Jsp、RDBMS、Tomcat、HTML、Linux、JDBC、SVN 2、解决扩展性的问题：Struts、Spring、SpringMVC、Hibernate、Mybatis 3、解决性能的问题：NoSQL、Java线程、Hadoop、Nginx、MQ、ElasticSearch 1.1.1Web1.0时代 Web1.0的时代，数据访问量很有限，用一夫当关的高性能的单点服务器可以解决大部分问题。 1.1.2Web2.0时代 随着Web2.0的时代的到来，用户访问量大幅度提升，同时产生了大量的用户数据。加上后来的智能移动设备的普及，所有的互联网平台都面临了巨大的性能挑战。 1.1.3解决CPU及内存压力 1.1.4解决IO压力 1.2NoSQL数据库 1.2.1NoSQL数据库概述 NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是SQL”，泛指非关系型的数据库。 NoSQL 不依赖业务逻辑方式存储，而以简单的key-value模式存储。因此大大的增加了数据库的扩展能力。 不遵循SQL标准。 不支持ACID。 远超于SQL的性能。 1.2.2NoSQL适用场景 对数据高并发的读写 海量数据的读写 对数据高可扩展性的 1.2.3NoSQL不适用场景 需要事务支持 基于sql的结构化查询存储，处理复杂的关系,需要即席查询。 （用不着sql的和用了sql也不行的情况，请考虑用NoSql） 1.2.4Memcache 很早出现的NoSql数据库数据都在内存中，一般不持久化支持简单的key-value模式，支持类型单一一般是作为缓存数据库辅助持久化的数据库 1.2.5Redis 几乎覆盖了Memcached的绝大部分功能数据都在内存中，支持持久化，主要用作备份恢复除了支持简单的key-value模式，还支持多种数据结构的存储，比如list、set、hash、zset等。一般是作为缓存数据库辅助持久化的数据库 1.2.6MongoDB 高性能、开源、模式自由(schema free)的文档型数据库数据都在内存中，如果内存不足，把不常用的数据保存到硬盘虽然是key-value模式，但是对value（尤其是json）提供了丰富的查询功能支持二进制数据及大型对象可以根据数据的特点替代RDBMS，成为独立的数据库。或者配合RDBMS，存储特定的数据。 1.3行式存储数据库（大数据时代） 1.3.1行式数据库 1.3.2列式数据库 Hbase HBase是Hadoop项目中的数据库。它用于需要对大量的数据进行随机、实时的读写操作的场景中。 HBase的目标就是处理数据量非常庞大的表，可以用普通的计算机处理超过10亿行数据，还可处理有数百万列元素的数据表。 Cassandra[kəˈsændrə] Apache Cassandra是一款免费的开源NoSQL数据库，其设计目的在于管理由大量商用服务器构建起来的庞大集群上的海量数据集(数据量通常达到PB级别)。在众多显著特性当中，Cassandra最为卓越的长处是对写入及读取操作进行规模调整，而且其不强调主集群的设计思路能够以相对直观的方式简化各集群的创建与扩展流程。 计算机存储单位计算机存储单位一般用B，KB，MB，GB，TB，EB，ZB，YB，BB来表示，它们之间的关系是：位bit (比特)(Binary Digits)：存放一位二进制数，即0或1，最小的存储单位。字节byte：8个二进制位为一个字节(B)，最常用的单位。1KB (Kilobyte千字节)=1024B，1MB (Megabyte兆字节简称“兆”)=1024KB，1GB (Gigabyte吉字节又称“千兆”)=1024MB，1TB (Trillionbyte万亿字节太字节)=1024GB，其中1024=2^10 ( 2的10次方)，1PB（Petabyte千万亿字节拍字节）=1024TB，1EB（Exabyte百亿亿字节艾字节）=1024PB，1ZB (Zettabyte十万亿亿字节泽字节)= 1024 EB,1YB (Jottabyte一亿亿亿字节尧字节)= 1024 ZB,1BB (Brontobyte一千亿亿亿字节)= 1024 YB.注：“兆”为百万级数量单位。 1.4图关系型数据库 主要应用：社会关系，公共交通网络，地图及网络拓谱(n*(n-1)/2) 1.5DB-Engines 数据库排名 http://db-engines.com/en/ranking 2.Redis概述安装 Redis是一个开源的key-value存储系统。 和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set --有序集合)和hash（哈希类型）。 这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。 在此基础上，Redis支持各种不同方式的排序。 与memcached一样，为了保证效率，数据都是缓存在内存中。 区别的是Redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件。 并且在此基础上实现了master-slave(主从)同步。 2.1应用场景 配合关系型数据库做高速缓存 高频次，热门访问的数据，降低数据库IO 分布式架构，做session共享 多样的数据结构存储持久化数据 2.2Redis安装 Redis官方网站 Redis中文官方网站 http://redis.io http://redis.cn/ 2.2.1安装版本 6.2.1 for Linux（redis-6.2.1.tar.gz） 不用考虑在windows环境下对Redis的支持 2.2.2安装步骤 准备工作：下载安装最新版的gcc编译器 安装C 语言的编译环境 yum install centos-release-scl scl-utils-build yum install -y devtoolset-8-toolchain scl enable devtoolset-8 bash 测试 gcc版本 gcc --version 下载redis-6.2.1.tar.gz放/opt目录 解压命令：tar -zxvf redis-6.2.1.tar.gz 解压完成后进入目录：cd redis-6.2.1 在redis-6.2.1目录下再次执行make命令（只是编译好） 如果没有准备好C语言编译环境，make 会报错—Jemalloc/jemalloc.h：没有那个文件 解决方案：运行make distclean 在redis-6.2.1目录下再次执行make命令（只是编译好） 跳过make test 继续执行: make install 2.2.3安装目录：/usr/local/bin 查看默认安装目录： redis-benchmark:性能测试工具，可以在自己本子运行，看看自己本子性能如何 redis-check-aof：修复有问题的AOF文件，rdb和aof后面讲 redis-check-dump：修复有问题的dump.rdb文件 redis-sentinel：Redis集群使用 redis-server：Redis服务器启动命令 redis-cli：客户端，操作入口 2.2.4前台启动（不推荐） 前台启动，命令行窗口不能关闭，否则服务器停止 2.2.5后台启动（推荐） 备份redis.conf 拷贝一份redis.conf到其他目录 cp /opt/redis-3.2.5/redis.conf /myredis 后台启动设置daemonize no改成yes 修改redis.conf(128行)文件将里面的daemonize no 改成 yes，让服务在后台启动 Redis启动 redis-server/myredis/redis.conf 用客户端访问：redis-cli 多个端口可以：redis-cli -p6379 测试验证： ping Redis关闭 单实例关闭：redis-cli shutdown 也可以进入终端后再关闭 多实例关闭，指定端口关闭：redis-cli -p 6379 shutdown Redis介绍相关知识 端口6379从何而来Alessia Merz 默认16个数据库，类似数组下标从0开始，初始默认使用0号库使用命令select 来切换数据库。如: select 8统一密码管理，所有库同样密码。dbsize查看当前数据库的key的数量flushdb清空当前库flushall通杀全部库 Redis是单线程+多路IO复用技术 多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用select和poll函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行（比如使用线程池） 串行 vs 多线程+锁（memcached） vs 单线程+多路IO复用(Redis) （与Memcache三点不同: 支持多数据类型，支持持久化，单线程+多路IO复用） 3.常用五大数据类型 哪里去获得redis常见数据类型操作命令http://www.redis.cn/commands.html 3.1Redis键(key) keys 查看当前库所有key (匹配：keys 1) exists key判断某个key是否存在 type key 查看你的key是什么类型 del key 删除指定的key数据 unlink key 根据value选择非阻塞删除 仅将keys从keyspace元数据中删除，真正的删除会在后续异步操作。 expire key 10 10秒钟：为给定的key设置过期时间 ttl key 查看还有多少秒过期，-1表示永不过期，-2表示已过期 select命令切换数据库 dbsize查看当前数据库的key的数量 flushdb清空当前库 flushall通杀全部库 3.2Redis字符串(String) 简介 String是Redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 String类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象。 String类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M 常用命令 set 添加键值对 *NX：当数据库中key不存在时，可以将key-value添加数据库 *XX：当数据库中key存在时，可以将key-value添加数据库，与NX参数互斥 *EX：key的超时秒数 *PX：key的超时毫秒数，与EX互斥 get 查询对应键值 append 将给定的 追加到原值的末尾 strlen 获得值的长度 setnx 只有在 key 不存在时 设置 key 的值 incr 将 key 中储存的数字值增1 只能对数字值操作，如果为空，新增值为1 decr 将 key 中储存的数字值减1 只能对数字值操作，如果为空，新增值为-1 incrby / decrby 将 key 中储存的数字值增减。自定义步长。 原子性所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。（1）在单线程中， 能够在单条指令中完成的操作都可以认为是\"原子操作\"，因为中断只能发生于指令之间。（2）在多线程中，不能被其它进程（线程）打断的操作就叫原子操作。Redis单命令的原子性主要得益于Redis的单线程。案例：java中的i++是否是原子操作？不是i=0;两个线程分别对i进行++100次,值是多少？ 2~200 mset ..... 同时设置一个或多个 key-value对 mget ..... 同时获取一个或多个 value msetnx ..... 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 原子性，有一个失败则都失败 getrange 获得值的范围，类似java中的substring，前包，后包 setrange 用 覆写所储存的字符串值，从开始(索引从0开始)。 setex 设置键值的同时，设置过期时间，单位秒。 getset 以新换旧，设置了新值同时获得旧值。 数据结构 String的数据结构为简单动态字符串(Simple Dynamic String,缩写SDS)。是可以修改的字符串，内部结构实现上类似于Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配. 如图中所示，内部为当前字符串实际分配的空间capacity一般要高于实际字符串长度len。当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。 3.3Redis列表(List) 简介 单键多值 Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。 常用命令 lpush/rpush .... 从左边/右边插入一个或多个值。 lpop/rpop 从左边/右边吐出一个值。值在键在，值光键亡。 rpoplpush 从列表右边吐出一个值，插到列表左边。 lrange 按照索引下标获得元素(从左到右) lrange mylist 0 -1 0左边第一个，-1右边第一个，（0-1表示获取所有） lindex 按照索引下标获得元素(从左到右) llen 获得列表长度 linsert before 在的后面插入插入值 lrem 从左边删除n个value(从左到右) lset将列表key下标为index的值替换成value 数据结构 List的数据结构为快速链表quickList。 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是ziplist，也即是压缩列表。 它将所有的元素紧挨着一起存储，分配的是一块连续的内存。 当数据量比较多的时候才会改成quicklist。 因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是int类型的数据，结构上还需要两个额外的指针prev和next。 Redis将链表和ziplist结合起来组成了quicklist。也就是将多个ziplist使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 3.4Redis集合(Set) 简介 Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 Redis的Set是string类型的无序集合。它底层其实是一个value为null的hash表，所以添加，删除，查找的复杂度都是O(1)。 一个算法，随着数据的增加，执行时间的长短，如果是O(1)，数据增加，查找数据的时间不变 常用命令 sadd ..... 将一个或多个 member 元素加入到集合 key 中，已经存在的 member 元素将被忽略 smembers 取出该集合的所有值。 sismember 判断集合是否为含有该值，有1，没有0 scard返回该集合的元素个数。 srem .... 删除集合中的某个元素。 spop 随机从该集合中吐出一个值。 srandmember 随机从该集合中取出n个值。不会从集合中删除 。 smove value把集合中一个值从一个集合移动到另一个集合 sinter 返回两个集合的交集元素。 sunion 返回两个集合的并集元素。 sdiff 返回两个集合的差集元素(key1中的，不包含key2中的) 数据结构 Set数据结构是dict字典，字典是用哈希表实现的。 Java中HashSet的内部实现使用的是HashMap，只不过所有的value都指向同一个对象。Redis的set结构也是一样，它的内部也使用hash结构，所有的value都指向同一个内部值。 3.5Redis哈希(Hash) 简介 Redis hash 是一个键值对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似Java里面的Map 用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储 主要有以下2种存储方式： 每次修改用户的某个属性需要，先反序列化改好后再序列化回去。开销较大。 用户ID数据冗余 通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题 :---- 常用命令 hset 给集合中的 键赋值 hget 从集合取出 value hmset ... 批量设置hash的值 hexists查看哈希表 key 中，给定域 field 是否存在。 hkeys 列出该hash集合的所有field hvals 列出该hash集合的所有value hincrby 为哈希表 key 中的域 field 的值加上增量 1 -1 hsetnx 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在 . 数据结构 Hash类型对应的数据结构是两种：ziplist（压缩列表），hashtable（哈希表）。当field-value长度较短且个数较少时，使用ziplist，否则使用hashtable。 3.6Redis有序集合Zset(sorted set) 简介 Redis有序集合zset与普通集合set非常相似，是一个没有重复元素的字符串集合。 不同之处是有序集合的每个成员都关联了一个评分（score）,这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复了 。 因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。 访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。 常用命令 zadd … 将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 zrange [WITHSCORES] 返回有序集 key 中，下标在之间的元素 带WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscore key minmax [withscores] [limit offset count] 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 zrevrangebyscore key maxmin [withscores] [limit offset count] 同上，改为从大到小排列。 zincrby 为元素的score加上增量 zrem 删除该集合下，指定值的元素 zcount 统计该集合，分数区间内的元素个数 zrank 返回该值在集合中的排名，从0开始。 案例：如何利用zset实现一个文章访问量的排行榜？ 数据结构 SortedSet(zset)是Redis提供的一个非常特别的数据结构，一方面它等价于Java的数据结构Map，可以给每一个元素value赋予一个权重score，另一方面它又类似于TreeSet，内部的元素会按照权重score进行排序，可以得到每个元素的名次，还可以通过score的范围来获取元素的列表。 zset底层使用了两个数据结构 （1）hash，hash的作用就是关联元素value和权重score，保障元素value的唯一性，可以通过元素value找到相应的score值。 （2）跳跃表，跳跃表的目的在于给元素value排序，根据score的范围获取元素列表。 跳跃表（跳表） 1、简介 有序集合在生活中比较常见，例如根据成绩对学生排名，根据得分对玩家排名等。对于有序集合的底层实现，可以用数组、平衡树、链表等。数组不便元素的插入、删除；平衡树或红黑树虽然效率高但结构复杂；链表查询需要遍历所有效率低。Redis采用的是跳跃表。跳跃表效率堪比红黑树，实现远比红黑树简单。 2、实例 对比有序链表和跳跃表，从链表中查询出51 有序链表 要查找值为51的元素，需要从第一个元素开始依次查找、比较才能找到。共需要6次比较。 跳跃表 从第2层开始，1节点比51节点小，向后比较。 21节点比51节点小，继续向后比较，后面就是NULL了，所以从21节点向下到第1层 在第1层，41节点比51节点小，继续向后，61节点比51节点大，所以从41向下 在第0层，51节点为要查找的节点，节点被找到，共查找4次。 从此可以看出跳跃表比有序链表效率要高 4.Redis配置文件介绍 自定义目录：/myredis/redis.conf Units单位 配置大小单位,开头定义了一些基本的度量单位，只支持bytes，不支持bit 大小写不敏感 INCLUDES包含 类似jsp中的include，多实例的情况可以把公用的配置文件提取出来 网络相关配置 bind 默认情况bind=127.0.0.1只能接受本机的访问请求 不写的情况下，无限制接受任何ip地址的访问 生产环境肯定要写你应用服务器的地址；服务器是需要远程访问的，所以需要将其注释掉 如果开启了protected-mode，那么在没有设定bind ip且没有设密码的情况下，Redis只允许接受本机的响应 保存配置，停止服务，重启启动查看进程，不再是本机访问了。 protected-mode 将本机访问保护模式设置no Port 端口号，默认 6379 tcp-backlog 设置tcp的backlog，backlog其实是一个连接队列，backlog队列总和=未完成三次握手队列 + 已经完成三次握手队列。 在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。 注意Linux内核会将这个值减小到/proc/sys/net/core/somaxconn的值（128），所以需要确认增大/proc/sys/net/core/somaxconn和/proc/sys/net/ipv4/tcp_max_syn_backlog（128）两个值来达到想要的效果 timeout 一个空闲的客户端维持多少秒会关闭，0表示关闭该功能。即永不关闭。 tcp-keepalive 对访问客户端的一种心跳检测，每个n秒检测一次。 单位为秒，如果设置为0，则不会进行Keepalive检测，建议设置成60 GENERAL通用 daemonize 是否为后台进程，设置为yes 守护进程，后台启动 pidfile 存放pid文件的位置，每个实例会产生一个不同的pid文件 loglevel 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为notice 四个级别根据使用阶段来选择，生产环境选择notice 或者warning logfile 日志文件名称 databases 16 设定库的数量 默认16，默认数据库为0，可以使用SELECT 命令在连接上指定数据库id SECURITY安全 设置密码 访问密码的查看、设置和取消 在命令中设置密码，只是临时的。重启redis服务器，密码就还原了。 永久设置，需要再配置文件中进行设置。 # LIMITS限制 maxclients 设置redis同时可以与多少个客户端进行连接。 默认情况下为10000个客户端。 如果达到了此限制，redis则会拒绝新的连接请求，并且向这些连接请求方发出“max number of clients reached”以作回应。 maxmemory 建议必须设置，否则，将内存占满，造成服务器宕机 设置redis可以使用的内存量。一旦到达内存使用上限，redis将会试图移除内部数据，移除规则可以通过maxmemory-policy来指定。 如果redis无法根据移除规则来移除内存中的数据，或者设置了“不允许移除”，那么redis则会针对那些需要申请内存的指令返回错误信息，比如SET、LPUSH等。 但是对于无内存申请的指令，仍然会正常响应，比如GET等。如果你的redis是主redis（说明你的redis有从redis），那么在设置内存使用上限时，需要在系统中留出一些内存空间给同步队列缓存，只有在你设置的是“不移除”的情况下，才不用考虑这个因素。 maxmemory-policy volatile-lru：使用LRU算法移除key，只对设置了过期时间的键；（最近最少使用） allkeys-lru：在所有集合key中，使用LRU算法移除key volatile-random：在过期集合中移除随机的key，只对设置了过期时间的键 allkeys-random：在所有集合key中，移除随机的key volatile-ttl：移除那些TTL值最小的key，即那些最近要过期的key noeviction：不进行移除。针对写操作，只是返回错误信息 maxmemory-samples 设置样本数量，LRU算法和最小TTL算法都并非是精确的算法，而是估算值，所以你可以设置样本的大小，redis默认会检查这么多个key并选择其中LRU的那个。 一般设置3到7的数字，数值越小样本越不准确，但性能消耗越小。 5.Redis的发布和订阅 什么是发布和订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 Redis的发布和订阅 1、客户端可以订阅频道如下图 2、当给这个频道发布消息后，消息就会发送给订阅的客户端 发布订阅命令行实现 打开一个客户端订阅channel1 SUBSCRIBE channel1 2、打开另一个客户端，给channel1发布消息hello publish channel1 hello 返回的1是订阅者数量 3、打开第一个客户端可以看到发送的消息 注：发布的消息没有持久化，如果在订阅的客户端收不到hello，只能收到订阅后发布的消息 6.Redis新数据类型 Bitmaps 简介 现代计算机用二进制（位） 作为信息的基础单位， 1个字节等于8位， 例如“abc”字符串是由3个字节组成， 但实际在计算机存储时将其用二进制表示， “abc”分别对应的ASCII码分别是97、 98、 99， 对应的二进制分别是01100001、 01100010和01100011，如下图 合理地使用操作位能够有效地提高内存使用率和开发效率。 Redis提供了Bitmaps这个“数据类型”可以实现对位的操作： Bitmaps本身不是一种数据类型， 实际上它就是字符串（key-value） ， 但是它可以对字符串的位进行操作。 Bitmaps单独提供了一套命令， 所以在Redis中使用Bitmaps和使用字符串的方法不太相同。 可以把Bitmaps想象成一个以位为单位的数组， 数组的每个单元只能存储0和1， 数组的下标在Bitmaps中叫做偏移量。 命令 1、setbit （1）格式 setbit设置Bitmaps中某个偏移量的值（0或1） *offset:偏移量从0开始 （2）实例 每个独立用户是否访问过网站存放在Bitmaps中， 将访问的用户记做1， 没有访问的用户记做0， 用偏移量作为用户的id。 设置键的第offset个位的值（从0算起） ， 假设现在有20个用户，userid=1， 6， 11， 15， 19的用户对网站进行了访问， 那么当前Bitmaps初始化结果如图 unique:users:20201106代表2020-11-06这天的独立访问用户的Bitmaps 注： 很多应用的用户id以一个指定数字（例如10000） 开头， 直接将用户id和Bitmaps的偏移量对应势必会造成一定的浪费， 通常的做法是每次做setbit操作时将用户id减去这个指定数字。 在第一次初始化Bitmaps时， 假如偏移量非常大， 那么整个初始化过程执行会比较慢， 可能会造成Redis的阻塞。 2、getbit （1）格式 getbit获取Bitmaps中某个偏移量的值 获取键的第offset位的值（从0开始算） （2）实例 获取id=8的用户是否在2020-11-06这天访问过， 返回0说明没有访问过： 注：因为100根本不存在，所以也是返回0 3、bitcount 统计字符串被设置为1的bit数。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。start 和 end 参数的设置，都可以使用负数值：比如 -1 表示最后一个位，而 -2 表示倒数第二个位，start、end 是指bit组的字节的下标数，二者皆包含。 （1）格式 bitcount[start end] 统计字符串从start字节到end字节比特值为1的数量 （2）实例 计算2022-11-06这天的独立访问用户数量 start和end代表起始和结束字节数， 下面操作计算用户id在第1个字节到第3个字节之间的独立访问用户数， 对应的用户id是11， 15， 19。 举例： K1 【01000001 01000000 00000000 00100001】，对应【0，1，2，3】 bitcount K1 1 2 ： 统计下标1、2字节组中bit=1的个数，即01000000 00000000 --》bitcount K1 1 2 　　--》1 bitcount K1 1 3 ： 统计下标1、2字节组中bit=1的个数，即01000000 00000000 00100001 --》bitcount K1 1 3　　--》3 bitcount K1 0 -2 ： 统计下标0到下标倒数第2，字节组中bit=1的个数，即01000001 01000000 00000000 --》bitcount K1 0 -2　　--》3 注意：redis的setbit设置或清除的是bit位置，而bitcount计算的是byte位置。 4、bitop (1)格式 bitop and(or/not/xor) [key…] bitop是一个复合操作， 它可以做多个Bitmaps的and（交集） 、 or（并集） 、 not（非） 、 xor（异或） 操作并将结果保存在destkey中。 (2)实例 2020-11-04 日访问网站的userid=1,2,5,9。 setbit unique:users:20201104 1 1 setbit unique:users:20201104 2 1 setbit unique:users:20201104 5 1 setbit unique:users:20201104 9 1 2020-11-03 日访问网站的userid=0,1,4,9。 setbit unique:users:20201103 0 1 setbit unique:users:20201103 1 1 setbit unique:users:20201103 4 1 setbit unique:users:20201103 9 1 计算出两天都访问过网站的用户数量 bitop and unique:users:and:20201104_03 unique:users:20201103unique:users:20201104 计算出任意一天都访问过网站的用户数量（例如月活跃就是类似这种） ， 可以使用or求并集 Bitmaps与set对比 假设网站有1亿用户， 每天独立访问的用户有5千万， 如果每天用集合类型和Bitmaps分别存储活跃用户可以得到表 set和Bitmaps存储一天活跃用户对比 数据类型 每个用户id占用空间 需要存储的用户量 全部内存量 集合类型 64位 50000000 64位*50000000 = 400MB Bitmaps 1位 100000000 1位*100000000 = 12.5MB 很明显， 这种情况下使用Bitmaps能节省很多的内存空间， 尤其是随着时间推移节省的内存还是非常可观的 set和Bitmaps存储独立用户空间对比 数据类型 一天 一个月 一年 集合类型 400MB 12GB 144GB Bitmaps 12.5MB 375MB 4.5GB 但Bitmaps并不是万金油， 假如该网站每天的独立访问用户很少， 例如只有10万（大量的僵尸用户） ， 那么两者的对比如下表所示， 很显然， 这时候使用Bitmaps就不太合适了， 因为基本上大部分位都是0。 set和Bitmaps存储一天活跃用户对比（独立用户比较少） 数据类型 每个userid占用空间 需要存储的用户量 全部内存量 集合类型 64位 100000 64位*100000 = 800KB Bitmaps 1位 100000000 1位*100000000 = 12.5MB HyperLogLog 简介 在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站PV（PageView页面访问量）,可以使用Redis的incr、incrby轻松实现。 但像UV（UniqueVisitor，独立访客）、独立IP数、搜索记录数等需要去重和计数的问题如何解决？这种求集合中不重复元素个数的问题称为基数问题。 解决基数问题有很多种方案： （1）数据存储在MySQL表中，使用distinct count计算不重复个数 （2）使用Redis提供的hash、set、bitmaps等数据结构来处理 以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。 能否能够降低一定的精度来平衡存储空间？Redis推出了HyperLogLog Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 命令 1、pfadd （1）格式 pfadd [element ...] 添加指定元素到 HyperLogLog 中 （2）实例 将所有元素添加到指定HyperLogLog数据结构中。如果执行命令后HLL估计的近似基数发生变化，则返回1，否则返回0。 2、pfcount （1）格式 pfcount [key ...] 计算HLL的近似基数，可以计算多个HLL，比如用HLL存储每天的UV，计算一周的UV可以使用7天的UV合并计算即可 （2）实例 3、pfmerge （1）格式 pfmerge [sourcekey ...] 将一个或多个HLL合并后的结果存储在另一个HLL中，比如每月活跃用户可以使用每天的活跃用户来合并计算可得 （2）实例 Geospatial 简介 Redis 3.2 中增加了对GEO类型的支持。GEO，Geographic，地理信息的缩写。该类型，就是元素的2维坐标，在地图上就是经纬度。redis基于该类型，提供了经纬度设置，查询，范围查询，距离查询，经纬度Hash等常见操作。 命令 1、geoadd （1）格式 geoadd [longitude latitude member...] 添加地理位置（经度，纬度，名称） （2）实例 geoadd china:city121.47 31.23 shanghai geoadd china:city 106.50 29.53 chongqing 114.05 22.52 shenzhen 116.38 39.90 beijing 两极无法直接添加，一般会下载城市数据，直接通过 Java 程序一次性导入。 有效的经度从 -180 度到 180 度。有效的纬度从 -85.05112878 度到 85.05112878 度。 当坐标位置超出指定范围时，该命令将会返回一个错误。 已经添加的数据，是无法再次往里面添加的。 2、geopos （1）格式 geopos [member...] 获得指定地区的坐标值 （2）实例 3、geodist （1）格式 geodist [m|km|ft|mi ] 获取两个位置之间的直线距离 （2）实例 获取两个位置之间的直线距离 单位： m 表示单位为米[默认值]。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位 4、georadius （1）格式 georadiusradius m|km|ft|mi 以给定的经纬度为中心，找出某一半径内的元素 经度 纬度 距离 单位 （2）实例 7.RedisJedis测试 Jedis所需要的jar包 redis.clientsjedis3.2.0 连接Redis注意事项 禁用Linux的防火墙：Linux(CentOS7)里执行命令 systemctl stop/disable firewalld.service redis.conf中注释掉bind 127.0.0.1 ,然后 protected-mode no Jedis常用操作 创建动态的工程 创建测试程序 package com.atguigu.jedis;import redis.clients.jedis.Jedis;public class Demo01 {public static void main(String[] args) {Jedis jedis = new Jedis(\"192.168.137.3\",6379);String pong = jedis.ping();System.out.println(\"连接成功：\"+pong);jedis.close();}} 测试相关数据类型 Jedis-API: Key jedis.set(\"k1\", \"v1\");jedis.set(\"k2\", \"v2\");jedis.set(\"k3\", \"v3\");Set keys = jedis.keys(\"*\");System.out.println(keys.size());for (String key : keys) {System.out.println(key);}System.out.println(jedis.exists(\"k1\"));System.out.println(jedis.ttl(\"k1\"));System.out.println(jedis.get(\"k1\")); Jedis-API: String jedis.mset(\"str1\",\"v1\",\"str2\",\"v2\",\"str3\",\"v3\");System.out.println(jedis.mget(\"str1\",\"str2\",\"str3\")); Jedis-API: List List list = jedis.lrange(\"mylist\",0,-1);for (String element : list) {System.out.println(element);} Jedis-API: set jedis.sadd(\"orders\", \"order01\");jedis.sadd(\"orders\", \"order02\");jedis.sadd(\"orders\", \"order03\");jedis.sadd(\"orders\", \"order04\");Set smembers = jedis.smembers(\"orders\");for (String order : smembers) {System.out.println(order);}jedis.srem(\"orders\", \"order02\"); Jedis-API: hash jedis.hset(\"hash1\",\"userName\",\"lisi\");System.out.println(jedis.hget(\"hash1\",\"userName\"));Map map = new HashMap();map.put(\"telphone\",\"13810169999\");map.put(\"address\",\"atguigu\");map.put(\"email\",\"abc@163.com\");jedis.hmset(\"hash2\",map);List result = jedis.hmget(\"hash2\", \"telphone\",\"email\");for (String element : result) {System.out.println(element);} Jedis-API: zset jedis.zadd(\"zset01\", 100d, \"z3\");jedis.zadd(\"zset01\", 90d, \"l4\");jedis.zadd(\"zset01\", 80d, \"w5\");jedis.zadd(\"zset01\", 70d, \"z6\");Set zrange = jedis.zrange(\"zset01\", 0, -1);for (String e : zrange) {System.out.println(e);} 8.RedisJedis实例 完成一个手机验证码功能 要求： 1、输入手机号，点击发送后随机生成6位数字码，2分钟有效 2、输入验证码，点击验证，返回成功或失败 3、每个手机号每天只能输入3次 9.Redis与Spring Boot整合 Spring Boot整合Redis非常简单，只需要按如下步骤整合即可 整合步骤 在pom.xml文件中引入redis相关依赖 org.springframework.bootspring-boot-starter-data-redisorg.apache.commonscommons-pool22.6.0 application.properties配置redis配置 #Redis服务器地址spring.redis.host=192.168.140.136#Redis服务器连接端口spring.redis.port=6379#Redis数据库索引（默认为0）spring.redis.database= 0#连接超时时间（毫秒）spring.redis.timeout=1800000#连接池最大连接数（使用负值表示没有限制）spring.redis.lettuce.pool.max-active=20#最大阻塞等待时间(负数表示没限制)spring.redis.lettuce.pool.max-wait=-1#连接池中的最大空闲连接spring.redis.lettuce.pool.max-idle=5#连接池中的最小空闲连接spring.redis.lettuce.pool.min-idle=0 添加redis配置类 @EnableCaching@Configurationpublic class RedisConfig extends CachingConfigurerSupport {@Beanpublic RedisTemplate redisTemplate(RedisConnectionFactory factory) {RedisTemplate template = new RedisTemplate<>();RedisSerializer redisSerializer = new StringRedisSerializer();Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);ObjectMapper om = new ObjectMapper();om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);jackson2JsonRedisSerializer.setObjectMapper(om);template.setConnectionFactory(factory);//key序列化方式template.setKeySerializer(redisSerializer);//value序列化template.setValueSerializer(jackson2JsonRedisSerializer);//value hashmap序列化template.setHashValueSerializer(jackson2JsonRedisSerializer);return template;}@Beanpublic CacheManager cacheManager(RedisConnectionFactory factory) {RedisSerializer redisSerializer = new StringRedisSerializer();Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);//解决查询缓存转换异常的问题ObjectMapper om = new ObjectMapper();om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);jackson2JsonRedisSerializer.setObjectMapper(om);//配置序列化（解决乱码的问题）,过期时间600秒RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig().entryTtl(Duration.ofSeconds(600)).serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(redisSerializer)).serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(jackson2JsonRedisSerializer)).disableCachingNullValues();RedisCacheManager cacheManager = RedisCacheManager.builder(factory).cacheDefaults(config).build();return cacheManager;}} 4、测试一下 RedisTestController中添加测试方法 @RestController@RequestMapping(\"/redisTest\")public class RedisTestController {@Autowiredprivate RedisTemplate redisTemplate;null@GetMappingpublic String testRedis() {//设置值到redisredisTemplate.opsForValue().set(\"name\",\"lucy\");//从redis获取值String name = (String)redisTemplate.opsForValue().get(\"name\");return name;}} 10.Redis事务锁机制_秒杀 Redis的事务定义 Redis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Redis事务的主要作用就是串联多个命令防止别的命令插队。 Multi、Exec、discard 从输入Multi命令开始，输入的命令都会依次进入命令队列中，但不会执行，直到输入Exec后，Redis会将之前的命令队列中的命令依次执行。 组队的过程中可以通过discard来放弃组队。 案例： 组队成功，提交成功 组队阶段报错，提交失败 组队成功，提交有成功有失败情况 事务的错误处理 组队中某个命令出现了报告错误，执行时整个的所有队列都会被取消。 如果执行阶段某个命令报出了错误，则只有报错的命令不会被执行，而其他的命令都会执行，不会回滚。 为什么要做成事务 想想一个场景：有很多人有你的账户,同时去参加双十一抢购 事务冲突的问题 例子 一个请求想给金额减8000 一个请求想给金额减5000 一个请求想给金额减1000 悲观锁 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 乐观锁 乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。Redis就是利用这种check-and-set机制实现事务的。 WATCH key [key ...] 在执行multi之前，先执行watch key1 [key2],可以监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 unwatch 取消 WATCH 命令对所有 key 的监视。 如果在执行 WATCH 命令之后，EXEC 命令或DISCARD 命令先被执行了的话，那么就不需要再执行UNWATCH 了。 http://doc.redisfans.com/transaction/exec.html Redis事务三特性 单独的隔离操作 事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念 队列中的命令没有提交之前都不会实际被执行，因为事务提交前任何指令都不会被实际执行 不保证原子性 事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 11.Redis事务秒杀案例 解决计数器和人员记录的事务操作 Redis事务--秒杀并发模拟 使用工具ab模拟测试 CentOS6 默认安装 CentOS7需要手动安装 联网：yum install httpd-tools 无网络 （1） 进入cd /run/media/root/CentOS 7 x86_64/Packages（路径跟centos6不同） （2） 顺序安装 apr-1.4.8-3.el7.x86_64.rpm apr-util-1.5.2-6.el7.x86_64.rpm httpd-tools-2.4.6-67.el7.centos.x86_64.rpm 测试及结果 通过ab测试 vim postfile 模拟表单提交参数,以&符号结尾;存放当前目录。 内容：prodid=0101& ab -n 2000 -c 200 -k -p ~/postfile -T application/x-www-form-urlencoded http://192.168.2.115:8081/Seckill/doseckill 超卖 超卖问题 利用乐观锁淘汰用户，解决超卖问题。 //增加乐观锁jedis.watch(qtkey);//3.判断库存String qtkeystr = jedis.get(qtkey);if(qtkeystr==null \\ \\ \"\".equals(qtkeystr.trim())) {System.out.println(\"未初始化库存\");jedis.close();return false ;}int qt = Integer.parseInt(qtkeystr);if(qtSystem.err.println(\"已经秒光\");jedis.close();return false;}//增加事务Transaction multi = jedis.multi();//4.减少库存//jedis.decr(qtkey);multi.decr(qtkey);//5.加人//jedis.sadd(usrkey, uid);multi.sadd(usrkey, uid);//执行事务List list = multi.exec();//判断事务提交是否失败if(list==null \\ \\ list.size()==0) {System.out.println(\"秒杀失败\");jedis.close();return false;}System.err.println(\"秒杀成功\");jedis.close(); 继续增加并发测试 连接有限制 ab -n 2000 -c 200 -k -p postfile -T 'application/x-www-form-urlencoded'http://192.168.140.1:8080/seckill/doseckill 增加-r参数，-r Don't exit on socket receive errors. ab -n 2000 -c 100 -r -p postfile -T 'application/x-www-form-urlencoded'http://192.168.140.1:8080/seckill/doseckill 已经秒光，可是还有库存 ab -n 2000 -c 100 -p postfile -T 'application/x-www-form-urlencoded'http://192.168.137.1:8080/seckill/doseckill 已经秒光，可是还有库存。原因，就是乐观锁导致很多请求都失败。先点的没秒到，后点的可能秒到了。 连接超时，通过连接池解决 连接池 节省每次连接redis服务带来的消耗，把连接好的实例反复利用。 通过参数管理连接的行为 代码见项目中 链接池参数 MaxTotal：控制一个pool可分配多少个jedis实例，通过pool.getResource()来获取；如果赋值为-1，则表示不限制；如果pool已经分配了MaxTotal个jedis实例，则此时pool的状态为exhausted。 maxIdle：控制一个pool最多有多少个状态为idle(空闲)的jedis实例； MaxWaitMillis：表示当borrow一个jedis实例时，最大的等待毫秒数，如果超过等待时间，则直接抛JedisConnectionException； testOnBorrow：获得一个jedis实例的时候是否检查连接可用性（ping()）；如果为true，则得到的jedis实例均是可用的； 解决库存遗留问题 LUA脚本 Lua 是一个小巧的脚本语言，Lua脚本可以很容易的被C/C++ 代码调用，也可以反过来调用C/C++的函数，Lua并没有提供强大的库，一个完整的Lua解释器不过200k，所以Lua不适合作为开发独立应用程序的语言，而是作为嵌入式脚本语言。 很多应用程序、游戏使用LUA作为自己的嵌入式脚本语言，以此来实现可配置性、可扩展性。 这其中包括魔兽争霸地图、魔兽世界、博德之门、愤怒的小鸟等众多游戏插件或外挂。 https://www.w3cschool.cn/lua/ LUA脚本在Redis中的优势 将复杂的或者多步的redis操作，写为一个脚本，一次提交给redis执行，减少反复连接redis的次数。提升性能。 LUA脚本是类似redis事务，有一定的原子性，不会被其他命令插队，可以完成一些redis事务性的操作。 但是注意redis的lua脚本功能，只有在Redis 2.6以上的版本才可以使用。 利用lua脚本淘汰用户，解决超卖问题。 redis 2.6版本以后，通过lua脚本解决争抢问题，实际上是redis 利用其单线程的特性，用任务队列的方式解决多任务并发问题。 Redis事务秒杀案例_代码 项目结构 第一版：简单版 老师点10次，正常秒杀 同学一起点试一试，秒杀也是正常的。这是因为还达不到并发的效果。 使用工具ab模拟并发测试，会出现超卖情况。查看库存会出现负数。 第二版：加事务-乐观锁(解决超卖),但出现遗留库存和连接超时 第三版：连接池解决超时问题 第四版：解决库存依赖问题，LUA脚本 local userid=KEYS[1];local prodid=KEYS[2];localqtkey=\"sk:\"..prodid..\":qt\";localusersKey=\"sk:\"..prodid.\":usr';local userExists=redis.call(\"sismember\",usersKey,userid);if tonumber(userExists)==1 thenreturn 2;endlocal num= redis.call(\"get\" ,qtkey);if tonumber(num)return 0;elseredis.call(\"decr\",qtkey);redis.call(\"sadd\",usersKey,userid);endreturn 1; 12.Redis持久化之RDB 总体介绍 官网介绍：http://www.redis.io Redis 提供了2个不同形式的持久化方式。 RDB（Redis DataBase） AOF（Append Of File） RDB（Redis DataBase） 官网介绍 是什么 在指定的时间间隔内将内存中的数据集快照写入磁盘， 也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里 备份是如何执行的 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到 一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能 如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 Fork Fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等） 数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程 在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，Linux中引入了“写时复制技术” 一般情况父进程和子进程会共用同一段物理内存，只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。 RDB持久化流程 dump.rdb文件 在redis.conf中配置文件名称，默认为dump.rdb 配置位置 rdb文件的保存路径，也可以修改。默认为Redis启动时命令行所在的目录下 dir \"/myredis/\" 如何触发RDB快照；保持策略 配置文件中默认的快照配置 命令save VS bgsave save ：save时只管保存，其它不管，全部阻塞。手动保存。不建议。 bgsave：Redis会在后台异步进行快照操作， 快照同时还可以响应客户端请求。 可以通过lastsave 命令获取最后一次成功执行快照的时间 flushall命令 执行flushall命令，也会产生dump.rdb文件，但里面是空的，无意义 SNAPSHOTTING快照 Save 格式：save 秒钟 写操作次数 RDB是整个内存的压缩过的Snapshot，RDB的数据结构，可以配置复合的快照触发条件， 默认是1分钟内改了1万次，或5分钟内改了10次，或15分钟内改了1次。 禁用 不设置save指令，或者给save传入空字符串 stop-writes-on-bgsave-error 当Redis无法写入磁盘的话，直接关掉Redis的写操作。推荐yes. rdbcompression 压缩文件 对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis会采用LZF算法进行压缩。 如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能。推荐yes. rdbchecksum 检查完整性 在存储快照后，还可以让redis使用CRC64算法来进行数据校验， 但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能 推荐yes. rdb的备份 先通过config get dir 查询rdb文件的目录 将*.rdb的文件拷贝到别的地方 rdb的恢复 关闭Redis 先把备份的文件拷贝到工作目录下 cp dump2.rdb dump.rdb 启动Redis, 备份数据会直接加载 优势 适合大规模的数据恢复 对数据完整性和一致性要求不高更适合使用 节省磁盘空间 恢复速度快 劣势 Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑 虽然Redis在fork时使用了写时拷贝技术,但是如果数据庞大时还是比较消耗性能。 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。 如何停止 动态停止RDB：redis-cli config set save \"\"#save后给空值，表示禁用保存策略 小总结 13.Redis持久化之AOF AOF（Append Only File） 是什么 以日志的形式来记录每个写操作（增量保存），将Redis执行过的所有写指令记录下来(读操作不记录)， 只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作 AOF持久化流程 （1）客户端的请求写命令会被append追加到AOF缓冲区内； （2）AOF缓冲区根据AOF持久化策略[always,everysec,no]将操作sync同步到磁盘的AOF文件中； （3）AOF文件大小超过重写策略或手动重写时，会对AOF文件rewrite重写，压缩AOF文件容量； （4）Redis服务重启时，会重新load加载AOF文件中的写操作达到数据恢复的目的； AOF默认不开启 可以在redis.conf中配置文件名称，默认为 appendonly.aof AOF文件的保存路径，同RDB的路径一致。 AOF和RDB同时开启，redis听谁的？ AOF和RDB同时开启，系统默认取AOF的数据（数据不会存在丢失） AOF启动/修复/恢复 AOF的备份机制和性能虽然和RDB不同, 但是备份和恢复的操作同RDB一样，都是拷贝备份文件，需要恢复时再拷贝到Redis工作目录下，启动系统即加载。 正常恢复 修改默认的appendonly no，改为yes 将有数据的aof文件复制一份保存到对应目录(查看目录：config get dir) 恢复：重启redis然后重新加载 异常恢复 修改默认的appendonly no，改为yes 如遇到AOF文件损坏，通过/usr/local/bin/redis-check-aof--fix appendonly.aof进行恢复 备份被写坏的AOF文件 恢复：重启redis，然后重新加载 AOF同步频率设置 appendfsync always 始终同步，每次Redis的写入都会立刻记入日志；性能较差但数据完整性比较好 appendfsync everysec 每秒同步，每秒记入日志一次，如果宕机，本秒的数据可能丢失。 appendfsync no redis不主动进行同步，把同步时机交给操作系统。 Rewrite压缩 1是什么： AOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制, 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩， 只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof 2重写原理，如何实现重写 AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，redis4.0版本后的重写，是指上就是把rdb 的快照，以二级制的形式附在新的aof头部，作为已有的历史数据，替换掉原来的流水账操作。 no-appendfsync-on-rewrite： 如果 no-appendfsync-on-rewrite=yes ,不写入aof文件只写入缓存，用户请求不会阻塞，但是在这段时间如果宕机会丢失这段时间的缓存数据。（降低数据安全性，提高性能） 如果 no-appendfsync-on-rewrite=no, 还是会把数据往磁盘里刷，但是遇到重写操作，可能会发生阻塞。（数据安全，但是性能降低） 触发机制，何时重写 Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发 重写虽然可以节约大量磁盘空间，减少恢复时间。但是每次重写还是有一定的负担的，因此设定Redis要满足一定条件才会进行重写。 auto-aof-rewrite-percentage：设置重写的基准值，文件达到100%时开始重写（文件是原来重写后文件的2倍时触发） auto-aof-rewrite-min-size：设置重写的基准值，最小文件64MB。达到这个值开始重写。 例如：文件达到70MB开始重写，降到50MB，下次什么时候开始重写？100MB 系统载入时或者上次重写完毕时，Redis会记录此时AOF大小，设为base_size, 如果Redis的AOF当前大小>= base_size +base_size*100% (默认)且当前大小>=64mb(默认)的情况下，Redis会对AOF进行重写。 3、重写流程 （1）bgrewriteaof触发重写，判断是否当前有bgsave或bgrewriteaof在运行，如果有，则等待该命令结束后再继续执行。 （2）主进程fork出子进程执行重写操作，保证主进程不会阻塞。 （3）子进程遍历redis内存中数据到临时文件，客户端的写请求同时写入aof_buf缓冲区和aof_rewrite_buf重写缓冲区保证原AOF文件完整以及新AOF文件生成期间的新的数据修改动作不会丢失。 （4）1).子进程写完新的AOF文件后，向主进程发信号，父进程更新统计信息。2).主进程把aof_rewrite_buf中的数据写入到新的AOF文件。 （5）使用新的AOF文件覆盖旧的AOF文件，完成AOF重写。 优势 备份机制更稳健，丢失数据概率更低。 可读的日志文本，通过操作AOF稳健，可以处理误操作。 劣势 比起RDB占用更多的磁盘空间。 恢复备份速度要慢。 每次读写都同步的话，有一定的性能压力。 存在个别Bug，造成恢复不能。 小总结 总结(Which one) 用哪个好 官方推荐两个都启用。 如果对数据不敏感，可以选单独用RDB。 不建议单独用 AOF，因为可能会出现Bug。 如果只是做纯内存缓存，可以都不用。 官网建议 RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储 AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾. Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大 只做缓存：如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式. 同时开启两种持久化方式 在这种情况下,当redis重启的时候会优先载入AOF文件来恢复原始的数据, 因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整. RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。那要不要只使用AOF呢？ 建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)， 快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。 性能建议 因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这条规则。如果使用AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了。代价,一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上。默认超过原大小100%大小时重写可以改到适当的数值。 14.Redis_主从复制 是什么 主机数据更新后根据配置和策略， 自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主 能干嘛 读写分离，性能扩展 容灾快速恢复 怎么玩：主从复制 拷贝多个redis.conf文件include(写绝对路径) 开启daemonize yes Pid文件名字pidfile 指定端口port Log文件名字 dump.rdb名字dbfilename Appendonly 关掉或者换名字 新建redis6379.conf，填写以下内容 include /myredis/redis.conf pidfile /var/run/redis_6379.pid port 6379 dbfilename dump6379.rdb 新建redis6380.conf，填写以下内容 新建redis6381.conf，填写以下内容 slave-priority 10 设置从机的优先级，值越小，优先级越高，用于选举主机时使用。默认100 启动三台redis服务器 查看系统进程，看看三台服务器是否启动 查看三台主机运行情况 info replication 打印主从复制的相关信息 配从(库)不配主(库) slaveof 成为某个实例的从服务器 1、在6380和6381上执行: slaveof 127.0.0.1 6379 2、在主机上写，在从机上可以读取数据 在从机上写数据报错 3、主机挂掉，重启就行，一切如初 4、从机重启需重设：slaveof 127.0.0.1 6379 可以将配置增加到文件中。永久生效。 常用3招 一主二仆 切入点问题？slave1、slave2是从头开始复制还是从切入点开始复制?比如从k4进来，那之前的k1,k2,k3是否也可以复制？ 从机是否可以写？set可否？ 主机shutdown后情况如何？从机是上位还是原地待命？ 主机又回来了后，主机新增记录，从机还能否顺利复制？ 其中一台从机down后情况如何？依照原有它能跟上大部队吗？ 薪火相传 上一个Slave可以是下一个slave的Master，Slave同样可以接收其他 slaves的连接和同步请求，那么该slave作为了链条中下一个的master, 可以有效减轻master的写压力,去中心化降低风险。 用 slaveof 中途变更转向:会清除之前的数据，重新建立拷贝最新的 风险是一旦某个slave宕机，后面的slave都没法备份 主机挂了，从机还是从机，无法写数据了 反客为主 当一个master宕机后，后面的slave可以立刻升为master，其后面的slave不用做任何修改。 用 slaveof no one 将从机变为主机。 复制原理 Slave启动成功连接到master后会发送一个sync命令 Master接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令， 在后台进程执行完毕之后，master将传送整个数据文件到slave,以完成一次完全同步 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master继续将新的所有收集到的修改命令依次传给slave,完成同步 但是只要是重新连接master,一次完全同步（全量复制)将被自动执行 哨兵模式(sentinel) 是什么 反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库 怎么玩(使用步骤) 调整为一主二仆模式，6379带着6380、6381 自定义的/myredis目录下新建sentinel.conf文件，名字绝不能错 配置哨兵,填写内容 sentinel monitor mymaster 127.0.0.1 6379 1 其中mymaster为监控对象起的服务器名称， 1 为至少有多少个哨兵同意迁移的数量。 启动哨兵 /usr/local/bin redis做压测可以用自带的redis-benchmark工具 执行redis-sentinel /myredis/sentinel.conf 当主机挂掉，从机选举中产生新的主机 (大概10秒左右可以看到哨兵窗口日志，切换了新的主机) 哪个从机会被选举为主机呢？根据优先级别：slave-priority 原主机重启后会变为从机。 复制延时 由于所有的写操作都是先在Master上操作，然后同步更新到Slave上，所以从Master同步到Slave机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重。 故障恢复 优先级在redis.conf中默认：slave-priority 100，值越小优先级越高 偏移量是指获得原主机数据最全的 每个redis实例启动后都会随机生成一个40位的runid 主从复制 private static JedisSentinelPool jedisSentinelPool=null;public static Jedis getJedisFromSentinel(){if(jedisSentinelPool==null){Set sentinelSet=new HashSet<>();sentinelSet.add(\"192.168.11.103:26379\");JedisPoolConfig jedisPoolConfig =new JedisPoolConfig();jedisPoolConfig.setMaxTotal(10); //最大可用连接数jedisPoolConfig.setMaxIdle(5); //最大闲置连接数jedisPoolConfig.setMinIdle(5); //最小闲置连接数jedisPoolConfig.setBlockWhenExhausted(true); //连接耗尽是否等待jedisPoolConfig.setMaxWaitMillis(2000); //等待时间jedisPoolConfig.setTestOnBorrow(true); //取连接的时候进行一下测试ping pongjedisSentinelPool=new JedisSentinelPool(\"mymaster\",sentinelSet,jedisPoolConfig);return jedisSentinelPool.getResource();}else{return jedisSentinelPool.getResource();}} 15.Redis集群 问题 容量不够，redis如何进行扩容？ 并发写操作， redis如何分摊？ 另外，主从模式，薪火相传模式，主机宕机，导致ip地址发生变化，应用程序中配置需要修改对应的主机地址、端口等信息。 之前通过代理主机来解决，但是redis3.0中提供了解决方案。就是无中心化集群配置。 什么是集群 Redis 集群实现了对Redis的水平扩容，即启动N个redis节点，将整个数据库分布存储在这N个节点中，每个节点存储总数据的1/N。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 删除持久化数据 将rdb,aof文件都删除掉。 制作6个实例，6379,6380,6381,6389,6390,6391 配置基本信息 开启daemonize yes Pid文件名字 指定端口 Log文件名字 Dump.rdb名字 Appendonly 关掉或者换名字 redis cluster配置修改 cluster-enabled yes 打开集群模式 cluster-config-file nodes-6379.conf 设定节点配置文件名 cluster-node-timeout 15000 设定节点失联时间，超过该时间（毫秒），集群自动进行主从切换。 include /home/bigdata/redis.confport 6379pidfile \"/var/run/redis_6379.pid\"dbfilename \"dump6379.rdb\"dir \"/home/bigdata/redis_cluster\"logfile \"/home/bigdata/redis_cluster/redis_err_6379.log\"cluster-enabled yescluster-config-file nodes-6379.confcluster-node-timeout 15000 修改好redis6379.conf文件，拷贝多个redis.conf文件 使用查找替换修改另外5个文件 例如：:%s/6379/6380 启动6个redis服务 将六个节点合成一个集群 组合之前，请确保所有redis实例启动后，nodes-xxxx.conf文件都生成正常。 合体： cd /opt/redis-6.2.1/src redis-cli --cluster create --cluster-replicas 1 192.168.11.101:6379192.168.11.101:6380 192.168.11.101:6381 192.168.11.101:6389 192.168.11.101:6390 192.168.11.101:6391 此处不要用127.0.0.1， 请用真实IP地址 --replicas 1 采用最简单的方式配置集群，一台主机，一台从机，正好三组。 普通方式登录 可能直接进入读主机，存储数据时，会出现MOVED重定向操作。所以，应该以集群方式登录。 -c 采用集群策略连接，设置数据会自动切换到相应的写主机 通过 cluster nodes 命令查看集群信息 redis cluster 如何分配这六个节点? 一个集群至少要有三个主节点。 选项 --cluster-replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。 分配原则尽量保证每个主数据库运行在不同的IP地址，每个从库和主库不在一个IP地址上。 什么是slots [OK] All nodes agree about slots configuration. Check for open slots... Check slots coverage... [OK] All16384slots covered. 一个 Redis 集群包含 16384 个插槽（hash slot）， 数据库中的每个键都属于这 16384 个插槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分插槽。 举个例子， 如果一个集群可以有主节点， 其中： 节点 A 负责处理 0 号至 5460 号插槽。 节点 B 负责处理 5461 号至 10922 号插槽。 节点 C 负责处理 10923 号至 16383 号插槽。 在集群中录入值 在redis-cli每次录入、查询键值，redis都会计算出该key应该送往的插槽，如果不是该客户端对应服务器的插槽，redis会报错，并告知应前往的redis实例地址和端口。 redis-cli客户端提供了 –c 参数实现自动重定向。 如 redis-cli-c–p 6379 登入后，再录入、查询键值对可以自动重定向。 不在一个slot下的键值，是不能使用mget,mset等多键操作。 可以通过{}来定义组的概念，从而使key中{}内相同内容的键值对放到一个slot中去。 查询集群中的值 CLUSTER GETKEYSINSLOT 返回 count 个 slot 槽中的键。 故障恢复 如果主节点下线？从节点能否自动升为主节点？注意：15秒超时 主节点恢复后，主从关系会如何？主节点回来变成从机。 如果所有某一段插槽的主从节点都宕掉，redis服务是否还能继续? 如果某一段插槽的主从都挂掉，而cluster-require-full-coverage 为yes ，那么 ，整个集群都挂掉 如果某一段插槽的主从都挂掉，而cluster-require-full-coverage 为no ，那么，该插槽数据全都不能使用，也无法存储。 redis.conf中的参数 cluster-require-full-coverage 集群的Jedis开发 即使连接的不是主机，集群会自动切换主机存储。主机写，从机读。 无中心化主从集群。无论从哪台主机写的数据，其他主机上都能读到数据。 public class JedisClusterTest {public static void main(String[] args) {Setset =new HashSet();set.add(new HostAndPort(\"192.168.31.211\",6379));JedisCluster jedisCluster=new JedisCluster(set);jedisCluster.set(\"k1\", \"v1\");System.out.println(jedisCluster.get(\"k1\"));}} Redis 集群提供了以下好处 实现扩容 分摊压力 无中心配置相对简单 Redis 集群的不足 多键操作是不被支持的 多键的Redis事务是不被支持的。lua脚本不被支持 由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁移至redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。 16.Redis应用问题解决 16.1缓存穿透 问题描述 key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会压到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。 解决方案 一个一定不存在缓存及查询不到的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。 解决方案： 对空值缓存：如果一个查询返回的数据为空（不管是数据是否不存在），我们仍然把这个空结果（null）进行缓存，设置空结果的过期时间会很短，最长不超过五分钟 设置可访问的名单（白名单）： 使用bitmaps类型定义一个可以访问的名单，名单id作为bitmaps的偏移量，每次访问和bitmap里面的id进行比较，如果访问id不在bitmaps里面，进行拦截，不允许访问。 采用布隆过滤器：(布隆过滤器（Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量(位图)和一系列随机映射函数（哈希函数）。 布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。) 将所有可能存在的数据哈希到一个足够大的bitmaps中，一个一定不存在的数据会被 这个bitmaps拦截掉，从而避免了对底层存储系统的查询压力。 进行实时监控：当发现Redis的命中率开始急速降低，需要排查访问对象和访问的数据，和运维人员配合，可以设置黑名单限制服务 17.2缓存击穿 问题描述 key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方案 key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题。 解决问题： （1）预先设置热门数据：在redis高峰访问之前，把一些热门数据提前存入到redis里面，加大这些热门数据key的时长 （2）实时调整：现场监控哪些数据热门，实时调整key的过期时长 （3）使用锁： 就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db。 先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX）去set一个mutex key 当操作返回成功时，再进行load db的操作，并回设缓存,最后删除mutex key； 当操作返回失败，证明有线程在load db，当前线程睡眠一段时间再重试整个get缓存的方法。 17.3缓存雪崩 问题描述 key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 缓存雪崩与缓存击穿的区别在于这里针对很多key缓存，前者则是某一个key 正常访问 缓存失效瞬间 解决方案 缓存失效时的雪崩效应对底层系统的冲击非常可怕！ 解决方案： 构建多级缓存架构：nginx缓存 + redis缓存 +其他缓存（ehcache等） 使用锁或队列： 用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。不适用高并发情况 设置过期标志更新缓存： 记录缓存数据是否过期（设置提前量），如果过期会触发通知另外的线程在后台去更新实际key的缓存。 将缓存失效时间分散开： 比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 17.4分布式锁 问题描述 随着业务发展的需要，原单体单机部署的系统被演化成分布式集群系统后，由于分布式系统多线程、多进程并且分布在不同机器上，这将使原单机部署情况下的并发控制锁策略失效，单纯的Java API并不能提供分布式锁的能力。为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题！ 分布式锁主流的实现方案： 基于数据库实现分布式锁 基于缓存（Redis等） 基于Zookeeper 每一种分布式锁解决方案都有各自的优缺点： 性能：redis最高 可靠性：zookeeper最高 这里，我们就基于redis实现分布式锁。 解决方案：使用redis实现分布式锁 redis:命令 set sku:1:info “OK” NX PX 10000 EX second ：设置键的过期时间为 second 秒。 SET key value EX second 效果等同于 SETEX key second value 。 PX millisecond ：设置键的过期时间为 millisecond 毫秒。 SET key value PX millisecond 效果等同于 PSETEX key millisecond value 。 NX ：只在键不存在时，才对键进行设置操作。 SET key value NX 效果等同于 SETNX key value 。 XX ：只在键已经存在时，才对键进行设置操作。 多个客户端同时获取锁（setnx） 获取成功，执行业务逻辑{从db获取数据，放入缓存}，执行完成释放锁（del） 其他客户端等待重试 编写代码 Redis: set num 0 @GetMapping(\"testLock\")public void testLock(){//1获取锁，setneBoolean lock = redisTemplate.opsForValue().setIfAbsent(\"lock\", \"111\");//2获取锁成功、查询num的值if(lock){Object value = redisTemplate.opsForValue().get(\"num\");//2.1判断num为空returnif(StringUtils.isEmpty(value)){return;}//2.2有值就转成成intint num = Integer.parseInt(value+\"\");//2.3把redis的num加1redisTemplate.opsForValue().set(\"num\", ++num);//2.4释放锁，delredisTemplate.delete(\"lock\");}else{//3获取锁失败、每隔0.1秒再获取try {Thread.sleep(100);testLock();} catch (InterruptedException e) {e.printStackTrace();}}} 重启，服务集群，通过网关压力测试： ab -n 1000 -c 100 http://192.168.140.1:8080/test/testLock 查看redis中num的值： 基本实现。 问题：setnx刚好获取到锁，业务逻辑出现异常，导致锁无法释放 解决：设置过期时间，自动释放锁。 优化之设置锁的过期时间 设置过期时间有两种方式： 首先想到通过expire设置过期时间（缺乏原子性：如果在setnx和expire之间出现异常，锁也无法释放） 在set时指定过期时间（推荐） 设置过期时间： 压力测试肯定也没有问题。自行测试 问题：可能会释放其他服务器的锁。 场景：如果业务逻辑的执行时间是7s。执行流程如下 index1业务逻辑没执行完，3秒后锁被自动释放。 index2获取到锁，执行业务逻辑，3秒后锁被自动释放。 index3获取到锁，执行业务逻辑 index1业务逻辑执行完成，开始调用del释放锁，这时释放的是index3的锁，导致index3的业务只执行1s就被别人释放。 最终等于没锁的情况。 解决：setnx获取锁时，设置一个指定的唯一值（例如：uuid）；释放前获取这个值，判断是否自己的锁 优化之UUID防误删 问题：删除操作缺乏原子性。 场景： index1执行删除时，查询到的lock值确实和uuid相等 uuid=v1 set(lock,uuid)； index1执行删除前，lock刚好过期时间已到，被redis自动释放 在redis中没有了lock，没有了锁。 index2获取了lock index2线程获取到了cpu的资源，开始执行方法 uuid=v2 set(lock,uuid)； index1执行删除，此时会把index2的lock删除 index1 因为已经在方法中了，所以不需要重新上锁。index1有执行的权限。index1已经比较完成了，这个时候，开始执行 删除的index2的锁！ 优化之LUA脚本保证删除的原子性 @GetMapping(\"testLockLua\")public void testLockLua() {//1声明一个uuid ,将做为一个value放入我们的key所对应的值中String uuid = UUID.randomUUID().toString();//2定义一个锁：lua脚本可以使用同一把锁，来实现删除！String skuId = \"25\"; //访问skuId为25号的商品100008348542String locKey = \"lock:\" + skuId; //锁住的是每个商品的数据// 3获取锁Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid, 3, TimeUnit.SECONDS);//第一种：lock与过期时间中间不写任何的代码。// redisTemplate.expire(\"lock\",10, TimeUnit.SECONDS);//设置过期时间//如果trueif (lock) {//执行的业务逻辑开始//获取缓存中的num数据Object value = redisTemplate.opsForValue().get(\"num\");//如果是空直接返回if (StringUtils.isEmpty(value)) {return;}//不是空如果说在这出现了异常！那么delete就删除失败！也就是说锁永远存在！int num = Integer.parseInt(value + \"\");//使num每次+1放入缓存redisTemplate.opsForValue().set(\"num\", String.valueOf(++num));/使用lua脚本来锁///定义lua脚本String script = \"if redis.call('get', KEYS[1]) == ARGV[1] then returnredis.call('del', KEYS[1]) else return 0 end\";//使用redis执行lua执行DefaultRedisScript redisScript = new DefaultRedisScript<>();redisScript.setScriptText(script);//设置一下返回值类型为Long//因为删除判断的时候，返回的0,给其封装为数据类型。如果不封装那么默认返回String类型，//那么返回字符串与0会有发生错误。redisScript.setResultType(Long.class);//第一个要是script脚本，第二个需要判断的key，第三个就是key所对应的值。redisTemplate.execute(redisScript, Arrays.asList(locKey), uuid);} else {//其他线程等待try {//睡眠Thread.sleep(1000);//睡醒了之后，调用方法。testLockLua();} catch (InterruptedException e) {e.printStackTrace();}}} Lua 脚本详解： 项目中正确使用： 定义key，key应该是为每个sku定义的，也就是每个sku有一把锁。String locKey =\"lock:\"+skuId; //锁住的是每个商品的数据Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid,3,TimeUnit.SECONDS); 总结 1、加锁 // 1.从redis中获取锁,set k1 v1 px 20000 nxString uuid = UUID.randomUUID().toString();Boolean lock = this.redisTemplate.opsForValue().setIfAbsent(\"lock\", uuid, 2, TimeUnit.SECONDS); 使用lua释放锁 // 2.释放锁delString script = \"if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end\";//设置lua脚本返回的数据类型DefaultRedisScript redisScript = new DefaultRedisScript<>();//设置lua脚本返回类型为LongredisScript.setResultType(Long.class);redisScript.setScriptText(script);redisTemplate.execute(redisScript, Arrays.asList(\"lock\"),uuid); 重试 Thread.sleep(500);testLock(); 为了确保分布式锁可用，我们至少要确保锁的实现同时满足以下四个条件： 互斥性。在任意时刻，只有一个客户端能持有锁。 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 加锁和解锁必须具有原子性。 17.Redis6.0新功能 17.1ACL 简介 Redis ACL是Access Control List（访问控制列表）的缩写，该功能允许根据可以执行的命令和可以访问的键来限制某些连接。 在Redis 5版本之前，Redis 安全规则只有密码控制 还有通过rename 来调整高危命令比如 flushdb ， KEYS* ， shutdown 等。Redis 6 则提供ACL的功能对用户进行更细粒度的权限控制 ： （1）接入权限:用户名和密码 （2）可以执行的命令 （3）可以操作的 KEY 参考官网：https://redis.io/topics/acl 命令 1、使用acl list命令展现用户权限列表 （1）数据说明 2、使用acl cat命令 （1）查看添加权限指令类别 （2）加参数类型名可以查看类型下具体命令 3、使用acl whoami命令查看当前用户 4、使用aclsetuser命令创建和编辑用户ACL （1）ACL规则 下面是有效ACL规则的列表。某些规则只是用于激活或删除标志，或对用户ACL执行给定更改的单个单词。其他规则是字符前缀，它们与命令或类别名称、键模式等连接在一起。 ACL规则 类型 参数 说明 启动和禁用用户 on 激活某用户账号 off 禁用某用户账号。注意，已验证的连接仍然可以工作。如果默认用户被标记为off，则新连接将在未进行身份验证的情况下启动，并要求用户使用AUTH选项发送AUTH或HELLO，以便以某种方式进行身份验证。 权限的添加删除 + 将指令添加到用户可以调用的指令列表中 - 从用户可执行指令列表移除指令 +@ 添加该类别中用户要调用的所有指令，有效类别为@admin、@set、@sortedset…等，通过调用ACL CAT命令查看完整列表。特殊类别@all表示所有命令，包括当前存在于服务器中的命令，以及将来将通过模块加载的命令。 -@ 从用户可调用指令中移除类别 allcommands +@all的别名 nocommand -@all的别名 可操作键的添加或删除 ~ 添加可作为用户可操作的键的模式。例如~*允许所有的键 （2）通过命令创建新用户默认权限 acl setuser user1 在上面的示例中，我根本没有指定任何规则。如果用户不存在，这将使用just created的默认属性来创建用户。如果用户已经存在，则上面的命令将不执行任何操作。 （3）设置有用户名、密码、ACL权限、并启用的用户 acl setuser user2 on >password ~cached:* +get (4)切换用户，验证权限 17.2IO多线程 简介 Redis6终于支撑多线程了，告别单线程了吗？ IO多线程其实指客户端交互部分的网络IO交互处理模块多线程，而非执行命令多线程。Redis6执行命令依然是单线程。 原理架构 Redis 6 加入多线程,但跟 Memcached 这种从 IO处理到数据访问多线程的实现模式有些差异。Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。之所以这么设计是不想因为多线程而变得复杂，需要去控制 key、lua、事务，LPUSH/LPOP 等等的并发问题。整体的设计大体如下: 另外，多线程IO默认也是不开启的，需要再配置文件中配置 io-threads-do-reads yes io-threads 4 工具支持 Cluster 之前老版Redis想要搭集群需要单独安装ruby环境，Redis 5 将 redis-trib.rb 的功能集成到 redis-cli 。另外官方 redis-benchmark 工具开始支持 cluster 模式了，通过多线程的方式对多个分片进行压测。 Redis新功能持续关注 Redis6新功能还有： 1、RESP3新的 Redis 通信协议：优化服务端与客户端之间通信 2、Client side caching客户端缓存：基于 RESP3 协议实现的客户端缓存功能。为了进一步提升缓存的性能，将客户端经常访问的数据cache到客户端。减少TCP网络交互。 3、Proxy集群代理模式：Proxy 功能，让 Cluster 拥有像单实例一样的接入方式，降低大家使用cluster的门槛。不过需要注意的是代理不改变 Cluster 的功能限制，不支持的命令还是不会支持，比如跨 slot 的多Key操作。 4、Modules API Redis 6中模块API开发进展非常大，因为Redis Labs为了开发复杂的功能，从一开始就用上Redis模块。Redis可以变成一个框架，利用Modules来构建不同系统，而不需要从头开始写然后还要BSD许可。Redis一开始就是一个向编写各种系统开放的平台。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 11:38:37 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/shardingsphere/":{"url":"distributed/shardingsphere/","title":"shardingsphere中间件","keywords":"","body":"数据库中间件 1.概述 2.核心概念 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 17:08:02 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/shardingsphere/概述.html":{"url":"distributed/shardingsphere/概述.html","title":"1.概述","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 为什么需要分库分表： 读写分离： 分库分表常见方式： 垂直 水平 分库分表之后带来的问题： 分库分表算法： 常见的中间件： 什么是分库分表： 一个数据库一张表分成N小表 不把鸡蛋放在一个篮子里 为什么需要分库分表： 业务越来越大，单表数据超出了数据库支持的容量。 持久化磁盘IO，传统的数据库性能瓶颈，产品经理业务必须这么做 改变程序。数据库下刀子切分优化 1、换数据库（缓存） 2、Sql、索引、字段优化 3、读写分离（业务有关优化） 4、分库分表（业务） 5、分区 读写分离： 什么是读写分离:我们一般应用程序访问数据库无非是读取数据、修改数据、插入数据、删除数据 CRUD。 分开》分库 前提条件：master》salve 主从（同步）架构 读写 互联网读多写少 Insert orders 1 select orders N 分库分表常见方式： 垂直 通俗的说法叫做“大表拆小表”，拆分是基于关系型数据库中的“列”（字段）进行的。 字段拆分 特点:1、每个库（表）的结构都不一样 ​ 2、每个库（表）的数据都（至少有一列）一样 ​ 3、每个库（表）的并集是全量数据 1、其实没太理解垂直拆分为什么要改表结构，之前在一个库里的时候 订单会员这种业务表之间也是要关联的啊？ 2、根据用户id分，会不会出现数据不平衡的情况 平衡 3、根据用户分，会不会出现数据不平衡的情况 4、老师，不明白刚刚分库的时候会有那种 链表查询，这个垂直分库不也是按数据内容分库吗 ？？？ 5、电商系统的订单量会很大，订单表数据也大，在分表的时候如果是对订单id作为分表规则，那么在查询某个用户的订单会涉及到很多个表的数据。相反，也会有同样的问题，问下老师如何解决？ 订单里面一般订单id userid（互联网网站 用户为中心 userid） 查看我的订单 Select * from orders where orderid=333 and userid=123 查看我的订单列表 Select * from orders where userid=123 1库 查看的商品？？购物车（用户）card 水平 以某个字段按照一定的规律（取模）讲一个表的数据分到多个库中 内容拆分 分库分表之后带来的问题： 读写分离：主从同步、数据一致性的问题、网络延迟的问题 分库分表： 增加了我们维护成本 分布式事务（跨库事务） 跨库join 分布式全局唯一ID（snowflake） 分库分表算法： 取模（Hash**）**通过userid用户表字段值进行123%3=xxxx 数据分散均衡，避免数据热点 一致性hash（扩容需要O（N）） 范围区分（range**）**按月 按省 A（0-6）B（7-8）C（9 10）热点数据 11那天 预定义（list**）**（100w 1亿数据 10库）风投 常见的中间件： 开源中间件：sharding-sphere atlas 当当网 张亮、高洪涛 sharding-jdbc 2018 分两种类型： Proxy**代理**：mycat（重）、mysql-proxy atlas、sharding-proxy（sharding-sphere ） Jdbc**直连：**TDDL（淘宝 半开源） 、sharding-jdbc（sharding-sphere ）缺点 讲清楚 不分上下 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 17:04:38 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/shardingsphere/核心概念.html":{"url":"distributed/shardingsphere/核心概念.html","title":"2.核心概念","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 Sharding-sphere相关特性与术语 分库分表-SQL： LogicTable逻辑表 ActualTable真实表 DataNode数据节点表 BindingTable绑定表 ShardingColumn 分片字段 Broadcast Table 广播表 Logic Index 逻辑索引 分库分表-算法： ShardingAlgorithm分片算法 StandardShardingStrategy分片策略 Config Map 读写分离： SQL Hint 主库 从库 主从同步 Sharding-sphere： Github：https://github.com/sharding-sphere/sharding-sphere 官网：http://shardingsphere.io/index_zh.html ShardingSphere是一套开源的分布式数据库中间件解决方案组成的生态圈，它由Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar（规划中）这3款相互独立，却又能够混合部署配合使用的产品组成。 它们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景。 Sharding-sphere相关特性与术语 分库分表-SQL： LogicTable逻辑表 数据分片的逻辑表，对于水平拆分的数据库(表)，同一类表的总称。例：订单数据根据主键尾数拆分为10张表,分别是t_order_0到t_order_9，他们的逻辑表名为t_order。 ActualTable真实表 在分片的数据库中真实存在的物理表。即上个示例中的t_order_0到t_order_9。 DataNode数据节点表 数据分片的最小单元。由数据源名称和数据表组成，例：ds_1.t_order_0。配置时默认各个分片数据库的表结构均相同，直接配置逻辑表和真实表对应关系即可。如果各数据库的表结果不同，可使用ds.actual_table配置。 BindingTable绑定表 指在任何场景下分片规则均一致的主表和子表。例：订单表和订单项表，均按照订单ID分片，则此两张表互为BindingTable关系。BindingTable关系的多表关联查询不会出现笛卡尔积关联，关联查询效率将大大提升。 ShardingColumn 分片字段 分片字段。用于将数据库(表)水平拆分的关键字段。例：订单表订单ID分片尾数取模分片，则订单ID为分片字段。SQL中如果无分片字段，将执行全路由，性能较差。Sharding-JDBC支持多分片字段。 Broadcast Table 广播表 指所有的分片数据源中都存在的表，表结构和表中的数据在每个数据库中均完全一致。适用于数据量不大且需要与海量数据的表进行关联查询的场景，例如：字典表。 Logic Index 逻辑索引 某些数据库（如：PostgreSQL）不允许同一个库存在名称相同索引，某些数据库（如：MySQL）则允许只要同一个表中不存在名称相同的索引即可。 逻辑索引用于同一个库不允许出现相同索引名称的分表场景，需要将同库不同表的索引名称改写为索引名 + 表名，改写之前的索引名称成为逻辑索引。 分库分表-算法： ShardingAlgorithm分片算法 。Sharding-JDBC通过分片算法将数据分片，支持通过等号、BETWEEN和IN分片。分片算法目前需要业务方开发者自行实现，可实现的灵活度非常高。未来Sharding-JDBC也将会实现常用分片算法，如range，hash和tag等。 http://shardingsphere.io/document/current/cn/features/sharding/concept/sharding/ StandardShardingStrategy分片策略 Config Map 通过ConfigMap可以配置分库分表或读写分离数据源的元数据，可通过调用ConfigMapContext.getInstance()获取ConfigMap中的shardingConfig和masterSlaveConfig数据。例：如果机器权重不同则流量可能不同，可通过ConfigMap配置机器权重元数据。 读写分离： Mybatis 1主多从 SQL Hint 对于分片字段非SQL决定，而由其他外置条件决定的场景，可使用SQL Hint灵活的注入分片字段。例：内部系统，按照员工登录ID分库，而数据库中并无此字段。SQL Hint支持通过ThreadLocal和SQL注释(待实现)两种方式使用。 主库 添加、更新以及删除数据操作所使用的数据库，目前仅支持单主库。 从库 查询数据操作所使用的数据库，可支持多从库。 主从同步 将主库的数据异步的同步到从库的操作。由于主从同步的异步性，从库与主库的数据会短时间内不一致。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 17:05:56 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/elk/":{"url":"distributed/elk/","title":"elk日志监控","keywords":"","body":"elk日志监控 1.分布式之ELK Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-07-21 16:36:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/elk/elk.html":{"url":"distributed/elk/elk.html","title":"1.分布式之ELK","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.Elasticsearch 1.1配置文件elasticsearch.yml 1.2启动 1.3访问 2.Logstash 2.1创建配置文件logstash.conf 2.2启动 3.Kibana 3.1配置kibana.yml 3.2启动 3.3访问 1.Elasticsearch 1.1配置文件elasticsearch.yml (如果是单机测试可以不用配置直接默认启动就可以了) 配置集群名称，保证每个节点的名称相同，如此就能都处于一个集群之内了 cluster.name: elasticsearch-cluster # 每一个节点的名称，必须不一样 node.name: es-node1 # http端口（使用默认即可） http.port: 9200 # 主节点，作用主要是用于来管理整个集群，负责创建或删除索引，管理其他非master节点（相当于企业老总） node.master: true # 数据节点，用于对文档数据的增删改查 node.data: true # 集群列表 discovery.seed_hosts: [\"192.168.0.100\", \"192.168.0.101\", \"192.168.0.102\"] # 启动的时候使用一个master节点 cluster.initial_master_nodes: [\"es-node1\"] 1.2启动 ./bin/elasticSearch –d 启动。-d表示后台启动 1.3访问 127.0.0.1:9200 2.Logstash 2.1创建配置文件logstash.conf input { file { type => \"log\" path => [\"/export/home/tomcat/domains/*/*/logs/*.out\"] start_position => \"end\" ignore_older => 0 codec=> multiline { //配置log换行问题 pattern => \"^%{TIMESTAMP_ISO8601}\" negate => true what => \"previous\" } } beats { port => 5044 } } output { if [type] == \"log\" { elasticsearch { hosts => [\"http://127.0.0.1:9200\"] index => \"log-%{+YYYY.MM}\" } } } 2.2启动 ./logstash -f ../config/logstash.conf 后台启动：nohup ./bin/logstash -f config/log.conf > log.log & 配置多个文件：./logstash -f ../config 指定启动目录，然后启动目录下配置多个*.conf文件。里面指定不同的logpath。 3.Kibana 3.1配置kibana.yml elasticsearch.url: http://localhost:9200 server.host: 0.0.0.0 3.2启动 启动命令：./kibana 后台启动：nohup ./bin/kibana & 3.3访问 http://localhost:5601 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 15:55:52 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/":{"url":"micro/","title":"六、微服务专题","keywords":"","body":"微服务专题 本章主要以spring boot为基础，说明自动加载原理，为后面springcloud-netflix和springcloud-alibaba二个spring cloud体系打下基础，最后说明docker部署微服务项目 springboot 1.快速开始 2.Springboot启动Tomcat热身 3.springboot自动装配原理 4.SpringBoot源码分析 springcloud-netflix 5.Eureka源码分析 6.Ribbon&Feign介绍及使用 7.Ribbon&Feign源码分析 8.Hystrix限流降级熔断 9.网关zuul 10.Hystrix&Zuul源码分析 12.配置中心Config 13.链路跟踪Sleuth springcloud-alibaba 5.Eureka源码分析 6.Ribbon&Feign介绍及使用 7.Ribbon&Feign源码分析 8.Hystrix限流降级熔断 9.网关zuul 10.Hystrix&Zuul源码分析 12.配置中心Config 13.链路跟踪Sleuth docker 14.Docker快速入门 15.DockerCompose微服务编排 16.Docker整体编排SpringCloud Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:40:11 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/springboot-index.html":{"url":"micro/springboot-index.html","title":"springboot","keywords":"","body":"springboot 1.快速开始 2.Springboot启动Tomcat热身 3.springboot自动装配原理 4.SpringBoot源码分析 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:50:10 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/fast.html":{"url":"micro/fast.html","title":"1.快速开始","keywords":"","body":"第一节:springboot快速开始(对三篇) 一:springboot微服务开发利器 1.1)什么是微服务，微服务和微服务架构的区别? 目前而已，对于微服务业界没有一个统一的标准定义,但是通常而言提倡把一个单一的应用程序划分为一组小 的服务， 每个小的服务都会运行在自己的进程中，服务之间通过轻量级的通信机制（http的rest api）进行通信,那么一个个的 小服务就是微服务。 ①：单体架构与微服务架构图示 传统的的单一电商应用来说，订单，支付，用户，商品，库存等模块都在一个项目中，若某一个模块出现线上bug，会导致整个版本发布回退. 若把单一应用拆分为一个一微服务，比如订单微服务，用户微服务，商品微服务，积分微服务等，若某一个微服务出错不会导致整个版本回退。 1.2）什么是微服务架构 微服务架构是一种架构模式（用于服务管理微服务的），它把一组小的服务互相协调、互相配合，并且完成功能。每个服务运行在其独立的进程中，服务与服务间采用轻量级的通信机制互相协作（通常是基于HTTP协议的RESTfulAPI）。每个服务都围绕着具体业务进行构建，并且能够被独立的部署到生产环境、类生产环境等。另外，应当尽量避免统一的、集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具对其进行构建。 1.3微服务的优缺点: 优点: ①:优点每个服务足够内聚，足够小，代码容易理解这样能聚焦一个指定的业务功能或业务需求(职责单 一) ②:开发简单、开发效率提高，一个服务可能就是专一的只干一件事,微服务能够被小团队单独开发，这个小团队是2到5人的开发人员组成。 ③:微服务能使用不同的语言开发。 ④:易于和第三方集成，微服务允许容易且灵活的方式集成自动部署，通过持续集成工具，如Jenkins,Hudson,bamboo。 ⑤:微服务只是业务逻辑的代码，不会和HTML,CSS或其他界面组件混合。 ⑥:每个微服务都有自己的存储能力，可以有自己的数据库。也可以有统一数据库。 ......................................... ......................................... 缺点: 开发人员要处理分布式系统的复杂性(分布式事物) 多服务运维难度，随着服务的增加，运维的压力也在增大 系统部署依赖 服务间通信成本 数据一致性 ................................................. ................................................. 二:springboot快速开始 2.1)(基于mavne版本构建) 2.1)先把maven的配置文件设置为如下配置 jdk‐1.8 true 1.8 1.8 1.8 1.8 2.1)配置IDE的环境（maven配置） 2.2)创建一个空的maven工程，然后导入springboot相关的jar包 //父工程依赖 org.springframework.boot spring-boot-starter-parent 2.0.8.RELEASE spring mvc-web的依赖 org.springframework.boot spring-boot-starter-web org.springframework.boot spring-boot-maven-plugin ①：编写主入口程序 /** * Created by smlz on 2019/3/18. */ @SpringBootApplication public class TulingStartMain { public static void main(String[] args) { SpringApplication.run(TulingStartMain.class,args); } } ②:其他业务组件 比如controller service repository compent注解标示的组件 **********自己写的组件必须放在主启动类(TulingStartMain)在所包的及其子包下????????将源码分析时候探究原理 /** * Created by smlz on 2019/3/18. */ @RestController public class TulingController { @RequestMapping(\"/tuling\") public String tulingHelloWorld() { return \"tuling,hello\"; } } ③:运行main函数启动程序，访问[http://localhost:8080/](http://localhost:8080/hello?fileGuid=DnEDhh3KIyMoGVBP)tuling，或者执行mvn package将项目打成jar包，用java -jar XXX.jar直接运行 ![图片](https://uploader.shimo.im/f/3PZXinbItnJHhIr7.jpeg!thumbnail?fileGuid=DnEDhh3KIyMoGVBP) 2.3)通过sts/idea创建 一个springboot项目 ![图片](https://uploader.shimo.im/f/4r85ihaUCMXt4W3C.png!thumbnail?fileGuid=DnEDhh3KIyMoGVBP) 编写自己的业务代码就maven构建springboot工程版本的一样 这里就不做累赘讲诉. 三:helloworld的探究，为啥我只要引入spring-boot-starter-parent和spring-boot-starter-web就可以快速开发mvc的项目 3.1）pom分析 org.springframework.boot spring-boot-starter-parent 2.0.8.RELEASE 真正的版本管理仲裁中心来决定应用的版本 org.springframework.boot spring-boot-dependencies 2.0.8.RELEASE ../../spring-boot-dependencies 以后我们导入依赖默认是不需要写版本；（没有在dependencies里面管理的依赖自然需要声明版本号） 3.2)我们来分析看下spring-boot-starter-web（场景启动器）为我项目中导入web开发需要的jar包依赖 4)多profile切换 我们在开发应用时，通常一个项目会被部署到不同的环境中，比如：开发、测试、生产等。其中每个环境的数据库地址、服务器端口等等配置都会不同，对于多环境的配置，大部分构建工具或是框架解决的基本思路是一致的，通过配置多份不同环境的配置文件，再通过打包命令指定需要打包的内容之后进行区分打包 4.1)yml支持多模块文档块 server: port: 8081 servlet: context-path: /tuling01 spring: profiles: active: dev 开发环境配置 spring: profiles: dev server: port: 8082 生产环境配置 spring: profiles: prod server: port: 8083 从上图看出，我们激活的配置是开发环境的配置,但是现在 我们还看到了servlet:context-path的配置形成互补配置 4.2)多yml|properties文件的环境切换 application.yml (用于激活不同环境的配置文件) spring: profiles: active: dev application-dev.yml server: port: 8081 servlet: context-path: /tl_dev application-prod.yml server: port: 8082 servlet: context-path: /tl_prod 4.3)激活指定环境配置的方法 ①:直接在application.yml的配置文件中使用spring.profiles.active=dev|prod|test ②:设置虚拟机参数-Dspring.profiles.active=dev|prod|test ③:命令行参数启动(打成Jar包时候) java -jar tuling-vip-springboot-02-0.0.1-SNAPSHOT.jar -- spring.profiles.active=prod 4.4)设置jvm参数 然后我们看是否设置成功 java -Xms128m -Xmx128m -jar tuling-vip-springboot-02-0.0.1-SNAPSHOT.jar -- server.port=8888 第一步:在cmd窗口中使用jps来看我们主进程的 第二步:使用jinfo命令+进程号来查看具体信息 4.5) springboot关于打包问题总结 4.5.1):打成指定的jar名称的 指定打包的文件名称 tulingVipSpringboot org.springframework.boot spring-boot-maven-plugin 4.5.2)若出现工程中出现多个mainclass的时候需要指定主启动类 tulingVipSpringboot org.springframework.boot spring-boot-maven-plugin com.tuling.TulingVipSpringboot02Application repackage 4.5.3）如何打出一个war包 第一步:指定springboot pom中的打包方式 由jar改为war 第二步:在spring-boot-starter-web模块打包比依赖与tomcat 第三步:主启动类上 实现SpringBootServletInitializer从写confiure方法(原理第三节课节讲) @SpringBootApplication public class TulingVipSpringboot03Application extends SpringBootServletInitializer { public static void main(String[] args) { SpringApplication.run(TulingVipSpringboot03Application.class, args); } @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(TulingVipSpringboot03Application.class); } } 第四步:打成war包 放在tomcat上运行. 6)springboot的web开发（） 6.1）什么是webJar：以jar包的形式来引入前端资源,比如jquery或者是BootStrap https://www.webjars.org/ 6.1.1）引入对应的jar包 org.webjars jquery 3.3.1-2 6.1.2)映射规则/webjars/**都会被映射到classpath:/META-INF/resources/webjars/目录下去处理 6.1.3)前端资源映射规则 核心源代码: public void addResourceHandlers(ResourceHandlerRegistry registry) { if(!this.resourceProperties.isAddMappings()) { logger.debug(\"Default resource handling disabled\"); } else { Duration cachePeriod = this.resourceProperties.getCache().getPeriod(); CacheControl cacheControl = this.resourceProperties.getCache().getCachecontrol().toHttpCacheControl(); //处理映射webjar的请求的 if(!registry.hasMappingForPattern(\"/webjars/**\")) { this.customizeResourceHandlerRegistration(registry.addResourceHandler(new String[]{\"/webjars/**\"}).addRes } //处理静态资源文件的 String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if(!registry.hasMappingForPattern(staticPathPattern)) { this.customizeResourceHandlerRegistration(registry.addResourceHandler(new String[]{staticPathPattern}).add } } } 6.1.4）http://localhost:8080/webjars/jquery/3.3.1-2/jquery.js 请求如何拦截处理请求的 ①根据日志打印，我们发现如下突破口 ②:第二步: org.springframework.web.servlet.resource.ResourceHttpRequestHandler#handleRequest方法 org.springframework.web.servlet.resource.ResourceHttpRequestHandler#getResource org.springframework.web.servlet.resource.ResourceResolverChain#resolveResource org.springframework.web.servlet.resource.PathResourceResolver#resolveResourceInternal org.springframework.web.servlet.resource.PathResourceResolver#getResource(真正的资源映射 处理逻辑) private Resource getResource(String resourcePath, @Nullable HttpServletRequest request, List locations) { for (Resource location : locations) { try { if (logger.isTraceEnabled()) { logger.trace(\"Checking location: \" + location); } String pathToUse = encodeIfNecessary(resourcePath, request, location); //真正的处理逻辑把jquery/3.3.1-2/jquery.js映射到 Resource resource = getResource(pathToUse, location); if (resource != null) { if (logger.isTraceEnabled()) { logger.trace(\"Found match: \" + resource); } return resource; } else if (logger.isTraceEnabled()) { logger.trace(\"No match for location: \" + location); } } catch (IOException ex) { logger.trace(\"Failure checking for relative resource - trying next location\", ex); } } return null; } 6.1.5)访问静态html页面 我们直接把静态页面放在static的目录下，直接可以在路径直接访问 6.1.6)映射原理/**请求都会被映射到 private static final String[] CLASSPATH_RESOURCE_LOCATIONS = { \"classpath:/META-INF/resources/\", \"classpath:/resources/\", \"classpath:/static/\", \"classpath:/public/\" }; public void addResourceHandlers(ResourceHandlerRegistry registry) { ..... ...... ...... String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if (!registry.hasMappingForPattern(staticPathPattern)) { customizeResourceHandlerRegistration( registry.addResourceHandler(staticPathPattern) .addResourceLocations(getResourceLocations( this.resourceProperties.getStaticLocations())) .setCachePeriod(getSeconds(cachePeriod)) .setCacheControl(cacheControl)); } } 6.1.7)欢迎页； 静态资源文件夹下的所有index.html页面；被\"/**\"映射； 6.1.8)使用webjar的方式修前端页面修改引用路径 6.2)springboot是如何整合springmvc功能的（WebMvcAutoConfiguration） 6.2.1）自动装配的组件 ①:ContentNegotiatingViewResolver和BeanNameViewResolver视图解析器 视图解析器的作用:根据方法的值找到对应的视图 ②:Support for serving static resources, including support for WebJars支持静态资源和webJars ③:Converter ,日期格式化器Formatter ④:消息装换器:HttpMessageConverters ⑤:首页设置index.html ⑥:图标支持Favicon 6.2.2)如何扩展springmvc的配置（springboot提我们自己配置的springmvc的功能不丢失的情况下） 比如我需要使用自己定义的拦截器 我们需要自己写一个配置类 继承WebMvcConfigurerAdapter需要什么组件 就注册什么组件 A:如何往容器中添加一个拦截器 第一步:创建一个拦截器 @Component public class TulingInterceptor implements HandlerInterceptor { public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)throws Exception System.out.println(\"我是TulingInterceptor的preHandle方法\"); return true; } public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler,@Nullable ModelAn System.out.println(\"我是TulingInterceptor的postHandle方法\"); } public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler,@Nullable Exce System.out.println(\"我是TulingInterceptor的afterCompletion方法\"); } } 第二步:注册拦截器 @Configuration public class TulingConfig extends WebMvcConfigurerAdapter { @Autowired private TulingInterceptor tulingInterceptor; /** 注册拦截器 @param registry */ public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(tulingInterceptor).addPathPatterns(\"/**\").excludePathPatterns(\"/index.html\",\"/\"); } } B:往容器中增加一个过滤器 public class TulingFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOE System.out.println(\"TulingFilter的doFilter方法\"); filterChain.doFilter(servletRequest,servletResponse); } @Override public void destroy() { } } /** 注册一个filter @return */ @Bean public FilterRegistrationBean tulingFilter(){ FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setFilter(new TulingFilter()); filterRegistrationBean.addUrlPatterns(\"/*\"); return filterRegistrationBean; } C:往容器中增加一个servlet public class TulingServlet extends HttpServlet { protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().write(\"hello......\"); } protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { doPost(req,resp); } } public class TulingServlet extends HttpServlet { protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().write(\"hello......\"); } protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { doPost(req,resp); } } 7）如何全面接管springboot的mvc配置(让springboot给我们自动配置的功能失效,自己像如何整合ssm一样的整合springmvc,不推荐) 官网原话: If you want to keep Spring Boot MVC features and you want to add additional MVCconfiguration (interceptors, formatters, view controllers, and other features), you canadd your own@Configurationclass of typeWebMvcConfigurerbut without@EnableWebMvc. If you wish to provide custom instances ofRequestMappingHandlerMapping,RequestMappingHandlerAdapter, orExceptionHandlerExceptionResolver, you can declare aWebMvcRegistrationsAdapterinstance to provide such components. 大概意思说，在配置文件中使用一个@EnableWebMvc来标识到配置类上,就会导致配置失效why?为什么会失 效????????????????? 原理: @EnableWebMvc为容器中导入了DelegatingWebMvcConfiguration的组件 @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Import(DelegatingWebMvcConfiguration.class) public @interface EnableWebMvc { } 1)我们来分析一下DelegatingWebMvcConfiguration是一个什么东西？？？？？ 我们发现DelegatingWebMvcConfiguration是WebMvcConfiurationSupport（只保证了springmvc的基本功能）类型的 @Configuration public class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport 2)我们来看下WebMvcAutoConfiguration上的注解 @Configuration @ConditionalOnWebApplication(type = Type.SERVLET) @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class }) //容器中没有WebMvcConfigurationSupport该配置文件才生生效,但是我们使用了@EnableWebMvc导入了WebMvcConfiuratio //只保存了springmvc的最基本的功能 @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10) @AutoConfigureAfter({ DispatcherServletAutoConfiguration.class, ValidationAutoConfiguration.class }) public class WebMvcAutoConfiguration 3)我们的webJar欢迎页 等全部失效 8)springboot错误处理机制?如何定制错误页面？ 案例:浏览器模拟发送的错误请求http://localhost:8080/aaaaaaaaaaaaaa 案例2:通过postman或者restlet发送的请求http://localhost:8080/testTuling/dddd 我们可以看出 不同的终端发送的请求 会返回不同的错误异常类容是根据什么原理？ 原理:是根据不同客户端发送的请求的请求头来区分是 返回页面还是json数据 8.1）我们来看springboot为我们自动配置的异常处理的一些bean org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration @Bean @ConditionalOnMissingBean(value = ErrorAttributes.class, search = SearchStrategy.CURRENT) public DefaultErrorAttributes errorAttributes() { return new DefaultErrorAttributes( this.serverProperties.getError().isIncludeException()); } @Bean @ConditionalOnMissingBean(value = ErrorController.class, search = SearchStrategy.CURRENT) public BasicErrorController basicErrorController(ErrorAttributes errorAttributes) { return new BasicErrorController(errorAttributes, this.serverProperties.getError(), this.errorViewResolvers); } @Bean public ErrorPageCustomizer errorPageCustomizer() { return new ErrorPageCustomizer(this.serverProperties, this.dispatcherServletPath); } @Bean @ConditionalOnBean(DispatcherServlet.class) @ConditionalOnMissingBean public DefaultErrorViewResolver conventionErrorViewResolver() { return new DefaultErrorViewResolver(this.applicationContext, this.resourceProperties); } @Configuration @ConditionalOnProperty(prefix = \"server.error.whitelabel\", name = \"enabled\", matchIfMissing = true) @Conditional(ErrorTemplateMissingCondition.class) protected static class WhitelabelErrorViewConfiguration { private final SpelView defaultErrorView = new SpelView( \"Whitelabel Error Page\" \"This application has no explicit mapping for /error, so you are seeing this as a fall \"${timestamp}\" \"There was an unexpected error (type=${error}, status=${status}).\" \"${message}\"); @Bean(name = \"error\") @ConditionalOnMissingBean(name = \"error\") public View defaultErrorView() { return this.defaultErrorView; } 我们具体来分析上诉源代码的组件 A: org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration.ErrorPageCustomizer（错误页面定制器） 作用：系统出现错误以后来到/error请求进行处理； /** Path of the error controller. */ @Value(\"${error.path:/error}\") private String path = \"/error\"; 那么当我们 发生错误，需要/error的请求映射来请求 接下来就会引出另外一个组件 来处理/error请求 B：org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController（基础错误控制器） @Controller @RequestMapping(\"${server.error.path:${error.path:/error}}\") public class BasicErrorController extends AbstractErrorController { //处理浏览器页面异常 @RequestMapping(produces = \"text/html\") public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { HttpStatus status = getStatus(request); Map model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView != null) ? modelAndView : new ModelAndView(\"error\", model); } //处理postman请求的Json数据异常错误 @RequestMapping @ResponseBody public ResponseEntity> error(HttpServletRequest request) { Map body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity<>(body, status); } } B1:我们来看下浏览器的响应过程怎么来处理请求异常信息的？ public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { //获取状态码 HttpStatus status = getStatus(request); //获取页面的模型数据 Map model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); //解析错误视图 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView != null) ? modelAndView : new ModelAndView(\"error\", model); } protected ModelAndView resolveErrorView(HttpServletRequest request, HttpServletResponse response, HttpStatus status, Map model) { //获取容器中的所有错误视图解析器DefaultErrorViewResolver for (ErrorViewResolver resolver : this.errorViewResolvers) { ModelAndView modelAndView = resolver.resolveErrorView(request, status, model); if (modelAndView != null) { return modelAndView; } } return null; } B2:我们接着分析 org.springframework.boot.autoconfigure.web.servlet.error.DefaultErrorViewResolver#DefaultErrorViewResolver 错误视图解析器 @Override public ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map model) { //解析视图 ModelAndView modelAndView = resolve(String.valueOf(status), model); //没有对应的解析精确匹配的状态码 使用模糊匹配比如4XX 5XX if (modelAndView == null && SERIES_VIEWS.containsKey(status.series())) { //返回4XX 5XX的页面 modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); } return modelAndView; } private ModelAndView resolve(String viewName, Map model) { error/404 String errorViewName = \"error/\" + viewName; //视图是否有模版引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); //有模版引擎解析直接返回 if (provider != null) { return new ModelAndView(errorViewName, model); } //静态html的页面解析 return resolveResource(errorViewName, model); } private ModelAndView resolveResource(String viewName, Map model) { for (String location : this.resourceProperties.getStaticLocations()) { try { //在static模版下需要创建一个error/404.html Resource resource = this.applicationContext.getResource(location); resource = resource.createRelative(viewName + \".html\"); //存在该页面直接返回 if (resource.exists()) { return new ModelAndView(new HtmlResourceView(resource), model); } } catch (Exception ex) { } } return null; } 浏览器模拟发送异常请求的流程视图解析过程 org.springframework.boot.autoconfigure.web.servlet.error.AbstractErrorController#resolveErrorView 开始解析视图，获取所有的异常错误视图解析器 org.springframework.boot.autoconfigure.web.servlet.error.DefaultErrorViewResolver#resolveErrorView默认错误视图解析器解析视图 org.springframework.boot.autoconfigure.web.servlet.error.DefaultErrorViewResolver#resolve响应码精准匹配视图 1)判断模版引擎是否能够处理错误视图,能处理就处理，不能处理交给静态页面解析处理 org.springframework.boot.autoconfigure.web.servlet.error.DefaultErrorViewResolver#resolveResour html资源视图 若不能精准匹配，那么就进行4XX 5XX模糊匹配 若不能精准匹配(error/状态码.html)的错误页面，也没有（error/状态码开头xx.html错误页面那就使用默认的错误空白页面） private final SpelView defaultErrorView = new SpelView( \"Whitelabel Error Page\" \"This application has no explicit mapping for /error, so you are seeing this as a fall \"${timestamp}\" \"There was an unexpected error (type=${error}, status=${status}).\" \"${message}\"); @Bean(name = \"error\") @ConditionalOnMissingBean(name = \"error\") public View defaultErrorView() { return this.defaultErrorView; } 我们怎么包含一个自己的错误异常信息的 自适应的效果 浏览器效果:(需要返回自己定义的错误页面 包含了自定义的错误异常信息) 其他客户端的效果: 第一步:我们定义一个全局异常处理器，然后返回看执行效果 @ControllerAdvice public class TulingExceptionHanlder { /** 浏览器和其他客户端都返回了json数组，不满足自适应 @param e @param request @return */ @ExceptionHandler(value= TulingException.class) @ResponseBody public Map dealException(TulingException e, HttpServletRequest request){ Map retInfo = new HashMap<>(); retInfo.put(\"code\",e.getCode()); retInfo.put(\"msg\",e.getMsg()); return retInfo; } } 效果:浏览器不满足 自适应效果返回的是一个json字符串,而不是一个页面 其他客户端满足要求，返回自己定义的错误异常信息 第二步:在异常处理器中 进行重定向 根据第一步的效果来看 浏览器不能满足自适应效果,那么我们看下BasicErrorController的类 @Controller @RequestMapping(\"${server.error.path:${error.path:/error}}\") public class BasicErrorController extends AbstractErrorController 他处理的请求是/error的请求，那么我们就想到 在全局异常处理器进行重定向 @ControllerAdvice public class TulingExceptionHanlder { @ExceptionHandler(value= TulingException.class) public String dealException(TulingException e, HttpServletRequest request){ Map retInfo = new HashMap<>(); retInfo.put(\"code\",e.getCode()); retInfo.put(\"msg\",e.getMsg()); //重定向，把请求转发到BasicErrorController来处理/error return \"forward:/error\"; } 执行效果: 分析过程 ①:根据上述执行效果我们发现 进行转发后 他的http状态码变为200那么错误异常处理就不能进行正常流程的处理 ②:那么我们需要分析错误异常处理器看下是如何获取异常状态码的. org.springframework.boot.autoconfigure.web.servlet.error.AbstractErrorController#getStatus 很明显，BasicErrorController的getStatus的过程中，都是从request中获取javax.servlet.error.status_code属性 protected HttpStatus getStatus(HttpServletRequest request) { Integer statusCode = (Integer) request .getAttribute(\"javax.servlet.error.status_code\"); if (statusCode == null) { return HttpStatus.INTERNAL_SERVER_ERROR; } try { return HttpStatus.valueOf(statusCode); } catch (Exception ex) { return HttpStatus.INTERNAL_SERVER_ERROR; } } 那么我们需要在我们的全局异常处理器中request中设置该属性 页面返回的属性字段是在哪里配置的？？？ 那我们来着重分析一下 org.springframework.boot.web.servlet.error.DefaultErrorAttributes#getErrorAttributes 疑问:我们来看下这个类的自动装配原理,发现容器中有ErrorAttributes主键，那么就不进行自动装配,我们可以来自己写一个类来继承他 @Component public class TulingErrorAttribute extends DefaultErrorAttributes { public Map getErrorAttributes(WebRequest webRequest, boolean includeStackTrace) { //获取父类的封装字段结果 Map retInfo = super.getErrorAttributes(webRequest,includeStackTrace); //获取全局异常自定义的结果 Map ext = (Map) webRequest.getAttribute(\"ext\",0); //封装自定义的错误信息 retInfo.put(\"company\",\"tuling\"); retInfo.put(\"ext\",ext); return retInfo; } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/tomcat.html":{"url":"micro/tomcat.html","title":"2.Springboot启动Tomcat热身","keywords":"","body":"Springboot启动Tomcat热身 不得不说的后置处理器org.springframework.beans.factory.config.BeanPostProcessor 调用时机:在每个Bean调用构造方法之后，初始化前后进行工作 public interface BeanPostProcessor { default Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { } default Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { return bean; } ｝ 关键触发时机: org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#doCreateBean org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#initializeBean org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#applyBeanPostProcessorsBeforeIni org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#invokeInitMethods org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#applyBeanPostProcessorsAfterIniti 那我们就来大致看下applyBeanPostProcessorsBeforeInitialization方法 调用所有后置处理器的postProcessAfterInitialization方法对当前bean进行拦截 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) { Object current = processor.postProcessAfterInitialization(result, beanName); if (current == null) { return result; } result = current; } return result; } invokeInitMethods方法 boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean && (mbd == null || !mbd.isExternallyManagedInitMethod(\"afterPropertiesSet\"))) { if (logger.isDebugEnabled()) { logger.debug(\"Invoking afterPropertiesSet() on bean with name '\" + beanName + \"'\"); } if (System.getSecurityManager() != null) { try { AccessController.doPrivileged((PrivilegedExceptionAction) () -> { ((InitializingBean) bean).afterPropertiesSet(); return null; }, getAccessControlContext()); } catch (PrivilegedActionException pae) { throw pae.getException(); } } //调用InitializingBean的afterPropertiesSet方法 else { ((InitializingBean) bean).afterPropertiesSet(); } } //调用自定义的init方法 if (mbd != null && bean.getClass() != NullBean.class) { String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) && !(isInitializingBean && \"afterPropertiesSet\".equals(initMethodName)) && !mbd.isExternallyManagedInitMethod(initMethodName)) { invokeCustomInitMethod(beanName, bean, mbd); } } 然后再看下:applyBeanPostProcessorsAfterInitialization //调用所有后置处理器的postProcessAfterInitialization方法 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) { Object current = processor.postProcessAfterInitialization(result, beanName); if (current == null) { return result; } result = current; } return result; } BeanPostProcessor是什么时候注册到容器中去的? org.springframework.context.support.AbstractApplicationContext#registerBeanPostProcessors org.springframework.context.support.PostProcessorRegistrationDelegate#registerBeanPostProcessors public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) { //找到所有的后置处理器的名称 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); Register BeanPostProcessorChecker that logs an info message when a bean is created during BeanPostProcessor instantiation, i.e. when a bean is not eligible for getting processed by all BeanPostProcessors. int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)) Separate between BeanPostProcessors that implement PriorityOrdered, Ordered, and the rest. List priorityOrderedPostProcessors = new ArrayList<>(); List internalPostProcessors = new ArrayList<>(); List orderedPostProcessorNames = new ArrayList<>(); List nonOrderedPostProcessorNames = new ArrayList<>(); //把后置处理器区分开来(出分包括创建bean对象) for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } //按照上面区分的后置处理器来进行注册(加入到容器中) //注册实现PriorityOrderd接口的 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); //注册实现Ordered接口的 List orderedPostProcessors = new ArrayList<>(); for (String ppName : orderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } sortPostProcessors(orderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, orderedPostProcessors); // Now, register all regular BeanPostProcessors. List nonOrderedPostProcessors = new ArrayList<>(); for (String ppName : nonOrderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); re-register all internal BeanPostProcessors. sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); Re-register post-processor for detecting inner beans as ApplicationListeners, moving it to the end of the processor chain (for picking up proxies etc). beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); } SpringBoot依靠 自动装配 来如何装配Tomcat的 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/springboot-auto.html":{"url":"micro/springboot-auto.html","title":"3.springboot自动装配原理","keywords":"","body":"springboot自动装配原理详解 1)传统ssm整合redis的时候 需要在xml的配置文件中 进行大量的配置Bean 我们在这里使用springboot来代替ssm的整合，只是通过xml的形式来整合redis 第一步:加入配置 org.springframework.data spring-data-redis 2.0.9.RELEASE redis.clients jedis 2.9.0 第二步:配置xml的bean的配置 //配置连接池 //配置连接工厂 //配置redisTemplate模版类 第三步:导入配置 @ImportResource(locations = \"classpath:beans.xml\")可以导入xml的配置文件 @SpringBootApplication @ImportResource(locations = \"classpath:beans.xml\") @RestController public class TulingOpenAutoconfigPrincipleApplication { @Autowired private RedisTemplate redisTemplate; public static void main(String[] args) { SpringApplication.run(TulingOpenAutoconfigPrincipleApplication.class, args); } @RequestMapping(\"/testRedis\") public String testRedis() { redisTemplate.opsForValue().set(\"smlz\",\"smlz\"); return \"OK\"; } } 2)综上所述 我们发现，若整合redis的时候通过传统的整合，进行了大量的配置,那么我们来看下通过springboot自动装配整合的对比 导入依赖: org.springframework.boot spring-boot-starter-data-redis 修改yml配置文件 spring.redis.host=47.104.128.12 spring.redis.port=6379 spring.redis.password=123456 直接使用(下述代码可以不要配置，为了解决保存使用jdk的序列方式才配置的) @Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate template = new RedisTemplate<>(); template.setDefaultSerializer(new Jackson2JsonRedisSerializer(Object.class)); template.setConnectionFactory(redisConnectionFactory); return template; } 3）传统整合和springboot自动装配 优劣势分析。。。。。。。。。。。。 4）自动装配原理前的不得不说的几个注解 4.1)通过@Import注解来导入ImportSelector组件 ①:写一个配置类在配置类上标注一个@Import的注解， @Configuration @Import(value = {TulingSelector.class}) public class TulingConfig { } ②：在@Import注解的value值 写自己需要导入的组件 在selectImports方法中 就是你需要导入组件的全类名 public class TulingSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { return new String[]{\"com.tuling.service.TulingServiceImpl\"}; } } 核心代码: @RestController public class TulingController { //自动注入tulingServiceImpl @Autowired private TulingServiceImpl tulingServiceImpl; @RequestMapping(\"testTuling\") public String testTuling() { tulingServiceImpl.testService(); return \"tulingOk\"; } } 这里是没有标注其他注解提供给spring包扫描的 public class TulingServiceImpl { public void testService() { System.out.println(\"我是通过importSelector导入进来的service\"); } } 1.2）通过@Import导入ImportBeanDefinitionRegistrar从而进来导入组件 核心代码: public class TulingImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata annotationMetadata, BeanDefinitionRegistry beanDefinitionReg //定义一个BeanDefinition RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(TulingDao.class); //把自定义的bean定义导入到容器中 beanDefinitionRegistry.registerBeanDefinition(\"tulingDao\",rootBeanDefinition); } } 通过ImportSelector功能导入进来的 public class TulingServiceImpl { @Autowired private TulingDao tulingDao; public void testService() { tulingDao.testTulingDao(); System.out.println(\"我是通过importSelector导入进来的service\"); } } 通过ImportBeanDefinitionRegistar导入进来的 public class TulingDao { public void testTulingDao() { System.out.println(\"我是通过ImportBeanDefinitionRegistrar导入进来tulingDao组件\"); } } 测试结果: 1.3)spring底层条件装配的原理@Conditional 应用要求:比如我有二个组件,一个是TulingLog一个是TulingAspect 而TulingLog是依赖TulingAspect的 只有容器中有TulingAspect组件才会加载TulingLog tulingLog组件 依赖TulingAspect组件 public class TulingLog { } tulingAspect组件 public class TulingAspect { } ①:自定义条件组件条件 public class TulingConditional implements Condition { @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { //容器中包含tulingAspect组件才返回Ture if(conditionContext.getBeanFactory().containsBean(\"tulingAspect\")){ return true; }else{ return false; } } } -------------------------------------该情况下会加载二个组件-------------------------------------------- @Bean public TulingAspect tulingAspect() { System.out.println(\"TulingAspect组件自动装配到容器中\"); return new TulingAspect(); @Bean @Conditional(value = TulingConditional.class) public TulingLog tulingLog() { System.out.println(\"TulingLog组件自动装配到容器中\"); return new TulingLog(); } -------------------------------------二个组件都不会被加载---------------------------------------- /@Bean*/ public TulingAspect tulingAspect() { System.out.println(\"TulingAspect组件自动装配到容器中\"); return new TulingAspect(); } @Bean @Conditional(value = TulingConditional.class) public TulingLog tulingLog() { System.out.println(\"TulingLog组件自动装配到容器中\"); return new TulingLog(); } 自动装配原理分析 从@SpringbootApplication入手分析 那我们仔细分析 org.springframework.boot.autoconfigure.AutoConfigurationImportSelector#selectImports public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return NO_IMPORTS; } AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = getAttributes(annotationMetadata); //去mata-info/spring.factories文件中查询EnableAutoConfiguration对于值List configurations = getCandidateConfigurations(annotationMetadata, attributes); //去除重复的配置类，若我们自己写的starter可能存主重复的 configurations = removeDuplicates(configurations); Set exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); //根据maven导入的启动器过滤出需要导入的配置类 configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return StringUtils.toStringArray(configurations); } } //去spring.factories中去查询EnableAutoConfirution类 private static Map> loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap result = cache.get(classLoader); if (result != null) { return result; } try { Enumeration urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap<>(); while (urls.hasMoreElements()) { URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry entry : properties.entrySet()) { List factoryClassNames = Arrays.asList( StringUtils.commaDelimitedListToStringArray((String) entry.getValue())); result.addAll((String) entry.getKey(), factoryClassNames); } } cache.put(classLoader, result); return result; } catch (IOException ex) { throw new IllegalArgumentException(\"Unable to load factories from location [\" + FACTORIES_RESOURCE_LOCATION + \"]\", ex); } } 然后我们分析RedisAutoConfiguration类 导入了三个组件RedisTemplateStringRedisTemplate JedisConnectionConfiguration @Configuration @ConditionalOnClass(RedisOperations.class) @EnableConfigurationProperties(RedisProperties.class) @Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class }) public class RedisAutoConfiguration { //导入redisTemplate @Bean @ConditionalOnMissingBean(name = \"redisTemplate\") public RedisTemplate redisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { RedisTemplate template = new RedisTemplate<>(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean public StringRedisTemplate stringRedisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } } =====================================JedisConnectionConfiguration=================== @Configuration @ConditionalOnClass({ GenericObjectPool.class, JedisConnection.class, Jedis.class }) class JedisConnectionConfiguration extends RedisConnectionConfiguration { private final RedisProperties properties; private final List builderCustomizers; JedisConnectionConfiguration(RedisProperties properties, ObjectProvider sentinelConfiguration, ObjectProvider clusterConfiguration, ObjectProvider> builderCustomizers) { super(properties, sentinelConfiguration, clusterConfiguration); this.properties = properties; this.builderCustomizers = builderCustomizers .getIfAvailable(Collections::emptyList); } @Bean @ConditionalOnMissingBean(RedisConnectionFactory.class) public JedisConnectionFactory redisConnectionFactory() throws UnknownHostException { return createJedisConnectionFactory(); } private JedisConnectionFactory createJedisConnectionFactory() { JedisClientConfiguration clientConfiguration = getJedisClientConfiguration(); if (getSentinelConfig() != null) { return new JedisConnectionFactory(getSentinelConfig(), clientConfiguration); } if (getClusterConfiguration() != null) { return new JedisConnectionFactory(getClusterConfiguration(), clientConfiguration); } return new JedisConnectionFactory(getStandaloneConfig(), clientConfiguration); } private JedisClientConfiguration getJedisClientConfiguration() { JedisClientConfigurationBuilder builder = applyProperties( JedisClientConfiguration.builder()); RedisProperties.Pool pool = this.properties.getJedis().getPool(); if (pool != null) { applyPooling(pool, builder); } if (StringUtils.hasText(this.properties.getUrl())) { customizeConfigurationFromUrl(builder); } customize(builder); return builder.build(); } private JedisClientConfigurationBuilder applyProperties( JedisClientConfigurationBuilder builder) { if (this.properties.isSsl()) { builder.useSsl(); } if (this.properties.getTimeout() != null) { Duration timeout = this.properties.getTimeout(); builder.readTimeout(timeout).connectTimeout(timeout); } return builder; } private void applyPooling(RedisProperties.Pool pool, JedisClientConfiguration.JedisClientConfigurationBuilder builder) { builder.usePooling().poolConfig(jedisPoolConfig(pool)); } private JedisPoolConfig jedisPoolConfig(RedisProperties.Pool pool) { JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(pool.getMaxActive()); config.setMaxIdle(pool.getMaxIdle()); config.setMinIdle(pool.getMinIdle()); if (pool.getMaxWait() != null) { config.setMaxWaitMillis(pool.getMaxWait().toMillis()); } return config; } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/Springboot-source.html":{"url":"micro/Springboot-source.html","title":"4.SpringBoot源码分析","keywords":"","body":"第三节:springboot源码解析(王炸篇) 今天内容 1:spring注解 热身 2:springboot自动装配原理 3:springboot启动原理(jar包启动) 4:springboot启动原理(War包启动) 5:作业:springboot的自定义启动器 一:spring注解之如何导入Bean的几种方式 1）@Bean注解，不做讲解 包扫描来加载Bean比如标识@Controller @Service @Repository @Compent不做讲解 @Import几种取值来注册bean ①：实现ImportSelector接口的类 ②：实现ImportBeanDefinitionRegistrar接口來注冊bean 4）实现factoryBean的方式来导入组件(不做讲解) 1.1)通过@Import注解来导入ImportSelector组件 ①:写一个配置类在配置类上标注一个@Import的注解， @Configuration @Import(value = {TulingSelector.class}) public class TulingConfig { } ②：在@Import注解的value值 写自己需要导入的组件 在selectImports方法中 就是你需要导入组件的全类名 public class TulingSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { return new String[]{\"com.tuling.service.TulingServiceImpl\"}; } } 核心代码: @RestController public class TulingController { //自动注入tulingServiceImpl @Autowired private TulingServiceImpl tulingServiceImpl; @RequestMapping(\"testTuling\") public String testTuling() { tulingServiceImpl.testService(); return \"tulingOk\"; } } 这里是没有标注其他注解提供给spring包扫描的 public class TulingServiceImpl { public void testService() { System.out.println(\"我是通过importSelector导入进来的service\"); } } 1.2）通过@Import导入ImportBeanDefinitionRegistrar从而进来导入组件 核心代码: public class TulingImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata annotationMetadata, BeanDefinitionRegistry beanDefinitionReg //定义一个BeanDefinition RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(TulingDao.class); //把自定义的bean定义导入到容器中 beanDefinitionRegistry.registerBeanDefinition(\"tulingDao\",rootBeanDefinition); } } 通过ImportSelector功能导入进来的 public class TulingServiceImpl { @Autowired private TulingDao tulingDao; public void testService() { tulingDao.testTulingDao(); System.out.println(\"我是通过importSelector导入进来的service\"); } } 通过ImportBeanDefinitionRegistar导入进来的 public class TulingDao { public void testTulingDao() { System.out.println(\"我是通过ImportBeanDefinitionRegistrar导入进来tulingDao组件\"); } } 测试结果: 1.3)spring底层条件装配的原理@Conditional 应用要求:比如我有二个组件,一个是TulingLog一个是TulingAspect 而TulingLog是依赖TulingAspect的 只有容器中有TulingAspect组件才会加载TulingLog tulingLog组件 依赖TulingAspect组件 public class TulingLog { } tulingAspect组件 public class TulingAspect { } ①:自定义条件组件条件 public class TulingConditional implements Condition { @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { //容器中包含tulingAspect组件才返回Ture if(conditionContext.getBeanFactory().containsBean(\"tulingAspect\")){ return true; }else{ return false; } } } -------------------------------------该情况下会加载二个组件-------------------------------------------- @Bean public TulingAspect tulingAspect() { System.out.println(\"TulingAspect组件自动装配到容器中\"); return new TulingAspect(); } @Bean @Conditional(value = TulingConditional.class) public TulingLog tulingLog() { System.out.println(\"TulingLog组件自动装配到容器中\"); return new TulingLog(); } -------------------------------------二个组件都不会被加载---------------------------------------- /@Bean*/ public TulingAspect tulingAspect() { System.out.println(\"TulingAspect组件自动装配到容器中\"); return new TulingAspect(); } @Bean @Conditional(value = TulingConditional.class) public TulingLog tulingLog() { System.out.println(\"TulingLog组件自动装配到容器中\"); return new TulingLog(); } ==================================到此结束spring自层注解 ============================== 二:springboot自动装配原理 2.1)@Springboot注解组合图 根据上面的@SpringBootApplication注解 我们来着重分析如下二个类 ①：AutoConfigurationImportSelector.class ②：AutoConfigurationPackages.Registrar.class 先分析AutoConfigurationImportSelector为我们干了什么活 ？？ public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return NO_IMPORTS; } AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = getAttributes(annotationMetadata); //获取候选的配置类 List configurations = getCandidateConfigurations(annotationMetadata, attributes); //移除重复的 configurations = removeDuplicates(configurations); Set exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); //返回出去 return StringUtils.toStringArray(configurations); } //获取候选的配置类 protected List getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, \"No auto configuration classes found in META-INF/spring.factories. If you \" \"are using a custom packaging, make sure that file is correct.\"); return configurations; } //加载配置类 public static List loadFactoryNames(Class factoryClass, @Nullable ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList()); } private static Map> loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap result = cache.get(classLoader); if (result != null) { return result; } try { //\"META-INF/spring.factories\"去类路径下该文件中加载EnableAutoConfiguration.class Enumeration urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap<>(); //遍历解析出来的集合 while (urls.hasMoreElements()) { URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); //放在Properties中 Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry entry : properties.entrySet()) { String factoryClassName = ((String) entry.getKey()).trim(); for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue()) result.add(factoryClassName, factoryName.trim()); } } } cache.put(classLoader, result); //返回 return result; } catch (IOException ex) { throw new IllegalArgumentException(\"Unable to load factories from location [\" + FACTORIES_RESOURCE_LOCATION + \"]\", ex); } } 主要是扫描spring-boot-autoconfigure\\2.0.8.RELEASE\\spring-boot-autoconfigure- 2.0.8.RELEASE.jar!\\META-INF\\spring.factories中EnableAutoConfiguration对应的全类名 org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\ org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\ org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\ org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\ org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\ org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\ org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\ org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\ org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\ org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.ldap.LdapDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.mongo.MongoReactiveDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.mongo.MongoReactiveRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.redis.RedisReactiveAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.rest.RepositoryRestMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.data.web.SpringDataWebAutoConfiguration,\\ org.springframework.boot.autoconfigure.elasticsearch.jest.JestAutoConfiguration,\\ org.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration,\\ org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration,\\ org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration,\\ org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration,\\ org.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration,\\ org.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration,\\ org.springframework.boot.autoconfigure.hazelcast.HazelcastJpaDependencyAutoConfiguration,\\ org.springframework.boot.autoconfigure.http.HttpMessageConvertersAutoConfiguration,\\ org.springframework.boot.autoconfigure.http.codec.CodecsAutoConfiguration,\\ org.springframework.boot.autoconfigure.influx.InfluxDbAutoConfiguration,\\ org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration,\\ org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration,\\ org.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.JndiDataSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.XADataSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\\ org.springframework.boot.autoconfigure.jms.JmsAutoConfiguration,\\ org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\\ org.springframework.boot.autoconfigure.jms.JndiConnectionFactoryAutoConfiguration,\\ org.springframework.boot.autoconfigure.jms.activemq.ActiveMQAutoConfiguration,\\ org.springframework.boot.autoconfigure.jms.artemis.ArtemisAutoConfiguration,\\ org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration,\\ org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration,\\ org.springframework.boot.autoconfigure.jooq.JooqAutoConfiguration,\\ org.springframework.boot.autoconfigure.jsonb.JsonbAutoConfiguration,\\ org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration,\\ org.springframework.boot.autoconfigure.ldap.embedded.EmbeddedLdapAutoConfiguration,\\ org.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration,\\ org.springframework.boot.autoconfigure.liquibase.LiquibaseAutoConfiguration,\\ org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration,\\ org.springframework.boot.autoconfigure.mail.MailSenderValidatorAutoConfiguration,\\ org.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongoAutoConfiguration,\\ org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\ org.springframework.boot.autoconfigure.mongo.MongoReactiveAutoConfiguration,\\ org.springframework.boot.autoconfigure.mustache.MustacheAutoConfiguration,\\ org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\\ org.springframework.boot.autoconfigure.quartz.QuartzAutoConfiguration,\\ org.springframework.boot.autoconfigure.reactor.core.ReactorCoreAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.servlet.SecurityRequestMatcherProviderAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.servlet.UserDetailsServiceAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.servlet.SecurityFilterAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.reactive.ReactiveSecurityAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.reactive.ReactiveUserDetailsServiceAutoConfiguration,\\ org.springframework.boot.autoconfigure.sendgrid.SendGridAutoConfiguration,\\ org.springframework.boot.autoconfigure.session.SessionAutoConfiguration,\\ org.springframework.boot.autoconfigure.security.oauth2.client.OAuth2ClientAutoConfiguration,\\ org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration,\\ org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\\ org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration,\\ org.springframework.boot.autoconfigure.transaction.jta.JtaAutoConfiguration,\\ org.springframework.boot.autoconfigure.validation.ValidationAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.client.RestTemplateAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.embedded.EmbeddedWebServerFactoryCustomizerAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.reactive.HttpHandlerAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.reactive.ReactiveWebServerFactoryAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.reactive.WebFluxAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.reactive.error.ErrorWebFluxAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.reactive.function.client.WebClientAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.websocket.reactive.WebSocketReactiveAutoConfiguration,\\ org.springframework.boot.autoconfigure.websocket.servlet.WebSocketServletAutoConfiguration,\\ org.springframework.boot.autoconfigure.websocket.servlet.WebSocketMessagingAutoConfiguration,\\ org.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration 上面的一些个XXXAutoConfiguration都是一个个自动配置类 我们就拿二个来分析一下 这些自动配置类是如何工作的??? 分析源码1: org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration @Configuration//标识是一个自动配置类 @EnableConfigurationProperties(HttpEncodingProperties.class)启动指定类的配置功能，并且把配置文件中的属性和HttpEncodi @ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET) //spring底层的@Conditional注解的变@ConditionalOnClass(CharacterEncodingFilter.class) ,判断环境中是否没有这个类 判断配置文件中是否存在某个配置spring.http.encoding.enabled；如果不存在，判断也是成立的//即使我们配置文件中不配置pring.http.encoding.enabled=true，也是默认生效的@ConditionalOnProperty(prefix = \"spring.http.encoding\", value = \"enabled\", matchIfMissing = true) public class HttpEncodingAutoConfiguration { 自动配置类的属性映射 private final HttpEncodingProperties properties; public HttpEncodingAutoConfiguration(HttpEncodingProperties properties) { this.properties = properties; } //配置一个CharacterEncodingFilter是springmvc解决乱码的，若容器中没有该组件，那么就会创建该组件 @Bean @ConditionalOnMissingBean public CharacterEncodingFilter characterEncodingFilter() { CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE)); return filter; } @Bean public LocaleCharsetMappingsCustomizer localeCharsetMappingsCustomizer() { return new LocaleCharsetMappingsCustomizer(this.properties); } private static class LocaleCharsetMappingsCustomizer implements WebServerFactoryCustomizer, Ordered { private final HttpEncodingProperties properties; LocaleCharsetMappingsCustomizer(HttpEncodingProperties properties) { this.properties = properties; } @Override public void customize(ConfigurableServletWebServerFactory factory) { if (this.properties.getMapping() != null) { factory.setLocaleCharsetMappings(this.properties.getMapping()); } } @Override public int getOrder() { return 0; } } } 我们来看下HttpEncodingProperties，这个类是用来什么的？ 就是我们yml中能配置什么类，在这个类中都会有一个属性一一对应 @ConfigurationProperties(prefix = \"spring.http.encoding\") //从配置文件中获取指定的值和bean的属 性进行绑定 public class HttpEncodingProperties { public static final Charset DEFAULT_CHARSET = Charset.forName(\"UTF‐8\"); 我们对应的配置文件(yml)中就会有对应属性来配置 以上 就是AutoConfigurationImportSelector为我们容器中注册了那些组件，然后根据maven依赖导入的jar包，根据条件装配来指定哪些组件 起作用 哪些组件不起作用。 三:上面我们分析了springboot自动装配原理，接下来我们依靠 自动装配原理来分析出spring Boot的jar包的启动流程. 3.1)我们先来看springboot怎么来自动装配tomcat相关的组件 EmbeddedWebServerFactoryCustomizerAutoConfiguration(内嵌web容器工厂自定义定制器装配类)? 疑问1？：定制器是用来干什么的？ 疑问2？:定制器何时工作？ 类的继承关系 我们就以tomcat作为内嵌容器来分析 @Configuration @ConditionalOnWebApplication @EnableConfigurationProperties(ServerProperties.class) public class EmbeddedWebServerFactoryCustomizerAutoConfiguration { //配置tomcat的 @Configuration @ConditionalOnClass({ Tomcat.class, UpgradeProtocol.class }) public static class TomcatWebServerFactoryCustomizerConfiguration { @Bean public TomcatWebServerFactoryCustomizer tomcatWebServerFactoryCustomizer( Environment environment, ServerProperties serverProperties) { return new TomcatWebServerFactoryCustomizer(environment, serverProperties); } } //配置jetty @Configuration @ConditionalOnClass({ Server.class, Loader.class, WebAppContext.class }) public static class JettyWebServerFactoryCustomizerConfiguration { @Bean public JettyWebServerFactoryCustomizer jettyWebServerFactoryCustomizer( Environment environment, ServerProperties serverProperties) { return new JettyWebServerFactoryCustomizer(environment, serverProperties); } } 配置undertow的 @Configuration @ConditionalOnClass({ Undertow.class, SslClientAuthMode.class }) public static class UndertowWebServerFactoryCustomizerConfiguration { @Bean public UndertowWebServerFactoryCustomizer undertowWebServerFactoryCustomizer( Environment environment, ServerProperties serverProperties) { return new UndertowWebServerFactoryCustomizer(environment, serverProperties); } } } 我们来看下tomat工厂定制器 是用来修改设置容器的内容的(把serverProperties的属性设置到tomcat的创建工厂中) public class TomcatWebServerFactoryCustomizer implements WebServerFactoryCustomizer ..........................其他代码省略。。。。。。。。。。。。。。 @Override public void customize(ConfigurableTomcatWebServerFactory factory) { ServerProperties properties = this.serverProperties; ServerProperties.Tomcat tomcatProperties = properties.getTomcat(); PropertyMapper propertyMapper = PropertyMapper.get(); propertyMapper.from(tomcatProperties::getBasedir).whenNonNull() .to(factory::setBaseDirectory); propertyMapper.from(tomcatProperties::getBackgroundProcessorDelay).whenNonNull() .as(Duration::getSeconds).as(Long::intValue) .to(factory::setBackgroundProcessorDelay); customizeRemoteIpValve(factory); propertyMapper.from(tomcatProperties::getMaxThreads).when(this::isPositive) .to((maxThreads) -> customizeMaxThreads(factory, tomcatProperties.getMaxThreads())); propertyMapper.from(tomcatProperties::getMinSpareThreads).when(this::isPositive) .to((minSpareThreads) -> customizeMinThreads(factory, minSpareThreads)); propertyMapper.from(() -> determineMaxHttpHeaderSize()).when(this::isPositive) .to((maxHttpHeaderSize) -> customizeMaxHttpHeaderSize(factory, maxHttpHeaderSize)); propertyMapper.from(tomcatProperties::getMaxHttpPostSize) .when((maxHttpPostSize) -> maxHttpPostSize != 0) .to((maxHttpPostSize) -> customizeMaxHttpPostSize(factory, maxHttpPostSize)); propertyMapper.from(tomcatProperties::getAccesslog) .when(ServerProperties.Tomcat.Accesslog::isEnabled) .to((enabled) -> customizeAccessLog(factory)); propertyMapper.from(tomcatProperties::getUriEncoding).whenNonNull() .to(factory::setUriEncoding); propertyMapper.from(properties::getConnectionTimeout).whenNonNull() .to((connectionTimeout) -> customizeConnectionTimeout(factory, connectionTimeout)); propertyMapper.from(tomcatProperties::getMaxConnections).when(this::isPositive) .to((maxConnections) -> customizeMaxConnections(factory, maxConnections)); propertyMapper.from(tomcatProperties::getAcceptCount).when(this::isPositive) .to((acceptCount) -> customizeAcceptCount(factory, acceptCount)); customizeStaticResources(factory); customizeErrorReportValve(properties.getError(), factory); } ServletWebServerFactoryAutoConfigurationServletweb工厂自动配置类 很重要*:@Import({ ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class}** @Configuration @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @ConditionalOnClass(ServletRequest.class) @ConditionalOnWebApplication(type = Type.SERVLET) @EnableConfigurationProperties(ServerProperties.class) @Import({ ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class, ServletWebServerFactoryConfiguration.EmbeddedTomcat.class, ServletWebServerFactoryConfiguration.EmbeddedJetty.class, ServletWebServerFactoryConfiguration.EmbeddedUndertow.class }) public class ServletWebServerFactoryAutoConfiguration { @Bean public ServletWebServerFactoryCustomizer servletWebServerFactoryCustomizer( ServerProperties serverProperties) { return new ServletWebServerFactoryCustomizer(serverProperties); } @Bean @ConditionalOnClass(name = \"org.apache.catalina.startup.Tomcat\") public TomcatServletWebServerFactoryCustomizer tomcatServletWebServerFactoryCustomizer( ServerProperties serverProperties) { return new TomcatServletWebServerFactoryCustomizer(serverProperties); } } .....................................................ServletWebServerFactoryCustomizer核心代码........................................... public void customize(ConfigurableServletWebServerFactory factory) { PropertyMapper map = PropertyMapper.get().alwaysApplyingWhenNonNull(); map.from(this.serverProperties::getPort).to(factory::setPort); map.from(this.serverProperties::getAddress).to(factory::setAddress); map.from(this.serverProperties.getServlet()::getContextPath) .to(factory::setContextPath); map.from(this.serverProperties.getServlet()::getApplicationDisplayName) .to(factory::setDisplayName); map.from(this.serverProperties.getServlet()::getSession).to(factory::setSession); map.from(this.serverProperties::getSsl).to(factory::setSsl); map.from(this.serverProperties.getServlet()::getJsp).to(factory::setJsp); map.from(this.serverProperties::getCompression).to(factory::setCompression); map.from(this.serverProperties::getHttp2).to(factory::setHttp2); map.from(this.serverProperties::getServerHeader).to(factory::setServerHeader); map.from(this.serverProperties.getServlet()::getContextParameters) .to(factory::setInitParameters); } ---------------------------------------------TomcatServletWebServerFactoryCustomizer核心定制代码---------- public void customize(TomcatServletWebServerFactory factory) { ServerProperties.Tomcat tomcatProperties = this.serverProperties.getTomcat(); if (!ObjectUtils.isEmpty(tomcatProperties.getAdditionalTldSkipPatterns())) { factory.getTldSkipPatterns() .addAll(tomcatProperties.getAdditionalTldSkipPatterns()); } if (tomcatProperties.getRedirectContextRoot() != null) { customizeRedirectContextRoot(factory, tomcatProperties.getRedirectContextRoot()); } if (tomcatProperties.getUseRelativeRedirects() != null) { customizeUseRelativeRedirects(factory, tomcatProperties.getUseRelativeRedirects()); } } ServletWebServerFactoryConfiguration容器工厂配置类 @Configuration class ServletWebServerFactoryConfiguration { @Configuration @ConditionalOnClass({ Servlet.class, Tomcat.class, UpgradeProtocol.class }) @ConditionalOnMissingBean(value = ServletWebServerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedTomcat { //配置tomcat容器工厂 @Bean public TomcatServletWebServerFactory tomcatServletWebServerFactory() { return new TomcatServletWebServerFactory(); } } 现在我们来分析一下启动流程。。。。。。。。。。。。。。。。。。。。 1)com.tuling.TulingvipSpringbootAutoconfigPrincipleApplication#main运行main方法 2)org.springframework.boot.SpringApplication#run(java.lang.Class, java.lang.String...) 2.1）传入主配置类，以及命令行参数 2.2)创建SpringApplication对象 ①:保存主配置类 ②：保存web应用的配置类型 ③：去mate-info/spring.factories文件中获取ApplicationContextInitializer(容器初始 化器)保存到springapplication对象中 ④：去mate-info/spring.factories文件中获取ApplicationListener(容器监听器)保存到springapplication对象中 ⑤：保存选取 主配置类 public SpringApplication(ResourceLoader resourceLoader, Class... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); //保存主配置类 this.primarySources = new LinkedHashSet<>(Arrays.asList(primarySources)); //保存web应用的类型 this.webApplicationType = WebApplicationType.deduceFromClasspath(); //保存容器初始化器(ApplicationContextInitializer类型的) setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); //把监听器保存到SpringApplication中[ApplicationListener] setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); //保存主配置类 this.mainApplicationClass = deduceMainApplicationClass(); } //还是去META-INFO/spring.factories中获取ApplicationContextInitializer类型，用于初始化容器private Collection getSpringFactoriesInstances(Class type, Class[] parameterTypes, Object... args) { ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); Use names and ensure unique to protect against duplicates Set names = new LinkedHashSet<>( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances; } //查找主配置类查询的依据就是看哪个方法是否有main方法 private Class deduceMainApplicationClass() { try { StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) { if (\"main\".equals(stackTraceElement.getMethodName())) { return Class.forName(stackTraceElement.getClassName()); } } } catch (ClassNotFoundException ex) { // Swallow and continue } return null; } 3)运行SpringbootApplication的run方法 public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); //创建一个容器对象 ConfigurableApplicationContext context = null; Collection exceptionReporters = new ArrayList<>(); configureHeadlessProperty(); //去meta-info/spring.factories中获取SpringApplicationRunListener监听器(事件发布监听器) SpringApplicationRunListeners listeners = getRunListeners(args); //发布容器starting事件(通过spring的事件多播器) listeners.starting(); try { //封装命令行参数 ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); //准备容器环境 1:获取或者创建环境 2：把命令行参数设置到环境中 3：通过监听器发布环境准备事件 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); //打印springboot的图标 Banner printedBanner = printBanner(environment); //创建容器根据webApplicationType来创建容器通过反射创建context = createApplicationContext(); //去meta-info类中获取异常报告exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context); //准备环境 1：把环境设置到容器中 循环调用AppplicationInitnazlier进行容器初始化工作 3:发布容器上下文准备完成事件 4:注册关于springboot特性的相关单例Bean 5:发布容器上下文加载完毕事件 prepareContext(context, environment, listeners, applicationArguments,printedBanner); refreshContext(context); //运行ApplicationRunner和CommandLineRunner afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } //发布容器启动事件listeners.started(context); //运行ApplicationRunner和CommandLineRunner callRunners(context, applicationArguments); } catch (Throwable ex) { //出现异常；调用异常分析保护类进行分析 handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); } try { //发布容器运行事件 listeners.running(context); } catch (Throwable ex) { handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); } return context; } 5)org.springframework.boot.SpringApplication#refreshContext 6)org.springframework.context.support.AbstractApplicationContext#refresh 7)org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#onRefresh 8)org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#createWebServer 8.1)org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#getWebServer获取web服务器工厂 以下是springioc容器启动的核心流程，在这里不做详细解释,大概步骤为如下: .................................. 8.2)org.springframework.boot.web.server.WebServerFactoryCustomizerBeanPostProcessor#postProces 8.3)org.springframework.boot.web.server.WebServerFactoryCustomizerBeanPostProcessor#postProcessBeforeInitializati 8.3.1)WebServerFactoryCustomizerBeanPostProcessor是一个什么东西？ 在哪里注册到容器中的??? 我们往容器中导入了BeanPostProcessorsRegistrar他实现了ImportBeanDefinitionRegistrar在他的registerBeanDefinitions注册Bean定义的时候 注册 webServerFactoryCustomizerBeanPostProcessor 想知道webServerFactoryCustomizerBeanPostProcessor何时在容器中注册的么？？？？？ public static class BeanPostProcessorsRegistrar implements ImportBeanDefinitionRegistrar, BeanFactoryAware { private ConfigurableListableBeanFactory beanFactory; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { if (beanFactory instanceof ConfigurableListableBeanFactory) { this.beanFactory = (ConfigurableListableBeanFactory) beanFactory; } } @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { if (this.beanFactory == null) { return; } registerSyntheticBeanIfMissing(registry, \"webServerFactoryCustomizerBeanPostProcessor\", WebServerFactoryCustomizerBeanPostProcessor.class); registerSyntheticBeanIfMissing(registry, \"errorPageRegistrarBeanPostProcessor\", ErrorPageRegistrarBeanPostProcessor.class); } org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory#getWebServer 创建tomcat并且容器启动 public WebServer getWebServer(ServletContextInitializer... initializers) { Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null) ? this.baseDirectory createTempDir(\"tomcat\"); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) { tomcat.getService().addConnector(additionalConnector); } prepareContext(tomcat.getHost(), initializers); return getTomcatWebServer(tomcat); } protected TomcatWebServer getTomcatWebServer(Tomcat tomcat) { //端口大于0启动启动 return new TomcatWebServer(tomcat, getPort() >= 0); } public TomcatWebServer(Tomcat tomcat, boolean autoStart) { Assert.notNull(tomcat, \"Tomcat Server must not be null\"); this.tomcat = tomcat; this.autoStart = autoStart; initialize(); } tomcat启动流程 private void initialize() throws WebServerException { TomcatWebServer.logger .info(\"Tomcat initialized with port(s): \" + getPortsDescription(false)); synchronized (this.monitor) { try { addInstanceIdToEngineName(); Context context = findContext(); context.addLifecycleListener((event) -> { if (context.equals(event.getSource()) * Lifecycle.START_EVENT.equals(event.getType())) { Remove service connectors so that protocol binding doesn't happen when the service is started. removeServiceConnectors(); } }); Start the server to trigger initialization listeners this.tomcat.start(); We can re-throw failure exception directly in the main thread rethrowDeferredStartupExceptions(); try { ContextBindings.bindClassLoader(context, context.getNamingToken(), getClass().getClassLoader()); } catch (NamingException ex) { // Naming is not enabled. Continue } Unlike Jetty, all Tomcat threads are daemon threads. We create a blocking non-daemon to stop immediate shutdown startDaemonAwaitThread(); } catch (Exception ex) { stopSilently(); throw new WebServerException(\"Unable to start embedded Tomcat\", ex); } } } 在IOC容器中的org.springframework.context.support.AbstractApplicationContext#refresh的 onReFresh（）带动tomcat启动 然后在接着执行ioc容器的其他步骤。 疑问？？？？？ 1）AutoConfigurationImportSelector#selectImports的方法是怎么触发 的？ 2）我们自己定义的一些@Controller @Service怎么到容器中去的？？？ 接下来 我们就一一解答你们的疑问?还是以debug的方式来为大家解答 1>AbstractApplicationContext#refresh(容器的刷新) 2>AbstractApplicationContext#invokeBeanFactoryPostProcessors调用bean工厂的后置处理器 3>PostProcessorRegistrationDelegate#invokeBeanDefinitionRegistryPostProcessors 4>ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry配置类的后置处理器5>ConfigurationClassPostProcessor#processConfigBeanDefinitions处理配置的bean定义 5.1）找到候选的配置类(tulingvipSpringbootAutoconfigPrincipleApplication)我们自己项目中的配 置类 5.2 )创建配置类解析器 6>ConfigurationClassParser#parse解析我们自己的配置类 (tulingvipSpringbootAutoconfigPrincipleApplication) 7>ConfigurationClassParser#processConfigurationClass处理配置类 7.1)处理配置类上的@PropertySource注解 ConfigurationClassParser#processPropertySource 7.2）处理@ComponentScan注解的ComponentScanAnnotationParser#parse ①:创建 类路径下的bean定义扫描器ClassPathBeanDefinitionScanner ..多个步骤 解析@ComponentScan注解的属性 ②:ClassPathBeanDefinitionScanner#doScan真正的扫描 (tulingvipSpringbootAutoconfigPrincipleApplication所在的包) ③:返回我们标志了@Controller @Service @Response @compent注解的bean定 义 7.3)处理@Import注解ConfigurationClassParser#processImports 7.4）处理@ImportSource注解 7.5)处理@Bean注解的 8>ConfigurationClassParser#processDeferredImportSelectors处理实现了 ImportSelectors接口的 9>AutoConfigurationGroup#process (获取 容器中的所有ImportSelector包含了AutoConfigurationImportSelector) 10>ImportSelector#selectImports回 AutoConfigurationImportSelector.selectImports ConfigurationClassBeanDefinitionReader#loadBeanDefinitions把解析出来的类的bean定 义 注册到容器中 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/netflix-index.html":{"url":"micro/netflix-index.html","title":"springcloud-netflix","keywords":"","body":"springcloud-netflix 5.Eureka源码分析 6.Ribbon&Feign介绍及使用 7.Ribbon&Feign源码分析 8.Hystrix限流降级熔断 9.网关zuul 10.Hystrix&Zuul源码分析 12.配置中心Config 13.链路跟踪Sleuth Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:49:48 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/Eureka-source.html":{"url":"micro/Eureka-source.html","title":"5.Eureka源码分析","keywords":"","body":"为什么要看源码： 1、提升技术功底：学习源码里的优秀设计思想，比如一些疑难问题的解决思路，还有一些优秀的设计模式，整体提升自己的技术功底 2、深度掌握技术框架：源码看多了，对于一个新技术或框架的掌握速度会有大幅提升，看下框架demo大致就能知道底层的实现，技术框架更新再快也不怕 3、快速定位线上问题：遇到线上问题，特别是框架源码里的问题(比如bug)，能够快速定位，这就是相比其他没看过源码的人的优势 4、对面试大有裨益：面试一线互联网公司对于框架技术一般都会问到源码级别的实现 5、技术追求：对技术有追求的人必做之事，使用了一个好的框架，很想知道底层是如何实现的 看源码方法： 1、先使用：先看官方文档快速掌握框架的基本使用 2、抓主线：找一个demo入手，顺藤摸瓜快速静态看一遍框架的主线源码(抓大放小)，画出源码主流程图，切勿一开始就陷入源码的细枝末节，否则会把自己绕晕 3、画图做笔记：总结框架的一些核心功能点，从这些功能点入手深入到源码的细节，边看源码边画源码走向图，并对关键源码的理解做笔记，把源码里的闪光点都记录下来，后续借鉴到工作项目中，理解能力强的可以直接看静态源码，也可以边看源码边debug源码执行过程，观察一些关键变量的值 4、整合总结：所有功能点的源码都分析完后，回到主流程图再梳理一遍，争取把自己画的所有图都在脑袋里做一个整合 1、Eureka架构图 2、Eureka核心功能点 服务注册(register)：Eureka Client会通过发送REST请求的方式向Eureka Server注册自己的服务，提供自身的元数据，比如ip地址、端口、运行状况指标的url、主页地址等信息。Eureka Server接收到注册请求后，就会把这些元数据信息存储在一个双层的Map中。 服务续约(renew)：在服务注册后，Eureka Client会维护一个心跳来持续通知Eureka Server，说明服务一直处于可用状态，防止被剔除。Eureka Client在默认的情况下会每隔30秒 (eureka.instance.leaseRenewallIntervalInSeconds)发送一次心跳来进行服务续约。 服务同步(replicate)：Eureka Server之间会互相进行注册，构建Eureka Server集群，不同Eureka Server之间会进行服务同步，用来保证服务信息的一致性。 获取服务(get registry)：服务消费者（Eureka Client）在启动的时候，会发送一个REST请求给Eureka Server，获取上面注册的服务清单，并且缓存在Eureka Client本地，默认缓存30秒(eureka.client.registryFetchIntervalSeconds)。同时，为了性能考虑，Eureka Server也会维护一份只读的服务清单缓存，该缓存每隔30秒更新一次。 服务调用：服务消费者在获取到服务清单后，就可以根据清单中的服务列表信息，查找到其他服务的地址，从而进行远程调用。Eureka有Region和Zone的概念，一个Region可以包含多个Zone，在进行服务调用时，优先访问处于同一个Zone中的服务提供者。 服务下线(cancel)：当Eureka Client需要关闭或重启时，就不希望在这个时间段内再有请求进来，所以，就需要提前先发送REST请求给Eureka Server，告诉Eureka Server自己要下线了，Eureka Server在收到请求后，就会把该服务状态置为下线（DOWN），并把该下线事件传播出去。 服务剔除(evict)：有时候，服务实例可能会因为网络故障等原因导致不能提供服务，而此时该实例也没有发送请求给Eureka Server来进行服务下线，所以，还需要有服务剔除的机制。Eureka Server在启动的时候会创建一个定时任务，每隔一段时间（默认60秒），从当前服务清单中把超时没有续约（默认90秒，eureka.instance.leaseExpirationDurationInSeconds）的服务剔除。 自我保护：既然Eureka Server会定时剔除超时没有续约的服务，那就有可能出现一种场景，网络一段时间内发生了异常，所有的服务都没能够进行续约，Eureka Server就把所有的服务都剔除了，这样显然不太合理。所以，就有了自我保护机制，当短时间内，统计续约失败的比例，如果达到一定阈值，则会触发自我保护的机制，在该机制下， Eureka Server不会剔除任何的微服务，等到正常后，再退出自我保护机制。自我保护开关(eureka.server.enable-self-preservation: false) 3、Eureka Server端源码分析 源码流程图参考： @Configuration @Import(EurekaServerInitializerConfiguration.class) @ConditionalOnBean(EurekaServerMarkerConfiguration.Marker.class) 4@EnableConfigurationProperties({EurekaDashboardProperties.class, 5InstanceRegistryProperties.class}) 6@PropertySource(\"classpath:/eureka/server.properties\") 7public classEurekaServerAutoConfigurationextendsWebMvcConfigurerAdapter{ 8 //此处省略大部分代码，仅抽取一些关键的代码片段 10 //加载EurekaController, spring‐cloud提供了一些额外的接口，用来获取eurekaServer的信息 @Bean @ConditionalOnProperty(prefix=\"eureka.dashboard\",name=\"enabled\",matchIfMissing=true) publicEurekaControllereurekaController() { return newEurekaController(this.applicationInfoManager); } 17 //初始化集群注册表 @Bean publicPeerAwareInstanceRegistrypeerAwareInstanceRegistry( ServerCodecs serverCodecs) { this.eurekaClient.getApplications();// force initialization return newInstanceRegistry(this.eurekaServerConfig,this.eurekaClientConfig, serverCodecs,this.eurekaClient, this.instanceRegistryProperties.getExpectedNumberOfRenewsPerMin(), this.instanceRegistryProperties.getDefaultOpenForTrafficCount()); } 28 //配置服务节点信息，这里的作用主要是为了配置Eureka的peer节点，也就是说当有收到有节点注册上来 //的时候，需要通知给那些服务节点， （互为一个集群） @Bean @ConditionalOnMissingBean publicPeerEurekaNodespeerEurekaNodes(PeerAwareInstanceRegistry registry, ServerCodecs serverCodecs) { return newPeerEurekaNodes(registry,this.eurekaServerConfig, this.eurekaClientConfig,serverCodecs,this.applicationInfoManager); } // EurekaServer的上下文 @Bean publicEurekaServerContexteurekaServerContext(ServerCodecs serverCodecs, PeerAwareInstanceRegistry registry,PeerEurekaNodes peerEurekaNodes) { return newDefaultEurekaServerContext(this.eurekaServerConfig,serverCodecs, registry,peerEurekaNodes,this.applicationInfoManager); } //这个类的作用是spring‐cloud和原生eureka的胶水代码，通过这个类来启动EurekaSever //后面这个类会在EurekaServerInitializerConfiguration被调用，进行eureka启动 @Bean publicEurekaServerBootstrapeurekaServerBootstrap(PeerAwareInstanceRegistry registry, EurekaServerContext serverContext) { return newEurekaServerBootstrap(this.applicationInfoManager, this.eurekaClientConfig,this.eurekaServerConfig,registry, serverContext); } //配置拦截器，ServletContainer里面实现了jersey框架，通过他来实现eurekaServer对外的restFull接口 @Bean publicFilterRegistrationBeanjerseyFilterRegistration( javax.ws.rs.core.Application eurekaJerseyApp) { FilterRegistrationBean bean=newFilterRegistrationBean(); bean.setFilter(newServletContainer(eurekaJerseyApp)); bean.setOrder(Ordered.LOWEST_PRECEDENCE); bean.setUrlPatterns( Collections.singletonList(EurekaConstants.DEFAULT_PREFIX+\"/*\")); 63 returnbean; } } EurekaServerAutoConfiguration会导入EurekaServerInitializerConfiguration /** *@author Dave Syer */ @Configuration 5@CommonsLog public classEurekaServerInitializerConfiguration implementsServletContextAware,SmartLifecycle,Ordered{ 8 @Autowired privateEurekaServerConfig eurekaServerConfig; 11 privateServletContext servletContext; 13 @Autowired privateApplicationContext applicationContext; 16 @Autowired privateEurekaServerBootstrap eurekaServerBootstrap; 19 privateboolean running; 21 privateint order=1; 23 @Override public voidsetServletContext(ServletContext servletContext) { this.servletContext=servletContext; } 28 @Override public voidstart() { //启动一个线程 newThread(newRunnable() { @Override public voidrun() { try{ //初始化EurekaServer，同时启动Eureka Server eurekaServerBootstrap.contextInitialized(EurekaServerInitializerConfiguration.this.servletCont xt); log.info(\"Started Eureka Server\"); //发布EurekaServer的注册事件 publish(newEurekaRegistryAvailableEvent(getEurekaServerConfig())); //设置启动的状态为true EurekaServerInitializerConfiguration.this.running=true; //发送Eureka Start事件 ， 其他还有各种事件，我们可以监听这种时间，然后做一些特定的业务需求 publish(newEurekaServerStartedEvent(getEurekaServerConfig())); } catch(Exception ex) { // Help! log.error(\"Could not initialize Eureka servlet context\",ex); } } }).start(); } 53 privateEurekaServerConfiggetEurekaServerConfig() { return this.eurekaServerConfig; } 57 private voidpublish(ApplicationEvent event) { this.applicationContext.publishEvent(event); } 61 @Override public voidstop() { this.running=false; eurekaServerBootstrap.contextDestroyed(this.servletContext); } 67 @Override publicbooleanisRunning() { return this.running; } 72 @Override publicintgetPhase() { return0; } 77 @Override publicbooleanisAutoStartup() { returntrue; } 82 @Override public voidstop(Runnable callback) { callback.run(); } 87 @Override publicintgetOrder() { return this.order; } 92 } EurekaServerBootstrap的contextInitialized初始化方法 //初始化EurekaServer的运行环境和上下文 public voidcontextInitialized(ServletContext context) {3try{ initEurekaEnvironment(); initEurekaServerContext(); 6 context.setAttribute(EurekaServerContext.class.getName(),this.serverContext); } catch(Throwable e) { log.error(\"Cannot bootstrap eureka server :\",e); throw newRuntimeException(\"Cannot bootstrap eureka server :\",e); } } 14 初始化EurekaServer的上下文 protected voidinitEurekaServerContext()throws Exception{ // For backward compatibility JsonXStream.getInstance().registerConverter(newV1AwareInstanceInfoConverter(), XStream.PRIORITY_VERY_HIGH); XmlXStream.getInstance().registerConverter(newV1AwareInstanceInfoConverter(), XStream.PRIORITY_VERY_HIGH); 22 if(isAws(this.applicationInfoManager.getInfo())) { this.awsBinder=newAwsBinderDelegate(this.eurekaServerConfig, this.eurekaClientConfig,this.registry,this.applicationInfoManager); this.awsBinder.start(); } 28 //初始化eureka server上下文 EurekaServerContextHolder.initialize(this.serverContext); 31 log.info(\"Initialized server context\"); 33 // Copy registry from neighboring eureka node //从相邻的eureka节点复制注册表 int registryCount=this.registry.syncUp(); //默认每30秒发送心跳，1分钟就是2次 //修改eureka状态为up //同时，这里面会开启一个定时任务，用于清理60秒没有心跳的客户端。自动下线 this.registry.openForTraffic(this.applicationInfoManager,registryCount); 41 // Register all monitoring statistics. EurekaMonitors.registerAllStats(); } 45 @Override publicintsyncUp() { // Copy entire entry from neighboring DS node int count=0; 50 for(int i=0; ((i if(i>0) { try{ Thread.sleep(serverConfig.getRegistrySyncRetryWaitMs()); }catch(InterruptedException e) { logger.warn(\"Interrupted during registry transfer..\"); break; } } Applications apps=eurekaClient.getApplications(); for(Application app:apps.getRegisteredApplications()) { for(InstanceInfo instance:app.getInstances()) { try{ if(isRegisterable(instance)) { //将其他节点的实例注册到本节点 register(instance,instance.getLeaseInfo().getDurationInSecs(),true); count++; } }catch(Throwable t) { logger.error(\"During DS init copy\",t); } } } } returncount; } 77 @Override public voidopenForTraffic(ApplicationInfoManager applicationInfoManager,int count) { // Renewals happen every 30 seconds and for a minute it should be a factor of 2. //计算每分钟最大续约数 this.expectedNumberOfRenewsPerMin=count*2; //每分钟最小续约数 this.numberOfRenewsPerMinThreshold= (int) (this.expectedNumberOfRenewsPerMin*serverConfig.getRenewalPercentThreshold()); logger.info(\"Got \"+count+\" instances from neighboring DS node\"); logger.info(\"Renew threshold is: \"+numberOfRenewsPerMinThreshold); this.startupTime=System.currentTimeMillis(); if(count>0) { this.peerInstancesTransferEmptyOnStartup=false; } DataCenterInfo.Name selfName=applicationInfoManager.getInfo().getDataCenterInfo().getName(); boolean isAws=Name.Amazon==selfName; if(isAws&&serverConfig.shouldPrimeAwsReplicaConnections()) { logger.info(\"Priming AWS connections for all replicas..\"); primeAwsReplicas(applicationInfoManager); } logger.info(\"Changing status to UP\"); //设置实例的状态为UP applicationInfoManager.setInstanceStatus(InstanceStatus.UP); //开启定时任务，默认60秒执行一次，用于清理60秒之内没有续约的实例 super.postInit(); } 104 protected voidpostInit() { renewsLastMin.start(); if(evictionTaskRef.get()!=null) { evictionTaskRef.get().cancel(); } evictionTaskRef.set(newEvictionTask()); //服务剔除任务 evictionTimer.schedule(evictionTaskRef.get(), serverConfig.getEvictionIntervalTimerInMs(), serverConfig.getEvictionIntervalTimerInMs()); } 从上面的EurekaServerAutoConfiguration类，我们可以看到有个初始化EurekaServerContext的方法 @Bean publicEurekaServerContexteurekaServerContext(ServerCodecs serverCodecs,3PeerAwareInstanceRegistry registry,PeerEurekaNodes peerEurekaNodes) { 4return newDefaultEurekaServerContext(this.eurekaServerConfig,serverCodecs,5registry,peerEurekaNodes,this.applicationInfoManager); } DefaultEurekaServerContext 这个类里面的的initialize()方法是被@PostConstruct 这个注解修饰的,在应用加载的时候，会执行这个方法 public voidinitialize()throws Exception{2logger.info(\"Initializing ...\"); 3//启动一个线程，读取其他集群节点的信息，后面后续复制 peerEurekaNodes.start(); // registry.init(peerEurekaNodes); logger.info(\"Initialized\"); } peerEurekaNodes.start()主要是启动一个只拥有一个线程的线程池，第一次进去会更新一下集群其他节点信息然后启动了一个定时线程，每60秒更新一次，也就是说后续可以根据配置动态的修改节点配置。（原生的spring cloud config支持） public voidstart() { taskExecutor=Executors.newSingleThreadScheduledExecutor( 3newThreadFactory() { @Override publicThreadnewThread(Runnable r) { Thread thread=newThread(r,\"Eureka‐PeerNodesUpdater\"); thread.setDaemon(true); 8returnthread; } } ); try{ //首次进来，更新集群节点信息 updatePeerEurekaNodes(resolvePeerUrls()); //搞个线程 Runnable peersUpdateTask=newRunnable() { @Override public voidrun() { try{ updatePeerEurekaNodes(resolvePeerUrls()); }catch(Throwable e) { logger.error(\"Cannot update the replica Nodes\",e); } 24 } }; taskExecutor.scheduleWithFixedDelay( peersUpdateTask, serverConfig.getPeerEurekaNodesUpdateIntervalMs(), serverConfig.getPeerEurekaNodesUpdateIntervalMs(), TimeUnit.MILLISECONDS ); }catch(Exception e) { throw newIllegalStateException(e); } for(PeerEurekaNode node:peerEurekaNodes) { logger.info(\"Replica node URL: \"+node.getServiceUrl()); } } //根据URL构建PeerEurekaNode信息 protectedPeerEurekaNodecreatePeerEurekaNode(String peerEurekaNodeUrl) { HttpReplicationClient replicationClient=JerseyReplicationClient.createReplicationClient(serv rConfig,serverCodecs,peerEurekaNodeUrl); String targetHost=hostFromUrl(peerEurekaNodeUrl); if(targetHost==null) { targetHost=\"host\"; } return newPeerEurekaNode(registry,targetHost,peerEurekaNodeUrl,replicationClient,serverConfig); } 4、Eureka Client端源码分析 源码流程图参考： client初始化 @Inject DiscoveryClient(ApplicationInfoManager applicationInfoManager,EurekaClientConfig config,AbstractDiscoveryClientOptionalArgs args, ProviderbackupRegistryProvider) { 4//省略非关键代码。。。 5 logger.info(\"Initializing Eureka in region {}\",clientConfig.getRegion()); 7 //省略非关键代码。。。 9 try{ // default size of 2 ‐ 1 each for heartbeat and cacheRefresh scheduler=Executors.newScheduledThreadPool(2, newThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient‐%d\") .setDaemon(true) .build()); 17 heartbeatExecutor=newThreadPoolExecutor( 1,clientConfig.getHeartbeatExecutorThreadPoolSize(),0,TimeUnit.SECONDS, newSynchronousQueue(), newThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient‐HeartbeatExecutor‐%d\") .setDaemon(true) .build() );// use direct handoff 26 cacheRefreshExecutor=newThreadPoolExecutor( 1,clientConfig.getCacheRefreshExecutorThreadPoolSize(),0,TimeUnit.SECONDS, newSynchronousQueue(), newThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient‐CacheRefreshExecutor‐%d\") .setDaemon(true) .build() );// use direct handoff 35 eurekaTransport=newEurekaTransport(); scheduleServerEndpointTask(eurekaTransport,args); 38 AzToRegionMapper azToRegionMapper; if(clientConfig.shouldUseDnsForFetchingServiceUrls()) { azToRegionMapper=newDNSBasedAzToRegionMapper(clientConfig); }else{ azToRegionMapper=newPropertyBasedAzToRegionMapper(clientConfig); } if(null!=remoteRegionsToFetch.get()) { azToRegionMapper.setRegionsToFetch(remoteRegionsToFetch.get().split(\",\")); } instanceRegionChecker=newInstanceRegionChecker(azToRegionMapper,clientConfig.getRegion()); }catch(Throwable e) { throw newRuntimeException(\"Failed to initialize DiscoveryClient!\",e); } 52 if(clientConfig.shouldFetchRegistry()&& !fetchRegistry(false)) { fetchRegistryFromBackup(); } 56 // call and execute the pre registration handler before all background tasks (inc registration) is started if(this.preRegistrationHandler!=null) { this.preRegistrationHandler.beforeRegistration(); } 61 if(clientConfig.shouldRegisterWithEureka()&&clientConfig.shouldEnforceRegistrationAtInit()) { try{ if(!register() ) { throw newIllegalStateException(\"Registration error at startup. Invalid server response.\"); } }catch(Throwable th) { logger.error(\"Registration error at startup: {}\",th.getMessage()); throw newIllegalStateException(th); } } 72 //最核心代码 // finally, init the schedule tasks (e.g. cluster resolvers, heartbeat, instanceInfo replicator fetch initScheduledTasks(); 76 try{ Monitors.registerObject(this); }catch(Throwable e) { logger.warn(\"Cannot register timers\",e); } 82 // This is a bit of hack to allow for existing code using DiscoveryManager.getInstance() // to work with DI'd DiscoveryClient DiscoveryManager.getInstance().setDiscoveryClient(this); DiscoveryManager.getInstance().setEurekaClientConfig(config); 87 initTimestampMs=System.currentTimeMillis(); logger.info(\"Discovery Client initialized at timestamp {} with initial instances count: {}\", initTimestampMs,this.getApplications().size()); } 初始化时启动核心功能定时任务 private voidinitScheduledTasks() { //获取服务注册列表信息 if(clientConfig.shouldFetchRegistry()) { //服务注册列表更新的周期时间 int registryFetchIntervalSeconds=clientConfig.getRegistryFetchIntervalSeconds(); int expBackOffBound=clientConfig.getCacheRefreshExecutorExponentialBackOffBound(); 7//定时更新服务注册列表 scheduler.schedule( newTimedSupervisorTask(10\"cacheRefresh\", 11scheduler, 12cacheRefreshExecutor, 13registryFetchIntervalSeconds, 14TimeUnit.SECONDS, 15expBackOffBound, 16newCacheRefreshThread()//该线程执行更新的具体逻辑 ), registryFetchIntervalSeconds,TimeUnit.SECONDS); } 20 if(clientConfig.shouldRegisterWithEureka()) { //服务续约的周期时间 int renewalIntervalInSecs=instanceInfo.getLeaseInfo().getRenewalIntervalInSecs(); int expBackOffBound=clientConfig.getHeartbeatExecutorExponentialBackOffBound(); //应用启动可见此日志，内容是：Starting heartbeat executor: renew interval is: 30 logger.info(\"Starting heartbeat executor: \"+\"renew interval is: \"+renewalIntervalInSecs); //服务定时续约 scheduler.schedule( newTimedSupervisorTask( \"heartbeat\", scheduler, heartbeatExecutor, renewalIntervalInSecs, TimeUnit.SECONDS, expBackOffBound, newHeartbeatThread()//该线程执行续约的具体逻辑 ), renewalIntervalInSecs,TimeUnit.SECONDS); 39 //这个Runable中含有服务注册的逻辑 instanceInfoReplicator=newInstanceInfoReplicator( this, instanceInfo, clientConfig.getInstanceInfoReplicationIntervalSeconds(), 2);// burstSize 46 statusChangeListener=newApplicationInfoManager.StatusChangeListener() { @Override publicStringgetId() { return\"statusChangeListener\"; } 52 @Override public voidnotify(StatusChangeEvent statusChangeEvent) { if(InstanceStatus.DOWN==statusChangeEvent.getStatus()|| InstanceStatus.DOWN==statusChangeEvent.getPreviousStatus()) { // log at warn level if DOWN was involved logger.warn(\"Saw local status change event {}\",statusChangeEvent); }else{ logger.info(\"Saw local status change event {}\",statusChangeEvent); } instanceInfoReplicator.onDemandUpdate(); } }; 65 if(clientConfig.shouldOnDemandUpdateStatusChange()) { applicationInfoManager.registerStatusChangeListener(statusChangeListener); } //服务注册 instanceInfoReplicator.start(clientConfig.getInitialInstanceInfoReplicationIntervalSeconds()); }else{ logger.info(\"Not registering with Eureka server per configuration\"); } } TimedSupervisorTask是一个Runnable接口实现，看下它的run方法 @Override public voidrun() { Futurefuture=null; 4try{ 5future=executor.submit(task); threadPoolLevelGauge.set((long)executor.getActiveCount()); //指定等待子线程的最长时间 future.get(timeoutMillis,TimeUnit.MILLISECONDS);// block until done or timeout 9//delay是个关键变量，后面会用到，这里记得每次执行任务成功都会将delay重置 10delay.set(timeoutMillis); 11threadPoolLevelGauge.set((long)executor.getActiveCount());12}catch(TimeoutException e) { 13logger.warn(\"task supervisor timed out\",e);14timeoutCounter.increment(); 15 long currentDelay=delay.get(); //任务线程超时的时候，就把delay变量翻倍，但不会超过外部调用时设定的最大延时时间 long newDelay=Math.min(maxDelay,currentDelay*2); //设置为最新的值，考虑到多线程，所以用了CAS delay.compareAndSet(currentDelay,newDelay); }catch(RejectedExecutionException e) { //一旦线程池的阻塞队列中放满了待处理任务，触发了拒绝策略，就会将调度器停掉 if(executor.isShutdown()||scheduler.isShutdown()) { logger.warn(\"task supervisor shutting down, reject the task\",e); }else{ logger.warn(\"task supervisor rejected the task\",e); } 28 rejectedCounter.increment(); }catch(Throwable e) { if(executor.isShutdown()||scheduler.isShutdown()) { logger.warn(\"task supervisor shutting down, can't accept the task\"); }else{ logger.warn(\"task supervisor threw an exception\",e); } 36 throwableCounter.increment(); }finally{ //这里任务要么执行完毕，要么发生异常，都用cancel方法来清理任务； if(future!=null) { future.cancel(true); } //只要调度器没有停止，就再指定等待时间之后在执行一次同样的任务 if(!scheduler.isShutdown()) { //假设外部调用时传入的超时时间为30秒（构造方法的入参timeout），最大间隔时间为50秒(构造方法的入参expBac kOffBound) //如果最近一次任务没有超时，那么就在30秒后开始新任务， //如果最近一次任务超时了，那么就在50秒后开始新任务（异常处理中有个乘以二的操作，乘以二后的60秒超过了最大间隔50秒） scheduler.schedule(this,delay.get(),TimeUnit.MILLISECONDS); } } } scheduler.schedule(this, delay.get(), TimeUnit.MILLISECONDS)，从代码注释上可以看出这个方法是一次性调用方法，但是实际上这个方法执行的任务会反复执行，秘密就在this对应的这个类TimedSupervisorTask的run方法 里，run方法任务执行完最后，会再次调用schedule方法，在指定的时间之后执行一次相同的任务，这个间隔时间和最近一次任务是否超时有关，如果超时了则下一次执行任务的间隔时间就会变大； 源码精髓： 从整体上看，TimedSupervisorTask是固定间隔的周期性任务，一旦遇到超时就会将下一个周期的间隔时间调大，如果连续超时，那么每次间隔时间都会增大一倍，一直到达外部参数设定的上限为止，一旦新任务不再超时，间隔时间又会自动恢复为初始值，另外还有CAS来控制多线程同步，这些是我们看源码需要学习到的设计技巧 定时更新服务注册列表线程CacheRefreshThread /** *The task that fetches the registry information at specified intervals. * */ classCacheRefreshThreadimplementsRunnable{6public voidrun() { refreshRegistry(); } } 10 @VisibleForTesting voidrefreshRegistry() { try{ boolean isFetchingRemoteRegionRegistries=isFetchingRemoteRegionRegistries(); 15 boolean remoteRegionsModified=false; // This makes sure that a dynamic change to remote regions to fetch is honored. String latestRemoteRegions=clientConfig.fetchRegistryForRemoteRegions(); //不做aws环境的配置这个if逻辑不会执行 if(null!=latestRemoteRegions) { String currentRemoteRegions=remoteRegionsToFetch.get(); if(!latestRemoteRegions.equals(currentRemoteRegions)) { // Both remoteRegionsToFetch and AzToRegionMapper.regionsToFetch need to be in sync synchronized(instanceRegionChecker.getAzToRegionMapper()) { if(remoteRegionsToFetch.compareAndSet(currentRemoteRegions,latestRemoteRegions)) { String[]remoteRegions=latestRemoteRegions.split(\",\"); remoteRegionsRef.set(remoteRegions); instanceRegionChecker.getAzToRegionMapper().setRegionsToFetch(remoteRegions); remoteRegionsModified=true; }else{ logger.info(\"Remote regions to fetch modified concurrently,\"+ \" ignoring change from {} to {}\",currentRemoteRegions,latestRemoteRegions); } } }else{ // Just refresh mapping to reflect any DNS/Property change instanceRegionChecker.getAzToRegionMapper().refreshMapping(); } } 40 //获取注册信息方法 boolean success=fetchRegistry(remoteRegionsModified); if(success) { registrySize=localRegionApps.get().size(); lastSuccessfulRegistryFetchTimestamp=System.currentTimeMillis(); } 47 //省略非关键代码。。。 }catch(Throwable e) { logger.error(\"Cannot fetch registry from server\",e); } } 53 privatebooleanfetchRegistry(boolean forceFullRegistryFetch) { Stopwatch tracer=FETCH_REGISTRY_TIMER.start(); 56 try{ // If the delta is disabled or if it is the first time, get all // applications //取出本地缓存之前获取的服务列表信息 Applications applications=getApplications(); 62 //判断多个条件，确定是否触发全量更新，如下任一个满足都会全量更新： //1.是否禁用增量更新； //2.是否对某个region特别关注； //3.外部调用时是否通过入参指定全量更新； //4.本地还未缓存有效的服务列表信息； if(clientConfig.shouldDisableDelta() ||(!Strings.isNullOrEmpty(clientConfig.getRegistryRefreshSingleVipAddress())) ||forceFullRegistryFetch ||(applications==null) ||(applications.getRegisteredApplications().size()==0) ||(applications.getVersion()== ‐1))//Client application does not have latest library supporting delta { logger.info(\"Disable delta property : {}\",clientConfig.shouldDisableDelta()); logger.info(\"Single vip registry refresh property : {}\",clientConfig.getRegistryRefreshSingleipAddress()); logger.info(\"Force full registry fetch : {}\",forceFullRegistryFetch); logger.info(\"Application is null : {}\", (applications==null)); logger.info(\"Registered Applications size is zero : {}\", (applications.getRegisteredApplications().size()==0)); logger.info(\"Application version is ‐1: {}\", (applications.getVersion()== ‐1)); //全量更新 getAndStoreFullRegistry(); }else{ //增量更新 getAndUpdateDelta(applications); } //重新计算和设置一致性hash码 applications.setAppsHashCode(applications.getReconcileHashCode()); logTotalInstances(); }catch(Throwable e) { logger.error(PREFIX+\"{} ‐ was unable to refresh its cache! status = {}\",appPathIdentifier,e.getMessage(),e); returnfalse; }finally{ if(tracer!=null) { tracer.stop(); } } 99 // Notify about cache refresh before updating the instance remote status //将本地缓存更新的事件广播给所有已注册的监听器，注意该方法已被CloudEurekaClient类重写 onCacheRefreshed(); 103 // Update remote status based on refreshed data held in the cache //检查刚刚更新的缓存中，有来自Eureka server的服务列表，其中包含了当前应用的状态， //当前实例的成员变量lastRemoteInstanceStatus，记录的是最后一次更新的当前应用状态， //上述两种状态在updateInstanceRemoteStatus方法中作比较 ，如果不一致，就更新lastRemoteInstanceStatu s，并且广播对应的事件 updateInstanceRemoteStatus(); 109 // registry was fetched successfully, so return true returntrue; } 全量更新getAndStoreFullRegistry private voidgetAndStoreFullRegistry()throws Throwable{ long currentUpdateGeneration=fetchRegistryGeneration.get(); 3 logger.info(\"Getting all instance registry info from the eureka server\"); 5 Applications apps=null; //由于并没有配置特别关注的region信息，因此会调用eurekaTransport.queryClient.getApplications方法从服务端获取服务列表 EurekaHttpResponsehttpResponse=clientConfig.getRegistryRefreshSingleVipAddress()==null ?eurekaTransport.queryClient.getApplications(remoteRegionsRef.get()) :eurekaTransport.queryClient.getVip(clientConfig.getRegistryRefreshSingleVipAddress(),remoteRegionsRef.get()); if(httpResponse.getStatusCode()==Status.OK.getStatusCode()) { //返回对象就是服务列表 apps=httpResponse.getEntity(); } logger.info(\"The response status is {}\",httpResponse.getStatusCode()); 16 if(apps==null) { logger.error(\"The application is null for some reason. Not storing this information\"); } //考虑到多线程同步，只有CAS成功的线程，才会把自己从Eureka server获取的数据来替换本地缓存 else if(fetchRegistryGeneration.compareAndSet(currentUpdateGeneration,currentUpdateGeneration+1)){ //localRegionApps就是本地缓存，是个AtomicReference实例 localRegionApps.set(this.filterAndShuffle(apps)); logger.debug(\"Got full registry with apps hashcode {}\",apps.getAppsHashCode()); }else{ logger.warn(\"Not updating applications as another thread is updating it already\"); } } 其中最重要的一段代码eurekaTransport.queryClient.getApplications(remoteRegionsRef.get())，和Eureka server交互的逻辑都在这里面，方法getApplications的具体实现是在EurekaHttpClientDecorator类 @Override publicEurekaHttpResponsegetApplications(final String...regions) {3returnexecute(newRequestExecutor() { @Override publicEurekaHttpResponseexecute(EurekaHttpClient delegate) { 6returndelegate.getApplications(regions); } 8 @Override publicRequestTypegetRequestType() { //本次向Eureka server请求的类型：获取服务列表 returnRequestType.GetApplications; } }); } debug进去delegate.getApplications(regions)方法会发现delegate实际用的是 AbstractJerseyEurekaHttpClient，里面都是具体的jersey实现的网络接口请求 @Override publicEurekaHttpResponsegetApplications(String...regions) {3//取全量数据的path是\"apps\" 4returngetApplicationsInternal(\"apps/\",regions); } 6 @Override publicEurekaHttpResponsegetDelta(String...regions) {9//取增量数据的path是\"apps/delta\" 10returngetApplicationsInternal(\"apps/delta\",regions); } 12 //具体的请求响应处理都在此方法中 privateEurekaHttpResponsegetApplicationsInternal(String urlPath,String[]regions) { ClientResponse response=null; String regionsParamValue=null; try{ //jersey、resource这些关键词都预示着这是个restful请求 WebResource webResource=jerseyClient.resource(serviceUrl).path(urlPath); if(regions!=null&&regions.length>0) { regionsParamValue=StringUtil.join(regions); webResource=webResource.queryParam(\"regions\",regionsParamValue); } Builder requestBuilder=webResource.getRequestBuilder(); addExtraHeaders(requestBuilder); //发起网络请求，将响应封装成ClientResponse实例 response=requestBuilder.accept(MediaType.APPLICATION_JSON_TYPE).get(ClientResponse.class); 28 Applications applications=null; if(response.getStatus()==Status.OK.getStatusCode()&&response.hasEntity()) { //取得全部应用信息 applications=response.getEntity(Applications.class); } returnanEurekaHttpResponse(response.getStatus(),Applications.class) .headers(headersOf(response)) .entity(applications) .build(); }finally{ if(logger.isDebugEnabled()) { logger.debug(\"Jersey HTTP GET {}/{}?{}; statusCode={}\", serviceUrl,urlPath, regionsParamValue==null?\"\":\"regions=\"+regionsParamValue, response==null?\"N/A\":response.getStatus() ); } if(response!=null) { response.close(); } } } 获取全量数据，是通过jersey-client库的API向Eureka server发起restful请求http://localhost:8761/eureka/apps实现的，并将响应的服务列表数据放在一个成员变量中作为本地缓存 1 UP_1_ 4 5MICROSERVICE‐PROVIDER‐USER6 7localhost:microservice‐provider‐user:80028192.168.101.1 9MICROSERVICE‐PROVIDER‐USER10192.168.101.1 11UP 12UNKNOWN138002 14443151 16 17MyOwn 18 19 2030 2190 221554360812763 231554360812763 0 1554360812763 8002 61822 http://192.168.101.1:8002/ http://192.168.101.1:8002/actuator/info http://192.168.101.1:8002/actuator/health microservice‐provider‐user microservice‐provider‐user false 1554360812764 1554360812649 ADDED 获取服务列表信息的增量更新getAndUpdateDelta private voidgetAndUpdateDelta(Applications applications)throws Throwable{2long currentUpdateGeneration=fetchRegistryGeneration.get(); 3 Applications delta=null; //增量信息是通过eurekaTransport.queryClient.getDelta方法完成的 EurekaHttpResponsehttpResponse=eurekaTransport.queryClient.getDelta(remoteRegi onsRef.get()); if(httpResponse.getStatusCode()==Status.OK.getStatusCode()) { 8//delta中保存了Eureka server返回的增量更新 9delta=httpResponse.getEntity(); } 11 if(delta==null) { logger.warn(\"The server does not allow the delta revision to be applied because it is not safe \" +\"Hence got the full registry.\"); //如果增量信息为空，就直接发起一次全量更新 getAndStoreFullRegistry(); } //考虑到多线程同步问题，这里通过CAS来确保请求发起到现在是线程安全的， //如果这期间fetchRegistryGeneration变了，就表示其他线程也做了类似操作，因此放弃本次响应的数据 else if(fetchRegistryGeneration.compareAndSet(currentUpdateGeneration,currentUpdateGeneration+1)){ logger.debug(\"Got delta update with apps hashcode {}\",delta.getAppsHashCode()); String reconcileHashCode=\"\"; if(fetchRegistryUpdateLock.tryLock()) { try{ //用Eureka返回的增量数据和本地数据做合并操作，这个方法稍后会细说 updateDelta(delta); //用合并了增量数据之后的本地数据来生成一致性哈希码 reconcileHashCode=getReconcileHashCode(applications); }finally{ fetchRegistryUpdateLock.unlock(); } }else{ logger.warn(\"Cannot acquire update lock, aborting getAndUpdateDelta\"); } //Eureka server在返回增量更新数据时，也会返回服务端的一致性哈希码， //理论上每次本地缓存数据经历了多次增量更新后，计算出的一致性哈希码应该是和服务端一致的， //如果发现不一致，就证明本地缓存的服务列表信息和Eureka server不一致了，需要做一次全量更新 if(!reconcileHashCode.equals(delta.getAppsHashCode())||clientConfig.shouldLogDeltaDiff()) { //一致性哈希码不同，就在reconcileAndLogDifference方法中做全量更新 reconcileAndLogDifference(delta,reconcileHashCode);// this makes a remoteCall } }else{ logger.warn(\"Not updating application delta as another thread is updating it already\"); logger.debug(\"Ignoring delta update with apps hashcode {}, as another thread is updating it aleady\",delta.getAppsHashCode()); } } updateDelta方法将增量更新数据和本地数据做合并 private voidupdateDelta(Applications delta) {2int deltaCount=0; 3//遍历所有服务 4for(Application app:delta.getRegisteredApplications()) { 5//遍历当前服务的所有实例 6for(InstanceInfo instance:app.getInstances()) { 7//取出缓存的所有服务列表，用于合并 8Applications applications=getApplications(); 9String instanceRegion=instanceRegionChecker.getInstanceRegion(instance); 10//判断正在处理的实例和当前应用是否在同一个region 11if(!instanceRegionChecker.isLocalRegion(instanceRegion)) { //如果不是同一个region，接下来合并的数据就换成专门为其他region准备的缓存 Applications remoteApps=remoteRegionVsApps.get(instanceRegion); if(null==remoteApps) { remoteApps=newApplications(); remoteRegionVsApps.put(instanceRegion,remoteApps); } applications=remoteApps; } 20 ++deltaCount; 22 if(ActionType.ADDED.equals(instance.getActionType())) {//对新增的实例的处理 Application existingApp=applications.getRegisteredApplications(instance.getAppName()); if(existingApp==null) { applications.addApplication(app); } logger.debug(\"Added instance {} to the existing apps in region {}\",instance.getId(),instanceR egion); applications.getRegisteredApplications(instance.getAppName()).addInstance(instance); }else if(ActionType.MODIFIED.equals(instance.getActionType())) {//对修改实例的处理 Application existingApp=applications.getRegisteredApplications(instance.getAppName()); if(existingApp==null) { applications.addApplication(app); } logger.debug(\"Modified instance {} to the existing apps \",instance.getId()); 36 applications.getRegisteredApplications(instance.getAppName()).addInstance(instance); 38 }else if(ActionType.DELETED.equals(instance.getActionType())) {//对删除实例的处理 Application existingApp=applications.getRegisteredApplications(instance.getAppName()); if(existingApp==null) { applications.addApplication(app); } logger.debug(\"Deleted instance {} to the existing apps \",instance.getId()); applications.getRegisteredApplications(instance.getAppName()).removeInstance(instance); } } } logger.debug(\"The total number of instances fetched by the delta processor : {}\",deltaCount); 50 getApplications().setVersion(delta.getVersion()); //整理数据，使得后续使用过程中，这些应用的实例总是以相同顺序返回 getApplications().shuffleInstances(clientConfig.shouldFilterOnlyUpInstances()); 54 //和当前应用不在同一个region的应用，其实例数据也要整理 for(Applications applications:remoteRegionVsApps.values()) { applications.setVersion(delta.getVersion()); applications.shuffleInstances(clientConfig.shouldFilterOnlyUpInstances()); } } 服务续约 //服务定时续约 scheduler.schedule( newTimedSupervisorTask( \"heartbeat\", scheduler, heartbeatExecutor, renewalIntervalInSecs, TimeUnit.SECONDS, 9expBackOffBound, 10newHeartbeatThread()//该线程执行续约的具体逻辑，会调用下面的renew()方法 ), renewalIntervalInSecs,TimeUnit.SECONDS); 13 private classHeartbeatThreadimplementsRunnable{ public voidrun() { if(renew()) { lastSuccessfulHeartbeatTimestamp=System.currentTimeMillis(); } } } 21 booleanrenew() { EurekaHttpResponsehttpResponse; try{ httpResponse=eurekaTransport.registrationClient.sendHeartBeat(instanceInfo.getAppName(),inst anceInfo.getId(),instanceInfo,null); logger.debug(PREFIX+\"{} ‐ Heartbeat status: {}\",appPathIdentifier,httpResponse.getStatusCode()); if(httpResponse.getStatusCode()==404) { REREGISTER_COUNTER.increment(); logger.info(PREFIX+\"{} ‐ Re‐registering apps/{}\",appPathIdentifier, instanceInfo.getAppName()); long timestamp=instanceInfo.setIsDirtyWithTime(); boolean success=register(); if(success) { instanceInfo.unsetIsDirty(timestamp); } returnsuccess; } returnhttpResponse.getStatusCode()==200; }catch(Throwable e) { logger.error(PREFIX+\"{} ‐ was unable to send heartbeat!\",appPathIdentifier,e); returnfalse; } } 服务注册 //服务注册 instanceInfoReplicator.start(clientConfig.getInitialInstanceInfoReplicationIntervalSeconds()); 3 public voidstart(int initialDelayMs) { if(started.compareAndSet(false,true)) { instanceInfo.setIsDirty();// for initial register Future next=scheduler.schedule(this,initialDelayMs,TimeUnit.SECONDS); scheduledPeriodicRef.set(next); } } 11 public voidrun() { try{ discoveryClient.refreshInstanceInfo(); 15 Long dirtyTimestamp=instanceInfo.isDirtyWithTime(); if(dirtyTimestamp!=null) { discoveryClient.register(); instanceInfo.unsetIsDirty(dirtyTimestamp); } }catch(Throwable t) { logger.warn(\"There was a problem with the instance info replicator\",t); }finally{ Future next=scheduler.schedule(this,replicationIntervalSeconds,TimeUnit.SECONDS); scheduledPeriodicRef.set(next); } } 5、Eureka Server服务端Jersey接口源码分析 服务端Jersey接口处理类ApplicationResource 其中有一个addInstance方法就是用来接收客户端的注册请求接口 //ApplicationResource.java 2@POST 3@Consumes({\"application/json\",\"application/xml\"}) 4publicResponseaddInstance(InstanceInfo info, 5@HeaderParam(PeerEurekaNode.HEADER_REPLICATION)String isReplication) { 6logger.debug(\"Registering instance {} (replication={})\",info.getId(),isReplication);7// validate that the instanceinfo contains all the necessary required fields //参数校验，不符合验证规则的，返回400状态码，此处不做详解 9if(isBlank(info.getId())) { 10returnResponse.status(400).entity(\"Missing instanceId\").build();11}else if(isBlank(info.getHostName())) { 12returnResponse.status(400).entity(\"Missing hostname\").build();13}else if(isBlank(info.getAppName())) { 14returnResponse.status(400).entity(\"Missing appName\").build();15}else if(!appName.equals(info.getAppName())) { returnResponse.status(400).entity(\"Mismatched appName, expecting \"+appName+\" but was \"+info.getAppName()).build(); }else if(info.getDataCenterInfo()==null) { returnResponse.status(400).entity(\"Missing dataCenterInfo\").build(); }else if(info.getDataCenterInfo().getName()==null) { returnResponse.status(400).entity(\"Missing dataCenterInfo Name\").build(); } 22 // handle cases where clients may be registering with bad DataCenterInfo with missing data DataCenterInfo dataCenterInfo=info.getDataCenterInfo(); if(dataCenterInfoinstanceofUniqueIdentifier) { String dataCenterInfoId=((UniqueIdentifier)dataCenterInfo).getId(); if(isBlank(dataCenterInfoId)) { boolean experimental=\"true\".equalsIgnoreCase(serverConfig.getExperimental(\"registration.validation.dataCenterInfoId\")); if(experimental) { String entity=\"DataCenterInfo of type \"+dataCenterInfo.getClass()+\" must contain a validid\"; returnResponse.status(400).entity(entity).build(); }else if(dataCenterInfoinstanceofAmazonInfo) { AmazonInfo amazonInfo=(AmazonInfo)dataCenterInfo; String effectiveId=amazonInfo.get(AmazonInfo.MetaDataKey.instanceId); if(effectiveId==null) { amazonInfo.getMetadata().put(AmazonInfo.MetaDataKey.instanceId.getName(),info.getId()); } }else{ logger.warn(\"Registering DataCenterInfo of type {} without an appropriate id\",dataCenterInfo.etClass()); } } } //重点在这里 registry.register(info,\"true\".equals(isReplication)); returnResponse.status(204).build();// 204 to be backwards compatible } AbstractInstanceRegistry的注册方法 public voidregister(InstanceInfo registrant,int leaseDuration,boolean isReplication) {2try{ 3//上只读锁 read.lock(); //从本地MAP里面获取当前实例的信息。 Map>gMap=registry.get(registrant.getAppName()); //增加注册次数到监控信息里面去。 REGISTER.increment(isReplication); 9if(gMap==null) { 10//如果第一次进来，那么gMap为空，则创建一个ConcurrentHashMap放入到registry里面去 final ConcurrentHashMap>gNewMap=newConcurrentHashMap>(); // putIfAbsent方法主要是在向ConcurrentHashMap中添加键—值对的时候，它会先判断该键值对是否已经存在。 //如果不存在（新的entry），那么会向map中添加该键值对，并返回null。 //如果已经存在，那么不会覆盖已有的值，直接返回已经存在的值。 gMap=registry.putIfAbsent(registrant.getAppName(),gNewMap); if(gMap==null) { //表明map中确实不存在，则设置gMap为最新创建的那个 gMap=gNewMap; } } //从MAP中查询已经存在的Lease信息 （比如第二次来） LeaseexistingLease=gMap.get(registrant.getId()); //当Lease的对象不为空时。 if(existingLease!=null&&(existingLease.getHolder()!=null)) { //当instance已经存在是，和客户端的instance的信息做比较，时间最新的那个，为有效instance信息 Long existingLastDirtyTimestamp=existingLease.getHolder().getLastDirtyTimestamp();// server Long registrationLastDirtyTimestamp=registrant.getLastDirtyTimestamp();// client logger.debug(\"Existing lease found (existing={}, provided={}\",existingLastDirtyTimestamp,reg strationLastDirtyTimestamp); if(existingLastDirtyTimestamp>registrationLastDirtyTimestamp) { logger.warn(\"There is an existing lease and the existing lease's dirty timestamp {} is greater + \" than the one that is being registered {}\",existingLastDirtyTimestamp,registrationLastDirtyimestamp); logger.warn(\"Using the existing instanceInfo instead of the new instanceInfo as theregistrant\"); registrant=existingLease.getHolder(); } }else{ //这里只有当existinglease不存在时，才会进来。 像那种恢复心跳，信息过期的，都不会进入这里。 // Eureka‐Server的自我保护机制做的操作，为每分钟最大续约数+2，同时重新计算每分钟最小续约数 synchronized(lock) { if(this.expectedNumberOfRenewsPerMin>0) { // Since the client wants to cancel it, reduce the threshold // (1 for 30 seconds, 2 for a minute) this.expectedNumberOfRenewsPerMin=this.expectedNumberOfRenewsPerMin+2; this.numberOfRenewsPerMinThreshold= (int) (this.expectedNumberOfRenewsPerMin*serverConfig.getRenewalPercentThreshold()); } } logger.debug(\"No previous lease information found; it is new registration\"); } //构建一个最新的Lease信息 Leaselease=newLease(registrant,leaseDuration); if(existingLease!=null) { //当原来存在Lease的信息时，设置他的serviceUpTimestamp,保证服务开启的时间一直是第一次的那个 lease.setServiceUpTimestamp(existingLease.getServiceUpTimestamp()); } //放入本地Map中 gMap.put(registrant.getId(),lease); //添加到最近的注册队列里面去，以时间戳作为Key， 名称作为value，主要是为了运维界面的统计数据。 synchronized(recentRegisteredQueue) { recentRegisteredQueue.add(newPair( System.currentTimeMillis(), registrant.getAppName()+\"(\"+registrant.getId()+\")\")); } // This is where the initial state transfer of overridden status happens //分析instanceStatus if(!InstanceStatus.UNKNOWN.equals(registrant.getOverriddenStatus())) { logger.debug(\"Found overridden status {} for instance {}. Checking to see if needs to be add tthe \" +\"overrides\",registrant.getOverriddenStatus(),registrant.getId()); if(!overriddenInstanceStatusMap.containsKey(registrant.getId())) { logger.info(\"Not found overridden id {} and hence adding it\",registrant.getId()); overriddenInstanceStatusMap.put(registrant.getId(),registrant.getOverriddenStatus()); } } InstanceStatus overriddenStatusFromMap=overriddenInstanceStatusMap.get(registrant.getId()); if(overriddenStatusFromMap!=null) { logger.info(\"Storing overridden status {} from map\",overriddenStatusFromMap); registrant.setOverriddenStatus(overriddenStatusFromMap); } 78 // Set the status based on the overridden status rules InstanceStatus overriddenInstanceStatus=getOverriddenInstanceStatus(registrant,existingLease,isReplication); registrant.setStatusWithoutDirty(overriddenInstanceStatus); 82 // If the lease is registered with UP status, set lease service up timestamp //得到instanceStatus，判断是否是UP状态， if(InstanceStatus.UP.equals(registrant.getStatus())) { lease.serviceUp(); } //设置注册类型为添加 registrant.setActionType(ActionType.ADDED); //租约变更记录队列，记录了实例的每次变化， 用于注册信息的增量获取、 recentlyChangedQueue.add(newRecentlyChangedItem(lease)); registrant.setLastUpdatedTimestamp(); //清理缓存 ，传入的参数为key invalidateCache(registrant.getAppName(),registrant.getVIPAddress(),registrant.getSecureVipAdd ress()); logger.info(\"Registered instance {}/{} with status {} (replication={})\", registrant.getAppName(),registrant.getId(),registrant.getStatus(),isReplication); }finally{ read.unlock(); } } 理解上面的register还需要先了解下注册实例信息存放的的map，这是个两层的ConcurrentHashMap>>，外层map的key是appName，也就是服务名，内层map的key是 instanceId，也就是实例名 注册表map数据示例如下： { MICROSERVICE - PROVIDER - USER = { DESKTOP - 1 SLJLB7: microservice - provider - user: 8002 = com.netflix.eureka.lease.Lease @2cd36af6, DESKTOP - 1 SLJLB7: microservice - provider - user: 8001 = com.netflix.eureka.lease.Lease @600b7073 } } 内层map的value对应的类Lease需要重点理解下 public classLease{ 2 enumAction{ Register,Cancel,Renew }; 6 public staticfinal intDEFAULT_DURATION_IN_SECS=90; 8 privateTholder; privatelong evictionTimestamp; privatelong registrationTimestamp; privatelong serviceUpTimestamp; // Make it volatile so that the expiration task would see this quicker privatevolatile long lastUpdateTimestamp; privatelong duration; 16 publicLease(Tr,int durationInSecs) { holder=r; registrationTimestamp=System.currentTimeMillis(); lastUpdateTimestamp=registrationTimestamp; duration=(durationInSecs*1000); 22 } 24 /** *Renew the lease,use renewal durationifit was specified by the *associated{@linkT}during registration,otherwisedefaultduration is *{@link #DEFAULT_DURATION_IN_SECS}. */ public voidrenew() { lastUpdateTimestamp=System.currentTimeMillis()+duration;//有个小bug，不应该加duration 32 } 34 /** *Cancels the lease by updating the eviction time. */ public voidcancel() { if(evictionTimestamp evictionTimestamp=System.currentTimeMillis(); } } 43 /** *Mark the serviceasup.This will only take affect the first time called, *subsequent calls will be ignored. */ public voidserviceUp() { if(serviceUpTimestamp==0) { serviceUpTimestamp=System.currentTimeMillis(); } } 53 /** *Set the leases serviceUPtimestamp. */ public voidsetServiceUpTimestamp(long serviceUpTimestamp) { this.serviceUpTimestamp=serviceUpTimestamp; } 60 /** *Checksifthe leaseofa given{@link com.netflix.appinfo.InstanceInfo}has expired or not. */ publicbooleanisExpired() { returnisExpired(0l); } 67 /** *Checksifthe leaseofa given{@link com.netflix.appinfo.InstanceInfo}has expired or not. * *Note that due torenew()doing the 'wrong\" thing and setting lastUpdateTimestamp to+duratiomore than what it should be,the expiry will actually be2duration.This is a minor bug and shouldnly affect *instances that ungracefully shutdown.Due to possible wide ranging impact to existing usage,thiswill *not be fixed. * *@param additionalLeaseMs any additional lease time to add to the lease evaluationinms. */ publicbooleanisExpired(long additionalLeaseMs) { return(evictionTimestamp>0||System.currentTimeMillis()>(lastUpdateTimestamp+duration+additionalLeaseMs)); } 81 /** *Gets the milliseconds since epoch when the lease was registered. * *@returnthe milliseconds since epoch when the lease was registered. */ publiclonggetRegistrationTimestamp() { returnregistrationTimestamp; } 90 /** *Gets the milliseconds since epoch when the lease was last renewed. *Note that the value returned here is actually not the last lease renewal time but the renewa+duration. * *@returnthe milliseconds since epoch when the lease was last renewed. */ publiclonggetLastRenewalTimestamp() { returnlastUpdateTimestamp; } 100 /** *Gets the milliseconds since epoch when the lease was evicted. * *@returnthe milliseconds since epoch when the lease was evicted. */ publiclonggetEvictionTimestamp() { returnevictionTimestamp; } 109 /** *Gets the milliseconds since epoch when the serviceforthe lease was markedasup. * *@returnthe milliseconds since epoch when the serviceforthe lease was markedasup. */ publiclonggetServiceUpTimestamp() { returnserviceUpTimestamp; } 118 /** *Returns the holderofthe lease. */ publicTgetHolder() { returnholder; } 125 } DEFAULT_DURATION_IN_SECS : 租约过期的时间常量，默认未90秒，也就说90秒没有心跳过来，那么这边将会自动剔除该节点 holder ：这个租约是属于谁的，目前占用这个属性的是 instanceInfo，也就是客户端实例信息。 evictionTimestamp ：租约是啥时候过期的，当服务下线的时候，会过来更新这个时间戳registrationTimestamp ：租约的注册时间 serviceUpTimestamp ：服务启动时间，当客户端在注册的时候，instanceInfo的status 为UP的时候，则更新这个时间戳 lastUpdateTimestamp ：最后更新时间，每次续约的时候，都会更新这个时间戳，在判断实例 是否过期时，需要用到这个属性。 duration：过期时间，毫秒单位 服务端Jersey接口处理类ApplicationsResource 其中有一个getContainers方法就是用来获取所有注册实例信息的接口 @GET publicResponsegetContainers(@PathParam(\"version\")String version,3@HeaderParam(HEADER_ACCEPT)String acceptHeader, 4@HeaderParam(HEADER_ACCEPT_ENCODING)String acceptEncoding, 5@HeaderParam(EurekaAccept.HTTP_X_EUREKA_ACCEPT)String eurekaAccept,6@Context UriInfo uriInfo, 7@Nullable @QueryParam(\"regions\")String regionsStr) { 8 boolean isRemoteRegionRequested=null!=regionsStr&& !regionsStr.isEmpty();10String[]regions=null; 11if(!isRemoteRegionRequested) { 12EurekaMonitors.GET_ALL.increment(); 13}else{ 14regions=regionsStr.toLowerCase().split(\",\"); Arrays.sort(regions);// So we don't have different caches for same regions queried in differet order. EurekaMonitors.GET_ALL_WITH_REMOTE_REGIONS.increment(); } 18 // Check if the server allows the access to the registry. The server can // restrict access if it is not // ready to serve traffic depending on various reasons. if(!registry.shouldAllowAccess(isRemoteRegionRequested)) { returnResponse.status(Status.FORBIDDEN).build(); } CurrentRequestVersion.set(Version.toEnum(version)); KeyType keyType=Key.KeyType.JSON; String returnMediaType=MediaType.APPLICATION_JSON; if(acceptHeader==null|| !acceptHeader.contains(HEADER_JSON_VALUE)) { keyType=Key.KeyType.XML; returnMediaType=MediaType.APPLICATION_XML; } 32 //获取服务实例对应的缓存key Key cacheKey=newKey(Key.EntityType.Application, ResponseCacheImpl.ALL_APPS, keyType,CurrentRequestVersion.get(),EurekaAccept.fromString(eurekaAccept),regions ); 38 Response response; if(acceptEncoding!=null&&acceptEncoding.contains(HEADER_GZIP_VALUE)) { response=Response.ok(responseCache.getGZIP(cacheKey)) .header(HEADER_CONTENT_ENCODING,HEADER_GZIP_VALUE) .header(HEADER_CONTENT_TYPE,returnMediaType) .build(); }else{ //从缓存里获取服务实例注册信息 response=Response.ok(responseCache.get(cacheKey)) .build(); } returnresponse; } 52 responseCache.get(cacheKey)对应的源码如下： @VisibleForTesting Stringget(final Key key,boolean useReadOnlyCache) { //从多级缓存里获取注册实例信息 Value payload=getValue(key,useReadOnlyCache); if(payload==null||payload.getPayload().equals(EMPTY_PAYLOAD)) { return null; }else{ returnpayload.getPayload(); } } 64 @VisibleForTesting ValuegetValue(final Key key,boolean useReadOnlyCache) { Value payload=null; try{ if(useReadOnlyCache) { final Value currentPayload=readOnlyCacheMap.get(key); if(currentPayload!=null) { payload=currentPayload; }else{ payload=readWriteCacheMap.get(key); readOnlyCacheMap.put(key,payload); } }else{ payload=readWriteCacheMap.get(key); } }catch(Throwable t) { logger.error(\"Cannot get value for key : {}\",key,t); } returnpayload; } 85 86 ResponseCacheImpl(EurekaServerConfig serverConfig,ServerCodecs serverCodecs,AbstractInstanceRgistry registry) { this.serverConfig=serverConfig; this.serverCodecs=serverCodecs; this.shouldUseReadOnlyResponseCache=serverConfig.shouldUseReadOnlyResponseCache(); this.registry=registry; 92 long responseCacheUpdateIntervalMs=serverConfig.getResponseCacheUpdateIntervalMs(); this.readWriteCacheMap= CacheBuilder.newBuilder().initialCapacity(1000) //读写缓存默认180秒会自动定时过期 .expireAfterWrite(serverConfig.getResponseCacheAutoExpirationInSeconds(),TimeUnit.SECONDS) .removalListener(newRemovalListener() { @Override public voidonRemoval(RemovalNotificationnotification) { Key removedKey=notification.getKey(); if(removedKey.hasRegions()) { Key cloneWithNoRegions=removedKey.cloneWithoutRegions(); regionSpecificKeys.remove(cloneWithNoRegions,removedKey); } } }) .build(newCacheLoader() { @Override publicValueload(Key key)throws Exception{ if(key.hasRegions()) { Key cloneWithNoRegions=key.cloneWithoutRegions(); regionSpecificKeys.put(cloneWithNoRegions,key); } Value value=generatePayload(key); returnvalue; } }); 119 if(shouldUseReadOnlyResponseCache) { //默认30秒用读写缓存的数据更新只读缓存的数据 timer.schedule(getCacheUpdateTask(), newDate(((System.currentTimeMillis()/responseCacheUpdateIntervalMs)*responseCacheUpdateIntervalMs) +responseCacheUpdateIntervalMs), responseCacheUpdateIntervalMs); } 127 try{ Monitors.registerObject(this); }catch(Throwable e) { logger.warn(\"Cannot register the JMX monitor for the InstanceRegistry\",e); } } 134 //初始化直接从注册表registry里那数据放入readWriteCacheMap privateValuegeneratePayload(Key key) { Stopwatch tracer=null; try{ String payload; switch(key.getEntityType()) { caseApplication: boolean isRemoteRegionRequested=key.hasRegions(); 143 if(ALL_APPS.equals(key.getName())) { if(isRemoteRegionRequested) { tracer=serializeAllAppsWithRemoteRegionTimer.start(); payload=getPayLoad(key,registry.getApplicationsFromMultipleRegions(key.getRegions())); }else{ tracer=serializeAllAppsTimer.start(); payload=getPayLoad(key,registry.getApplications()); } }else if(ALL_APPS_DELTA.equals(key.getName())) { if(isRemoteRegionRequested) { tracer=serializeDeltaAppsWithRemoteRegionTimer.start(); versionDeltaWithRegions.incrementAndGet(); versionDeltaWithRegionsLegacy.incrementAndGet(); payload=getPayLoad(key, registry.getApplicationDeltasFromMultipleRegions(key.getRegions())); }else{ tracer=serializeDeltaAppsTimer.start(); versionDelta.incrementAndGet(); versionDeltaLegacy.incrementAndGet(); payload=getPayLoad(key,registry.getApplicationDeltas()); } }else{ tracer=serializeOneApptimer.start(); payload=getPayLoad(key,registry.getApplication(key.getName())); } break; caseVIP: caseSVIP: tracer=serializeViptimer.start(); payload=getPayLoad(key,getApplicationsForVip(key,registry)); break; default: logger.error(\"Unidentified entity type: {} found in the cache key.\",key.getEntityType()); payload=\"\"; break; } return newValue(payload); }finally{ if(tracer!=null) { tracer.stop(); } } } 187 //用读写缓存的数据更新只读缓存的数据 privateTimerTaskgetCacheUpdateTask() { return newTimerTask() { @Override public voidrun() { logger.debug(\"Updating the client cache from response cache\"); for(Key key:readOnlyCacheMap.keySet()) { if(logger.isDebugEnabled()) { logger.debug(\"Updating the client cache from response cache for key : {} {} {} {}\", key.getEntityType(),key.getName(),key.getVersion(),key.getType()); } try{ CurrentRequestVersion.set(key.getVersion()); Value cacheValue=readWriteCacheMap.get(key); Value currentCacheValue=readOnlyCacheMap.get(key); if(cacheValue!=currentCacheValue) { readOnlyCacheMap.put(key,cacheValue); } }catch(Throwable th) { logger.error(\"Error while updating the client cache from response cache for key {}\",key.toStringCompact(),th); } } } }; } 源码精髓：多级缓存设计思想 在拉取注册表的时候： 首先从ReadOnlyCacheMap里查缓存的注册表。 若没有，就找ReadWriteCacheMap里缓存的注册表。 如果还没有，就从内存中获取实际的注册表数据。 在注册表发生变更的时候： 会在内存中更新变更的注册表数据，同时过期掉ReadWriteCacheMap。 此过程不会影响ReadOnlyCacheMap提供人家查询注册表。 默认每30秒Eureka Server会将ReadWriteCacheMap更新到ReadOnlyCacheMap里 默认每180秒Eureka Server会将ReadWriteCacheMap里是数据失效 下次有服务拉取注册表，又会从内存中获取最新的数据了，同时填充各级缓存。 多级缓存机制的优点： 尽可能保证了内存注册表数据不会出现频繁的读写冲突问题。 并且进一步保证对Eureka Server的大量请求，都是快速从纯内存走，性能极高（可以稍微估计下对于一线互联网公司，内部上千个eureka client实例，每分钟对eureka上千次的访问，一天就是上千万次的访问） 看源码彻底搞懂一些诡异的问题： 看完多级缓存这块源码我们可以搞清楚一个常见的问题，就是当我们eureka服务实例有注册或下线或有实例发生故障，内存注册表虽然会及时更新数据，但是客户端不一定能及时感知到，可能会过30秒才能感知到，因为客户端拉取注册表实例这里面有一个多级缓存机制 还有服务剔除的不是默认90秒没心跳的实例，剔除的是180秒没心跳的实例(eureka的bug导致) Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/Hystrix-source.html":{"url":"micro/Hystrix-source.html","title":"10.Hystrix&Zuul源码分析","keywords":"","body":"Hystrix方法执行与降级方法执行源码分析 主要对加了@HystrixCommand注解的方法用AOP拦截实现类HystrixCommandAspect去拦截，主要拦截执行方法如下 publicObjectmethodsAnnotatedWithHystrixCommand(final ProceedingJoinPoint joinPoint)throwshrowable{ //被@HystrixCommand标记的hello()方法 Method method=getMethodFromTarget(joinPoint); MetaHolderFactory metaHolderFactory= ...get(HystrixPointcutType.of(method)); 5MetaHolder metaHolder=metaHolderFactory.create(joinPoint); 6//准备各种材料后，创建HystrixInvokable 7HystrixInvokable invokable=HystrixCommandFactory.getInstance().create(metaHolder);8Object result; 9try{ 10if(!metaHolder.isObservable()) { 11//利用工具CommandExecutor来执行具体的方法 12result=CommandExecutor.execute(invokable,executionType,metaHolder); } } returnresult; } HystrixInvokable 只是一个空接口，没有任何方法，只是用来标记具备可执行的能力。 那 HystrixInvokable 又是如何创建的？它具体的实现类又是什么？先看 看H ystrixC om m andFactory.getInstance().create()的代码。 1publicHystrixInvokablecreate(MetaHolder metaHolder) { 2return newGenericCommand(...create(metaHolder)); } GenericCommand 负责执行具体的方法和fallback时的方法 //执行具体的方法，如：OrderController的findById() 2protectedObjectrun()throws Exception{ 3returnprocess(newAction() { @Override Objectexecute() { returngetCommandAction().execute(getExecutionType()); } }); } //执行fallback方法，如：OrderController的findByIdFallback() protectedObjectgetFallback() { final CommandAction commandAction=getFallbackAction(); returnprocess(newAction() { @Override Objectexecute() { MetaHolder metaHolder=commandAction.getMetaHolder(); Object[]args=createArgsForFallback(...); returncommandAction.executeWithArgs(...,args); } }); } toObservable()核心源码解析 publicObservabletoObservable() {2final AbstractCommand_cmd=this; 3//命令执行结束后的清理者 4final Action0 terminateCommandCleanup=newAction0() {...}; 5//取消订阅时处理者 6final Action0 unsubscribeCommandCleanup=newAction0() {...}; 7//重点：Hystrix核心逻辑:断路器、隔离 8final Func0>applyHystrixSemantics=newFunc0>() {...}; 9//发射数据(OnNext表示发射数据)时的Hook 10final Func1wrapWithAllOnNextHooks=newFunc1() {...}; 11//命令执行完成的Hook 12final Action0 fireOnCompletedHook=newAction0() {...};13//通过Observable.defer()创建一个Observable 14returnObservable.defer(newFunc0>() {15@Override 16publicObservablecall() { 17final boolean requestCacheEnabled=isRequestCachingEnabled(); 18final String cacheKey=getCacheKey(); 19//首先尝试从请求缓存中获取结果 20if(requestCacheEnabled) { HystrixCommandResponseFromCachefromCache=(HystrixCommandResponseFromCache)requestC ache.get(cacheKey); if(fromCache!=null) { isResponseFromCache=true; returnhandleRequestCacheHitAndEmitValues(fromCache,_cmd); } } //使用上面的Func0：applyHystrixSemantics来创建Observable ObservablehystrixObservable= Observable.defer(applyHystrixSemantics) .map(wrapWithAllOnNextHooks); ObservableafterCache; //如果启用请求缓存，将Observable包装成HystrixCachedObservable并进行相关处理 if(requestCacheEnabled&&cacheKey!=null) { HystrixCachedObservabletoCache=HystrixCachedObservable.from(hystrixObservable,_cmd); ... }else{ afterCache=hystrixObservable; } //返回Observable returnafterCache .doOnTerminate(terminateCommandCleanup) .doOnUnsubscribe(unsubscribeCommandCleanup) .doOnCompleted(fireOnCompletedHook); } }); } 断路器、隔离核心代码applyHystrixSemantics() // Semantics译为语义,应用Hystrix语义很拗口，其实就是应用Hystrix的断路器、隔离特性 2privateObservableapplyHystrixSemantics(final AbstractCommand_cmd) { //源码中有很多executionHook、eventNotifier的操作，这是Hystrix拓展性的一种体现。这里面啥事也没做，留了个口子，开发人员可以拓展 executionHook.onStart(_cmd); //判断断路器是否开启 if(circuitBreaker.attemptExecution()) { 7//获取执行信号 8final TryableSemaphore executionSemaphore=getExecutionSemaphore(); 9final AtomicBoolean semaphoreHasBeenReleased=newAtomicBoolean(false);10final Action0 singleSemaphoreRelease=newAction0() {...}; 11final Action1markExceptionThrown=newAction1() {...}; 12//判断是否信号量拒绝 13if(executionSemaphore.tryAcquire()) { 14try{ 15//重点：处理隔离策略和Fallback策略 16returnexecuteCommandAndObserve(_cmd) 17.doOnError(markExceptionThrown) 18.doOnTerminate(singleSemaphoreRelease) 19.doOnUnsubscribe(singleSemaphoreRelease); 20}catch(RuntimeException e) { 21returnObservable.error(e); } }else{ returnhandleSemaphoreRejectionViaFallback(); } } //开启了断路器,执行Fallback else{ returnhandleShortCircuitViaFallback(); } } executeCommandAndObserve()处理隔离策略和各种Fallback privateObservableexecuteCommandAndObserve(final AbstractCommand_cmd) { final HystrixRequestContext currentRequestContext=HystrixRequestContext.getContextForCurretThread(); final Action1markEmits=newAction1() {...}; 4final Action0 markOnCompleted=newAction0() {...};5//利用Func1获取处理Fallback的Observable final Func1>handleFallback=newFunc1>() { @Override publicObservablecall(Throwable t) { 9circuitBreaker.markNonSuccess(); Exception e=getExceptionFromThrowable(t); executionResult=executionResult.setExecutionException(e); //拒绝处理 if(einstanceofRejectedExecutionException) { returnhandleThreadPoolRejectionViaFallback(e); //超时处理 }else if(tinstanceofHystrixTimeoutException) { returnhandleTimeoutViaFallback(); }else if(tinstanceofHystrixBadRequestException) { returnhandleBadRequestByEmittingError(e); }else{ ... returnhandleFailureViaFallback(e); } } }; final Action1>setRequestContext... Observableexecution; //利用特定的隔离策略来处理 if(properties.executionTimeoutEnabled().get()) { execution=executeCommandWithSpecifiedIsolation(_cmd) .lift(newHystrixObservableTimeoutOperator(_cmd)); }else{ execution=executeCommandWithSpecifiedIsolation(_cmd); } returnexecution.doOnNext(markEmits) .doOnCompleted(markOnCompleted) //绑定Fallback的处理者 .onErrorResumeNext(handleFallback) .doOnEach(setRequestContext); } 隔离特性的处理：executeCommandWithSpecifiedIsolation() privateObservableexecuteCommandWithSpecifiedIsolation(final AbstractCommand_cmd) { 2//线程池隔离 3if(properties.executionIsolationStrategy().get()==ExecutionIsolationStrategy.THREAD) {4//再次使用Observable.defer(),通过执行Func0来得到Observable 5returnObservable.defer(newFunc0>() { @Override publicObservablecall() { 8//收集metric信息 9metrics.markCommandStart(commandKey,threadPoolKey,ExecutionIsolationStrategy.THREAD);10... 11try{ 12...//获取真正的用户Task 13returngetUserExecutionObservable(_cmd); 14}catch(Throwable ex) { 15returnObservable.error(ex); } ... } //绑定各种处理者 }).doOnTerminate(newAction0() {...}) .doOnUnsubscribe(newAction0() {...}) //线程隔离，绑定超时处理者 .subscribeOn(threadPool.getScheduler(newFunc0() { @Override publicBooleancall() { returnproperties.executionIsolationThreadInterruptOnTimeout().get()&&_cmd.isCommandTimedOut.get()==TimedOutStatus.TIMED_OUT; } })); } //信号量隔离，和线程池大同小异 else{ returnObservable.defer(newFunc0>() {...} } } 线程隔离threadPool.getScheduler //隔离线程池初始化 private staticHystrixThreadPoolinitThreadPool(HystrixThreadPool fromConstructor,HystrixThradPoolKey threadPoolKey,HystrixThreadPoolProperties.Setter threadPoolPropertiesDefaults) { if(fromConstructor==null) { // get the default implementation of HystrixThreadPool returnHystrixThreadPool.Factory.getInstance(threadPoolKey,threadPoolPropertiesDefaults); 6}else{ 7returnfromConstructor; } } 10 11 staticHystrixThreadPoolgetInstance(HystrixThreadPoolKey threadPoolKey,HystrixThreadPoolProperties.Setter propertiesBuilder) { // get the key to use instead of using the object itself so that if people forget to impleme nt equals/hashcode things will still work String key=threadPoolKey.name(); 15 // this should find it for all but the first time HystrixThreadPool previouslyCached=threadPools.get(key); if(previouslyCached!=null) { returnpreviouslyCached; } 21 // if we get here this is the first time so we need to initialize synchronized(HystrixThreadPool.class) { if(!threadPools.containsKey(key)) { threadPools.put(key,newHystrixThreadPoolDefault(threadPoolKey,propertiesBuilder)); } } returnthreadPools.get(key); } 30 publicHystrixThreadPoolDefault(HystrixThreadPoolKey threadPoolKey,HystrixThreadPoolProperties.Setter propertiesDefaults) { this.properties=HystrixPropertiesFactory.getThreadPoolProperties(threadPoolKey,propertiesDefaults); HystrixConcurrencyStrategy concurrencyStrategy=HystrixPlugins.getInstance().getConcurrencyStrategy(); this.queueSize=properties.maxQueueSize().get(); 35 this.metrics=HystrixThreadPoolMetrics.getInstance(threadPoolKey, concurrencyStrategy.getThreadPool(threadPoolKey,properties), properties); this.threadPool=this.metrics.getThreadPool(); this.queue=this.threadPool.getQueue(); 41 / strategy: HystrixMetricsPublisherThreadPool / HystrixMetricsPublisherFactory.createOrRetrievePublisherForThreadPool(threadPoolKey,this.me trics,this.properties); } 命令真正的调用逻辑入口getUserExecutionObservable privateObservablegetUserExecutionObservable(final AbstractCommand_cmd) { 2ObservableuserObservable; 3 try{ userObservable=getExecutionObservable(); 6}catch(Throwable ex) { // the run() method is a user provided implementation so can throw instead of using Observab e.onError // so we catch it here and turn it into Observable.error 9userObservable=Observable.error(ex); } 11 returnuserObservable .lift(newExecutionHookApplication(_cmd)) .lift(newDeprecatedOnRunHookApplication(_cmd)); } 16 finalprotectedObservablegetExecutionObservable() { returnObservable.defer(newFunc0>() { @Override publicObservablecall() { try{ //调用命令的真正方法run()入口 returnObservable.just(run()); }catch(Throwable ex) { returnObservable.error(ex); } } }).doOnSubscribe(newAction0() { @Override public voidcall() { // Save thread on which we get subscribed so that we can interrupt it later if needed executionThread.set(Thread.currentThread()); } }); } 上面方法层层调用，倒过来看，就是先创建一个Observable，然后绑定各种事件对应的处理者，如下图 断路器源码分析 Hystrix 有点类似，例如：以秒为单位来统计请求的处理情况(成功请求数量、失败请求数、超时请求数、被拒绝的请求数)，然后每次取最近10秒的数据来进行计算，如果失败率超过50%，就进行熔断，不再处理任何请求。这是Hystrix官网的一张图： 它演示了 Hystrix 滑动窗口 策略，假定以秒为单位来统计请求处理情况，上面每个格子代表1秒，格子中的数据就是1秒内各处理结果的请求数量，格子称为 Bucket(译为桶)。 若每次的决策都以10个Bucket的数据为依据，计算10个Bucket的请求处理情况，当失败率超过50%时就熔断。 10个Bucket就是10秒，这个10秒就是一个 滑动窗口(Rolling window)。 为什么叫滑动窗口？因为在没有熔断时，每当收集好一个新的Bucket后，就会丢弃掉最旧的一个Bucket。上图中的深色的(23 5 2 0)就是被丢弃的桶。 下面是官方完整的流程图，策略是：不断收集数据，达到条件就熔断；熔断后拒绝所有请求一段时间(sleepWindow)；然后放一个请求过去，如果请求成功，则关闭熔断器，否则继续打开熔断器。 相关配置 默认配置都在H ystrixC om m andProperties类中。 先看两个metrics收集的配置。 metrics.rollingStats.timeInMilliseconds 表示滑动窗口的时间(the duration of the statistical rolling window)，默认10000(10s)，也是熔断器计算的基 本单位。 metrics.rollingStats.numBuckets 滑动窗口的Bucket数量(the number of buckets the rolling statistical window is divided into)，默认10. 通过timeInMilliseconds和numBuckets可以计算出每个Bucket的时长。 metrics.rollingStats.timeInMilliseconds % metrics.rollingStats.numBuckets 必须等于 0，否则将抛异常。再看看熔断器的配置。 circuitBreaker.requestVolumeThreshold 滑动窗口触发熔断的最小请求数。如果值是20，但滑动窗口的时间内请求数只有19，那即使19个请求全部失败，也不会熔断，必须达到这个值才行，否则样本太少，没有意义。 circuitBreaker.sleepWindowInMilliseconds 这个和熔断器自动恢复有关，为了检测后端服务是否恢复，可以放一个请求过去试探一下。sleepWindow指的发生熔断后，必须隔sleepWindow这么长的时间，才能放请求过去试探下服务是否恢复。默认是5s circuitBreaker.errorThresholdPercentage 错误率阈值，表示达到熔断的条件。比如默认的50%，当一个滑动窗口内，失败率达到50%时就会触发熔断。 断路器的初始化是在AbstractCommand构造器中做的初始化 private staticHystrixCircuitBreakerinitCircuitBreaker(boolean enabled,HystrixCircuitBreakefromConstructor,HystrixCommandKey commandKey...) { //如果启用了熔断器 if(enabled) { //若commandKey没有对应的CircuitBreaker,则创建 5if(fromConstructor==null) { 6returnHystrixCircuitBreaker.Factory.getInstance(commandKey,groupKey,properties,metrics);7}else{ //如果有则返回现有的 returnfromConstructor; } }else{ return newNoOpCircuitBreaker(); } } 再看看 HystrixCircuitBreaker.Factory.getInstance(commandKey, groupKey, properties, metrics) 如何 创建circuit-breakder？ circuitBreaker以commandKey为维度，每个commandKey都会有对应的circuitBreaker public staticHystrixCircuitBreakergetInstance(HystrixCommandKey key,HystrixCommandGroupKeygroup,HystrixCommandProperties properties,HystrixCommandMetrics metrics) { //如果有则返回现有的, key.name()即command的name作为检索条件 HystrixCircuitBreaker previouslyCached=circuitBreakersByCommand.get(key.name()); 4if(previouslyCached!=null) { 5returnpreviouslyCached; } //如果没有则创建并cache HystrixCircuitBreaker cbForCommand=circuitBreakersByCommand.putIfAbsent(key.name(),newHytrixCircuitBreakerImpl(key,group,properties,metrics)); if(cbForCommand==null) { returncircuitBreakersByCommand.get(key.name()); }else{ returncbForCommand; } } 15 16 //初始化断路器 protectedHystrixCircuitBreakerImpl(HystrixCommandKey key,HystrixCommandGroupKey commandGroup,final HystrixCommandProperties properties,HystrixCommandMetrics metrics) { this.properties=properties; //这是Command中的metrics对象,metrics对象也是commandKey维度的 this.metrics=metrics; //重点:订阅事件流 Subscription s=subscribeToStream(); activeSubscription.set(s); } //订阅事件流,各事件以结构化数据汇入了Stream中 privateSubscriptionsubscribeToStream() { // HealthCountsStream是重点 returnmetrics.getHealthCountsStream() .observe() //利用数据统计的结果HealthCounts,实现熔断器 .subscribe(newSubscriber() { @Override public voidonCompleted() {} @Override public voidonError(Throwable e) {} @Override public voidonNext(HealthCounts hc) { //检查是否达到最小请求数,默认20个;未达到的话即使请求全部失败也不会熔断 if(hc.getTotalRequests() //啥也不做 }else{ //错误百分比未达到设定的阀值 if(hc.getErrorPercentage() }else{ //错误率过高,进行熔断 if(status.compareAndSet(Status.CLOSED,Status.OPEN)) { circuitOpened.set(System.currentTimeMillis()); } } } } }); } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/alibaba-index.html":{"url":"micro/alibaba-index.html","title":"springcloud-alibaba","keywords":"","body":"springcloud-alibaba 5.Eureka源码分析 6.Ribbon&Feign介绍及使用 7.Ribbon&Feign源码分析 8.Hystrix限流降级熔断 9.网关zuul 10.Hystrix&Zuul源码分析 12.配置中心Config 13.链路跟踪Sleuth Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:49:31 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/docker-index.html":{"url":"micro/docker-index.html","title":"docker","keywords":"","body":"docker 14.Docker快速入门 15.DockerCompose微服务编排 16.Docker整体编排SpringCloud Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:49:36 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/docker.html":{"url":"micro/docker.html","title":"14.Docker快速入门","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 uname -r yum -y update yum remove docker docker-common docker-selinux docker-engine yum list docker-ce --showduplicates | sort -r docker version yum -y remove docker-engine docker search java docker search java cd /etc/docker docker pull java:8 docker images docker rmi java docker run -d -p 91:80 nginx docker ps docker stop f0b1c8ab3633 docker kill f0b1c8ab3633 docker start f0b1c8ab3633 docker inspect f0b1c8ab3633 docker topf0b1c8ab3633 docker exec -it f0b1c8ab3633 /bin/bash docker rm f0b1c8ab3633 docker build -t nginx:tuling . docker run -d -p 92:80 nginx:tuling 作者：诸葛老师 2013年发布至今，Docker 一直广受瞩目，被认为可能会改变软件行业。 但是，许多人并不清楚 Docker 到底是什么，要解决什么问题，好处又在哪里？今天就来详细解释，帮助大家理解它，还带有简单易懂的实例，教你如何将它用于日常开发。 Docker简介 Docker是一个开源的容器引擎，它有助于更快地交付应用。 Docker可将应用程序和基础设施层隔离，并且能将基础设施当作程序一样进行管理。使用 Docker可更快地打包、测试以及部署应用程序，并可以缩短从编写到部署运行代码的周期。 Docker的优点如下： 1、简化程序 Docker 让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，便可以实现虚拟化。Docker改变了虚拟化的方式，使开发者可以直接将自己的成果放入Docker中进行管理。方便快捷已经是 Docker的最大优势，过去需要用数天乃至数周的任务，在Docker容器的处理下，只需要数秒就能完成。 2、避免选择恐惧症 如果你有选择恐惧症，还是资深患者。Docker 帮你打包你的纠结！比如 Docker 镜像； Docker 镜像中包含了运行环境和配置，所以 Docker 可以简化部署多种应用实例工作。比如 Web 应用、后台应用、数据库应用、大数据应用比如 Hadoop 集群、消息队列等等都可以打包成一个镜像部署。 3、节省开支 一方面，云计算时代到来，使开发者不必为了追求效果而配置高额的硬件，Docker 改变了高性能必然高价格的思维定势。Docker 与云的结合，让云空间得到更充分的利用。不仅解决了硬件管理的问题，也改变了虚拟化的方式。 Docker的架构 Docker daemon（ Docker守护进程） Docker daemon是一个运行在宿主机（ DOCKER-HOST）的后台进程。可通过 Docker 客户端与之通信。 Client（ Docker客户端） Docker客户端是 Docker的用户界面，它可以接受用户命令和配置标识，并与 Docker daemon通信。图中， docker build等都是 Docker的相关命令。 Images（ Docker镜像） Docker镜像是一个只读模板，它包含创建 Docker容器的说明。它和系统安装光盘有点像，使用系统安装光盘可以安装系统，同理，使用Docker镜像可以运行 Docker镜像中的程序。 Container（容器） 容器是镜像的可运行实例。镜像和容器的关系有点类似于面向对象中，类和对象的关系。 可通过 Docker API或者 CLI命令来启停、移动、删除容器。 Registry Docker Registry是一个集中存储与分发镜像的服务。构建完 Docker镜像后，就可在当前宿主机上运行。但如果想要在其他机器上运行这个镜像，就需要手动复制。此时可借助 Docker Registry来避免镜像的手动复制。 一个 Docker Registry可包含多个 Docker仓库，每个仓库可包含多个镜像标签，每个标签对应一个 Docker镜像。这跟 Maven的仓库有点类似，如果把 Docker Registry比作Maven仓库的话，那么 Docker仓库就可理解为某jar包的路径，而镜像标签则可理解为jar包的版本号。 Docker Registry可分为公有Docker Registry和私有Docker Registry。 最常⽤的Docker Registry莫过于官⽅的Docker Hub，这也是默认的Docker Registry。 Docker Hub上存放着⼤量优秀的镜像，我们可使⽤Docker命令下载并使⽤。 Docker 的安装 Docker 是一个开源的商业产品，有两个版本：社区版（Community Edition，缩写为 CE）和企业版（Enterprise Edition，缩写为 EE）。企业版包含了一些收费服务，个人开发者一般用不到。下面的介绍都针对社区版。 Docker CE 的安装请参考官方文档，我们这里以CentOS为例： 1、Docker 要求 CentOS 系统的内核版本高于 3.10 通过 uname -r 命令查看你当前的内核版本 uname -r 2、使用 root 权限登录 Centos。确保 yum 包更新到最新。 yum -y update 3、卸载旧版本(如果安装过旧版本的话) yum remove docker docker-common docker-selinux docker-engine 4、安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 yum install -y yum-utils device-mapper-persistent-data lvm2 5、设置yum源，并更新yum的包索引 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast 6、可以查看所有仓库中所有docker版本，并选择特定版本安装 yum list docker-ce --showduplicates | sort -r 7、安装docker yum install docker-ce #由于repo中默认只开启stable仓库，故这里安装的是最新稳定版 18.03.1 yum install # 例如：yum -y install docker-ce-18.03.1.ce 8、启动并加入开机启动 systemctl start docker systemctl enable docker 9、验证安装是否成功(有client和service两部分表示docker安装启动都成功了) docker version 10、卸载docker yum -y remove docker-engine Docker常用命令 镜像相关命令 1、搜索镜像 可使用 docker search命令搜索存放在 Docker Hub中的镜像。执行该命令后， Docker就会在Docker Hub中搜索含有 java这个关键词的镜像仓库。 docker search java 以上列表包含五列，含义如下： NAME:镜像仓库名称。 DESCRIPTION:镜像仓库描述。 STARS：镜像仓库收藏数，表示该镜像仓库的受欢迎程度，类似于 GitHub的 stars0 OFFICAL:表示是否为官方仓库，该列标记为[0K]的镜像均由各软件的官方项目组创建和维护。 AUTOMATED：表示是否是自动构建的镜像仓库。 注意：需要配置镜像加速器 docker search java Error response from daemon: Get https://index.docker.io/v1/search?q=java: read tcp 52.200.132.201:443:i/o timeout 我们可以借助阿里云的镜像加速器，登录阿里云 (https://cr.console.aliyun.com/#/accelerator)可以看到镜像加速地址如下图： cd /etc/docker 查看有没有 daemon.json。这是docker默认的配置文件。 如果没有新建，如果有，则修改。 vim daemon.json { \"registry-mirrors\": [\"https://m9r2r2uj.mirror.aliyuncs.com\"] } 保存退出。 重启docker服务 service docker restart 成功！ 2、下载镜像 使用命令docker pull命令即可从 Docker Registry上下载镜像，执行该命令后，Docker会从 Docker Hub中的 java仓库下载最新版本的 Java镜像。如果要下载指定版本则在java后面加冒号指定版本，例如：docker pull java:8 docker pull java:8 3、列出镜像 使用 docker images命令即可列出已下载的镜像 docker images 以上列表含义如下 REPOSITORY：镜像所属仓库名称。 TAG:镜像标签。默认是 latest,表示最新。 IMAGE ID：镜像 ID，表示镜像唯一标识。 CREATED：镜像创建时间。 SIZE: 镜像大小。 4、删除本地镜像 使用 docker rmi命令即可删除指定镜像 docker rmi java 容器相关命令 1、新建并启动容器 使用以下docker run命令即可新建并启动一个容器，该命令是最常用的命令，它有很多选项，下面将列举一些常用的选项。 -d选项：表示后台运行 -P选项：随机端口映射 -p选项：指定端口映射，有以下四种格式。 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort --net选项：指定网络模式，该选项有以下可选参数：--net=bridge:默认选项，表示连接到默认的网桥。--net=host:容器使用宿主机的网络。 --net=container:NAME-or-ID：告诉 Docker让新建的容器使用已有容器的网络配置。 --net=none：不配置该容器的网络，用户可自定义网络配置。 docker run -d -p 91:80 nginx 这样就能启动一个 Nginx容器。在本例中，为 docker run添加了两个参数，含义如下： -d 后台运行 -p 宿主机端口:容器端口 #开放容器端口到宿主机端口 访问 http://Docker宿主机 IP:91/，将会看到nginx的主界面如下： 需要注意的是，使用 docker run命令创建容器时，会先检查本地是否存在指定镜像。如果本地不存在该名称的镜像， Docker就会自动从 Docker Hub下载镜像并启动一个 Docker容器。 2、列出容器 docker ps命令即可列出运行中的容器 docker ps 如需列出所有容器（包括已停止的容器），可使用-a参数。该列表包含了7列，含义如下 CONTAINER_ID：表示容器 ID。 IMAGE:表示镜像名称。 COMMAND：表示启动容器时运行的命令。 CREATED：表示容器的创建时间。 STATUS：表示容器运行的状态。UP表示运行中， Exited表示已停止。 PORTS:表示容器对外的端口号。 NAMES:表示容器名称。该名称默认由 Docker自动生成，也可使用 docker run命令的-- name选项自行指定。 3、停止容器 使用 docker stop命令，即可停止容器 docker stop f0b1c8ab3633 其中f0b1c8ab3633是容器 ID,当然也可使用 docker stop容器名称来停止指定容器 4、强制停止容器 可使用 docker kill命令发送 SIGKILL信号来强制停止容器 docker kill f0b1c8ab3633 5、启动已停止的容器 使用docker run命令，即可新建并启动一个容器。对于已停止的容器，可使用 docker start命令来启动 docker start f0b1c8ab3633 6、查看容器所有信息 docker inspect f0b1c8ab3633 7、查看容器日志 docker container logsf0b1c8ab3633 8、查看容器里的进程 docker topf0b1c8ab3633 9、进入容器 使用docker exec命令用于进入一个正在运行的docker容器。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了 docker exec -it f0b1c8ab3633 /bin/bash 9、删除容器 使用 docker rm命令即可删除指定容器 docker rm f0b1c8ab3633 该命令只能删除已停止的容器，如需删除正在运行的容器，可使用-f参数 将微服务运行在docker上 使用Dockerfile构建Docker镜像 Dockerfile是一个文本文件，其中包含了若干条指令，指令描述了构建镜像的细节 先来编写一个最简单的Dockerfile，以前文下载的Nginx镜像为例，来编写一个Dockerfile修改该Nginx镜像的首页 1、新建文件夹/app，在app目录下新建一个名为Dockerfile的文件，在里面增加如下内容： FROM nginx RUN echo 'This is Tuling Nginx!!!' > /usr/share/nginx/html/index.html该Dockerfile非常简单，其中的 FORM、 RUN都是 Dockerfile的指令。 FROM指令用于指定基础镜像， RUN指令用于执行命令。 2、在Dockerfile所在路径执行以下命令构建镜像： docker build -t nginx:tuling . 其中，-t指定镜像名字，命令最后的点（.）表示Dockerfile文件所在路径 3、执行以下命令，即可使用该镜像启动一个 Docker容器 docker run -d -p 92:80 nginx:tuling 4、访问 http://Docker宿主机IP:92/，可看到下图所示界面 Dockerfile常用指令 注意：RUN命令在 image 文件的构建阶段执行，执行结果都会打包进入 image 文件；CMD命令则是在容器启动后执行。另外，一个 Dockerfile 可以包含多个RUN命令，但是只能有一个CMD命令。 注意，指定了CMD命令以后，docker container run命令就不能附加命令了（比如前面的/bin/bash），否则它会覆盖CMD命令。 使用Dockerfile构建微服务镜像 以项目05-ms-eureka-server为例，将该微服务的可运行jar包构建成docker镜像 1、将jar包上传linux服务器/app/eureka目录，在jar包所在目录创建名为Dockerfile的文件 2、在Dockerfile中添加以下内容 基于哪个镜像 From java:8 复制文件到容器 声明需要暴露的端口 EXPOSE 8761 配置容器启动后执行的命令 ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]3、使用docker build命令构建镜像 docker build -t microservice-eureka-server:0.0.1 . 格式： docker build -t 镜像名称:标签 Dockerfile的相对位置 在这里，使用-t选项指定了镜像的标签。执行该命令后，终端将会输出如下的内容 4、启动镜像，加-d可在后台启动 docker run -p 8761:8761 microservice-eureka-server:0.0.1 使用 -v 可以挂载一个主机上的目录到容器的目录 docker run -p 8761:8761 -v /tmp:/tmp microservice-eureka-server:0.0.1 5、访问http://Docker宿主机IP:8761/，可正常显示Eureka Server首页 Docker虚拟化原理 传统虚拟化和容器技术结构比较：传统虚拟化技术是在硬件层面实现虚拟化，增加了系统调用链路的环节，有性能损耗；容器虚拟化技术以共享Kernel的方式实现，几乎没有性能损耗 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/Docker-Compose.html":{"url":"micro/Docker-Compose.html","title":"15.DockerCompose微服务编排","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 pip install docker-compose docker-compose version docker-compose up Just specify a path and let the Engine create a volume Specify an absolute path mapping Path on the host, relative to the Compose file User-relative path Named volume docker-compose up docker-compose up 作者：诸葛老师 Docker Compose介绍 使用微服务架构的应用系统一般包含若干个微服务，每个微服务一般都会部署多个实例。如果每个微服务都要手动启停，那么效率之低、维护量之大可想而知。本节课将讨论如何使用 Docker Compose来轻松、高效地管理容器。为了简单起见将 Docker Compose简称为Compose。 Compose 是一个用于定义和运行多容器的Docker应用的工具。使用Compose，你可以在一个配置文件（yaml格式）中配置你应用的服务，然后使用一个命令，即可创建并启动配置中引用的所有服务。下面我们进入Compose的实战吧 Docker Compose的安装 Compose的安装有多种方式，例如通过shell安装、通过pip安装、以及将compose作为容器安装等等。本文讲解通过pip安装的方式。其他安装方式如有兴趣，可以查看Docker的官方文档：https://docs.docker.com/compose/install/ 1、安装python-pip yum -y install epel-release yum -y install python-pip 2、安装docker-compose pip install docker-compose 3、待安装完成后，执行查询版本的命令 docker-compose version Docker Compose入门示例 Compose的使用非常简单，只需要编写一个docker-compose.yml，然后使用docker-compose 命令操作即可。docker-compose.yml描述了容器的配置，而docker-compose 命令描述了对容器的操作。我们首先通过一个示例快速入门： 还记得上节课，我们使用Dockerfile为项目microservice-eureka-server构建Docker镜像吗？我们还以此项目为例测试 我们在microservice-eureka-server-0.0.1-SNAPSHOT.jar所在目录的上一级目录，创建docker-compose.yml文件。 目录树结构如下： ├── docker-compose.yml └── eureka ├── Dockerfile └── microservice-eureka-server-0.0.1-SNAPSHOT.jar 然后在docker-compose.yml中添加内容如下： 在docker-compose.yml所在路径执行： docker-compose up Compose就会自动构建镜像并使用镜像启动容器。也可使用 docker-compose up -d后台启动并运行这些容器 访问：http://宿主机IP:8761/，发现可以正常启动。 Docker Compose管理容器的结构 Docker Compose将所管理的容器分为三层，分别是工程（ project），服务（service）以及容器（ container）。 Docker Compose运行目录下的所有文件（ docker-compose.yml、 extends文件或环境变量文件等）组成一个工程（默认为 docker-compose.yml所在目录的目录名称）。一个工程可包含多个服务，每个服务中定义了容器运行的镜像、参数和依赖，一个服务可包括多个容器实例。 上节示例里工程名称是 docker-compose.yml所在的目录名。该工程包含了1个服务，服务名称是 eureka，执行 docker-compose up 时，启动了eureka服务的1个容器实例 docker­compose.yml常用指令 image 指定镜像名称或者镜像id，如果该镜像在本地不存在，Compose会尝试pull下来。 示例： image: java build 指定Dockerfile文件的路径。可以是一个路径，例如： build: ./dir 也可以是一个对象，用以指定Dockerfile和参数，例如： build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 command 覆盖容器启动后默认执行的命令。 示例： command: bundle exec thin -p 3000 也可以是一个list，类似于Dockerfile总的CMD指令，格式如下： command: [bundle, exec, thin, -p, 3000] links 链接到其他服务中的容器。可以指定服务名称和链接的别名使用SERVICE:ALIAS的形式，或者只指定服务名称，示例： web: links: db db:database redis external_links 表示链接到docker­compose.yml外部的容器，甚至并非Compose管理的容器，特别是对于那些提供共享容器或共同服务。格式跟links类似，示例： external_links: redis_1 project_db_1:mysql project_db_1:postgresql ports 暴露端口信息。使用宿主端口:容器端口的格式，或者仅仅指定容器的端口（此时宿主机将会随机指定端口），类似于docker run -p，示例： ports: \"3000\" \"3000-3005\" \"8000:8000\" \"9090-9091:8080-8081\" \"49100:22\" \"127.0.0.1:8001:8001\" \"127.0.0.1:5000-5010:5000-5010\" expose 暴露端口，只将端口暴露给连接的服务，而不暴露给宿主机，示例： expose: \"3000\" \"8000\" volumes 卷挂载路径设置。可以设置宿主机路径（HOST:CONTAINER）或加上访问模式（HOST:CONTAINER:ro）。示例： volumes: Just specify a path and let the Engine create a volume /var/lib/mysql Specify an absolute path mapping /opt/data:/var/lib/mysql Path on the host, relative to the Compose file ./cache:/tmp/cache User-relative path ~/configs:/etc/configs/:ro Named volume datavolume:/var/lib/mysql volumes_from 从另一个服务或者容器挂载卷。可以指定只读或者可读写，如果访问模式没有指定，则默认是可读写。示例： volumes_from: service_name service_name:ro container:container_name container:container_name:rw environment 设置环境变量。可以使用数组或者字典两种方式。只有一个key的环境变量可以在运行Compose的机器上找到对应的值，这有助于加密的或者特殊主机的值。示例： environment: RACK_ENV: development SHOW: 'true' SESSION_SECRET: environment: RACK_ENV=development SHOW=true SESSION_SECRET env_file 从文件中获取环境变量，可以为单独的文件路径或列表。如果通过docker-compose -f FILE指定了模板文件，则env_file中路径会基于模板文件路径。如果有变量名称与environment指令冲突，则以envirment为准。示例： env_file: .env env_file: ./common.env ./apps/web.env /opt/secrets.env extends 继承另一个服务，基于已有的服务进行扩展。 net 设置网络模式。示例： net: \"bridge\" net: \"host\" net: \"none\" net: \"container:[service name or container name/id]\" dns 配置dns服务器。可以是一个值，也可以是一个列表。示例： dns: 8.8.8.8 dns: 8.8.8.8 9.9.9.9 dns_search 配置DNS的搜索域，可以是一个值，也可以是一个列表，示例： dns_search: example.com dns_search: dc1.example.com dc2.example.com 其他 docker­compose.yml还有很多其他命令，本文仅挑选常用命令进行讲解，其他不不作赘述。如果感兴趣的，可以参考docker­ compose.yml文件官方文档：https://docs.docker.com/compose/compose­file/ 用Docker Compose编排Spring Cloud微服务 如果微服务较多，则可以用docker compose来统一编排，我们打算用docker compose来统一编排三个微服务：eureka服务(项目05-ms-eureka-server)，user服务(项目05-ms-provider-user)，order服务(项目05-ms-consumer-order-ribbon) 编排微服务 1、在根目录创建文件夹/app 2、在app目录下新建docker-compose.yml文件和三个文件夹eureka，user，order 3、在eureka，user，order三个文件夹下分别构建eureka服务镜像，user服务镜像，order服务镜像，以构建eureka服务镜像为例，在eureka文件夹下新建dockerfile文件并且将eureka服务的可运行jar包上传到该目录(注意：需要将配置eureka.client.serviceUrl.defaultZone的值改为http://eureka:8761/eureka/，默认情况下Compose以服务名称作为hostname被其他容器访问)，dockerfile文件内容如下 基于哪个镜像 From java:8 将本地文件夹挂载到当前容器 VOLUME /tmp 复制文件到容器 声明需要暴露的端口 EXPOSE 8761 配置容器启动后执行的命令 ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]4、docker-compose.yml内容如下 version: '2'#docker的文件格式版本 services: eureka:#docker服务名 image: eureka #docker镜像 ports: \"8761:8761\" user: image: user ports: \"8000:8000\" order: image: order ports: \"8010:8010\" 5、启动所有微服务，在命令后面加-d可以后台启动： docker-compose up 6、访问三个微服务是否正常 编排高可用微服务 1、在根目录创建文件夹/app-ha 2、在app-ha目录下新建docker-compose.yml文件和三个文件夹eureka-ha，user-ha，order-ha 3、在eureka-ha，user-ha，order-ha三个文件夹下分别构建eureka-ha服务镜像，user-ha服务镜像，order-ha服务镜像，eureka-ha服务参考项目08-ms-eureka-server-ha，（注意：需要修改user服务和order服务配置文件eureka.client.serviceUrl.defaultZone的值为http://peer1:8761/eureka/,http://peer2:8762/eureka/) 4、docker-compose.yml内容如下 version: '2'#docker的文件格式版本 services: peer1:#docker微服务名称 image: eureka-ha #docker镜像 ports: \"8761:8761\" environment: spring.profiles.active=peer1 peer2: image: eureka-ha ports: \"8762:8762\" environment: spring.profiles.active=peer2 user: image: user-ha ports: \"8000:8000\" order: image: order-ha ports: \"8010:8010\" 5、启动所有微服务，在命令后面加-d可以后台启动： docker-compose up 6、访问三个微服务是否正常 动态扩容微服务 有时我们需要扩容微服务，比如我们想把用户和订单微服务各部署两个微服务，则docker-compose.yml文件应该如下配置docker-compose.yml内容如下 version: '2'#docker的文件格式版本 services: peer1:#docker微服务名称 image: eureka-ha #docker镜像 ports: \"8761:8761\" environment: spring.profiles.active=peer1 peer2: image: eureka-ha ports: \"8762:8762\" environment: spring.profiles.active=peer2 user: image: user-ha order: image: order-ha 执行如下扩容命令： docker-compose up #必须先正常编排微服务，然后才能动态扩容 docker-compose scale user=2 order=2 注意：如果是在同一台物理机上做动态扩容，则需要在docker-compose.yml里去掉除了eureka其它微服务ports端口映射运行完查看eureka注册中心如下图所示： Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"micro/SpringCloudDocker.html":{"url":"micro/SpringCloudDocker.html","title":"16.Docker整体编排SpringCloud","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 Docker容器中间件部署 erureka-server config-server service-member service-product service-order mall-portal 项目模块简介 名称 介绍 说明 config-server 远程配置管理服务 spring cloud config配置中心，演示基本的config功能 erureka-server cloud微服务注册中心 基于REST的定位服务，以实现云端中间层服务发现和故障转移 mall-portal mall商城入口服务 用于演示 feign+ribbon+hystrix+zuul 等组件基本的使用和配置 service-member mall会员微服务(仅演示) SpringBoot+MybatisPlus框架的业务模块微服务，可随意扩展重建，附带feign+config+bus等组件的演示，整合Redis+RabbitMQ中间件的基本使用 service-order mall订单微服务(仅演示) SpringBoot+MybatisPlus框架的业务模块微服务，可随意扩展重建，附带feign+config+bus等组件的演示，整合Redis+RabbitMQ中间件的基本使用 service-product mall商品微服务(仅演示) SpringBoot+MybatisPlus框架的业务模块微服务，可随意扩展重建，附带feign+config+bus等组件的演示，整合Redis+RabbitMQ中间件的基本使用 Docker容器中间件部署 Mysql docker pull mysql:5.7 sudo docker run --name pwc-mysql -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:5.7 Redis docker pull redis:3.2 docker run -p 6379:6379 --name redis -v /etc/localtime:/etc/localtime:ro -v /mnt/docker/redis/data:/data -d redis:3.2 Rabbitmq docker pull rabbitmq:management docker run -p 5672:5672 -p 15672:15672 --name rabbitmq -v /etc/localtime:/etc/localtime:ro -d rabbitmq:management Service启动后的测试地址 erureka-server http://localhost:1001/ config-server http://localhost:2001/service-member-dev.yml http://localhost:2001/service-member-test.yml http://localhost:2001/service-member-prd.yml http://localhost:2001/service-product-dev.yml http://localhost:2001/service-product-test.yml http://localhost:2001/service-product-prd.yml http://localhost:2001/service-order-dev.yml http://localhost:2001/service-order-test.yml http://localhost:2001/service-order-prd.yml http://localhost:2001/actuator/bus-refreshPOST service-member http://localhost:8001/profile http://localhost:8001/api/user?username=baicai http://localhost:8001/api/user/cache?username=baicai service-product http://localhost:8003/profile http://localhost:8003/api/product?sn=SN123456 http://localhost:8003/api/product/cache?sn=SN123456 service-order http://localhost:8002/profile http://localhost:8002/api/order?sn=Q123456 http://localhost:8002/api/order/cache?sn=Q123456 http://localhost:8002/api/order/tx?sn=Q123456&productId=1&memberId=1 mall-portal http://localhost:9001/api/find/data?username=baicai&productSn=SN123456&orderSn=Q123456 http://localhost:9001/apigateway/member/api/user?username=baicai http://localhost:9001/apigateway/product/api/product?sn=SN123456 http://localhost:9001/apigateway/order/api/order?sn=Q123456 Docker Compose编排项目 注意： 1、因为order，member和product服务启动需要读取config服务，所以需要将docker-compose拆分，让eureka和config先启动 2、因为容器没法直接访问宿主机的真实ip，只能访问宿主机给容器映射的ip(用ifconfig命令查出来的docker0网卡的ip)，所以容器要访问安装在宿主机上的那些基础服务必须用宿主机的映射ip docker-compose.yml内容如下， version: '2' #docker的文件格式版本 services: eureka-server: #docker服务名 image: eureka-server #docker镜像 ports: \"1001:1001\" config-server: image: config-server command: \"--mysql.address=172.17.0.1\" ports: \"2001:2001\" service-member: image: service-member command: \"--mysql.address=172.17.0.1\" \"--redis.address=172.17.0.1\" \"--rabbitmq.address=172.17.0.1\" ports: \"8001:8001\" service-order: image: service-order command: \"--mysql.address=172.17.0.1\" \"--redis.address=172.17.0.1\" \"--rabbitmq.address=172.17.0.1\" ports: \"8002:8002\" service-product: image: service-product command: \"--mysql.address=172.17.0.1\" \"--redis.address=172.17.0.1\" \"--rabbitmq.address=172.17.0.1\" ports: \"8003:8003\" mall-portal: image: mall-portal ports: \"9001:9001\" Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-20 10:32:49 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/":{"url":"project/","title":"七、项目实战专题","keywords":"","body":"七、项目实战专题 经过前面多章节的学习，本节会从以下几个方面了解一个项目，系统架构，商品&会员模块，交易&营销模块，支付模块，分库分表 1.系统架构 2.商品&会员模块 3.交易&营销模块 4.支付模块 5.分库分表一 6.分库分表二 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-23 15:40:38 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/SystemArchitecture.html":{"url":"project/SystemArchitecture.html","title":"1.系统架构","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.电商演变 2.运行效果 3.环境详解 4.架构详解 1.电商演变 2.运行效果 项目下载地址：http://git.jiagouedu.com/java-vip/tuling-shop.git 前端：wukong/ aaa111 后端：admin/123456 前提条件：Redis、mysql、zookeeper 创建数据库脚本 代码下载来之后编译。 启动顺序：shop-goods、shop-member、shop-trade、shop-pay、 shop-admin、shop-web 3.环境详解 192.168.0.15：base 服务 192.168.0.16：app 服务 Depoly.sh 下载、解压 env-set.sh 赋值、杀进程、启动 Pom.sh 每次解压会从 app-conf 目录下 copy 环境的配置 第三方 jar 包如何上传到私服。 mvn deploy:deploy-file -DgroupId=com.alipay -DartifactId=alipay-sdk-java -Dversion=3.3.0 -Dpackaging=jar -Dfile=F:\\workspace\\vipdev\\tuling-shop\\alipay-sdk-java-3.3.0.jar -Durl=http://192.168.0.15:7777/nexus/content/repositories/thirdparty/ 4.架构详解 新增加能： 1、完善部分模块(shop-web 和 shop-admin) 2、冗余代码 3、模块分离（shop-admin、shop-web） 4、支付功能（shop-pay、shop-pay-client） 5、分布式事务 6、版本模块 Jeeshop 开源网站改过来的 单体应用 半个时间 分布式网站 秒杀、商品详细、库存、分库分表、分布式事务。 *Client 都是 dubbo 对外暴露接口 问题？为什么要这么设计？解耦 对外接口（不需要业务） 并且我们接口升级之后 不会影响我们的服务 Dubbo 所有接口模块 不同的业务按不同的 dubbo 模块 优点：解耦，不会暴露给调用方具体的业务代码、职责单一。 缺点：带来一定工作量 繁琐 拆的还不够细。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 16:08:59 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/member.html":{"url":"project/member.html","title":"2.商品&会员模块","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.商品中心 1.1分类+商品（1对多） 1.2分类+商品+品牌： 1.3分类+商品+品牌+属性： 1.4分类+商品+品牌+属性+规格： 1.5技术点 2.会员中心 2.1t_account会员表 2.2t_accountRank会员级别表 2.3t_address配送地址信息表 1.商品中心 商品中心： 1.1分类+商品（1对多） t_catalog商品分类表 参数 名称 类型 备注 id 自增长 int 唯一 name 分类名称 String code 编码，简码 String 唯一 pid 父ID Int order1 排序 Int type 类型 String 类型，a:文章目录；p:产品目录 showInNav 是否显示在首页的导航条上 String y:显示,n:不显示；默认n。仅对type=p有效 t_product商品表 字段名 数据类型 默认值 描述 id int 唯一，商品ID catalogID varchar 商品类别catalog表id name varchar 商品名称 introduce text 商品简介 price DECIMAL(9,2) 定价 nowPrice DECIMAL(9,2) 现价 picture String 小图片地址 score Int 0 赠送积分 maxPicture String 大图片地址 isnew String n 是否新品。n：否，y：是 sale String n 是否特价。n：否，y：是 activityID String 绑定的活动ID giftID String 绑定的礼品ID hit int 0 浏览次数 unit String 商品单位。默认“item:件” createAccount String 录入人账号 createtime datetime 录入时间 updateAccount String 最后修改人账号 updatetime String 最后修改时间 isTimePromotion String n 是否限时促销。n：否，y：是 status Int 0 商品状态。1：新增，2：已上架，3：已下架 productHTML LONGTEXT 商品介绍 images String 商品多张图片集合，逗号分割 sellcount Int 销售数量 默认：0 stock Int 剩余库存数 默认：0 searchKey String 搜索关键词 title String 页面标题 description String 页面描述 keywords String 页面关键词 1.2分类+商品+品牌： 我要查看苹果所有的产品（手机苹果、电脑苹果）需求 1.3分类+商品+品牌+属性： 更加快速找到我们想购买的商品 t_attribute商品属性(参数)表 参数 名称 类型 备注 id 自增长 int 唯一 name 属性/参数名称 String catalogID 类别ID Int pid 父ID Int 该字段具有双重含义。0表示属性大类，一般情况下产品只有两层attribute，一层为属性名称类别，一层为属性；-1：参数 order1 排序 Int t_attribute_link商品属性(参数)中间表 参数 名称 类型 备注 id 自增长 int 唯一 attrID 属性(参数)ID Int productID 商品ID Int value 商品参数值 String 名称从属性表中取得 1.4分类+商品+品牌+属性+规格： t_spec商品规格表 字段名 数据类型 默认值 描述 id int 唯一 productID String 商品ID specColor String 颜色 specSize String 尺寸 specStock Int 此规格的商品库存数 specPrice Double 此规格的商品价格 specStatus String y:显示规格；n:不显示规格 SPU ：SPU(Standard Product Unit)：标准化产品单元。是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗点讲，属性值、特性相同的商品就可以称为一个SPU。 举例：Iphone6 SKU： SKU=Stock Keeping Unit（库存量单位）。即库存进出计量的基本单元，可以是以件，盒，托盘等为单位。 举例：Iphone6+土豪金+32G C2C店铺 1.5技术点 商品检索：ES、SOLR （数据源来自数据库，那就意味着同步）、分词 商品展示：商品+图片+库存+店铺+商品相关的信息 图片：GFS、TFS、FastDFs底层原理？特点：文件小、图片小 并发下问题 缓存：查询的速度、内存》硬盘（数据源来自数据库，那就意味着同步） 增量：增加、修改、下架 全量：预热数据（某个活动所有商品加载缓存中） 静态化: 把html+CDN 缺点:更新上需要更新HTML 2.会员中心 会员基础服务：登录、注册、购物、评价、晒单 会员成长体系：购物、评价、晒单 会员等级（注册会员、铜牌会员、银牌会员、金牌会员、钻石会员 ） 2.1t_account会员表 字段名 数据类型 主键 唯一 描述 id int Y 会员ID nickname varchar y 昵称 account varchar y 用户名out当前时间戳 password varchar 密码 accountType String 会员类型。qq,sinawb,alipay trueName String 真实姓名 sex String 性别。m:男,f：女,s:保密 birthday Date 出生年月日 province String 省份 city varchar 所在城市 address varchar 联系地址 postcode varchar 邮政编码 cardNO varchar 证件号码 cardType varchar 证件类型 grade int 等级 amount money 消费额 tel varchar 电话 email varchar y Email地址 emailIsActive String 邮箱是否已激活。y:已激活,n:未激活。默认n freeze String 是否冻结 n：未冻结，y：已冻结；默认n freezeStartdate Date 冻结的开始日期 freezeEnddate Date 冻结的结束日期 lastLoginTime Date 最后登录时间 lastLoginIp String 最后登录IP lastLoginArea String 最后登陆地点 diffAreaLogin String 是否是异地登陆y:是,n:否 regeistDate Date 注册日期 addressID Int 配送信息ID openId String y QQ登陆返回 accessToken String QQ登陆返回 alipayUseId String y 支付宝快捷登陆返回的用户ID sinaWeiboID String y 新浪微博登陆返回的用户ID rank String 会员等级。和t_accountType.code进行挂钩。默认R1 score Int 会员积分。默认0 2.2t_accountRank会员级别表 字段名 数据类型 是否主键 描述 id int 是 自增 code String 级别编码R1：普通会员，0-499R2：铜牌会员，积分范围500-999R3：银牌会员，1000-1999R4：金牌会员，2000-4000R5：钻石会员，大于4000 name String 级别名称 minScore Int 最小积分 maxScore Int 最大积分 remark String 备注 2.3t_address配送地址信息表 字段名 数据类型 是否主键 描述 id Int 是 自增 account String 会员账号 name String 收货人姓名 province String 省份 city String 城市 area String 区域 pcadetail String 省市区的地址中文合并 address String 收货人详细地址 zip String 收货人邮编 phone String 收货人电话号码 mobile String 收货人手机号码 isdefault String 默认n 是否默认；n=不是,y=默认 技术点： 单点登录、业务功能、会员迁移（分库分表） Login.jd.com item.jd.com 分布式会话解决方案 分库：hash取模、list、range 16个库（800万数据量） hash、不好扩展 1.28亿用户 均匀（解决热点数据） 分库分库 如何设计一个不扩容方案。 可视化的黑客小工具 调式bug比较有用 测试排查问题（查看最新0代码） 系统比较庞大 不需要重启 Admin admin 会员部门 xxx部门 非常非常简单 原理 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 16:09:11 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/order.html":{"url":"project/order.html","title":"3.交易&营销模块","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.交易模块 1.1t_order 订单表 1.2t_orderdetail订单明细表 1.3t_discount折扣表 1.4t_orderpay 支付记录表 1.5t_ordership 订单配送表 1.6t_orderlog订单日志表 1.7t_express快递配送表 2.营销 交易中心 交易中心业务 技术难点 营销 营销中心业务 技术难点1.交易模块 1.1t_order 订单表 字段名 数据类型 是否主键 描述 id int 是 订单编号。 account String 账号 payType varchar 付款方式 carry varchar 运送方式 rebate DECIMAL(9,2) 折扣 createdate datetime 创建日期 remark varchar 备注、支付宝的WIDsubject status String init:未审核；pass:已审核；send：已发货；sign：已签收；cancel:已取消；file:已归档；finish：交易完成； refundStatus String 退款状态(直接借用了支付宝的退款状态)。WAIT_SELLER_AGREE：等待卖家同意退款；WAIT_BUYER_RETURN_GOODS：卖家同意退款，等待买家退货；WAIT_SELLER_CONFIRM_GOODS：买家已退货，等待卖家收到退货；REFUND_SUCCESS：卖家收到退货，退款成功，交易关闭 paystatus String n:未支付；p:部分支付；y:全部支付 lowStocks String n:库存不足；y:库存充足。默认n amount Double 订单总金额 amountExchangeScore Int 订单总兑换积分 fee Double 运费总金额 ptotal Double 商品总金额 quantity Int 商品总数量 updateAmount String n:没有修改过；y:修改过；默认:n expressCode String 配送方式编码 expressName String 配送方式名称 expressNo String 快递运单号 expressCompanyName String 快递公司名称 confirmSendProductRemark String 确认发货备注 otherRequirement String 客户的附加要求 closedComment String 此订单的所有订单项对应的商品都进行了评论，则此值为y，表示此订单的评论功能已经关闭，默认为null，在订单状态为已发货后，则用户可以对订单进行评价。 score Int 订单获赠的积分 1.2t_orderdetail订单明细表 字段名 数据类型 是否主键 描述 ID int 是 ID号 orderID int 与t_order表的id字段关联 orderdetailID String 订单项ID productID int 商品ID giftID String 商品赠品ID productName String 商品名称 price money 价格 number int 数量 total0 Double 总金额（数量*价格） fee Double 配送费 isComment String 是否评价过。n:未评价,y:已评价；默认n lowStocks String n:库存不足；y:库存充足。默认n s String 商品规格信息 1.3t_discount折扣表 字段名 数据类型 是否主键 描述 ID int 是 ID号 discount decimal(9,1) 折扣,比如9.5折 name String 折扣宣传名称 1.4t_orderpay 支付记录表 字段名 数据类型 是否主键 描述 id int 是 自增 orderid String 订单ID paystatus String 支付状态。y:支付成功,n:支付失败 payamount Double 支付金额 createtime String 支付时间 paymethod String 支付方式 confirmdate String 确认日期 confirmuser String 确认人 remark String 备注 tradeNo String 支付宝交易号，以后用来发货 1.5t_ordership 订单配送表 字段名 数据类型 是否主键 描述 id int 是 自增 orderid String 订单ID shipname String 收货人姓名 shipaddress String 收货人详细地址 provinceCode String 省份代码 province String 省份 cityCode String 城市代码 city String 城市 areaCode String 区域代码 area String 区域 phone String 手机 tel String 座机 zip String 邮编 sex String 性别 remark String 备注 1.6t_orderlog订单日志表 字段名 数据类型 允许为空 描述 id int 自增 orderid String 订单ID account String 操作人 createdate date 记录时间，默认是当前时间 content String 日志内容 accountType String w:会员;m:后台管理人员;p:第三方支付系统异步通知 1.7t_express快递配送表 参数 名称 类型 备注 id 自增长 int 唯一 code 快递编码 String 三个值可选：EXPRESS（快递）、POST（平邮）、EMS（EMS） name 快递名称 String fee 物流费用 Double order1 排序 Int 交易中 库存（超卖）、重复支付、唯一主键、秒杀（库存） 、购物车 库存超卖、秒杀：锁、队列 for update 重复支付：幂等性（前端通知、后端通知）（下单 通知多次 调用多次 ）Redis incr 唯一主键：雪花算法、redis、数据库特性、UUID等等（分库分表实战） 下单：RPC调用 显示状态 订单状态 支付状态 发货状态 已付款 活动订单 已支付 未发货 已发货 活动订单 已支付 已发货 待自提 活动订单 已支付 自提点签收 已签收 活动订单 已支付 用户签收 已拒收 活动订单 已支付 用户拒收 配送成功 活动订单 已支付 配送成功 配送失败 活动订单 已支付 配送失败 交易成功 已完成 已支付 配送成功 交易失败 已完成 已支付 配送失败 取消中 取消中 已支付 未发货 已取消 订单取消 未发货 消息中间件：异步、解耦 购物车-技术点 面试题：不未登入时，购物车redis key该怎么做？ Userid+goodsid+shopid ?goodsid+shopid 未登录商品 购物车 Redis 同一台电脑上 跟换游览器没有关系 时序图 存储在redis+mysql redis存储的（shopid+goodsid） 增加一个商品购物车 插入一次数据库。 Userid xxxx id pc ios rpc调用goodsid 商品服务 下单 购物车清空 购物车数据结构：B2C(跨店铺) com.jiagouedu.web.action.front.orders.CartInfo com.jiagouedu.web.action.front.orders. CartGroup(一个店铺catgroup) cartPkg(一个店铺下可能会产生多个包裹) List(商品明细) 2个技术点： 排序的问题 排序（put数据）重写排序算法 实时性的问题 10点 5% 10：1 10% xxx奶粉 囤货 1千奶粉 羊毛党（抓）结算 查一次海关系统（下单） 拆单 业务上问题 2个iphone 长沙 仓库武汉（库存 1 个）（河南 1个 广州 1个） Goodsid+shopid+userid pkg（包裹） JD 充值 100 95 （9.5折）99. 总结： 师，上节课的 黑科技上传了麽 上传了 技术难点说了一大堆，解决方案一个没有 库存锁定、扣减 下单： U l 扣减 10 1 0 支付： 9 0 1 ERP系统 30分钟支付时间 锁定 2.营销 商品级别、订单级别、全站级别。 技术实现？ 营销活动存储（下节课 技术点） 数据的回滚 ，用户手上 订单 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 16:09:21 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/pay.html":{"url":"project/pay.html","title":"4.支付模块","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.营销系统 2.支付宝对接 3.微信对接 4.技术注意点： 4.1接口的幂等性 4.2后台系统 4.2.1t_user用户表 4.2.2t_role角色表 4.2.3t_privilege权限表 4.2.4t_systemsetting系统设置表 4.2.5t_systemlog系统日志表 1.营销系统 技术如何去实现？ 一个商品会对应多个活动 购物车N个商品N个活动 100个商品 300个活动（300条记录 数据库查询） 100个悟空人 3w条记录 缓存？Redis mysql Jvm map 数据同步的 在分布式下 不同的jvm 缓存数据不一样？ 活动优先级问题：怎么解决？权重 jvm 也是内存 redis也是内存 那就不要redis了 将来发生 job 执行的 计划 时间 2.支付宝对接 https://openhome.alipay.com/platform/demoManage.htm#/alipay.trade.pay 3.微信对接 4.技术注意点： 4.1接口的幂等性 4.2后台系统 后台管理： 电商没有关系、很多系统的权限这块 公共。 权限系统一般有：账号、角色、权限、资源。 权限可以分为三种：页面权限，操作权限，数据权限。 页面权限： 控制你可以看到哪个页面，看不到哪个页面。 很多系统都只做到了控制页面这一层级，它实现起来比较简单，一些系统会这样设计，但是比较古板，控制的权限不精细，难以在页面上对权限进行更下一层级的划分。 If else 操作权限： 则控制你可以在页面上操作哪些按妞。 延伸：当我们进入一个页面，我们的目的无非是在这个页面上进行增删改查，那在页面上对应的操作可能是：查询，删除，编辑，新增四个按钮。 可能你在某个页面上，只能查询数据，而不能修改数据。 数据权限： 数据权限则是控制你可以看到哪些数据，比如市场A部的人只能看到或者修改A部创建的数据，他看不到或者不能修改B部的数据。 延伸：数据的控制我们一般通过部门去实现，每条记录都有一个创建人，而每一个创建人都属于某个部门，因此部门分的越细，数据的控制层级也就越精细，这里是否有其他好的方式除了部门这个维度还有其他什么方式可以控制数据权限。 4.2.1t_user用户表 t_user用户表 参数 名称 类型 备注 id 自增长 int 唯一 username 帐号 String 唯一 password 密码 string MD5加密 createtime 创建时间 String createAccount 创建人 String updatetime 最后修改时间 String updateAccount 最后修改人 String status 状态 String y:启用,n:禁用；默认y rid 角色ID Int nickname 昵称 String email 邮箱 String 4.2.2t_role角色表 t_role角色表 参数 名称 类型 备注 id 自增长 int 唯一 role_name 角色名称 String role_desc 角色描述 string role_dbPrivilege 数据库权限 String status 角色状态，如果角色被禁用，则该角色下的所有的账号都不能使用，除非角色被解禁。 String y:启用，n:禁用；默认y 4.2.3t_privilege权限表 参数 名称 类型 备注 id 自增长 int 唯一 rid 角色ID int mid 资源ID int t_menu资源表 参数 名称 类型 备注 id 自增长 int 唯一 pid 父ID Int url 资源 String name 资源名称 string orderNum 序号 int type 功能类型 String module：模块page：页面button：功能 4.2.4t_systemsetting系统设置表 字段名 数据类型 允许为空 描述 基本设置 id int ID号 systemCode String 系统代号 name String 网站名称 www String 门户地址根http路径 manageHttp String 后台地址根http路径 log String 网站门户的Log图标地址 title String 网站标题 description String 网站的描述 keywords String 网站的关键字 shortcuticon String 网站的图标 address String 联系地址 tel String 联系电话 email String 邮箱 qqHelpHtml String Qq沟通组建的HTML内容 icp String 备案号 isopen String 是否开放网站。y:开放,n不开放 closeMsg String 网站关闭消息 qq String Qq号码 statisticsCode String 站长统计代码 version String 系统版本相关信息 显示设置 openResponsive String y:启用响应式；n:禁用响应式。默认y imageRootPath String 图片根路径，以后可以专门弄个图片服务器，图片和项目分离，提高站点访问速度。 defaultProductImg String 商品的默认图片 images 图集 String 多张图片之间用分号分割。如果广告的useImagesRandom为n，则优先显示html；否则显示images图集的图片，每一次都会从图集中随机选取一张图片来显示。 manageLeftTreeLeafIcon String 后台左侧菜单叶子节点的图标 4.2.5t_systemlog系统日志表 字段名 数据类型 允许为空 描述 id int 自增 title String 日志标题 content String 日志内容 type Int 日志类型。1：登陆日志，2：版本日志， createdate date 记录时间，默认是当前时间 account String 操作人 loginIP String 登陆人IP地址 logintime String 登陆时间 loginArea String 登陆区域 diffAreaLogin String 是否是异地登陆y:是;n:否 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 16:09:32 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/Depots-table-1.html":{"url":"project/Depots-table-1.html","title":"5.分库分表一","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.数据库的压力 2.如何优化？有什么方式 3.问题 4.如何优化：散列+增量（数据热点+扩容） 5.总结 1.数据库的压力 订单相关表都已经是超大表，最大表的数据量已经是几十亿，数据库处理能力已经到了极限； 单库包含多个超大表，占用的硬盘空间已经接近了服务器的硬盘极限，很快将无空间可用； 过度解决：我们可以考虑到最直接的方式是增加大容量硬盘，或者对IO有更高要求，还可以考虑增加SSD硬盘来解决容量的问题。此方法无法解决单表数据量问题。 可以对数据表历史数据进行归档，但也需要频繁进行归档操作，而且不能解决性能问题 硬件上（大小）、单表容量（性能） 2.如何优化？有什么方式 读写分离、换mysql》oracle、分库分表。 分库分表： 散列hash：hashmap可以很好的去解决数据热点的问题，但是扩容 短板 Range增量：他的库容很多好，但是他就是没法解决数据热点的问题。 实战： 老的版本：不支持spring管理（分布式主键） 新的版本:支持（分布式主键） 3.问题 表增加是可以通过创建，但是有一个地方我们改了也会有问题 面对这两种方案都是比较难的处境，shard扩容显得难了、 扩容的问题？ 分库扩容理想状态： 最好不数据迁移（给团队带来的工作压力） 可以达到1的要求，并且数据热点的问题。 根据硬件资源设置不同阈值（判定） 4.如何优化：散列+增量（数据热点+扩容） 热点：解决数据热点的问题（因为我们局部用了散列） 扩容： 5.总结 多查一次数据库（字典表） 依赖全局的ID生成（美团+业务ID在区间自增） 其他： 前端：吞吐量高、并发高、相应速度快、但是业务简单（我） 我的订单 全部订单（我） userid=1 商品表（shopid） 后端：(并发不高、业务逻辑复杂、相应不快) 后台（另外）业务复杂、吞吐量不高 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 16:09:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"project/Depots-table-2.html":{"url":"project/Depots-table-2.html","title":"6.分库分表二","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.分库分表难点 2.分布式唯一ID解决方案 2.1Uuid: 2.2Snowflke 2.3Mysql 2.4Redis: 3.分布式事务解决方案 1.分库分表难点 分布式事务。 分布式主键。 跨库的查询。 数据迁移的问题。2.分布式唯一ID解决方案 2.1Uuid: 通用唯一识别码 组成部分：当前日期和时间+时钟序列+全局唯一网卡 mac 地址获取 执行任务数：10000 所有线程共耗时：91.292 s 并发执行完耗时：1.221 s 单任务平均耗时：9.1292 ms 单线程最小耗时：0.0 ms 单线程最大耗时：470.0 ms 优点： 代码实现简单、不占用宽带、数据迁移不影响 缺点： 无序、无法保证趋势递增、字符存储、传输、查询慢 2.2Snowflke snowflake 是 Twitter 开源的分布式 ID 生成算法。 传统数据库软件开发中，主键自动生成技术是基本需求。而各个数据库对于该需求也提供了相应的支持，比如 MySQL 的自增键，Oracle 的自增序列等。 数据分片后，不同数据节点生成全局唯一主键是非常棘手的问题。同一个逻辑表内的不同实际表之间的自增键由于无法互相感知而产生重复主键。 虽然可通过约束自增主键初始值和步长的方式避免碰撞，但需引入额外的运维规则，使解决方案缺乏完整性和可扩展性。 io.shardingsphere.core.keygen.DefaultKeyGenerator 执行任务数：10000 所有线程共耗时：15.111 s 并发执行完耗时：217.0 ms 单任务平均耗时：1.5111 ms 单线程最小耗时：0.0 ms 单线程最大耗时：97.0 ms 优点： 不占用宽带、本地生成、高位是毫秒，低位递增 缺点： 强依赖时钟，如果时间回拨，数据递增不安全 2.3Mysql 利用数据库的步长来做的。 CREATE TABLE bit_table ( id varchar(255) NOT NULL,//字符 PRIMARY KEY (id) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE bit_num ( id bigint(11) NOT NULL AUTO_INCREMENT, KEY (id) USING BTREE ) ENGINE=InnoDB auto_increment=1 DEFAULT CHARSET=utf8; SET global auto_increment_offset=1; --初始化 SET global auto_increment_increment=100; --初始步长 show global variables; 缺点: 受限数据库、单点故障、扩展麻烦 优点： 性能可以、可读性强、数字排序 2.4Redis: redis 原子性：对存储在指定 key 的数值执行原子的加 1 操作。如果指定的 key 不存在，那么在执行 incr 操作之前，会先将它的值设定为 0 组成部分：年份+当天当年第多少天+天数+小时+redis 自增 执行任务数：10000 所有线程共耗时：746.767 s 并发执行完耗时：9.381 s 单任务平均耗时：74.6767 ms 单线程最小耗时：0.0 ms 单线程最大耗时：4.119 s 优点： 有序递增、可读性强、符合刚才我们那个扩容方案的 id 缺点： 占用宽带（网络）、依赖第三方、redis 一个小时内生产 99 万的订单 ？ 场景： 名称 场景 适用指数 Uuid Token、图片 id ★★ Snowflake ELK、MQ、业务系统 ★★★★ 数据库 非大型电商系统 ★★★ Redis 大型系统 ★★★★★ 3.分布式事务解决方案 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 16:09:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}