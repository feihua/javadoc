{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"源码框架专题/":{"url":"源码框架专题/","title":"一、源码框架专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"源码框架专题/spring/":{"url":"源码框架专题/spring/","title":"spring","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"源码框架专题/mybatis/":{"url":"源码框架专题/mybatis/","title":"mybatis","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"并发编程专题/":{"url":"并发编程专题/","title":"二、并发编程专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/":{"url":"performance/","title":"三、性能调优专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/mysql/":{"url":"performance/mysql/","title":"mysql","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/":{"url":"performance/nginx/","title":"nginx","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx-config.html":{"url":"performance/nginx/Nginx-config.html","title":"1.Nginx核心模块与配置实践","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Nginx 简介与安装 知识点： 1、Nginx简介： 2、编译与安装 二、Nginx 架构说明 三、Nginx 配置与使用 知识点 1、配置文件的语法格式： 2、配置第一个静态WEB服务 3、日志配置： 主讲：鲁班 时间：2018/10/12 概要： Nginx 简介 Nginx 架构说明 Nginx 基础配置与使用一、Nginx 简介与安装 知识点： Nginx 简介 Nginx 编译与安装1、Nginx简介： Nginx是一个高性能WEB服务器，除它之外Apache、Tomcat、Jetty、IIS，它们都是Web服务器，或者叫做WWW（World Wide Web）服务器，相应地也都具备Web服务器的基本功能。Nginx 相对基它WEB服务有什么优势呢？ Tomcat、Jetty 面向java语言，先天就是重量级的WEB服务器，其性能与Nginx没有可比性。 IIS只能在Windows操作系统上运行。Windows作为服务器在稳定性与其他一些性能上都不如类UNIX操作系统，因此，在需要高性能Web服务器的场合下IIS并不占优。 Apache的发展时期很长，而且是目前毫无争议的世界第一大Web服务器，其有许多优点，如稳定、开源、跨平台等，但它出现的时间太长了，在它兴起的年代，互联网的产业规模远远比不上今天，所以它被设计成了一个重量级的、不支持高并发的Web服务器。在Apache服务器上，如果有数以万计的并发HTTP请求同时访问，就会导致服务器上消耗大量内存，操作系统内核对成百上千的Apache进程做进程间切换也会消耗大量CPU资源，并导致HTTP请求的平均响应速度降低，这些都决定了Apache不可能成为高性能Web服务器，这也促使了Lighttpd和Nginx的出现。 下图可以看出07年到17 年强劲增长势头。 2、编译与安装 安装环境准备： （1）linux 内核2.6及以上版本: 只有2.6之后才支持epool ，在此之前使用select或pool多路复用的IO模型，无法解决高并发压力的问题。通过命令uname -a 即可查看。 #查看 linux 内核 uname -a （2）GCC编译器 GCC（GNU Compiler Collection）可用来编译C语言程序。Nginx不会直接提供二进制可执行程序,只能下载源码进行编译。 （3）PCRE库 PCRE（Perl Compatible Regular Expressions，Perl兼容正则表达式）是由Philip Hazel开发的函数库，目前为很多软件所使用，该库支持正则表达式。 （4）zlib库 zlib库用于对HTTP包的内容做gzip格式的压缩，如果我们在nginx.conf里配置了gzip on，并指定对于某些类型（content-type）的HTTP响应使用gzip来进行压缩以减少网络传输量。 （5）OpenSSL开发库 如果我们的服务器不只是要支持HTTP，还需要在更安全的SSL协议上传输HTTP，那么就需要拥有OpenSSL了。另外，如果我们想使用MD5、SHA1等散列函数，那么也需要安装它。 上面几个库都是Nginx 基础功能所必需的，为简单起见我们可以通过yum 命令统一安装。 #yum 安装nginx 环境 yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel pcre pcre-devel 源码获取： nginx 下载页：http://nginx.org/en/download.html 。 # 下载nginx 最新稳定版本 wget http://nginx.org/download/nginx-1.14.0.tar.gz #解压 tar -zxvf nginx-1.14.0.tar.gz 最简单的安装： # 全部采用默认安装 ./configure make && make install 执行完成之后 nginx 运行文件 就会被安装在 /usr/local/nginx 下。 基于参数构建 ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-debug 控制命令： #默认方式启动： ./sbin/nginx #指定配置文件启动 ./sbing/nginx -c /tmp/nginx.conf #指定nginx程序目录启动 ./sbin/nginx -p /usr/local/nginx/ #快速停止 ./sbin/nginx -s stop #优雅停止 ./sbin/nginx -s quit # 热装载配置文件 ./sbin/nginx -s reload # 重新打开日志文件 ./sbin/nginx -s reopen 模块更新： 二、Nginx 架构说明 Nginx 架构图: 架构说明： 1）nginx启动时，会生成两种类型的进程，一个是主进程（Master），一个（windows版本的目前只有一个）和多个工作进程（Worker）。主进程并不处理网络请求，主要负责调度工作进程，也就是图示的三项：加载配置、启动工作进程及非停升级。所以，nginx启动以后，查看操作系统的进程列表，我们就能看到至少有两个nginx进程。 2）服务器实际处理网络请求及响应的是工作进程（worker），在类unix系统上，nginx可以配置多个worker，而每个worker进程都可以同时处理数以千计的网络请求。 3）模块化设计。nginx的worker，包括核心和功能性模块，核心模块负责维持一个运行循环（run-loop），执行网络请求处理的不同阶段的模块功能，如网络读写、存储读写、内容传输、外出过滤，以及将请求发往上游服务器等。而其代码的模块化设计，也使得我们可以根据需要对功能模块进行适当的选择和修改，编译成具有特定功能的服务器。 4）事件驱动、异步及非阻塞，可以说是nginx得以获得高并发、高性能的关键因素，同时也得益于对Linux、Solaris及类BSD等操作系统内核中事件通知及I/O性能增强功能的采用，如kqueue、epoll及event ports。 Nginx 核心模块： 三、Nginx 配置与使用 知识点 配置文件语法格式 配置第一个静态WEB服务 配置案例 动静分离实现 防盗链 多域名站点 下载限速 IP 黑名单 基于user-agent分流 日志配置1、配置文件的语法格式： 先来看一个简单的nginx 配置worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } location /nginx_status { stub_status on; access_log off; } } } 上述配置中的events、http、server、location、upstream等属于配置项块。而worker_processes 、worker_connections、include、listen 属于配置项块中的属性。 /nginx_status 属于配置块的特定参数参数。其中server块嵌套于http块，其可以直接继承访问Http块当中的参数。 | 配置块 | 名称开头用大口号包裹其对应属性 | |:----|:----| | 属性 | 基于空格切分属性名与属性值，属性值可能有多个项 都以空格进行切分 如： access_log logs/host.access.log main | | 参数 | 其配置在 块名称与大括号间，其值如果有多个也是通过空格进行拆 | 注意 如果配置项值中包括语法符号，比如空格符，那么需要使用单引号或双引号括住配置项值，否则Nginx会报语法错误。例如： log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; 2、配置第一个静态WEB服务 基础站点演示： [ ] 创建站点目录 mkdir -p /usr/www/luban [ ] 编写静态文件 [ ] 配置 nginx.conf [ ] 配置server [ ] 配置location 基本配置介绍说明： （1）监听端口 语法：listen address： 默认：listen 80; 配置块：server （2）主机名称 语法：server_name name[……]; 默认：server_name \"\"; 配置块：server server_name后可以跟多个主机名称，如server_name www.testweb.com、download.testweb.com;。 支持通配符与正则 （3）location 语法：location[=|～|～*|^～|@]/uri/{……} 配置块：server / 基于uri目录匹配 =表示把URI作为字符串，以便与参数中的uri做完全匹配。 ～表示正则匹配URI时是字母大小写敏感的。 ～*表示正则匹配URI时忽略字母大小写问题。 ^～表示正则匹配URI时只需要其前半部分与uri参数匹配即可。 动静分离演示： [ ] 创建静态站点 [ ] 配置 location /static [ ] 配置 ~* .(gif|png|css|js)$ 基于目录动静分离 server { listen 80; server_name *.luban.com; root /usr/www/luban; location / { index luban.html; } location /static { alias /usr/www/static; } } 基于正则动静分离 location ~* \\.(gif|jpg|png|css|js)$ { root /usr/www/static; } 防盗链配置演示： # 加入至指定location 即可实现 valid_referers none blocked *.luban.com; if ($invalid_referer) { return 403; } 下载限速： location /download { limit_rate 1m; limit_rate_after 30m; } 创建IP黑名单 # 创建黑名单文件 echo 'deny 192.168.0.132;' >> balck.ip #http 配置块中引入 黑名单文件 include black.ip; 3、日志配置： 日志格式： log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log logs/access.log main; #基于域名打印日志 access_log logs/$host.access.log main; error日志的设置 语法：error_log /path/file level; 默认：error_log logs/error.log error; level是日志的输出级别，取值范围是debug、info、notice、warn、error、crit、alert、emerg， 针对指定的客户端输出debug级别的日志 语法：debug_connection[IP|CIDR] events { debug_connection 192.168.0.147; debug_connection 10.224.57.0/200; } nginx.conf Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/nginx/Nginx-In-actual-combat.html":{"url":"performance/nginx/Nginx-In-actual-combat.html","title":"2.Nginx 性能优化实践","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Nginx 反向代理实现 知识点： 2.负载均衡配置与参数解析 3.upstream 负载均衡算法介绍 二、Nginx 高速缓存 知识点： 1、案例分析： 2.Nginx 静态缓存基本配置 3.缓存的清除： 三、Nginx 性能参数调优 主讲：鲁班 时间：2018/10/14 概要： Nginx 反向代理与负载均衡 Nginx 实现高速缓存 Nginx 性能参数调优 一、Nginx 反向代理实现 知识点： 反向代理基本配置 负载均衡配置与参数解析 负载均衡算法详解 反向代理基本配置 提问：什么是反向代理其与正向代理有什么区别？ 正向代理的概念： 正向代理是指客户端与目标服务器之间增加一个代理服务器，客户端直接访问代理服务器，在由代理服务器访问目标服务器并返回客户端并返回 。这个过程当中客户端需要知道代理服务器地址，并配置连接。 反向代理的概念： 反向代理是指 客户端访问目标服务器，在目标服务内部有一个统一接入网关将请求转发至后端真正处理的服务器并返回结果。这个过程当中客户端不需要知道代理服务器地址，代理对客户端而言是透明的。 反向代理与正向代理的区别 | | 正向代理 | 反向代理 | |:----|:----|:----| | 代理服务器位置 | 客户端与服务都能连接的们位置 | 目标服务器内部 | | 主要作用 | 屏蔽客户端IP、集中式缓存、解决客户端不能直连服务端的问题。 | 屏蔽服务端内部实现、负载均衡、缓存。 | | 应用场景 | 爬虫、翻墙、maven 的nexus 服务 | Nginx 、Apache负载均衡应用 | Nginx代理基本配置 Nginx 代理只需要配置 location 中配置proxy_pass 属性即可。其指向代理的服务器地址。 # 正向代理到baidu 服务 location = /baidu.html { proxy_pass http://www.baidu.com; } # 反向代理至 本机的8010服务 location /luban/ { proxy_pass http://127.0.0.1:8010; } 代理相关参数： proxy_pass # 代理服务 proxy_redirect off; # 是否允许重定向 proxy_set_header Host $host; # 传 header 参数至后端服务 proxy_set_header X-Forwarded-For $remote_addr; # 设置request header 即客户端IP 地址 proxy_connect_timeout 90; # 连接代理服务超时时间 proxy_send_timeout 90; # 请求发送最大时间 proxy_read_timeout 90; # 读取最大时间 proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; 2.负载均衡配置与参数解析 通过proxy_pass 可以把请求代理至后端服务，但是为了实现更高的负载及性能， 我们的后端服务通常是多个， 这个是时候可以通过upstream 模块实现负载均衡。 演示upstream 的实现。 upstream backend { server 127.0.0.1:8010 weight=1; server 127.0.0.1:8080 weight=2; } location / { proxy_pass http://backend; } upstream 相关参数: service 反向服务地址 加端口 weight 权重 max_fails 失败多少次 认为主机已挂掉则，踢出 fail_timeout 踢出后重新探测时间 backup 备用服务 max_conns 允许最大连接数 slow_start 当节点恢复，不立即加入,而是等待 slow_start 后加入服务对列。3.upstream 负载均衡算法介绍 ll+weight： 轮询加权重 (默认) ip_hash : 基于Hash 计算 ,用于保持session 一至性 url_hash: 静态资源缓存,节约存储，加快速度（第三方） least_conn ：最少链接（第三方） least_time ：最小的响应时间,计算节点平均响应时间，然后取响应最快的那个，分配更高权重（第三方）二、Nginx 高速缓存 知识点： 缓存案例分析 Nginx 静态缓存基本配置 缓存更新 1、案例分析： 某电商平台商品详情页需要实现 700+ QPS，如何着手去做？ 首先为分析一下一个商品详情页有哪些信息 从中得出 商品详情页依懒了 对于商品详情页涉及了如下主要服务： 商品详情页HTML页面渲染 价格服务 促销服务 库存状态/配送至服务 广告词服务 预售/秒杀服务 评价服务 试用服务 推荐服务 商品介绍服务 各品类相关的一些特殊服务 解决方案： 采用Ajax 动态加载 价格、广告、库存等服务 采用key value 缓存详情页主体html。 方案架构： 问题： 当达到500QPS 的时候很难继续压测上去。 分析原因：一个详情页html 主体达平均150 kb 那么在500QPS 已接近千M局域网宽带极限。必须减少内网通信。 基于Nginx 静态缓存的解决方案： 2.Nginx 静态缓存基本配置 一、在http元素下添加缓存区声明。 #proxy_cache_path 缓存路径 #levels 缓存层级及目录位数 #keys_zone 缓存区内存大小 #inactive 有效期 #max_size 硬盘大小 proxy_cache_path /data/nginx/cache_luban levels=1:2 keys_zone=cache_luban:500m inactive=20d max_size=1g; 二、为指定location 设定缓存策略。 # 指定缓存区 proxy_cache cache_luban; #以全路径md5值做做为Key proxy_cache_key $host$uri$is_args$args; #对不同的HTTP状态码设置不同的缓存时间 proxy_cache_valid 200 304 12h; 演示缓存生效过程 [ ] 配置声明缓存路径 [ ] 为location 配置缓存策略 [ ] 重启nginx（修改了） [ ] 查看缓存目录生成 缓存参数详细说明| 父元素 | 名称 | 描述 | |:----|:----|:----| | http | proxy_cache_path | 指定缓存区的根路径 | | | levels | 缓存目录层级最高三层，每层1~2个字符表示。如1:1:2 表示三层。 | | | keys_zone | 缓存块名称 及内存块大小。如 cache_item:500m 。表示声明一个名为cache_item 大小为500m。超出大小后最早的数据将会被清除。 | | | inactive | 最长闲置时间 如:10d 如果一个数据被闲置10天将会被清除 | | | max_size | 缓存区硬盘最大值。超出闲置数据将会被清除 | | location | proxy_cache | 指定缓存区，对应keys_zone 中设置的值 | | | proxy_cache_key | 通过参数拼装缓存key 如：$host$uri$is_args$args 则会以全路径md5值做做为Key | | | proxy_cache_valid | 为不同的状态码设置缓存有效期 | 3.缓存的清除： 该功能可以采用第三方模块 ngx_cache_purge 实现。 为nginx 添加 ngx_cache_purge 模块 #下载ngx_cache_purge 模块包 ,这里nginx 版本为1.6.2 purge 对应2.0版 wget http://labs.frickle.com/files/ngx_cache_purge-2.3.tar.gz #查看已安装模块 ./sbin/nginx -V #进入nginx安装包目录 重新安装 --add-module为模块解压的全路径 ./configure --prefix=/root/svr/nginx --with-http_stub_status_module --with-http_ssl_module --add-module=/root/svr/nginx/models/ngx_cache_purge-2.0 #重新编译 make #拷贝 安装目录/objs/nginx 文件用于替换原nginx 文件 #检测查看安装是否成功 nginx -t 清除配置： location ~ /clear(/.*) { #允许访问的IP allow 127.0.0.1; allow 192.168.0.193; #禁止访问的IP deny all; #配置清除指定缓存区和路径(与proxy_cache_key一至) proxy_cache_purge cache_item $host$1$is_args$args; } 配置好以后 直接访问 ： # 访问生成缓存文件 http://www.luban.com/?a=1 # 清除生成的缓存,如果指定缓存不存在 则会报404 错误。 http://www.luban.com/clear/?a=1 三、Nginx 性能参数调优 worker_processes number; 每个worker进程都是单线程的进程，它们会调用各个模块以实现多种多样的功能。如果这些模块确认不会出现阻塞式的调用，那么，有多少CPU内核就应该配置多少个进程；反之，如果有可能出现阻塞式调用，那么需要配置稍多一些的worker进程。例如，如果业务方面会致使用户请求大量读取本地磁盘上的静态资源文件，而且服务器上的内存较小，以至于大部分的请求访问静态资源文件时都必须读取磁盘（磁头的寻址是缓慢的），而不是内存中的磁盘缓存，那么磁盘I/O调用可能会阻塞住worker进程少量时间，进而导致服务整体性能下降。 每个worker 进程的最大连接数 语法：worker_connections number; 默认：worker_connections 1024 worker_cpu_affinity cpumask[cpumask……] 绑定Nginx worker进程到指定的CPU内核 为什么要绑定worker进程到指定的CPU内核呢？假定每一个worker进程都是非常繁忙的，如果多个worker进程都在抢同一个CPU，那么这就会出现同步问题。反之，如果每一个worker进程都独享一个CPU，就在内核的调度策略上实现了完全的并发。 例如，如果有4颗CPU内核，就可以进行如下配置： worker_processes 4; worker_cpu_affinity 1000 0100 0010 0001; 注意 worker_cpu_affinity配置仅对Linux操作系统有效。 Nginx worker 进程优先级设置 语法：worker_priority nice; 默认：worker_priority 0; 优先级由静态优先级和内核根据进程执行情况所做的动态调整（目前只有±5的调整）共同决定。nice值是进程的静态优先级，它的取值范围是–20～+19，–20是最高优先级，+19是最低优先级。因此，如果用户希望Nginx占有更多的系统资源，那么可以把nice值配置得更小一些，但不建议比内核进程的nice值（通常为–5）还要小 Nginx worker进程可以打开的最大句柄描述符个数 语法： worker_rlimit_nofile limit; 默认：空 更改worker进程的最大打开文件数限制。如果没设置的话，这个值为操作系统的限制。设置后你的操作系统和Nginx可以处理比“ulimit -a”更多的文件，所以把这个值设高，这样nginx就不会有“too many open files”问题了。 是否打开accept锁 语法：accept_mutex[on|off] 默认：accept_mutext on; accept_mutex是Nginx的负载均衡锁，当某一个worker进程建立的连接数量达到worker_connections配置的最大连接数的7/8时，会大大地减小该worker进程试图建立新TCP连接的机会，accept锁默认是打开的，如果关闭它，那么建立TCP连接的耗时会更短，但worker进程之间的负载会非常不均衡，因此不建议关闭它。 使用accept锁后到真正建立连接之间的延迟时间 语法：accept_mutex_delay Nms; 默认：accept_mutex_delay 500ms; 在使用accept锁后，同一时间只有一个worker进程能够取到accept锁。这个accept锁不是堵塞锁，如果取不到会立刻返回。如果只有一个worker进程试图取锁而没有取到，他至少要等待accept_mutex_delay定义的时间才能再次试图取锁。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/tomcat/":{"url":"performance/tomcat/","title":"tomcat","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance/tomcat/Tomcat-pro-use.html":{"url":"performance/tomcat/Tomcat-pro-use.html","title":"1.Tomcat生产环境应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Tomcat各组件认知 2.Tomcat 各组件及关系 3.Tomcat启动参数说明 二、Tomcat server.xml 配置详解 三、Tomcat IO模型介绍 1、Tomcat支持的IO模型说明 2、BIO 与NIO有什么区别 概要： Tomcat各核心组件认知 Tomcat server.xml 配置详解 Tomcat IO模型介绍 一、Tomcat各组件认知 知识点： Tomcat架构说明 Tomcat组件及关系详情介绍 Tomcat启动参数说明 Tomcat架构说明 Tomcat是一个基于JAVA的WEB容器，其实现了JAVA EE中的 Servlet 与 jsp 规范，与Nginx apache 服务器不同在于一般用于动态请求处理。在架构设计上采用面向组件的方式设计。即整体功能是通过组件的方式拼装完成。另外每个组件都可以被替换以保证灵活性。 那么是哪些组件组成了Tomcat呢？ 2.Tomcat 各组件及关系 Server 和 Service Connector 连接器 HTTP 1.1 SSL https AJP（ Apache JServ Protocol） apache 私有协议，用于apache 反向代理Tomcat Container Engine 引擎 catalina Host 虚拟机 基于域名 分发请求 Context 隔离各个WEB应用 每个Context的 ClassLoader都是独立 Component Manager （管理器） logger （日志管理） loader （载入器） pipeline (管道) valve （管道中的阀） 3.Tomcat启动参数说明 我们平时启动Tomcat过程是怎么样的？ 复制WAR包至Tomcat webapp 目录。 执行starut.bat 脚本启动。 启动过程中war 包会被自动解压装载。 但是我们在Eclipse 或idea 中启动WEB项目的时候 也是把War包复杂至webapps 目录解压吗？显然不是，其真正做法是在Tomcat程序文件之外创建了一个部署目录，在一般生产环境中也是这么做的 即：Tomcat 程序目录和部署目录分开 。 我们只需要在启动时指定CATALINA_HOME 与 CATALINA_BASE 参数即可实现。 启动参数 描述说明 JAVA_OPTS jvm 启动参数 , 设置内存 编码等 -Xms100m -Xmx200m -Dfile.encoding=UTF-8 JAVA_HOME 指定jdk 目录，如果未设置从java 环境变量当中去找。 CATALINA_HOME Tomcat 程序根目录 CATALINA_BASE 应用部署目录，默认为$CATALINA_HOME CATALINA_OUT 应用日志输出目录：默认$CATALINA_BASE/log CATALINA_TMPDIR 应用临时目录：默认：$CATALINA_BASE/temp 可以编写一个脚本 来实现自定义配置： 演示自定义启动Tomcat [ ] 下载并解压Tomcat [ ] 创建并拷贝应用目录 [ ] 创建Tomcat.sh [ ] 编写Tomcat.sh [ ] chmod +x tomcat.sh 添加执行权限 [ ] 拷贝conf 、webapps 、logs至应用目录。 [ ] 执行启动测试。 ```powershell!/bin/bash export JAVA_OPTS=\"-Xms100m -Xmx200m\" export JAVA_HOME=/root/svr/jdk/ export CATALINA_HOME=/usr/local/apache-tomcat-8.5.34 export CATALINA_BASE=\"pwd\" case $1 in start) $CATALINA_HOME/bin/catalina.sh start echo start success!! ;; stop) $CATALINA_HOME/bin/catalina.sh stop echo stop success!! ;; restart) $CATALINA_HOME/bin/catalina.sh stop echo stop success!! sleep 2 $CATALINA_HOME/bin/catalina.sh start echo start success!! ;; version) $CATALINA_HOME/bin/catalina.sh version ;; configtest) $CATALINA_HOME/bin/catalina.sh configtest ;; esac exit 0 ## 二、Tomcat server.xml 配置详解 --- Server 的基本基本配置： ```xml 元素说明： server root元素：server 的顶级配置 主要属性: port：执行关闭命令的端口号 shutdown：关闭命令 [ ] 演示shutdown的用法#基于telent 执行SHUTDOWN 命令即可关闭 telent 127.0.0.1 8005 SHUTDOWN service 服务：将多个connector 与一个Engine组合成一个服务，可以配置多个服务。 Connector 连接器：用于接收 指定协议下的连接 并指定给唯一的Engine 进行处理。 主要属性： protocol 监听的协议，默认是http/1.1 port 指定服务器端要创建的端口号 minThread 服务器启动时创建的处理请求的线程数 maxThread 最大可以创建的处理请求的线程数 enableLookups 如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址 redirectPort 指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号 acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理 connectionTimeout 指定超时的时间数(以毫秒为单位) SSLEnabled 是否开启 sll 验证，在Https 访问时需要开启。 [ ] 演示配置多个Connector Engine 引擎：用于处理连接的执行器，默认的引擎是catalina。一个service 中只能配置一个Engine。 主要属性：name 引擎名称 defaultHost 默认host Host 虚拟机：基于域名匹配至指定虚拟机。类似于nginx 当中的server,默认的虚拟机是localhost. 主要属性： [ ] 演示配置多个Host Context 应用上下文：一个host 下可以配置多个Context ，每个Context 都有其独立的classPath。相互隔离，以免造成ClassPath 冲突。 主要属性： [ ] 演示配置多个Context Valve 阀门：可以理解成request 的过滤器，具体配置要基于具体的Valve 接口的子类。以下即为一个访问日志的Valve. 三、Tomcat IO模型介绍 知识点： Tomcat支持的IO模型说明 BIO 与NIO的区别 IO模型源码解读1、Tomcat支持的IO模型说明 | | 描述 | |:----|:----| | BIO | 阻塞式IO，即Tomcat使用传统的java.io进行操作。该模式下每个请求都会创建一个线程，对性能开销大，不适合高并发场景。优点是稳定，适合连接数目小且固定架构。 | | NIO | 非阻塞式IO，jdk1.4 之后实现的新IO。该模式基于多路复用选择器监测连接状态在通知线程处理，从而达到非阻塞的目的。比传统BIO能更好的支持并发性能。Tomcat 8.0之后默认采用该模式 | | APR | 全称是 Apache Portable Runtime/Apache可移植运行库)，是Apache HTTP服务器的支持库。可以简单地理解为，Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作。使用需要编译安装APR 库 | | AIO | 异步非阻塞式IO，jdk1.7后之支持 。与nio不同在于不需要多路复用选择器，而是请求处理线程执行完程进行回调调知，已继续执行后续操作。Tomcat 8之后支持。 | | | | 使用指定IO模型的配置方式: 配置 server.xml 文件当中的 修改即可。 默认配置 8.0 protocol=“HTTP/1.1” 8.0 之前是 BIO 8.0 之后是NIO BIO protocol=“org.apache.coyote.http11.Http11Protocol“ NIO protocol=”org.apache.coyote.http11.Http11NioProtocol“ AIO protocol=”org.apache.coyote.http11.Http11Nio2Protocol“ APR protocol=”org.apache.coyote.http11.Http11AprProtocol“ 2、BIO 与NIO有什么区别 分别演示在高并发场景下BIO与NIO的线程数的变化？ 演示数据： | | 每秒提交数 | BIO执行线程 | NIO执行线程 | | |:----|:----|:----|:----|:----| | 预测 | 200 | 200 | 50 | | | 实验环境 | 200 | 48 | 37 | | | 生产环境 | 200 | 419 | 23 | | 结论： BIO 线程模型讲解 NIO 线程模型讲解 BIO 源码解读 Http11Protocol Http BIO协议解析器 JIoEndpoint Acceptor implements Runnable SocketProcessor implements Runnable Http11NioProtocol Http Nio协议解析器 NioEndpoint Acceptor implements Runnable Poller implements Runnable SocketProcessor implements Runnable Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/":{"url":"distributed/","title":"四、分布式框架专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/":{"url":"distributed/zookeeper/","title":"zookeeper","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-base-use.html":{"url":"distributed/zookeeper/zookeeper-base-use.html","title":"1.特性与节点说明","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、zookeeper概要、背景及作用 zookeeper产生背景： zookeeper概要： znode 节点 二、部署与常规配置 版本说明： 常规配置文件说明： 客户端命令： 三、Zookeeper节点介绍 知识点： 节点类型 节点属性 节点的监听： acl权限设置 主题概要 zookeeper概要、背景及作用 部署与常规配置 节点类型 一、zookeeper概要、背景及作用 zookeeper产生背景： 项目从单体到分布式转变之后，将会产生多个节点之间协同的问题。如： 每天的定时任务由谁哪个节点来执行？ RPC调用时的服务发现? 如何保证并发请求的幂等 .... 这些问题可以统一归纳为多节点协调问题，如果靠节点自身进行协调这是非常不可靠的，性能上也不可取。必须由一个独立的服务做协调工作，它必须可靠，而且保证性能。 zookeeper概要： ZooKeeper是用于分布式应用程序的协调服务。它公开了一组简单的API，分布式应用程序可以基于这些API用于同步，节点状态、配置等信息、服务注册等信息。其由JAVA编写，支持JAVA 和C两种语言的客户端。 znode 节点 zookeeper 中数据基本单元叫节点，节点之下可包含子节点，最后以树级方式程现。每个节点拥有唯一的路径path。客户端基于PATH上传节点数据，zookeeper 收到后会实时通知对该路径进行监听的客户端。 二、部署与常规配置 zookeeper 基于JAVA开发，下载后只要有对应JVM环境即可运行。其默认的端口号是2181运行前得保证其不冲突。 版本说明： 2019年5月20日发行的3.5.5是3.5分支的第一个稳定版本。此版本被认为是3.4稳定分支的后续版本，可以用于生产。基于3.4它包含以下新功能 动态重新配置 本地会议 新节点类型：容器，TTL 原子广播协议的SSL支持 删除观察者的能力 多线程提交处理器 升级到Netty 4.1 Maven构建 另请注意：建议的最低JDK版本为1.8 文件说明： apache-zookeeper-xxx-tar.gz 代表源代码 apache-zookeeper-xxx-bin.tar.gz 运行版本 下载地址：https://zookeeper.apache.org/releases.html#download 具体部署流程： #下载 wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/current/apache-zookeeper-3.5.5-bin.tar.gz #解压 tar -zxvf apache-zookeeper-3.5.5-bin.tar.gz #拷贝默认配置 cd {zookeeper_home}/conf cp zoo_sample.cfg zoo.cfg #启动 {zookeeper_home}/bin/zkServer.sh 常规配置文件说明： # zookeeper时间配置中的基本单位 (毫秒) tickTime=2000 # 允许follower初始化连接到leader最大时长，它表示tickTime时间倍数 即:initLimit*tickTime initLimit=10 # 允许follower与leader数据同步最大时长,它表示tickTime时间倍数 syncLimit=5 #zookeper 数据存储目录 dataDir=/tmp/zookeeper #对客户端提供的端口号 clientPort=2181 #单个客户端与zookeeper最大并发连接数 maxClientCnxns=60 # 保存的数据快照数量，之外的将会被清除 autopurge.snapRetainCount=3 #自动触发清除任务时间间隔，小时为单位。默认为0，表示不自动清除。 autopurge.purgeInterval=1 客户端命令： 基本命令列表 close 关闭当前会话 connect host:port 重新连接指定Zookeeper服务 create [-s] [-e] [-c] [-t ttl] path [data] [acl] 创建节点 delete [-v version] path 删除节点，(不能存在子节点） deleteall path 删除路径及所有子节点 setquota -n|-b val path 设置节点限额 -n 子节点数 -b 字节数 listquota path 查看节点限额 delquota [-n|-b] path 删除节点限额 get [-s] [-w] path 查看节点数据 -s 包含节点状态 -w 添加监听 getAcl [-s] path ls [-s] [-w] [-R] path 列出子节点 -s状态 -R 递归查看所有子节点 -w 添加监听 printwatches on|off 是否打印监听事件 quit 退出客户端 history 查看执行的历史记录 redo cmdno 重复 执行命令，history 中命令编号确定 removewatches path [-c|-d|-a] [-l] 删除指定监听 set [-s] [-v version] path data 设置值 setAcl [-s] [-v version] [-R] path acl 为节点设置ACL权限 stat [-w] path 查看节点状态 -w 添加监听 sync path 强制同步节点 node数据的增删改查 # 列出子节点 ls / #创建节点 create /luban \"luban is good man\" # 查看节点 get /luban # 创建子节点 create /luban/sex \"man\" # 删除节点 delete /luban/sex # 删除所有节点 包括子节点 deleteall /luban 三、Zookeeper节点介绍 知识点： 节点类型 节点的监听(watch) 节点属性说明(stat) 权限设置(acl) zookeeper 中节点叫znode存储结构上跟文件系统类似，以树级结构进行存储。不同之外在于znode没有目录的概念，不能执行类似cd之类的命令。znode结构包含如下： path:唯一路径 childNode：子节点 stat:状态属性 type:节点类型 节点类型 | 类型 | 描述 | |:----|:----| | PERSISTENT | 持久节点 | | PERSISTENT_SEQUENTIAL | 持久序号节点 | | EPHEMERAL | 临时节点(不可在拥有子节点) | | EPHEMERAL_SEQUENTIAL | 临时序号节点(不可在拥有子节点) | PERSISTENT（持久节点） 持久化保存的节点，也是默认创建的 #默认创建的就是持久节点 create /test PERSISTENT_SEQUENTIAL(持久序号节点) 创建时zookeeper 会在路径上加上序号作为后缀，。非常适合用于分布式锁、分布式选举等场景。创建时添加 -s 参数即可。 #创建序号节点 create -s /test #返回创建的实际路径 Created /test0000000001 create -s /test #返回创建的实际路径2 Created /test0000000002 EPHEMERAL（临时节点） 临时节点会在客户端会话断开后自动删除。适用于心跳，服务发现等场景。创建时添加参数-e 即可。 #创建临时节点， 断开会话 在连接将会自动删除 create -e /temp EPHEMERAL_SEQUENTIAL（临时序号节点） 与持久序号节点类似，不同之处在于EPHEMERAL_SEQUENTIAL是临时的会在会话断开后删除。创建时添加 -e -s create -e -s /temp/seq 节点属性 # 查看节点属性 stat /luban 其属性说明如下表： #创建节点的事物ID cZxid = 0x385 #创建时间 ctime = Tue Sep 24 17:26:28 CST 2019 #修改节点的事物ID mZxid = 0x385 #最后修改时间 mtime = Tue Sep 24 17:26:28 CST 2019 # 子节点变更的事物ID pZxid = 0x385 #这表示对此znode的子节点进行的更改次数（不包括子节点） cversion = 0 # 数据版本，变更次数 dataVersion = 0 #权限版本，变更次数 aclVersion = 0 #临时节点所属会话ID ephemeralOwner = 0x0 #数据长度 dataLength = 17 #子节点数(不包括子子节点) numChildren = 0 节点的监听： 客户添加 -w 参数可实时监听节点与子节点的变化，并且实时收到通知。非常适用保障分布式情况下的数据一至性。其使用方式如下： | 命令 | 描述 | |:----|:----| | ls -w path | 监听子节点的变化（增，删） | | get -w path | 监听节点数据的变化 | | stat -w path | 监听节点属性的变化 | | printwatches on|off | 触发监听后，是否打印监听事件(默认on) | acl权限设置 ACL全称为Access Control List（访问控制列表），用于控制资源的访问权限。ZooKeeper使用ACL来控制对其znode的防问。基于scheme:id:permission的方式进行权限控制。scheme表示授权模式、id模式对应值、permission即具体的增删改权限位。 scheme:认证模型 | 方案 | 描述 | |:----|:----| | world | 开放模式，world表示全世界都可以访问（这是默认设置） | | ip | ip模式，限定客户端IP防问 | | auth | 用户密码认证模式，只有在会话中添加了认证才可以防问 | | digest | 与auth类似，区别在于auth用明文密码，而digest 用sha-1+base64加密后的密码。在实际使用中digest 更常见。 | permission权限位 | 权限位 | 权限 | 描述 | |:----|:----|:----| | c | CREATE | 可以创建子节点 | | d | DELETE | 可以删除子节点（仅下一级节点） | | r | READ | 可以读取节点数据及显示子节点列表 | | w | WRITE | 可以设置节点数据 | | a | ADMIN | 可以设置节点访问控制列表权限 | acl 相关命令： | 命令 | 使用方式 | 描述 | |:----|:----|:----| | getAcl | getAcl | 读取ACL权限 | | setAcl | setAcl | 设置ACL权限 | | addauth | addauth | 添加认证用户 | world权限**示例** 语法： setAcl world:anyone: 注：world模式中anyone是唯一的值,表示所有人 查看默认节点权限： #创建一个节点 create -e /testAcl #查看节点权限 getAcl /testAcl #返回的默认权限表示 ，所有人拥有所有权限。 'world,'anyone: cdrwa 修改默认权限为 读写 #设置为rw权限 setAcl /testAcl world:anyone:rw # 可以正常读 get /testAcl # 无法正常创建子节点 create -e /testAcl/t \"hi\" # 返回没有权限的异常 Authentication is not valid : /testAcl/t IP权限示例： 语法： setAcl ip:: auth模式示例: 语法： setAcl auth::: addauth digest : digest 权限示例： 语法： setAcl digest ::: addauth digest : 注1：密钥 通过sha1与base64组合加密码生成，可通过以下命令生成 echo -n : | openssl dgst -binary -sha1 | openssl base64 注2：为节点设置digest 权限后，访问前必须执行addauth，当前会话才可以防问。 设置digest 权限 #先 sha1 加密，然后base64加密 echo -n luban:123456 | openssl dgst -binary -sha1 | openssl base64 #返回密钥 2Rz3ZtRZEs5RILjmwuXW/wT13Tk= #设置digest权限 setAcl /luban digest:luban:2Rz3ZtRZEs5RILjmwuXW/wT13Tk=:cdrw 查看节点将显示没有权限 #查看节点 get /luban #显示没有权限访问 org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /luban 给当前会话添加认证后在次查看 #给当前会话添加权限帐户 addauth digest luban:123456 #在次查看 get /luban #获得返回结果 luban is good man ACL的特殊说明： 权限仅对当前节点有效，不会让子节点继承。如限制了IP防问A节点，但不妨碍该IP防问A的子节点 /A/B。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-client-use.html":{"url":"distributed/zookeeper/zookeeper-client-use.html","title":"2.客户端使用与集群特性","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、客户端API常规应用 2.创建、查看节点 3.监听节点 4.设置节点ACL权限 5.第三方客户端ZkClient 二、Zookeeper集群 3.选举机制 5.四字运维命令 主要内容： 客户端 zookeeper客户端简介，C客户端 客户端连接参数说明 客户端CRUD 客户端监听 集群 集群架构说明 集群配置及参数说明 选举投票机制 主从复制机制一、客户端API常规应用 zookeeper 提供了java与C两种语言的客户端。我们要学习的就是java客户端。引入最新的maven依赖： org.apache.zookeeper zookeeper 3.5.5 知识点： 初始连接 创建、查看节点 监听节点 设置节点权限 第三方客户端ZkClient 初始连接： 常规的客户端类是 org.apache.zookeeper.ZooKeeper，实例化该类之后将会自动与集群建立连接。构造参数说明如下： | 参数名称 | 类型 | 说明 | |:----|:----|:----|:----:| | connectString | String | 连接串，包括ip+端口 ,集群模式下用逗号隔开 192.168.0.149:2181,192.168.0.150:2181 | | sessionTimeout | int | 会话超时时间，该值不能超过服务端所设置的 minSessionTimeout 和maxSessionTimeout | | watcher | Watcher | 会话监听器，服务端事件将会触该监听 | | sessionId | long | 自定义会话ID | | sessionPasswd | byte[] | 会话密码 | | canBeReadOnly | boolean | 该连接是否为只读的 | | hostProvider | HostProvider | 服务端地址提供者，指示客户端如何选择某个服务来调用，默认采用StaticHostProvider实现 | 2.创建、查看节点 创建节点 通过org.apache.zookeeper.ZooKeeper#create()即可创建节点，其参数说明如下： | 参数名称 | 类型 | 说明 | |:----|:----|:----| | path | String | | | data | byte[] | | | acl | List | | | createMode | CreateMode | | | cb | StringCallback | | | ctx | Object | | 查看节点： 通过org.apache.zookeeper.ZooKeeper#getData()即可创建节点，其参数说明如下： | 参数名称 | 类型 | 说明 | |:----|:----|:----| | path | String | | | watch | boolean | | | watcher | Watcher | | | cb | DataCallback | | | ctx | Object | | 查看子节点： 通过org.apache.zookeeper.ZooKeeper#getChildren()即可获取子节点，其参数说明如下： | 参数名称 | 类型 | 说明 | |:----:|:----:|:----| | path | String | | | watch | boolean | | | watcher | Watcher | | | cb | Children2Callback | | | ctx | Object | | 3.监听节点 在getData() 与getChildren()两个方法中可分别设置监听数据变化和子节点变化。通过设置watch为true，当前事件触发时会调用zookeeper()构建函数中Watcher.process()方法。也可以添加watcher参数来实现自定义监听。一般采用后者。 注：所有的监听都是一次性的，如果要持续监听需要触发后在添加一次监听。 4.设置节点ACL权限 ACL包括结构为scheme:id:permission（有关ACL的介绍参照第一节课关于ACL的讲解） 客户端中由org.apache.zookeeper.data.ACL 类表示，类结构如下： ACL Id scheme // 对应权限模式scheme id // 对应模式中的id值 perms // 对应权限位permission 关于权限位的表示方式： 每个权限位都是一个唯一数字，将其合时通过或运行生成一个全新的数字即可 @InterfaceAudience.Public public interface Perms { int READ = 1 5.第三方客户端ZkClient zkClient 是在zookeeper客户端基础之上封装的，使用上更加友好。主要变化如下： 可以设置持久监听，或删除某个监听 可以插入JAVA对象，自动进行序列化和反序列化 简化了基本的增删改查操作。 二、Zookeeper集群 知识点： 集群部署 集群角色说明 选举机制 数据提交机制 集群配置说明 zookeeper集群的目的是为了保证系统的性能承载更多的客户端连接设专门提供的机制。通过集群可以实现以下功能： 读写分离：提高承载，为更多的客户端提供连接，并保障性能。 主从自动切换：提高服务容错性，部分节点故障不会影响整个服务集群。 半数以上运行机制说明： 集群至少需要三台服务器，并且强烈建议使用奇数个服务器。因为zookeeper 通过判断大多数节点的存活来判断整个服务是否可用。比如3个节点，挂掉了2个表示整个集群挂掉，而用偶数4个，挂掉了2个也表示其并不是大部分存活，因此也会挂掉。 集群部署 配置语法： server.=:: 节点**ID**：服务id手动指定1至125之间的数字，并写到对应服务节点的 {dataDir}/myid 文件中。 IP地址：节点的远程IP地址，可以相同。但生产环境就不能这么做了，因为在同一台机器就无法达到容错的目的。所以这种称作为伪集群。 数据同步端口：主从同时数据复制端口，（做伪集群时端口号不能重复）。 远举端口：主从节点选举端口，（做伪集群时端口号不能重复）。 配置文件示例： tickTime=2000 dataDir=/var/lib/zookeeper/ clientPort=2181 initLimit=5 syncLimit=2 #以下为集群配置，必须配置在所有节点的zoo.cfg文件中 server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 集群配置流程： 分别创建3个data目录用于存储各节点数据mkdir data mkdir data/1 mkdir data/3 mkdir data/3 编写myid文件echo 1 > data/1/myid echo 3 > data/3/myid echo 2 > data/2/myid 3、编写配置文件 conf/zoo1.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=data/1 clientPort=2181 #集群配置 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 conf/zoo2.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=data/2 clientPort=2182 #集群配置 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 conf/zoo3.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=data/3 clientPort=2183 #集群配置 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 4.分别启动 ./bin/zkServer.sh start conf/zoo1.cfg ./bin/zkServer.sh start conf/zoo2.cfg ./bin/zkServer.sh start conf/zoo3.cfg 5.分别查看状态 ./bin/zkServer.sh status conf/zoo1.cfg Mode: follower ./bin/zkServer.sh status conf/zoo2.cfg Mode: leader ./bin/zkServer.sh status conf/zoo3.cfg Mode: follower 检查集群复制情况： 1、分别连接指定节点 zkCli.sh 后加参数-server 表示连接指定IP与端口。 ./bin/zkCli.sh -server 127.0.0.1:2181 ./bin/zkCli.sh -server 127.0.0.1:2182 ./bin/zkCli.sh -server 127.0.0.1:2183 [ ] 任意节点中创建数据，查看其它节点已经同步成功。 注意： -server参数后同时连接多个服务节点，并用逗号隔开 127.0.0.1:2181,127.0.0.1:2182 集群角色说明 zookeeper 集群中总共有三种角色，分别是leader（主节点）follower(子节点) observer（次级子节点） | 角色 | 描述 | |:----|:----| | leader | 主节点，又名领导者。用于写入数据，通过选举产生，如果宕机将会选举新的主节点。 | | follower | 子节点，又名追随者。用于实现数据的读取。同时他也是主节点的备选节点，并用拥有投票权。 | | observer | 次级子节点，又名观察者。用于读取数据，与fllower区别在于没有投票权，不能选为主节点。并且在计算集群可用状态时不会将observer计算入内。 | observer配置： 只要在集群配置中加上observer后缀即可，示例如下： server.3=127.0.0.1:2889:3889:observer 3.选举机制 通过 ./bin/zkServer.sh status 命令可以查看到节点状态 ./bin/zkServer.sh status conf/zoo1.cfg Mode: follower ./bin/zkServer.sh status conf/zoo2.cfg Mode: leader ./bin/zkServer.sh status conf/zoo3.cfg Mode: follower 可以发现中间的2182 是leader状态.其选举机制如下图： 投票机制说明： 第一轮投票全部投给自己 第二轮投票给myid比自己大的相邻节点 如果得票超过半数，选举结束。 选举触发： 当集群中的服务器出现已下两种情况时会进行Leader的选举 服务节点初始化启动 半数以上的节点无法和Leader建立连接 当节点初始起动时会在集群中寻找Leader节点，如果找到则与Leader建立连接，其自身状态变化follower或observer。如果没有找到Leader，当前节点状态将变化LOOKING，进入选举流程。 在集群运行其间如果有follower或observer节点宕机只要不超过半数并不会影响整个集群服务的正常运行。但如果leader宕机，将暂停对外服务，所有follower将进入LOOKING 状态，进入选举流程。 数据同步机制 zookeeper 的数据同步是为了保证各节点中数据的一至性，同步时涉及两个流程，一个是正常的客户端数据提交，另一个是集群某个节点宕机在恢复后的数据同步。 客户端写入请求： 写入请求的大至流程是，收leader接收客户端写请求，并同步给各个子节点。如下图： 但实际情况要复杂的多，比如client 它并不知道哪个节点是leader 有可能写的请求会发给follower ，由follower在转发给leader进行同步处理 客户端写入流程说明： client向zk中的server发送写请求，如果该server不是leader，则会将该写请求转发给leader server，leader将请求事务以proposal形式分发给follower； 当follower收到收到leader的proposal时，根据接收的先后顺序处理proposal； 当Leader收到follower针对某个proposal过半的ack后，则发起事务提交，重新发起一个commit的proposal Follower收到commit的proposal后，记录事务提交，并把数据更新到内存数据库； 当写成功后，反馈给client。 服务节点初始化同步： 在集群运行过程当中如果有一个follower节点宕机，由于宕机节点没过半，集群仍然能正常服务。当leader 收到新的客户端请求，此时无法同步给宕机的节点。造成数据不一至。为了解决这个问题，当节点启动时，第一件事情就是找当前的Leader，比对数据是否一至。不一至则开始同步,同步完成之后在进行对外提供服务。 如何比对Leader的数据版本呢，这里通过ZXID事物ID来确认。比Leader就需要同步。 ZXID说明： ZXID是一个长度64位的数字，其中低32位是按照数字递增，任何数据的变更都会导致,低32位的数字简单加1。高32位是leader周期编号，每当选举出一个新的leader时，新的leader就从本地事物日志中取出ZXID,然后解析出高32位的周期编号，进行加1，再将低32位的全部设置为0。这样就保证了每次新选举的leader后，保证了ZXID的唯一性而且是保证递增的。 思考题： 如果leader 节点宕机，在恢复后它还能被选为leader吗？ 5.四字运维命令 ZooKeeper响应少量命令。每个命令由四个字母组成。可通过telnet或nc向ZooKeeper发出命令。 这些命令默认是关闭的，需要配置4lw.commands.whitelist来打开，可打开部分或全部示例如下： #打开指定命令 4lw.commands.whitelist=stat, ruok, conf, isro #打开全部 4lw.commands.whitelist=* 安装Netcat工具，已使用nc命令 #安装Netcat 工具 yum install -y nc #查看服务器及客户端连接状态 echo stat | nc localhost 2181 命令列表 conf：3.3.0中的新增功能：打印有关服务配置的详细信息。 缺点：3.3.0中的新增功能：列出了连接到该服务器的所有客户端的完整连接/会话详细信息。包括有关已接收/已发送的数据包数量，会话ID，操作等待时间，最后执行的操作等信息。 crst：3.3.0中的新增功能：重置所有连接的连接/会话统计信息。 dump：列出未完成的会话和临时节点。这仅适用于领导者。 envi：打印有关服务环境的详细信息 ruok：测试服务器是否以非错误状态运行。如果服务器正在运行，它将以imok响应。否则，它将完全不响应。响应“ imok”不一定表示服务器已加入仲裁，只是服务器进程处于活动状态并绑定到指定的客户端端口。使用“ stat”获取有关状态仲裁和客户端连接信息的详细信息。 srst：重置服务器统计信息。 srvr：3.3.0中的新功能：列出服务器的完整详细信息。 stat：列出服务器和连接的客户端的简要详细信息。 wchs：3.3.0中的新增功能：列出有关服务器监视的简要信息。 wchc：3.3.0中的新增功能：按会话列出有关服务器监视的详细信息。这将输出具有相关监视（路径）的会话（连接）列表。请注意，根据手表的数量，此操作可能会很昂贵（即影响服务器性能），请小心使用。 dirs：3.5.1中的新增功能：以字节为单位显示快照和日志文件的总大小 wchp：3.3.0中的新增功能：按路径列出有关服务器监视的详细信息。这将输出具有关联会话的路径（znode）列表。请注意，根据手表的数量，此操作可能会很昂贵（即影响服务器性能），请小心使用。 mntr：3.4.0中的新增功能：输出可用于监视集群运行状况的变量列表。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-sence-use.html":{"url":"distributed/zookeeper/zookeeper-sence-use.html","title":"3.典型使用场景实践","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、 分布式集群管理 分布式集群管理的需求： 架构设计： 功能实现： 二 、分布式注册中心 Dubbo 对zookeeper的使用 Dubbo Zookeeper注册中心存储结构： 示例演示： 三、分布式JOB 分布式JOB需求： 架构设计： 四、分布式锁 锁的的基本概念： 锁的获取： 关于羊群效应： 示例演示： 课程概要： 分布式集群管理 分布式注册中心 分布式JOB 分布式锁 一、 分布式集群管理 分布式集群管理的需求： 主动查看线上服务节点 查看服务节点资源使用情况 服务离线通知 服务资源（CPU、内存、硬盘）超出阀值通知架构设计： 节点结构： tuling-manger // 根节点 server00001 : //服务节点 1 server00002 ://服务节点 2 server........n ://服务节点 n 服务状态信息: ip cpu memory disk功能实现： 数据生成与上报： 创建临时节点： 定时变更节点状态信息： 主动查询： 1、实时查询 zookeeper 获取集群节点的状态信息。 被动通知： 监听根节点下子节点的变化情况,如果CPU 等硬件资源低于警告位则发出警报。 关键示例代码： package com.tuling; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.tuling.os.CPUMonitorCalc; import com.tuling.os.OsBean; import org.I0Itec.zkclient.IZkChildListener; import org.I0Itec.zkclient.ZkClient; import java.io.IOException; import java.lang.instrument.Instrumentation; import java.lang.management.ManagementFactory; import java.lang.management.MemoryUsage; import java.net.InetAddress; import java.net.UnknownHostException; import java.util.ArrayList; import java.util.List; import java.util.stream.Collectors; /** * @author Tommy * Created by Tommy on 2019/9/22 **/ public class Agent { private String server = \"192.168.0.149:2181\"; ZkClient zkClient; private static Agent instance; private static final String rootPath = \"/tuling-manger\"; private static final String servicePath = rootPath + \"/service\"; private String nodePath; private Thread stateThread; List list = new ArrayList<>(); public static void premain(String args, Instrumentation instrumentation) { instance = new Agent(); if (args != null) { instance.server = args; } instance.init(); } // 初始化连接 public void init() { zkClient = new ZkClient(server, 5000, 10000); System.out.println(\"zk连接成功\" + server); buildRoot(); createServerNode(); stateThread = new Thread(() -> { while (true) { updateServerNode(); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } }, \"zk_stateThread\"); stateThread.setDaemon(true); stateThread.start(); } // 构建根节点 public void buildRoot() { if (!zkClient.exists(rootPath)) { zkClient.createPersistent(rootPath); } } // 生成服务节点 public void createServerNode() { nodePath = zkClient.createEphemeralSequential(servicePath, getOsInfo()); System.out.println(\"创建节点:\" + nodePath); } // 监听服务节点状态改变 public void updateServerNode() { zkClient.writeData(nodePath, getOsInfo()); } // 更新服务节点状态 public String getOsInfo() { OsBean bean = new OsBean(); bean.lastUpdateTime = System.currentTimeMillis(); bean.ip = getLocalIp(); bean.cpu = CPUMonitorCalc.getInstance().getProcessCpu(); MemoryUsage memoryUsag = ManagementFactory.getMemoryMXBean().getHeapMemoryUsage(); bean.usableMemorySize = memoryUsag.getUsed() / 1024 / 1024; bean.usableMemorySize = memoryUsag.getMax() / 1024 / 1024; ObjectMapper mapper = new ObjectMapper(); try { return mapper.writeValueAsString(bean); } catch (JsonProcessingException e) { throw new RuntimeException(e); } } public void updateNode(String path, Object data) { if (zkClient.exists(path)) { zkClient.writeData(path, data); } else { zkClient.createEphemeral(path, data); } } public static String getLocalIp() { InetAddress addr = null; try { addr = InetAddress.getLocalHost(); } catch (UnknownHostException e) { throw new RuntimeException(e); } return addr.getHostAddress(); } } 实现效果图： 二 、分布式注册中心 在单体式服务中，通常是由多个客户端去调用一个服务，只要在客户端中配置唯一服务节点地址即可，当升级到分布式后，服务节点变多，像阿里一线大厂服务节点更是上万之多，这么多节点不可能手动配置在客户端，这里就需要一个中间服务，专门用于帮助客户端发现服务节点，即许多技术书籍经常提到的服务发现。 一个完整的注册中心涵盖以下功能特性： 服务注册：提供者上线时将自提供的服务提交给注册中心。 服务注销：通知注册心提供者下线。 服务订阅：动态实时接收服务变更消息。 可靠：注册服务本身是集群的，数据冗余存储。避免单点故障，及数据丢失。 容错：当服务提供者出现宕机，断电等极情况时，注册中心能够动态感知并通知客户端服务提供者的状态。Dubbo 对zookeeper的使用 阿里著名的开源项目Dubbo 是一个基于JAVA的RCP框架，其中必不可少的注册中心可基于多种第三方组件实现，但其官方推荐的还是Zookeeper做为注册中心服务。 Dubbo Zookeeper注册中心存储结构： 节点说明： | 类别 | 属性 | 说明 | |:----|:----|:----| | Root | 持久节点 | 根节点名称，默认是 \"dubbo\" | | Service | 持久节点 | 服务名称，完整的服务类名 | | type | 持久节点 | 可选值：providers(提供者)、consumers（消费者）、configurators(动态配置)、routers | | URL | 临时节点 | url名称 包含服务提供者的 IP 端口 及配置等信息。 | 流程说明： 服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。示例演示： 服务端代码： package com.tuling.zk.dubbo; import com.alibaba.dubbo.config.ApplicationConfig; import com.alibaba.dubbo.config.ProtocolConfig; import com.alibaba.dubbo.config.RegistryConfig; import com.alibaba.dubbo.config.ServiceConfig; import java.io.IOException; /** * @author Tommy * Created by Tommy on 2019/10/8 **/ public class Server { public void openServer(int port) { // 构建应用 ApplicationConfig config = new ApplicationConfig(); config.setName(\"simple-app\"); // 通信协议 ProtocolConfig protocolConfig = new ProtocolConfig(\"dubbo\", port); protocolConfig.setThreads(200); ServiceConfig serviceConfig = new ServiceConfig(); serviceConfig.setApplication(config); serviceConfig.setProtocol(protocolConfig); serviceConfig.setRegistry(new RegistryConfig(\"zookeeper://192.168.0.149:2181\")); serviceConfig.setInterface(UserService.class); UserServiceImpl ref = new UserServiceImpl(); serviceConfig.setRef(ref); //开始提供服务 开张做生意 serviceConfig.export(); System.out.println(\"服务已开启!端口:\"+serviceConfig.getExportedUrls().get(0).getPort()); ref.setPort(serviceConfig.getExportedUrls().get(0).getPort()); } public static void main(String[] args) throws IOException { new Server().openServer(-1); System.in.read(); } } 客户端代码： package com.tuling.zk.dubbo; import com.alibaba.dubbo.config.ApplicationConfig; import com.alibaba.dubbo.config.ReferenceConfig; import com.alibaba.dubbo.config.RegistryConfig; import java.io.IOException; /** * @author Tommy * Created by Tommy on 2018/11/20 **/ public class Client { UserService service; // URL 远程服务的调用地址 public UserService buildService(String url) { ApplicationConfig config = new ApplicationConfig(\"young-app\"); // 构建一个引用对象 ReferenceConfig referenceConfig = new ReferenceConfig<>(); referenceConfig.setApplication(config); referenceConfig.setInterface(UserService.class); // referenceConfig.setUrl(url); referenceConfig.setRegistry(new RegistryConfig(\"zookeeper://192.168.0.149:2181\")); referenceConfig.setTimeout(5000); // 透明化 this.service = referenceConfig.get(); return service; } static int i = 0; public static void main(String[] args) throws IOException { Client client1 = new Client(); client1.buildService(\"\"); String cmd; while (!(cmd = read()).equals(\"exit\")) { UserVo u = client1.service.getUser(Integer.parseInt(cmd)); System.out.println(u); } } private static String read() throws IOException { byte[] b = new byte[1024]; int size = System.in.read(b); return new String(b, 0, size).trim(); } } 查询zk 实际存储内容： /dubbo /dubbo/com.tuling.zk.dubbo.UserService /dubbo/com.tuling.zk.dubbo.UserService/configurators /dubbo/com.tuling.zk.dubbo.UserService/routers /dubbo/com.tuling.zk.dubbo.UserService/providers /dubbo/com.tuling.zk.dubbo.UserService/providers/dubbo://192.168.0.132:20880/com.tuling.zk.dubbo.UserService?anyhost=true&application=simple-app&dubbo=2.6.2&generic=false&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=11128&side=provider&threads=200&timestamp=1570518302772 /dubbo/com.tuling.zk.dubbo.UserService/providers/dubbo://192.168.0.132:20881/com.tuling.zk.dubbo.UserService?anyhost=true&application=simple-app&dubbo=2.6.2&generic=false&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=12956&side=provider&threads=200&timestamp=1570518532382 /dubbo/com.tuling.zk.dubbo.UserService/providers/dubbo://192.168.0.132:20882/com.tuling.zk.dubbo.UserService?anyhost=true&application=simple-app&dubbo=2.6.2&generic=false&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=2116&side=provider&threads=200&timestamp=1570518537021 /dubbo/com.tuling.zk.dubbo.UserService/consumers /dubbo/com.tuling.zk.dubbo.UserService/consumers/consumer://192.168.0.132/com.tuling.zk.dubbo.UserService?application=young-app&category=consumers&check=false&dubbo=2.6.2&interface=com.tuling.zk.dubbo.UserService&methods=getUser&pid=9200&side=consumer&timeout=5000&timestamp=1570518819628 三、分布式JOB 分布式JOB需求： 多个服务节点只允许其中一个主节点运行JOB任务。 当主节点挂掉后能自动切换主节点，继续执行JOB任务。架构设计： node结构： tuling-master server0001:master server0002:slave server000n:slave 选举流程： 服务启动： 在tuling-maste下创建server子节点，值为slave 获取所有tuling-master 下所有子节点 判断是否存在master 节点 如果没有设置自己为master节点 子节点删除事件触发： 获取所有tuling-master 下所有子节点 判断是否存在master 节点 如果没有设置最小值序号为master 节点四、分布式锁 锁的的基本概念： 开发中锁的概念并不陌生，通过锁可以实现在多个线程或多个进程间在争抢资源时，能够合理的分配置资源的所有权。在单体应用中我们可以通过 synchronized 或ReentrantLock 来实现锁。但在分布式系统中，仅仅是加synchronized 是不够的，需要借助第三组件来实现。比如一些简单的做法是使用 关系型数据行级锁来实现不同进程之间的互斥，但大型分布式系统的性能瓶颈往往集中在数据库操作上。为了提高性能得采用如Redis、Zookeeper之内的组件实现分布式锁。 共享锁：也称作只读锁，当一方获得共享锁之后，其它方也可以获得共享锁。但其只允许读取。在共享锁全部释放之前，其它方不能获得写锁。 排它锁：也称作读写锁，获得排它锁后，可以进行数据的读写。在其释放之前，其它方不能获得任何锁。 锁的获取： 某银行帐户，可以同时进行帐户信息的读取，但读取其间不能修改帐户数据。其帐户ID为:888 获得读锁流程： 1、基于资源ID创建临时序号读锁节点 /lock/888.R0000000002 Read 2、获取 /lock 下所有子节点，判断其最小的节点是否为读锁，如果是则获锁成功 3、最小节点不是读锁，则阻塞等待。添加lock/ 子节点变更监听。 4、当节点变更监听触发，执行第2步 数据结构： 获得写锁： 1、基于资源ID创建临时序号写锁节点 /lock/888.R0000000002 Write 2、获取 /lock 下所有子节点，判断其最小的节点是否为自己，如果是则获锁成功 3、最小节点不是自己，则阻塞等待。添加lock/ 子节点变更监听。 4、当节点变更监听触发，执行第2步 释放锁： 读取完毕后，手动删除临时节点，如果获锁期间宕机，则会在会话失效后自动删除。 关于羊群效应： 在等待锁获得期间，所有等待节点都在监听 Lock节点，一但lock 节点变更所有等待节点都会被触发，然后在同时反查Lock 子节点。如果等待对例过大会使用Zookeeper承受非常大的流量压力。 为了改善这种情况，可以采用监听链表的方式，每个等待对列只监听前一个节点，如果前一个节点释放锁的时候，才会被触发通知。这样就形成了一个监听链表。 示例演示： package com.tuling.zookeeper.lock; import org.I0Itec.zkclient.IZkDataListener; import org.I0Itec.zkclient.ZkClient; import java.util.List; import java.util.stream.Collectors; /** * @author Tommy * Created by Tommy on 2019/9/23 **/ public class ZookeeperLock { private String server = \"192.168.0.149:2181\"; private ZkClient zkClient; private static final String rootPath = \"/tuling-lock\"; public ZookeeperLock() { zkClient = new ZkClient(server, 5000, 20000); buildRoot(); } // 构建根节点 public void buildRoot() { if (!zkClient.exists(rootPath)) { zkClient.createPersistent(rootPath); } } public Lock lock(String lockId, long timeout) { Lock lockNode = createLockNode(lockId); lockNode = tryActiveLock(lockNode);// 尝试激活锁 if (!lockNode.isActive()) { try { synchronized (lockNode) { lockNode.wait(timeout); } } catch (InterruptedException e) { throw new RuntimeException(e); } } if (!lockNode.isActive()) { throw new RuntimeException(\" lock timeout\"); } return lockNode; } public void unlock(Lock lock) { if (lock.isActive()) { zkClient.delete(lock.getPath()); } } // 尝试激活锁 private Lock tryActiveLock(Lock lockNode) { // 判断当前是否为最小节点 List list = zkClient.getChildren(rootPath) .stream() .sorted() .map(p -> rootPath + \"/\" + p) .collect(Collectors.toList()); String firstNodePath = list.get(0); if (firstNodePath.equals(lockNode.getPath())) { lockNode.setActive(true); } else { String upNodePath = list.get(list.indexOf(lockNode.getPath()) - 1); zkClient.subscribeDataChanges(upNodePath, new IZkDataListener() { @Override public void handleDataChange(String dataPath, Object data) throws Exception { } @Override public void handleDataDeleted(String dataPath) throws Exception { // 事件处理 与心跳 在同一个线程，如果Debug时占用太多时间，将导致本节点被删除，从而影响锁逻辑。 System.out.println(\"节点删除:\" + dataPath); Lock lock = tryActiveLock(lockNode); synchronized (lockNode) { if (lock.isActive()) { lockNode.notify(); } } zkClient.unsubscribeDataChanges(upNodePath, this); } }); } return lockNode; } public Lock createLockNode(String lockId) { String nodePath = zkClient.createEphemeralSequential(rootPath + \"/\" + lockId, \"lock\"); return new Lock(lockId, nodePath); } } Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/zookeeper/zookeeper-zab.html":{"url":"distributed/zookeeper/zookeeper-zab.html","title":"4.ZAB协议实现源码分析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、启动流程 1.工程结构介绍 2.启动宏观流程图： 3.集群启动详细流程 4.netty 服务启动流程： 二、快照与事务日志存储结构 概要: 存储结构: 快照相关配置： 快照装载流程： 课程概要： 启动流程源码分析 快照与事物日志的存储结构一、启动流程 知识点： 工程结构介绍 启动流程宏观图 集群启动详细流程 netty 服务工作机制1.工程结构介绍 项目地址:https://github.com/apache/zookeeper.git 分支tag ：3.5.5 zookeeper-recipes: 示例源码 zookeeper-client: C语言客户端 zookeeper-server：主体源码 2.启动宏观流程图： [ ] 启动示例演示： 服务端：ZooKeeperServerMain 客户端：ZooKeeperMain 3.集群启动详细流程 装载配置： # zookeeper 启动流程堆栈 >QuorumPeerMain#initializeAndRun //启动工程 >QuorumPeerConfig#parse // 加载config 配置 >QuorumPeerConfig#parseProperties// 解析config配置 >new DatadirCleanupManager // 构造一个数据清器 >DatadirCleanupManager#start // 启动定时任务 清除过期的快照 代码堆栈 ： >QuorumPeerMain#main //启动main方法 >QuorumPeerConfig#parse // 加载zoo.cfg 文件 >QuorumPeerConfig#parseProperties // 解析配置 >DatadirCleanupManager#start // 启动定时任务清除日志 >QuorumPeerConfig#isDistributed // 判断是否为集群模式 >ServerCnxnFactory#createFactory() // 创建服务默认为NIO，推荐netty //***创建 初始化集群管理器**/ >QuorumPeerMain#getQuorumPeer >QuorumPeer#setTxnFactory >new FileTxnSnapLog // 数据文件管理器，用于检测快照与日志文件 /** 初始化数据库*/ >new ZKDatabase >ZKDatabase#createDataTree //创建数据树，所有的节点都会存储在这 // 启动集群：同时启动线程 > QuorumPeer#start // > QuorumPeer#loadDataBase // 从快照文件以及日志文件 加载节点并填充到dataTree中去 > QuorumPeer#startServerCnxnFactory // 启动netty 或java nio 服务，对外开放2181 端口 > AdminServer#start// 启动管理服务，netty http服务，默认端口是8080 > QuorumPeer#startLeaderElection // 开始执行选举流程 > quorumPeer.join() // 防止主进程退出 流程说明: main方法启动 加载zoo.cfg 配置文件 解析配置 创建服务工厂 创建集群管理线程 设置数据库文件管理器 设置数据库 ....设置设置 start启动集群管理线程 加载数据节点至内存 启动netty 服务，对客户端开放端口 启动管理员Http服务，默认8080端口 启动选举流程 join 管理线程，防止main 进程退出 4.netty 服务启动流程： 服务UML类图 设置netty启动参数 -Dzookeeper.serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory 初始化： 关键代码： #初始化管道流 #channelHandler 是一个内部类是具体的消息处理器。 protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); if (secure) { initSSL(pipeline); } pipeline.addLast(\"servercnxnfactory\", channelHandler); } channelHandler 类结构 执行堆栈： NettyServerCnxnFactory#NettyServerCnxnFactory // 初始化netty服务工厂 > NettyUtils.newNioOrEpollEventLoopGroup // 创建IO线程组 > NettyUtils#newNioOrEpollEventLoopGroup() // 创建工作线程组 >ServerBootstrap#childHandler(io.netty.channel.ChannelHandler) // 添加管道流 >NettyServerCnxnFactory#start // 绑定端口，并启动netty服务 创建连接： 每当有客户端新连接进来，就会进入该方法 创建 NettyServerCnxn对象。并添加至cnxns对例 执行堆栈 CnxnChannelHandler#channelActive >new NettyServerCnxn // 构建连接器 >NettyServerCnxnFactory#addCnxn // 添加至连接器，并根据客户端IP进行分组 >ipMap.get(addr) // 基于IP进行分组 读取消息： 执行堆栈 CnxnChannelHandler#channelRead >NettyServerCnxn#processMessage // 处理消息 >NettyServerCnxn#receiveMessage // 接收消息 >ZooKeeperServer#processPacket //处理消息包 >org.apache.zookeeper.server.Request // 封装request 对象 >org.apache.zookeeper.server.ZooKeeperServer#submitRequest // 提交request >org.apache.zookeeper.server.RequestProcessor#processRequest // 处理请求 二、快照与事务日志存储结构 概要: ZK中所有的数据都是存储在内存中，即zkDataBase中。但同时所有对ZK数据的变更都会记录到事物日志中，并且当写入到一定的次数就会进行一次快照的生成。已保证数据的备份。其后缀就是ZXID（唯一事物ID）。 事物日志：每次增删改，的记录日志都会保存在文件当中 快照日志：存储了在指定时间节点下的所有的数据存储结构: zkDdataBase 是zk数据库基类，所有节点都会保存在该类当中，而对Zk进行任何的数据变更都会基于该类进行。zk数据的存储是通过DataTree 对象进行，其用了一个map 来进行存储。 UML 类图： 读取快照日志： org.apache.zookeeper.server.SnapshotFormatter 读取事物日志： org.apache.zookeeper.server.LogFormatter 快照相关配置： dataLogDir 事物日志目录 zookeeper.preAllocSize 预先开辟磁盘空间，用于后续写入事务日志，默认64M zookeeper.snapCount 每进行snapCount次事务日志输出后，触发一次快照，默认是100,000 autopurge.snapRetainCount 自动清除时 保留的快照数 autopurge.purgeInterval 清除时间间隔，小时为单位 -1 表示不自动清除。 快照装载流程： >ZooKeeperServer#loadData // 加载数据 >FileTxnSnapLog#restore // 恢复数据 >FileSnap#deserialize() // 反序列化数据 >FileSnap#findNValidSnapshots // 查找有效的快照 >Util#sortDataDir // 基于后缀排序文件 >persistence.Util#isValidSnapshot // 验证是否有效快照文件 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/netty/":{"url":"distributed/netty/","title":"netty","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/":{"url":"distributed/dubbo/","title":"dubbo","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-introduce.html":{"url":"distributed/dubbo/dubbo-introduce.html","title":"1.从0到1整体认知分布式系统","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、分布式架构的发展历史与背景 架构的发展历史： 分布式架构所带来的成本与风险: 二、如何选型分布式架构 基于反向代理的集中式分布式架构 嵌入应用内部的去中心化架构 基于独立代理进程的架构(Service Mesh) 三种架构的比较 **三、Dubbo 架构与设计说明 ** dubbo架构简要讲解 Dubbo 整体设计 Dubbo 中的SPI机制 概要： 分布式架构的发展历史与背景 如何着手架构一套分布示式系统 Dubbo 架构与设计说明 一、分布式架构的发展历史与背景 场景一： 一家做政务OA系统的公司老板发现跟竞争对手比发现自己的系统的架构不是分布示的，找到技术负责人问，把系统架构升级成分布示架构要多长时间？技术负责人网上查了查 dubbo官网看了看 Demo 这不很简单吗，拍着胸脯一个月能升级好。 现在我的问题是：这位技术理在改造过程中可能会遇到什么风险和问题？ 新功能和旧BUG的问题 业务完整性的问题 团队协作方式转变 开发人员技能提升 系统交付方式转变 这些问题解决涉及业务部门及整个技术部门（开发、测试、运维）协商与工作标准的制定。业务相关问题暂不做讨论,技术架构上应该要清楚自己的职责是，如何通过技术手段把业务波动降至最低、开发成本最低、实施风险最低？ 架构的发展历史： 单体式架构： 垂直架构: 分布示架构： 分布式架构所带来的成本与风险: 分布式事物： 分布式事物是指一个操作，分成几个小操作在多个服务器上执行，要么多成功，要么多失败这些分布事物要做的 不允许服务有状态（**stateless service**） 无状态服务是指对单次请求的处理，不依赖其他请求，也就是说，处理一次请求所需的全部信息，要么都包含在这个请求里，要么可以从外部获取到（比如说数据库），服务器本身不存储任何信息。 服务依懒关系复杂 服务 A --> B--> C 那和服务C 的修改 就可能会影响 B 和C，事实上当服务越来 越多的时候，C的变动将会越来越困难。 部署运维成本增加 不用说了，相比之前几个节点，运维成本的增加必须的。 源码管理成本增加： 原本一套或几套源码现在拆分成几十个源码库，其中分支、tag都要进行相应管理。 如何保证系统的伸缩性： 伸缩性是指,当前服务器硬件升级后或新增服务器处理能力就能相对应的提升。 分布式会话： 此仅针对应用层服务，不能将Session 存储在一个服务器上。 分布式JOB 通常定时任务只需要在一台机器上触发执行，分布式的情况下在哪台执行呢？ 最后通过一张图直观感受一下 单体到分布式的区别： 二、如何选型分布式架构 提问：实现一个分布示框架最核心功能是什么? RPC远程调用技术： 大家知道的 有哪些远程调用的 方式？拿几个大家比较熟悉的来举例：RMI 、Web Service、Http | 协议 | 描述 | 优点 | 缺点 | |:----|:----|:----|:----| | RMI | JAVA 远程方法调用、使用原生二进制方式进行序列化 | 简单易用、SDK支持，提高开发效率 | 不支持跨语言 | | Web Service | 比较早系统调用解决方案 ，跨语言, 其基于WSDL 生成 SOAP 进行消息的传递。 | SDK支持、跨语言 | 实现较重，发布繁琐 | | Http | 采用htpp +json 实现 | 简单、轻量、跨语言 | 不支持SDK | 基于比较上述比较，大家会选择哪个方案，综合考虑 RMI是比较合适的方案，基本没有学习成本。而跨语言问题基本可以勿略。 如果服务端不是单个的话，这个方案差点我就用了。实际上服务端是多个的 ，好了新的问题又来了。 负载均衡：这么多个机器调用哪一台? 服务发现：样发现新的服务地址呢？ 健康检测：服务关宕机或恢复后怎么办？ 容错：如果调用其中一台调用出错了怎么办？ 这些功能怎么解决呢？一个一个的去编码实现么？。有没有现成的方案可以直接借鉴呢？ 分布式架构的三种解决方案： 基于反向代理的中心化架构 嵌入应用内部的去中心化架构 基于独立代理进程的Service Mesh架构基于反向代理的集中式分布式架构 这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。 Http+Nginx 方案总结： 优点：简单快速、几乎没有学习成本 适用场景：轻量级分布式系统、局部分布式架构。 瓶颈：Nginx中心负载、Http传输、JSON序列化、开发效率、运维效率。 嵌入应用内部的去中心化架构 这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。我们所熟悉的 duboo 和spring cloud Eureka +Ribbon/'rɪbən/ 都是这种方式实现。 相比第一代架构它有以下特点几点： 去中心化，客户端直连服务端 动态注册和发现服务 高效稳定的网络传输 高效可容错的序列化基于独立代理进程的架构(Service Mesh) 这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同第二代架构。 三种架构的比较 模式 优点 缺点 适应场景 案例 集中式负载架构 简单 集中式治理 与语言无关 配置维护成本高 多了一层IO 单点问题 大部分公司都适用，对运维有要求 亿贝、携程、早期互联网公司 客户端嵌入式架构 无单点 性能更好 客户端复杂 语言栈要求 中大规模公司、语言栈统一 Dubbo 、 Twitter finagle、 Spring Cloud Ribbon 独立进程代理架构 无单点 性能更好 与语言无关 运维部署复杂 开发联调复杂 中大规模公司 对运维有要求 Smart Stack Service Mesh 三、Dubbo 架构与设计说明 dubbo架构简要讲解 架构图 流程说明： Provider(提供者)绑定指定端口并启动服务 指供者连接注册中心，并发本机IP、端口、应用信息和提供服务信息发送至注册中心存储 Consumer(消费者），连接注册中心 ，并发送应用信息、所求服务信息至注册中心 注册中心根据 消费 者所求服务信息匹配对应的提供者列表发送至Consumer 应用缓存。 Consumer 在发起远程调用时基于缓存的消费者列表择其一发起调用。 Provider 状态变更会实时通知注册中心、在由注册中心实时推送至Consumer 这么设计的意义： Consumer 与Provider 解偶，双方都可以横向增减节点数。 注册中心对本身可做对等集群，可动态增减节点，并且任意一台宕掉后，将自动切换到另一台 去中心化，双方不直接依懒注册中心，即使注册中心全部宕机短时间内也不会影响服务的调用 服务提供者无状态，任意一台宕掉后，不影响使用 Dubbo 整体设计 config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成动态代理 扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 其协作流程如下： Dubbo 中的SPI机制 在了解Dubbo的spi之前 先来了解一下 JAVA自带的SPI java spi的具体约定为:当服务的提供者，提供了服务接口的一种实现之后，在jar包的META-INF/services/目录里同时创建一个以服务接口命名的文件。该文件里就是实现该服务接口的具体实现类。而当外部程序装配这个模块的时候，就能通过该jar包META-INF/services/里的配置文件找到具体的实现类名，并装载实例化，完成模块的注入。 基于这样一个约定就能很好的找到服务接口的实现类，而不需要再代码里制定。jdk提供服务实现查找的一个工具类java.util.ServiceLoader 演示JAVA SPI机制 [ ] 编写接口 [ ] 编写实现类 [ ] 编辑META-INF/services/xxx 文件 [ ] 演示spi 实现 spi 目录文件： META-INF/services/tuling.dubbo.server.UserService 中的值： tuling.dubbo.server.impl.UserServiceImpl2 装载获取SPI实现类： public static void main(String[] args) { Iterator services = ServiceLoader.load(UserService.class).iterator(); UserService service = null; while (services.hasNext()) { service = services.next(); } System.out.println(service.getUser(111)); } Dubbo的SPI机制： dubbo spi 在JAVA自带的SPI基础上加入了扩展点的功能，即每个实现类都会对应至一个扩展点名称，其目的是 应用可基于此名称进行相应的装配。 演示Dubbo SPI机制： [ ] 编写Filter 过滤器 [ ] 编写 dubbo spi 配置文件 [ ] 装配自定义Filter dubbo spi 目录文件 dubbo spi 文件内容： luban=tuling.dubbo.server.LubanFilter 装配自定义Filter Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-base-use.html":{"url":"distributed/dubbo/dubbo-base-use.html","title":"2.快速掌握Dubbo常规应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Dubbo 快速入门 Dubbo核心功能解释 快速演示Dubbo的远程调用 基于Dubbo实现服务集群： 二、Dubbo常规配置说明 Dubbo配置的整体说明： dubbo 配置的一些套路: 一般建议配置示例： Copyright &copy ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 概要： Dubbo 快速入门 Dubbo 常规配置说明一、Dubbo 快速入门 Dubbo核心功能解释 dubbo 阿里开源的一个SOA服务治理框架，从目前来看把它称作是一个RPC远程调用框架更为贴切。单从RPC框架来说，功能较完善，支持多种传输和序列化方案。所以想必大家已经知道他的核心功能了：就是远程调用。 快速演示Dubbo的远程调用 实现步骤 [ ] 创建服务端项目 [ ] 引入dubbo 依赖 [ ] 编写服务端代码 [ ] 创建客户端项目 [ ] 引入dubbo 依赖 [ ] 编写客户端调用代码 dubbo 引入： com.alibaba dubbo 2.6.2 dubbo 默认依懒： 客户端代码： static String remoteUrl = \"dubbo://127.0.0.1:12345/tuling.dubbo.server.UserService\"; // 构建远程服务对象 public UserService buildRemoteService(String remoteUrl) { ApplicationConfig application = new ApplicationConfig(); application.setName(\"young-app\"); ReferenceConfig referenceConfig = new ReferenceConfig<>(); referenceConfig.setApplication(application); referenceConfig.setInterface(UserService.class); referenceConfig.setUrl(remoteUrl); UserService userService = referenceConfig.get(); return userService; } 服务端代码： public void openServer(int port) { ApplicationConfig config = new ApplicationConfig(); config.setName(\"simple-app\"); ProtocolConfig protocolConfig=new ProtocolConfig(); protocolConfig.setName(\"dubbo\"); protocolConfig.setPort(port); protocolConfig.setThreads(20); ServiceConfig serviceConfig=new ServiceConfig(); serviceConfig.setApplication(config); serviceConfig.setProtocol(protocolConfig); serviceConfig.setRegistry(new RegistryConfig(RegistryConfig.NO_AVAILABLE)); serviceConfig.setInterface(UserService.class); serviceConfig.setRef(new UserServiceImpl()); serviceConfig.export(); } 基于Dubbo实现服务集群： 在上一个例子中如多个服务的集群？即当有多个服务同时提供的时候，客户端该调用哪个？以什么方式进行调用以实现负载均衡？ 一个简单的办法是将多个服务的URL同时设置到客户端并初始化对应的服务实例，然后以轮询的方式进行调用。 但如果访问增大，需要扩容服务器数量，那么就必须增加配置重启客户端实例。显然这不是我们愿意看到的。Dubbo引入了服务注册中的概念，可以解决动态扩容的问题。 演示基于注册中心实现服集群： [ ] 修改服务端代码，添加multicast 注册中心。 [ ] 修改客户端代码，添加multicast 注册中心。 [ ] 观察 多个服务时，客户端如何调用。 [ ] 观察 动态增减服务，客户端的调用。 # 服务端连接注册中心 serviceConfig.setRegistry(new RegistryConfig(\"multicast://224.1.1.1:2222\")); # 客户端连接注册中心 referenceConfig.setRegistry(new RegistryConfig(\"multicast://224.1.1.1:2222\")); #查看 基于UDP 占用的2222 端口 netstat -ano|findstr 2222 基于spring IOC维护Dubbo 实例 在前面两个例子中 出现了,ApplicationConfig、ReferenceConfig、RegistryConfig、com.alibaba.dubbo.config.ServiceConfig等实例 ，很显然不需要每次调用的时候都去创建该实例那就需要一个IOC 容器去管理这些实例，spring 是一个很好的选择。 提供者配置---------------------------------- 提供者服务暴露代码： ApplicationContext context = new ClassPathXmlApplicationContext(\"/spring-provide.xml\"); ((ClassPathXmlApplicationContext) context).start(); System.in.read(); 消费者配置--------------------------------------- 消费者调用代码： ApplicationContext context = new ClassPathXmlApplicationContext(\"/spring-consumer.xml\"); UserService userService = context.getBean(UserService.class); UserVo u = userService.getUser(1111); System.out.println(u); 二、Dubbo常规配置说明 Dubbo配置的整体说明： 标签 用途 解释 公共 用于配置当前应用信息，不管该应用是提供者还是消费者 公共 用于配置连接注册中心相关信息 服务 用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受 服务 用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心 服务 当 ProtocolConfig 和 ServiceConfig 某属性没有配置时，采用此缺省值，可选 引用 当 ReferenceConfig 某属性没有配置时，采用此缺省值，可选 引用 用于创建一个远程服务代理，一个引用可以指向多个注册中心 公共 用于 ServiceConfig 和 ReferenceConfig 指定方法级的配置信息 公共 用于指定方法参数配置 配置关系图： 配置分类 所有配置项分为三大类。 服务发现：表示该配置项用于服务的注册与发现，目的是让消费方找到提供方。 服务治理：表示该配置项用于治理服务间的关系，或为开发测试提供便利条件。 性能调优：表示该配置项用于调优性能，不同的选项对性能会产生影响。dubbo 配置的一些套路: 先来看一个简单配置 通过字面了解 timeout即服务的执行超时时间。但当服务执行真正超时的时候 报的错跟timeout并没有半毛钱的关系，其异常堆栈如下： 可以看到错误表达的意思是 因为Channel 关闭导致 无法返回 Response 消息。 出现这情况的原因在于 虽然timeout 配置在服务端去是用在客户端，其表示的是客户端调用超时间，而非服务端方法的执行超时。当我们去看客户端的日志时候就能看到timeout异常了 类似这种配在服务端用在客户端的配置还有很多，如retries/riː'traɪ/(重试次数)、async/əˈsɪŋk/（是否异步）、loadbalance(负载均衡)。。。等。 套路一：*服务端配置客户端来使用*。 注：其参数传递机制是 服务端所有配置都会封装到URL参数，在通过注册中心传递到客户端 如果需要暴露多个服务的时候，每个服务都要设置其超时时间，貌似有点繁琐。Dubbo中可以通过 来实现服务端缺省配置。它可以同时为 和 两个标签提供缺省配置。如： #相当于每个服务提供者设置了超时时间 和重试次数 同样客户端也有缺省配置标签：，这些缺省设置可以配置多个 通过 ,如果没指定就用第一个。 、 套路二：与 ，与傻傻分不清楚 在服务端配置timeout 之后 所有客户端都会采用该方超时时间，其客户端可以自定义超时时间吗？通过 可以设定或者在 也可以设定 甚至可以设定到方法级别 。加上服务端的配置，超时总共有6处可以配置。如果6处都配置了不同的值，最后肯定只会有一个超时值生效，其优先级如下： 小提示：通过DefaultFuture的get 方法就可观测到实际的超时设置。 com.alibaba.dubbo.remoting.exchange.support.DefaultFuture 套路三：同一属性到处配置，优先级要小心。 一般建议配置示例： 提供端：--------------------------- 消费端示例：-------------------- Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-high-use.html":{"url":"distributed/dubbo/dubbo-high-use.html","title":"3.Dubbo企业级应用进阶","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、分布式项目开发与联调 接口暴露与引用 自动化构建与协作 接口平滑升级： 开发联调： 二、Dubbo控制管理后台使用 Dubbo 控制后台版本说明： Dubbo 控制后台的安装： 三、Dubbo注册中心详解 注册中心的作用 Dubbo所支持的注册中心 Redis 注册中心 Zookeeper 注册中心 课程概要： 分布式项目开发与联调 控制管理后台使用 Dubbo注册中心详解 一、分布式项目开发与联调 接口暴露与引用 在一个RPC场景中 ，调用方是通过接口来调用服务端，传入参数并获得返回结果。这样服务端的接口和模型必须暴露给调用方项目。服务端如何暴露呢？客户端如何引用呢？ 接口信息 、模型信息 、异常 暴露接口的通常做法是 接口与实现分离，服务端将 接口、模型、异常 等统一放置于一个模块，实现置于另一个模块。调用方通过Maven进行引用。 自动化构建与协作 当项目越来越多，服务依懒关系越发复杂的时候，为了提高协作效率，必须采用自动化工具 完成 接口从编写到构建成JAR包，最后到引用的整个过程。 流程描述： 服务提供者项目发人员编写Client 接口 push 至远程仓库 jenkins 构建指定版本 jenkins Deploye 至私服仓库 nexus 服务消费者项目开发人员基于maven 从私服务仓库下载接口平滑升级： 在项目迭代过程当中， 经常会有多个项目依懒同一个接口，如下图 项目B、C都依懒了项目A当中的接口1，此时项目B业务需要，需要接口1多增加一个参数，升级完成后。项目B能正确构建上线，项目C却不行。 解决办法与原则： 接口要做到向下兼容：接口参数尽量以对象形式进行封装。Model属性只增不删，如果需要作废，可以添加@Deprecated 标识。 如果出现了不可兼容的变更，则必须通知调用方整改，并制定上线计划。 开发联调： 在项目开发过程当中，一个开发或测试环境的注册中心很有可能会同时承载着多个服务，如果两组服务正在联调，如何保证调用的是目标服务呢？ 1、基于临时分组联调 group 分组 在reference 和server 当中采用相同的临时组 ,通过group 进行设置 2、直连提供者： 在reference 中指定提供者的url即可做到直连 3、只注册： 一个项目有可能同是为即是服务提供者又消费者，在测试时需要调用某一服务同时又不希望正在开发的服务影响到其它订阅者如何实现？ 通过修改 register=false 即可实现 二、Dubbo控制管理后台使用 Dubbo 控制后台版本说明： dubbo 在2.6.0 以前 使用dubbo-admin 作为管理后台，2.6 以后已经去掉dubbo-admin 并采用 incubator-dubbo-ops 作为新的管理后台，目前该后台还在开发中还没有发布正式的版本 ，所以本节课还是采用的旧版的dubbo-admin 来演示。 Dubbo 控制后台的安装： #从github 中下载dubbo 项目 git clone https://github.com/apache/incubator-dubbo.git #更新项目 git fetch #临时切换至 dubbo-2.5.8 版本 git checkout dubbo-2.5.8 #进入 dubbo-admin 目录 cd dubbo-admin #mvn 构建admin war 包 mvn clean pakcage -DskipTests #得到 dubbo-admin-2.5.8.war 即可直接部署至Tomcat #修改 dubbo.properties 配置文件 dubbo.registry.address=zookeeper://127.0.0.1:2181 注：如果实在懒的构建 可直接下载已构建好的： 链接：https://pan.baidu.com/s/1zJFNPgwNVgZZ-xobAfi5eQ 提取码：gjtv 控制后台基本功能介绍 ： 服务查找： 服务关系查看: 服务权重调配： 服务路由： 服务禁用 三、Dubbo注册中心详解 注册中心的作用 为了到达服务集群动态扩容的目的，注册中心存储了服务的地址信息与可用状态信息，并实时推送给订阅了相关服务的客户端。 一个完整的注册中心需要实现以下功能： 接收服务端的注册与客户端的引用，即将引用与消费建立关联，并支持多对多。 当服务非正常关闭时能即时清除其状态 当注册中心重启时，能自动恢复注册数据，以及订阅请求 注册中心本身的集群 Dubbo所支持的注册中心 Multicast 注册中心 基于组网广播技术，只能用在局域网内，一般用于简单的测试服务 Zookeeper 注册中心(**推荐**) Zookeeper 是 Apacahe Hadoop 的子项目，是一个树型的目录服务，支持变更推送，适合作为 Dubbo 服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 Redis 注册中心 基于Redis的注册中心 Simple 注册中心 基于本身的Dubbo服务实现（SimpleRegistryService），不支持集群可作为自定义注册中心的参考，但不适合直接用于生产环境。 Redis 注册中心 关于Redis注册中心我们需要了解两点， 如何存储服务的注册与订阅关系 是当服务状态改变时如何即时更新 演示使用Redis 做为注册中心的使用。 [ ] 启动Redis服务 [ ] 服务端配置注册中心 [ ] 启动两个服务端 [ ] 通过RedisClient 客户端观察Redis中的数据 redis 注册中心配置： 当我们启动两个服务端后发现，Reids中增加了一个Hash 类型的记录，其key为/dubbo/tuling.dubbo.server.UserService/providers。Value中分别存储了两个服务提供者的URL和有效期。 同样消费者也是类似其整体结构如下： //服务提供者注册信息 /dubbbo/com.tuling.teach.service.DemoService/providers dubbo://192.168.246.1:20880/XXX.DemoService=1542619052964 dubbo://192.168.246.2:20880/XXX.DemoService=1542619052964 //服务消费订阅信息 /dubbbo/com.tuling.teach.service.DemoService/consumers dubbo://192.168.246.1:20880/XXX.DemoService=1542619788641 主 Key 为服务名和类型 Map 中的 Key 为 URL 地址 Map 中的 Value 为过期时间，用于判断脏数据，脏数据由监控中心删除 接下来回答第二个问题 当提供者突然 宕机状态能即里变更吗？ 这里Dubbo采用的是定时心跳的机制 来维护服务URL的有效期，默认每30秒更新一次有效期。即URL对应的毫秒值。具体代码参见：com.alibaba.dubbo.registry.redis.RedisRegistry#expireExecutor com.alibaba.dubbo.registry.redis.RedisRegistry#deferExpired com.alibaba.dubbo.registry.integration.RegistryDirectory com.alibaba.dubbo.registry.support.ProviderConsumerRegTable Zookeeper 注册中心 关于Zookeeper 注册中心同样需要了解其存储结构和更新机制。 Zookeper是一个树型的目录服务，本身支持变更推送相比redis的实现Publish/Subscribe功能更稳定。 结构： 失败重连 com.alibaba.dubbo.registry.support.FailbackRegistry 提供者突然断开： 基于Zookeeper 临时节点机制实现，在客户端会话超时后 Zookeeper会自动删除所有临时节点，默认为40秒。 // 创建临时节点 com.alibaba.dubbo.remoting.zookeeper.curator.CuratorZookeeperClient#createEphemeral 提问： 在zookeeper 断开的40秒内 如果 有客户端加入 会调用 已失效的提供者连接吗？ 答：不会，提供者宕机后 ，其与客户端的链接也随即断开，客户端在调用前会检测长连接状态。 // 检测连接是否有效 com.alibaba.dubbo.rpc.protocol.dubbo.DubboInvoker#isAvailable 创建 configurators与routers 会创建持久节点 // 创建持久节点 com.alibaba.dubbo.remoting.zookeeper.curator.CuratorZookeeperClient#createPersistent 服务订阅机制实现： // 注册目录 com.alibaba.dubbo.registry.integration.RegistryDirectory 源码解析： com.alibaba.dubbo.registry.integration.RegistryDirectory Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-module-detail.html":{"url":"distributed/dubbo/dubbo-module-detail.html","title":"2.Dubb调用模块详解","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、Dubbo 调用模块基本组成 Dubbo调用模块概述： 透明代理： 负载均衡 TODO 一至性hash 演示 容错 异步调用 过滤器 TODO 演示添加日志访问过滤: 二 、Dubbo 调用非典型使用场景 泛化提供&引用 TODO 示例演示 隐示传参 令牌验证 三、调用通信内部实现源码分析 网络传输的实现组成 Dubbo 长连接实现与配置 dubbo传输uml类图: Dubbo 传输协作线程 概要： 一、Dubbo 调用模块基本组成 二 、Dubbo 调用非典型使用场景 三、调用通信内部实现源码分析 一、Dubbo 调用模块基本组成 Dubbo调用模块概述： dubbo调用模块核心功能是发起一个远程方法的调用并顺利拿到返回结果，其体系组成如下： 透明代理：通过动态代理技术，屏蔽远程调用细节以提高编程友好性。 负载均衡：当有多个提供者是，如何选择哪个进行调用的负载算法。 容错机制：当服务调用失败时采取的策略 调用方式：支持同步调用、异步调用 透明代理： 参见源码： com.alibaba.dubbo.config.ReferenceConfig#createProxy com.alibaba.dubbo.common.bytecode.ClassGenerator com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory 负载均衡 Dubbo 目前官方支持以下负载均衡策略： 随机(random)：按权重设置随机概率。此为默认算法. 轮循 (roundrobin):按公约后的权重设置轮循比率。 最少活跃调用数(leastactive):相同活跃数的随机，活跃数指调用前后计数差。 一致性Hash(consistenthash ):相同的参数总是发到同一台机器 设置方式支持如下四种方式设置，优先级由低至高 TODO 一至性hash 演示 [ ] 配置loadbalance [ ] 配置需要hash 的参数与虚拟节点数 [ ] 发起远程调用 一至性hash 算法详解： 容错 Dubbo 官方目前支持以下容错策略： 失败自动切换：调用失败后基于retries=“2” 属性重试其它服务器 快速失败：快速失败，只发起一次调用，失败立即报错。 勿略失败：失败后勿略，不抛出异常给客户端。 失败重试：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作 并行调用: 只要一个成功即返回，并行调用指定数量机器，可通过 forks=\"2\" 来设置最大并行数。 广播调用：广播调用所有提供者，逐个调用，任意一台报错则报错 设置方式支持如下两种方式设置，优先级由低至高 注：容错机制 在基于 API设置时无效 如 referenceConfig.setCluster(\"failback\"); 经测试不启作用 异步调用 异步调用是指发起远程调用之后获取结果的方式。 同步等待结果返回（默认） 异步等待结果返回 不需要返回结果 Dubbo 中关于异步等待结果返回的实现流程如下图： 异步调用配置: 注：在进行异步调用时 容错机制不能为 cluster=\"forking\" 或 cluster=\"broadcast\" 异步获取结果演示： [ ] 编写异步调用代码 [ ] 编写同步调用代码 [ ] 分别演示同步调用与异步调用耗时 异步调用结果获取Demo demoService.sayHello1(\"han\"); Future future1 = RpcContext.getContext().getFuture(); demoService.sayHello2(\"han2\"); Future future2 = RpcContext.getContext().getFuture(); Object r1 = null, r2 = null; // wait 直到拿到结果 获超时 r1 = future1.get(); // wait 直到拿到结果 获超时 r2 = future2.get(); 过滤器 类似于 WEB 中的Filter ，Dubbo本身提供了Filter 功能用于拦截远程方法的调用。其支持自定义过滤器与官方的过滤器使用： TODO 演示添加日志访问过滤: 以上配置 就是 为 服务提供者 添加 日志记录过滤器， 所有访问日志将会集中打印至 accesslog 当中 二 、Dubbo 调用非典型使用场景 泛化提供&引用 泛化提供 是指不通过接口的方式直接将服务暴露出去。通常用于Mock框架或服务降级框架实现。 TODO 示例演示 public static void doExportGenericService() { ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName(\"demo-provider\"); // 注册中心 RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setProtocol(\"zookeeper\"); registryConfig.setAddress(\"192.168.0.147:2181\"); ProtocolConfig protocol=new ProtocolConfig(); protocol.setPort(-1); protocol.setName(\"dubbo\"); GenericService demoService = new MyGenericService(); ServiceConfig service = new ServiceConfig(); // 弱类型接口名 service.setInterface(\"com.tuling.teach.service.DemoService\"); // 指向一个通用服务实现 service.setRef(demoService); service.setApplication(applicationConfig); service.setRegistry(registryConfig); service.setProtocol(protocol); // 暴露及注册服务 service.export(); } 泛化引用 是指不通过常规接口的方式去引用服务，通常用于测试框架。 ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName(\"demo-provider\"); // 注册中心 RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setProtocol(\"zookeeper\"); registryConfig.setAddress(\"192.168.0.147:2181\"); // 引用远程服务 ReferenceConfig reference = new ReferenceConfig(); // 弱类型接口名 reference.setInterface(\"com.tuling.teach.service.DemoService\"); // 声明为泛化接口 reference.setGeneric(true); reference.setApplication(applicationConfig); reference.setRegistry(registryConfig); // 用com.alibaba.dubbo.rpc.service.GenericService可以替代所有接口引用 GenericService genericService = reference.get(); Object result = genericService.$invoke(\"sayHello\", new String[]{\"java.lang.String\"}, new Object[]{\"world\"}); 隐示传参 是指通过非常方法参数传递参数，类似于http 调用当中添加cookie值。通常用于分布式追踪框架的实现。使用方式如下 ： //客户端隐示设置值 RpcContext.getContext().setAttachment(\"index\", \"1\"); // 隐式传参，后面的远程调用都会隐 //服务端隐示获取值 String index = RpcContext.getContext().getAttachment(\"index\"); 令牌验证 通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者，另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者 使用： 三、调用通信内部实现源码分析 网络传输的实现组成 IO模型： BIO 同步阻塞 NIO 同步非阻塞 AIO 异步非阻塞 连接模型： 长连接 短连接 线程分类： IO线程 服务端业务线程 客户端调度线程 客户端结果exchange线程。 保活心跳线程 重连线程 线程池模型： 固定数量线程池 缓存线程池 有限线程池Dubbo 长连接实现与配置 初始连接： 引用服务增加提供者==>获取连接===》是否获取共享连接==>创建连接客户端==》开启心跳检测状态检查定时任务===》开启连接状态检测 源码见：com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol#getClients 心跳发送： 在创建一个连接客户端同时也会创建一个心跳客户端，客户端默认基于60秒发送一次心跳来保持连接的存活，可通过 heartbeat 设置。 源码见：com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeClient#startHeatbeatTimer 断线重连： 每创建一个客户端连接都会启动一个定时任务每两秒中检测一次当前连接状态，如果断线则自动重连。 源码见：com.alibaba.dubbo.remoting.transport.AbstractClient#initConnectStatusCheckCommand 连接销毁: 基于注册中心通知，服务端断开后销毁 源码见：com.alibaba.dubbo.remoting.transport.AbstractClient#close() dubbo传输uml类图: Dubbo 传输协作线程 客户端调度线程：用于发起远程方法调用的线程。 客户端结果**Exchange**线程：当远程方法返回response后由该线程填充至指定ResponseFuture，并叫醒等待的调度线程。 客户端IO线程：由传输框架实现，用于request 消息流发送、response 消息流读取与解码等操作。 服务端IO线程：由传输框架实现，用于request消息流读取与解码 与Response发送。 业务执行线程：服务端具体执行业务方法的线程 客户端线程协作流程： 调度线程 调用远程方法 对request 进行协议编码 发送request 消息至IO线程 等待结果的获取 IO线程 读取response流 response 解码 提交Exchange 任务 Exchange线程 填写response值 至 ResponseFuture 唤醒调度线程，通知其获取结果 调用调试： 客户端的执行线程: 1、业务线程 1) DubboInvoker#doInvoke(隐示传公共参数、获取客户端、异步、单向、同步（等待返回结果）) 2)AbstractPeer#send// netty Client客户端发送消息 写入管道 3)DubboCodec#encodeRequestData // Request 协议编码 2、IO线程 DubboCodec#decodeBody //Response解码 AllChannelHandler#received //// 派发消息处理线程 3、调度线程 DefaultFuture#doReceived // 设置返回结果 服务端线程协作： IO线程： request 流读取 request 解码 提交业务处理任务 业务线程： 业务方法执行 response 编码 回写结果至channel 线程池 fixed：固定线程池,此线程池启动时即创建固定大小的线程数，不做任何伸缩， cached：缓存线程池,此线程池可伸缩，线程空闲一分钟后回收，新请求重新创建线程 Limited：有限线程池,此线程池一直增长，直到上限，增长后不收缩。 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/dubbo/dubbo-proto-detail.html":{"url":"distributed/dubbo/dubbo-proto-detail.html","title":"3.Dubbo协议模块源码剖析","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 RPC协议基本组成 RPC 协议名词解释 协议基本组成： Dubbo中所支持RPC协议使用 协议的使用与配置: TODO 演示采用其它协议来配置Dubbo Hessian 序列化： 三 、RPC协议报文编码与实现详解 RPC 传输实现： 拆包与粘包产生的原因： 拆包与粘包解决办法： Dubbo 协议报文编码： Dubbo协议的编解码过程： 主讲：鲁班 时间：2018/12/2 8:10 地址：腾讯课堂图灵学院 课程概要： RPC协议基本组成 RPC协议报文编码与实现详解 Dubbo中所支持RPC协议与使用 RPC协议基本组成 RPC 协议名词解释 在一个典型RPC的使用场景中，包含了服务发现、负载、容错、网络传输、序列化等组件，其中RPC协议就指明了程序如何进行网络传输和序列化 。也就是说一个RPC协议的实现就等于一个非透明的远程调用实现，如何做到的的呢？ 协议基本组成： 地址：服务提供者地址 端口：协议指定开放的端口 报文编码：协议报文编码 ，分为请求头和请求体两部分。 序列化方式：将请求体序列化成对象 Hessian2Serialization、 DubboSerialization、 JavaSerialization JsonSerialization 运行服务: 网络传输实现 netty mina RMI 服务 servlet 容器（jetty、Tomcat、Jboss） Dubbo中所支持RPC协议使用 dubbo 支持的RPC协议列表 | 名称 | 实现描述 | 连接描述 | 适用场景 | |:----|:----|:----|:----| | dubbo | 传输服务: mina, netty(默认), grizzy序列化: hessian2(默认), java, fastjson自定义报文 | 单个长连接NIO异步传输 | 1、常规RPC调用2、传输数据量小3、提供者少于消费者 | | rmi | 传输：java rmi 服务序列化：java原生二进制序列化 | 多个短连接BIO同步传输 | 1、常规RPC调用2、与原RMI客户端集成3、可传少量文件4、不支持防火墙穿透 | | hessian | 传输服务：servlet容器序列化：hessian二进制序列化 | 基于Http 协议传输，依懒servlet容器配置 | 1、提供者多于消费者2、可传大字段和文件3、跨语言调用 | | http | 传输服务：servlet容器序列化：java原生二进制序列化 | 依懒servlet容器配置 | 1、数据包大小混合 | | thrift | 与thrift RPC 实现集成，并在其基础上修改了报文头 | 长连接、NIO异步传输 | | 关于RMI不支持防火墙穿透的补充说明： 原因在于RMI 底层实现中会有两个端口，一个是固定的用于服务发现的注册端口，另外会生成一个随机端口用于网络传输。因为这个随机端口就不能在防火墙中提前设置开放开。所以存在防火墙穿透问题 协议的使用与配置: Dubbo框架配置协议非常方便，用户只需要在 provider 应用中 配置 元素即可。 TODO 演示采用其它协议来配置Dubbo [ ] dubbo 协议采用 json 进行序列化 (源码参见：com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol) [ ] 采用RMI协议 (源码参见：com.alibaba.dubbo.rpc.protocol.rmi.RmiProtocol) [ ] 采用Http协议 (源码参见：com.alibaba.dubbo.rpc.protocol.http.HttpProtocol.InternalHandler) [ ] 采用Heason协议 (源码参见:com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol.HessianHandler) netstat -aon|findstr \"17732\" 序列化： | | 特点 | |:----|:----| | fastjson | 文本型：体积较大，性能慢、跨语言、可读性高 | | fst | 二进制型：体积小、兼容 JDK 原生的序列化。要求 JDK 1.7 支持。 | | hessian2 | 二进制型：跨语言、容错性高、体积小 | | java | 二进制型：在JAVA原生的基础上 可以写入Null | | compactedjava | 二进制型：与java 类似，内容做了压缩 | | nativejava | 二进制型：原生的JAVA 序列化 | | kryo | 二进制型：体积比hessian2 还要小，但容错性 没有hessian2 好 | Hessian 序列化： 参数及返回值需实现 Serializable 接口 参数及返回值不能自定义实现 List, Map, Number, Date, Calendar 等接口，只能用 JDK 自带的实现，因为 hessian 会做特殊处理，自定义实现类中的属性值都会丢失。 Hessian 序列化，只传成员属性值和值的类型，不传方法或静态变量，兼容情况 [1][2]： | 数据通讯 | 情况 | 结果 | |:----|:----|:----| | A->B | 类A多一种 属性（或者说类B少一种 属性） | 不抛异常，A多的那 个属性的值，B没有， 其他正常 | | A->B | 枚举A多一种 枚举（或者说B少一种 枚举），A使用多 出来的枚举进行传输 | 抛异常 | | A->B | 枚举A多一种 枚举（或者说B少一种 枚举），A不使用 多出来的枚举进行传输 | 不抛异常，B正常接 收数据 | | A->B | A和B的属性 名相同，但类型不相同 | 抛异常 | | A->B | serialId 不相同 | 正常传输 | 接口增加方法，对客户端无影响，如果该方法不是客户端需要的，客户端不需要重新部署。输入参数和结果集中增加属性，对客户端无影响，如果客户端并不需要新属性，不用重新部署。 输入参数和结果集属性名变化，对客户端序列化无影响，但是如果客户端不重新部署，不管输入还是输出，属性名变化的属性值是获取不到的。 总结：服务器端和客户端对领域对象并不需要完全一致，而是按照最大匹配原则。 [ ] 演示Hession2 序列化的容错性 三 、RPC协议报文编码与实现详解 RPC 传输实现： RPC的协议的传输是基于 TCP/IP 做为基础使用Socket 或Netty、mina等网络编程组件实现。但有个问题是TCP是面向字节流的无边边界协议，其只管负责数据传输并不会区分每次请求所对应的消息，这样就会出现TCP协义传输当中的拆包与粘包问题 拆包与粘包产生的原因： 我们知道tcp是以流动的方式传输数据，传输的最小单位为一个报文段（segment）。tcp Header中有个Options标识位，常见的标识为mss(Maximum Segment Size)指的是，连接层每次传输的数据有个最大限制MTU(Maximum Transmission Unit)，一般是1500比特，超过这个量要分成多个报文段，mss则是这个最大限制减去TCP的header，光是要传输的数据的大小，一般为1460比特。换算成字节，也就是180多字节。 tcp为提高性能，发送端会将需要发送的数据发送到缓冲区，等待缓冲区满了之后，再将缓冲中的数据发送到接收方。同理，接收方也有缓冲区这样的机制，来接收数据。这时就会出现以下情况： 应用程序写入的数据大于MSS大小，这将会发生拆包。 应用程序写入数据小于MSS大小，这将会发生粘包。 接收方法不及时读取套接字缓冲区数据，这将发生粘包。拆包与粘包解决办法： 设置定长消息，服务端每次读取既定长度的内容作为一条完整消息。 {\"type\":\"message\",\"content\":\"hello\"}\\n 使用带消息头的协议、消息头存储消息开始标识及消息长度信息，服务端获取消息头的时候解析出消息长度，然后向后读取该长度的内容。 比如：Http协议 heade 中的 Content-Length 就表示消息体的大小。 (注①：http 报文编码) Dubbo 协议报文编码： 注②Dubbo 协议报文编码： | | 0-7 | 8-15 | 16-20 | 21 | 22 | 23 | 24-31 | | |:----|:----|:----|:----|:----|:----|:----|:----|:----| | | | 1 | 1 | | | | | | | 32-95 | | | | | | | | | | 96-127 | | | | | | | | | magic：类似java字节码文件里的魔数，用来判断是不是dubbo协议的数据包。魔数是常量0xdabb,用于判断报文的开始。 flag：标志位, 一共8个地址位。低四位用来表示消息体数据用的序列化工具的类型（默认hessian），高四位中，第一位为1表示是request请求，第二位为1表示双向传输（即有返回response），第三位为1表示是心跳ping事件。 status：状态位, 设置请求响应状态，dubbo定义了一些响应的类型。具体类型见 com.alibaba.dubbo.remoting.exchange.Response invoke id：消息id, long 类型。每一个请求的唯一识别id（由于采用异步通讯的方式，用来把请求request和返回的response对应上） body length：消息体 body 长度, int 类型，即记录Body Content有多少个字节。 （注：相关源码参见 com.alibaba.dubbo.rpc.protocol.dubbo.DubboCodec**） Dubbo协议的编解码过程： Dubbo 协议编解码实现过程 (源码来源于**dubbo2.5.8 ) 1、DubboCodec.encodeRequestData() 116L // 编码request 2、DecodeableRpcInvocation.decode() 89L // 解码request 3、DubboCodec.encodeResponseData() 184L // 编码response 4、DecodeableRpcResult.decode() 73L // 解码response Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rocketmq/":{"url":"distributed/rocketmq/","title":"rocketmq","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/rabbitmq/":{"url":"distributed/rabbitmq/","title":"rabbitmq","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/kafka/":{"url":"distributed/kafka/","title":"kafka","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mysql/":{"url":"distributed/mysql/","title":"mysql","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/":{"url":"distributed/mongo/","title":"mongo","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/mongoDb-data.html":{"url":"distributed/mongo/mongoDb-data.html","title":"1.准备数据","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.职工信息 2. 学生信息 3. 学生科目 4. 课程项目 1.职工信息 db.emp.insert([ {_id:1101,name:'鲁班' ,job:'讲师' ,dep:'讲师部',salary:10000}, {_id:1102,name:'悟空' ,job:'讲师' ,dep:'讲师部',salary:10000}, {_id:1103,name:'诸葛' ,job:'讲师' ,dep:'讲师部',salary:10000}, {_id:1105,name:'赵云' ,job:'讲师' ,dep:'讲师部',salary:8000}, {_id:1106,name:'韩信',job:'校长' ,dep:'校办',salary:20000}, {_id:1107,name:'貂蝉' ,job:'班主任' ,dep:'客服部',salary:8000}, {_id:1108,name:'安其' ,job:'班主任' ,dep:'客服部',salary:8000}, {_id:1109,name:'李白' ,job:'教务' ,dep:'教务处',salary:8000}, {_id:1110,name:'默子' ,job:'教务',dep:'教务处',salary:8000}, {_id:1111,name:'大乔',job:'助教' ,dep:'客服部',salary:5000}, {_id:1112,name:'小乔' ,job:'助教' ,dep:'客服部',salary:3000}, ]); 2. 学生信息 db.student.insertMany([ {_id:\"001\",name:\"陈霸天\",age:5,grade:{redis:87,zookeper:85,dubbo:90}}, {_id:\"002\",name:\"张明明\",age:3,grade:{redis:86,zookeper:82,dubbo:59}}, {_id:\"003\",name:\"肖炎炎\",age:2,grade:{redis:81,zookeper:94,dubbo:88}}, {_id:\"004\",name:\"李鬼才\",age:6,grade:{redis:48,zookeper:87,dubbo:48}} ]) 3. 学生科目 db.subject.insertMany([ {_id:\"001\",name:\"陈霸天\",subjects:[\"redis\",\"zookeper\",\"dubbo\"]}, {_id:\"002\",name:\"张明明\",subjects:[\"redis\",\"Java\",\"mySql\"]}, {_id:\"003\",name:\"肖炎炎\",subjects:[\"mySql\",\"zookeper\",\"bootstrap\"]}, {_id:\"004\",name:\"李鬼才\",subjects:[\"Java\",\"dubbo\",\"Java\"]}, ]) db.subject2.insertMany([ {_id:\"001\",name:\"陈霸天\",subjects:[{name:\"redis\",hour:12},{name:\"dubbo\",hour:120},{name:\"zookeper\",hour:56}]}, {_id:\"002\",name:\"张明明\",subjects:[{name:\"java\",hour:120},{name:\"mysql\",hour:10},{name:\"oracle\",hour:30}]}, {_id:\"003\",name:\"肖炎炎\",subjects:[{name:\"mysql\",hour:12},{name:\"html5\",hour:120},{name:\"netty\",hour:56}]}, {_id:\"004\",name:\"李鬼才\",subjects:[{name:\"redis\",hour:12},{name:\"dubbo\",hour:120},{name:\"netty\",hour:56}]} ]) 4. 课程项目 db.project.insert([ { _id: 1, name: \"Java Script\", description: \"name is js and jquery\" }, { _id: 2, name: \"Git\", description: \"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency\" }, { _id: 3, name: \"Apache dubbo\", description: \"Apache Dubbo is a high-performance, java based open source RPC framework.阿里 开源 项目\" }, { _id: 4, name: \"Redis\", description: \"Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures\" }, { _id: 5, name: \"Apache ZooKeeper\", description: \"Apache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination\" } ]) Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/mongoDb-quick-start.html":{"url":"distributed/mongo/mongoDb-quick-start.html","title":"2.快速上手","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、MongoDb的体系结构 1、NoSql的概念 2、NoSql的应用场景 3、MongoDb的逻辑组成 二、MongoDb安装配置与基础命令 2.mongoDb启动参数说明 3.客户端Shell 的使用及参数说明 4.数据库与集合的基础操作 三、MongoDB CRUD与全文索引 2、数据的查询 排序与分页： 修改 3、数据的修改与删除 4、全文索引 大纲： 1、MongoDb的体系结构 2、MongoDb安装配置与基础命令 3、MongoDB CRUD与全文索引 数据脚本.txt 一、MongoDb的体系结构 概要： NoSql的概念 NoSql的应用场景 MongoDb的逻辑组成 1、NoSql的概念 NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是 [SQL](https://baike.baidu.com/item/SQL) ”，互联网的早期我们的数据大多以关系型数据库来存储的。其特点是规范的数据结构（预定义模式）、强一至性、表与表之间通过外键进行关联，这些特征使我们对数据的管理更加清晰和严谨，但随着互联网的发展数据成爆炸式的增长我们对数据库需要更好的灵活性和更快的速度。这就是NoSql可以做到的。它不需要预先定义模式，没有主外键关联、支持分片、支持复本。 NoSql的分类： 键值(Key-Value)存储数据库 这一类数据库主要会使用到一个哈希表，这个表中有一个特定的键和一个指针指向特定的数据。Key/value模型对于IT系统来说的优势在于简单、易部署。但是如果DBA只对部分值进行查询或更新的时候，Key/value就显得效率低下了。举例如：Tokyo Cabinet/Tyrant, Redis, Voldemort, Oracle BDB. 列存储数据库。 这部分数据库通常是用来应对分布式存储的海量数据。键仍然存在，但是它们的特点是指向了多个列。这些列是由列家族来安排的。如：Cassandra, HBase, Riak. 文档型数据库 文档型数据库的灵感是来自于Lotus Notes办公软件的，而且它同第一种键值存储相类似。该类型的数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可 以看作是键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高。如：CouchDB, MongoDb. 国内也有文档型数据库SequoiaDB，已经开源。 图形(Graph)数据库 图形结构的数据库同其他行列以及刚性结构的SQL数据库不同，它是使用灵活的图形模型，并且能够扩展到多个服务器上。NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型。许多NoSQL数据库都有REST式的数据接口或者查询API。如：Neo4J, InfoGrid, Infinite Graph. 2、NoSql的应用场景 NoSQL数据库在以下的这几种情况下比较适用： 1、数据模型比较简单； 2、需要灵活性更强的IT系统； 3、对数据库性能要求较高； 4、不需要高度的数据一致性； [ ] 基于豆瓣电影举例说明NoSQL的应用场景 [ ] 电影基本信息分析 [ ] 电影与明星关系存储 3、MongoDb的逻辑组成 体系结构： 逻辑结构与关系数据库的对比： | 关系型数据库 | MongoDb | |:----|:----| | database(数据库) | database（数据库） | | table （表） | collection（ 集合） | | row（ 行） | document（ BSON 文档） | | column （列） | field （字段） | | index（唯一索引、主键索引） | index （全文索引） | | join （主外键关联） | embedded Document (嵌套文档) | | primary key(指定1至N个列做主键) | primary key (指定_id field做为主键) | | aggreation(groupy) | aggreation (pipeline mapReduce) | 二、MongoDb安装配置与基础命令 概要： mongoDb版本说明 mongoDb启动参数说明 客户端Shell 的使用及参数说明 数据库与集合的基础操作 mongoDb社区版说明 下载地址：https://www.mongodb.com/download-center/community #下载 wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.5.tgz # 解压 tar -zxvf mongodb-linux-x86_64-4.0.5.tgz 2.mongoDb启动参数说明 mongoDb 由C++编写，下载下来的包可以直接启动 #创建数据库目录 mkdir -p /data/mongo # 启动mongo ./bin/mongod --dbpath=/data/mongo/ 常规参数 | 参数 | 说明 | |:----|:----| | dbpath | 数据库目录，默认/data/db | | bind_ip | 监听IP地址，默认全部可以访问 | | port | 监听的端口，默认27017 | | logpath | 日志路径 | | logappend | 是否追加日志 | | auth | 是开启用户密码登陆 | | fork | 是否已后台启动的方式登陆 | | config | 指定配置文件 | 配置文件示例 vim mongo.conf 内容： dbpath=/data/mongo/ port=27017 bind_ip=0.0.0.0 fork=true logpath = /data/mongo/mongodb.log logappend = true auth=false 已配置文件方式启动 ./bin/mongod -f mongo.conf 3.客户端Shell 的使用及参数说明 #启动客户端 连接 本机的地的默认端口 ./bin/mongo # 指定IP和端口 ./bin/mongo --host=127.0.0.1 --port=27017 mongo shell 是一个js 控台，可以执行js 相关运算如: > 1+1 2 > var i=123; > print(i) 123 > 4.数据库与集合的基础操作 #查看数据库 show dbs; #切换数据库 use luban; #创建数据库与集合，在插入数据时会自动 创建数据库与集和 db.friend.insertOne({name:\"wukong\"，sex:\"man\"}); #查看集合 show tables; show collections; #删除集合 db.friend.drop(); #删除数据库 db.dropDatabase(); 三、MongoDB CRUD与全文索引 概要： 数据的新增的方式 数据的查询 数据的修改删除 全文索引查询 数据的新增的方式 关于Mongodb数据插入的说明 数据库的新增不需要序先设计模型结构，插入数据时会自动创建。 同一个集合中不同数据字段结构可以不一样 插入相关方法： //插入单条 db.friend.insertOne({name:\"wukong\"，sex:\"man\"}); // 插入多条 db.friend.insertMany([ {name:\"wukong\",sex:\"man\"},{name:\"diaocan\",sex:\"woman\",age:18,birthday:new Date(\"1995-11-02\")},{name:\"zixiao\",sex:\"woman\"} ]); // 指定ID db.friend.insert([ {_id:1,name:\"wokong\",sex:\"man\",age:1}, {_id:2,name:\"diaocan\",sex:\"women\",birthday:new Date(\"1988-11- 11\")} ]) 2、数据的查询 概要： 基于条件的基础查询 $and、$or、$in、$gt、$gte、$lt、$lte 运算符 基于 sort skip limit 方法实现排序与分页 嵌套查询 数组查询 数组嵌套查询 基础查询： #基于ID查找 db.emp.find({_id:1101}) #基于属性查找 db.emp.find({\"name\":\"鲁班\"}) # && 运算 与大于 运算 db.emp.find({\"job\":\"讲师\",\"salary\":{$gt:8000}}) # in 运算 db.emp.find({\"job\":{$in:[\"讲师\",\"客服部\"]}}) # or 运算 db.emp.find({$or:[{job:\"讲师\" },{job:\"客服部\"}] }) 排序与分页： // sort skip limit db.emp.find().sort({dep:1,salary:-1}).skip(5).limit(2) 嵌套查询： # 错误示例：无结果 db.student.find({grade:{redis:87,dubbo:90 }); #错误示例：无结果 db.student.find({grade:{redis:87,dubbo:90,zookeper:85} }) # 基于复合属性查找 时必须包含其所有的值 并且顺序一至 db.student.find({grade:{redis:87,zookeper:85,dubbo:90} }) #基于复合属性当中的指定值 查找。注：名称必须用双引号 db.student.find({\"grade.redis\":87}); db.student.find({\"grade.redis\":{\"$gt\":80}}); 数组查询： db.subject.insertMany([ {_id:\"001\",name:\"陈霸天\",subjects:[\"redis\",\"zookeper\",\"dubbo\"]}, {_id:\"002\",name:\"张明明\",subjects:[\"redis\",\"Java\",\"mySql\"]}, {_id:\"003\",name:\"肖炎炎\",subjects:[\"mySql\",\"zookeper\",\"bootstrap\"]}, {_id:\"004\",name:\"李鬼才\",subjects:[\"Java\",\"dubbo\",\"Java\"]}, ]) #无结果 db.subject.find({subjects:[\"redis\",\"zookeper\"]}) #无结果 db.subject.find({subjects:[\"zookeper\",\"redis\",\"dubbo\"]}) # 与嵌套查询一样，必须是所有的值 并且顺序一至 db.subject.find({subjects:[\"redis\",\"zookeper\",\"dubbo\"]}) # $all 匹配数组中包含该两项的值。注：顺序不作要求 db.subject.find({subjects:{\"$all\": [\"redis\",\"zookeper\"]}}) 注： # 简化数组查询 db.subject.find({subjects:\"redis\"}) # 简化数组查询 ，匹配数组中存在任意一值。与$all相对应 db.subject.find({subjects:{$in: [\"redis\",\"zookeper\"]}}) 数组嵌套查询： #基础查询 ，必须查询全部，且顺序一至 db.subject2.find({subjects:{name:\"redis\",hour:12} }) #指定查询第一个数组 课时大于12 db.subject2.find({\"subjects.0.hour\":{$gt:12}}) #查询任科目 课时大于12 db.subject2.find({\"subjects.hour\":{$gt:12}}) # $elemMatch 元素匹配，指定属性满足，且不要求顺序一至 db.subject2.find({subjects:{$elemMatch:{name:\"redis\",hour:12}}}) # 数组中任意元素匹配 不限定在同一个对象当中 db.subject2.find({\"subjects.name\":\"mysql\",\"subjects.hour\":120}) 修改 #设置值 db.emp.update({_id:1101} ,{ $set:{salary:10300} }) #自增 db.emp.update({_id:1101} ,{ $inc:{salary:200}}) #基于条件 更新多条数据 # 只会更新第一条 db.emp.update({\"dep\":\"客服部\"},{$inc:{salary:100}}) # 更新所有 匹配的条件 db.emp.updateMany({\"dep\":\"客服部\"},{$inc:{salary:100}}) 3、数据的修改与删除 修改 #设置值 db.emp.update({_id:1101} ,{ $set:{salary:10300} }) #自增 db.emp.update({_id:1101} ,{ $inc:{salary:200}}) #基于条件 更新多条数据 # 只会更新第一条 db.emp.update({\"dep\":\"客服部\"},{$inc:{salary:100}}) # 更新所有 匹配的条件 db.emp.updateMany({\"dep\":\"客服部\"},{$inc:{salary:100}}) 删除： // 基于查找删除 db.emp.deleteOne({_id:1101}) // 删除整个集合 db.project.drop() // 删除库 db.dropDatabase() 4、全文索引 索引的创建 db.project.createIndex({name:\"text\",description:\"text\"}) 基于索引分词进行查询 db.project.find({$text:{$search:\"java jquery\"}}) 基于索引 短语 db.project.find({$text:{$search:\"\\\"Apache ZooKeeper\\\"\"}}) 过滤指定单词 db.project.find({$text:{$search:\"java apache -阿里\"}}) 查看执行计划 db.project.find({$text:{$search:\"java -阿里\"}}).explain(\"executionStats\") Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/mongo/mongoDb-enterprise.html":{"url":"distributed/mongo/mongoDb-enterprise.html","title":"3.企业应用","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 一、 mongoDB的聚合操作 1.pipeline 聚合 2.mapRedurce 聚合 3.在聚合中使用索引 二、mongodb 的主从复制机制 1.复制集群的架构 2.复制集群搭建基础示例 子节点配置2 3.复制集群选举操作 三、mongodb 中的分片机制 1.为什么需要分片？ 1.mongodb 中的分片架构 2.分片示例流程： 节点1 config1-37017.conf 节点2 config2-37018.conf 配置 shard 节点集群============== 节点 route-27017.conf 四、用户管理与数据集验证 概要： mongoDB的聚合操作 mongodb 集群：复制 mongodb 集群：分片 一、 mongoDB的聚合操作 知识点： pipeline 聚合 mapRedurce 聚合 在聚合中使用索引 1.pipeline 聚合 pipeline相关运算符： $match ：匹配过滤聚合的数据 $project：返回需要聚合的字段 $group：统计聚合数据 示例： # $match 与 $project使用 db.emp.aggregate( {$match:{\"dep\":{$eq:\"客服部\"}}}, {$project:{name:1,dep:1,salary:1}} ); # $group 与 $sum 使用 db.emp.aggregate( {$project:{dep:1,salary:1}}, {$group:{\"_id\":\"$dep\",total:{$sum:\"$salary\"}}} ); # 低于4000 忽略 db.emp.aggregate( {$match:{salary:{$gt:4000}}}, {$project:{dep:1,salary:1}}, {$group:{\"_id\":\"$dep\",total:{$sum:\"$salary\"}}} ); # 基于多个字段 进行组合group 部门+职位进行统计 db.emp.aggregate( {$project:{dep:1,job:1,salary:1}}, {$group:{\"_id\":{\"dep\":\"$dep\",\"job\":\"$job\"},total:{$sum:\"$salary\"}}} ); 二次过滤 db.emp.aggregate( {$project:{dep:1,job:1,salary:1}}, {$group:{\"_id\":{\"dep\":\"$dep\",\"job\":\"$job\"},total:{$sum:\"$salary\"}}}， {$match:{\"$total\":{$gt:10000}}} ); 2.mapRedurce 聚合 mapRedurce 说明： 为什么需要 MapReduce？ (1) 海量数据在单机上处理因为硬件资源限制，无法胜任 (2) 而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 (3) 引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理 mongodb中mapRedurce的使用流程 创建Map函数， 创建Redurce函数 将map、Redurce 函数添加至集合中，并返回新的结果集 查询新的结果集 示例操作 // 创建map 对象 var map1=function (){ emit(this.job,1); } // 创建reduce 对象 var reduce1=function(job,count){ return Array.sum(count); } // 执行mapReduce 任务 并将结果放到新的集合 result 当中 db.emp.mapReduce(map1,reduce1,{out:\"result\"}) // 查询新的集合 db.result.find() # 使用复合对象作为key var map2=function (){ emit({\"job\":this.job,\"dep\":this.dep},1); } var reduce2=function(key,values){ return values.length; } db.emp.mapReduce(map2,reduce2,{out:\"result2\"}).find() mapRedurce的原理 在map函数中使用emit函数添加指定的 key 与Value ，相同的key 将会发给Redurce进行聚合操作，所以Redurce函数中第二个参数 就是 所有集的数组。return 的显示就是聚合要显示的值。 3.在聚合中使用索引 通过$Math内 可以包合对$text 的运算 示例： db.project.aggregate( {$match:{$text:{$search:\"apache\"}}}, {$project:{\"name\":1,\"price\":1}}, {$group:{_id:\"$name\",price:{$sum:\"$price\"}}} ) 关于索引 除了全文索引之外，还有单键索引。即整个字段的值作为索引。单键索引用值1和-1表示，分别代表正序和降序索引。 示例： de 创建单键索引 db.emp.createIndex({\"dep\":1}) 查看基于索引的执行计划 db.emp.find({\"dep\":\"客服部\"}).explain() 除了单键索引外还可以创建联合索引如下： db.emp.createIndex({\"dep\":1,\"job\":-1}) 查看 复合索引的执行计划 db.emp.find({\"dep\":\"ddd\"}).explain() 查看索引在排序当中的使用 db.emp.find().sort({\"job\":-1,\"dep\":1}).explain() 二、mongodb 的主从复制机制 知识点： 复制集群的架构 复制集群搭建 复制集群的选举配置 1.复制集群的架构 2.复制集群搭建基础示例 主节点配置 dbpath=/data/mongo/master port=27017 fork=true logpath=master.log replSet=tulingCluster 从节点配置 dbpath=/data/mongo/slave port=27018 fork=true logpath=slave.log replSet=tulingCluster 子节点配置2 dbpath=/data/mongo/slave2 port=27019 fork=true logpath=slave2.log replSet=tulingCluster [ ] 分别启动三个节点 [ ] 进入其中一个节点 集群复制配置管理 #查看复制集群的帮助方法 rs.help() 添加配置 // 声明配置变量 var cfg ={\"_id\":\"tulingCluster\", \"members\":[ {\"_id\":0,\"host\":\"127.0.0.1:27017\"}, {\"_id\":1,\"host\":\"127.0.0.1:27018\"} ] } // 初始化配置 rs.initiate(cfg) // 查看集群状态 rs.status() 变更节点示例： // 插入新的复制节点 rs.add(\"127.0.0.1:27019\") // 删除slave 节点 rs.remove(\"127.0.0.1:27019\") [ ] 演示复制状态 [ ] 进入主节点客户端 [ ] 插入数据 [ ] 进入从节点查看数据 [ ] 尝试在从节点下插入数据 注：默认节点下从节点不能读取数据。调用 rs.slaveOk() 解决。 3.复制集群选举操作 为了保证高可用，在集群当中如果主节点挂掉后，会自动 在从节点中选举一个 重新做为主节点。 [ ] 演示节点的切换操作 [ ] kill 主节点 [ ] 进入从节点查看集群状态 。rs.status() 选举的原理： 在mongodb 中通过在 集群配置中的 rs.属性值大小来决定选举谁做为主节点，通时也可以设置arbiterOnly 为true 表示 做为裁判节点用于执行选举操作，该配置下的节点 永远不会被选举为主节点和从节点。 示例： 重新配置节点 var cfg ={\"_id\":\"tulingCluster\", \"protocolVersion\" : 1, \"members\":[ {\"_id\":0,\"host\":\"127.0.0.1:27017\",\"priority\":10}, {\"_id\":1,\"host\":\"127.0.0.1:27018\",\"priority\":2}, {\"_id\":2,\"host\":\"127.0.0.1:27019\",\"arbiterOnly\":true} ] } // 重新装载配置，并重新生成集群节点。 rs.reconfig(cfg) //重新查看集群状态 rs.status() 节点说明： PRIMARY 节点： 可以查询和新增数据 SECONDARY 节点：只能查询 不能新增 基于priority 权重可以被选为主节点 RBITER 节点： 不能查询数据 和新增数据 ，不能变成主节点 三、mongodb 中的分片机制 知识点： 分片的概念 mongodb 中的分片架构 分片示例 1.为什么需要分片？ 随着数据的增长，单机实例的瓶颈是很明显的。可以通过复制的机制应对压力，但mongodb中单个集群的 节点数量限制到了12个以内，所以需要通过分片进一步横向扩展。此外分片也可节约磁盘的存储。 1.mongodb 中的分片架构 分片中的节点说明： 路由节点(mongos)：用于分发用户的请求，起到反向代理的作用。 配置节点(config)：用于存储分片的元数据信息，路由节基于元数据信息 决定把请求发给哪个分片。（3.4版本之后，该节点，必须使用复制集。） 分片节点(shard):用于实际存储的节点，其每个数据块默认为64M，满了之后就会产生新的数据库。 2.分片示例流程： 配置 并启动config 节点集群 配置集群信息 配置并启动2个shard 节点 配置并启动路由节点 添加shard 节点 添加shard 数据库 添加shard 集合 插入测试数据 检查数据的分布 插入大批量数据查看shard 分布 设置shard 数据块为一M 插入10万条数据 配置 并启动config 节点集群 节点1 config1-37017.conf dbpath=/data/mongo/config1 port=37017 fork=true logpath=logs/config1.log replSet=configCluster configsvr=true 节点2 config2-37018.conf dbpath=/data/mongo/config2 port=37018 fork=true logpath=logs/config2.log replSet=configCluster configsvr=true 进入shell 并添加 config 集群配置： var cfg ={\"_id\":\"configCluster\", \"protocolVersion\" : 1, \"members\":[ {\"_id\":0,\"host\":\"127.0.0.1:37017\"}, {\"_id\":1,\"host\":\"127.0.0.1:37018\"} ] } // 重新装载配置，并重新生成集群。 rs.initiate(cfg) 配置 shard 节点集群============== # 节点1 shard1-47017.conf dbpath=/data/mongo/shard1 port=47017 fork=true logpath=logs/shard1.log shardsvr=true # 节点2 shard2-47018.conf dbpath=/data/mongo/shard2 port=47018 fork=true logpath=logs/shard2.log shardsvr=true 配置 路由节点 mongos ============== 节点 route-27017.conf port=27017 bind_ip=0.0.0.0 fork=true logpath=logs/route.log configdb=configCluster/127.0.0.1:37017,127.0.0.1:37018 // 添加分片节点 sh.status() sh.addShard(\"127.0.0.1:47017\"); sh.addShard(\"127.0.0.1:47018\"); 为数据库开启分片功能 sh.enableSharding(\"tuling\") 为指定集合开启分片功能 sh.shardCollection(\"tuling.emp\",{\"_id\":1}) 修改分片大小 use config db.settings.find() db.settings.save({_id:\"chunksize\",value:1}) 尝试插入1万条数据： for(var i=1;i 四、用户管理与数据集验证 // 创建管理员用户 use admin; db.createUser({\"user\":\"admin\",\"pwd\":\"123456\",\"roles\":[\"root\"]}) #验证用户信息 db.auth(\"admin\",\"123456\") #查看用户信息 db.getUsers() # 修改密码 db.changeUserPassword(\"admin\",\"123456\") 以auth 方式启动mongod，需要添加auth=true 参数 ，mongdb 的权限体系才会起作用： #以auth 方向启动mongod （也可以在mongo.conf 中添加auth=true 参数） ./bin/mongod -f conf/mongo.conf --auth # 验证用户 use admin; db.auth(\"admin\",\"123456\") 创建只读用户 db.createUser({\"user\":\"dev\",\"pwd\":\"123456\",\"roles\":[\"read\"]}) 重新登陆 验证用户权限 use luban ; db.auth(\"dev\",\"123456\") [ ] 演示查看数据 [ ] 演示插入数据 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/redis/":{"url":"distributed/redis/","title":"redis","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/shardingsphere/":{"url":"distributed/shardingsphere/","title":"shardingsphere","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"distributed/elk/":{"url":"distributed/elk/","title":"elk","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:18:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"微服务专题/":{"url":"微服务专题/","title":"五、微服务框架专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"项目实战专题/":{"url":"项目实战专题/","title":"六、项目实战专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/":{"url":"automation/","title":"七、自动化运维专题","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/git/":{"url":"automation/git/","title":"git","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/git/git-base-use.html":{"url":"automation/git/git-base-use.html","title":"1.Git基本概念与核心命令掌握","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.GIT体系概述 1.1GIT 与 svn 主要区别： 1.1.1存储方式区别 1.1.2使用方式区别 1.1.3版本管理模式区别 2.GIT 核心命令使用 2.1安装git客户端安装 2.2认识git的基本使用 快捷提交至本地仓库 2.3分支管理 2.4远程仓库管理 2.5tag 管理 2.6日志管理 3.git 底层原理 3.1GIT存储对像(hashMap) 3.2GIT树对像 3.3git提交对象 3.4GIT引用 4.基于 gogs 快速搭建企业私有 GIT 服务 4.1gogs 介绍安装 前台运行 后台运行 4.2gogs 基础配置 4.3gogs 定时备份与恢复 查看备份相关参数 默认备份,备份在当前目录 参数化备份 --target 输出目录 --database-only 只备份 db 恢复。执行该命令前要先删除 custom.bak 自动备份脚本 !/bin/sh -e 执行备份命令 查找并删除 7 天前的备份 添加定时任务 每天 4：00 执行备份 打开任务编辑器 输入如下命令 00 04 * * * 每天凌晨 4 点执行 do-backup.sh 并输出日志至 #backup.log 4、客户端公钥配置与添加 Git 安装完之后，需做最后一步配置。打开 git bash，分别执行以下两句命令 git 自动记住用户和密码操作 概要： GIT体系概述 GIT 核心命令使用 GIT 底层原理 基于 gogs 搭建 WEB 管理服务1.GIT体系概述 提问： 大家公司是用什么工具来管理代码版本？SVN、CVS、GIT GIT 和 SVN 有什么区别呢？ 1.1GIT 与 svn 主要区别： 存储方式不一样 使用方式不一样 管理模式不一样1.1.1存储方式区别 GIT 把内容按元数据方式存储类似k/v 数据库，而 SVN 是按文件(新版 svn 已改成元数据存储) 1.1.2使用方式区别 从本地把文件推送远程服务，SVN 只需要commint而 GIT 需要add、commint、push 三个步骤 SVN 基本使用过程 Git 基本使用过程 1.1.3**版本管理模式区别** git 是一个分布式的版本管理系统，而要 SVN 是一个远程集中式的管理系统 集中式 分布式 2.GIT 核心命令使用 主要内容: git 客户端安装配置 整体认识 GIT 的基本使用 分支管理 标签管理 远程仓库配置2.1安装git客户端安装 官方客户端： httpsd://git-scm.com/downloads 其它客户端： https://tortoisegit.org/download/ 2.2认识git的基本使用 git 项目创建与克隆 文件提交与推送 完整模拟从项目添加到 push 过程 创建项目 初始化 git 仓库 提交文件 远程关联 push 至远程仓库 本地初始化 GIT 仓库: 基于远程仓库克隆至本地 git clone 当前目录初始化为 git 本地仓库 git init 基于 mvn 模板创建项目 mvn archetype:generate 本地添加 添加指定文件至暂存区 git add 添加指定目录至暂存区 git add 添加所有 git add -A 将指定目录及子目录移除出暂存区 git rm --cached target -r 添加勿略配置文件 .gitignore 本地提交 提交至本地仓库 git commit file -m '提交评论' 快捷提交至本地仓库 git commit -am '快添加与提交' 2.3分支管理 查看当前分支 git branch [-avv] 基于当前分支新建分支 git branch 基于提交新建分支 git branch git branch -d {dev} 切换分支 git checkout 合并分支 git merge 解决冲突，如果因冲突导致自动合并失败，此时 status 为 mergeing 状态. 需要手动修改后重新提交（commit） 2.4远程仓库管理 查看远程配置 git remote [-v] 添加远程地址 git remote add origin http:xxx.xxx 删除远程地址 git remote remove origin 上传新分支至远程 git push --set-upstream origin master 将本地分支与远程建立关联 git branch --track --set-upstream-to=origin/test test 2.5tag 管理 查看当前 git tag 创建分支 git tag 删除分支 git tag -d 2.6日志管理 查看当前分支下所有提交日志 git log 查看当前分支下所有提交日志 git log {branch} 单行显示日志 git log --oneline 比较两个版本的区别 git log master..experiment 以图表的方式显示提交合并网络 git log --pretty=format:'%h %s' --graph 3.git 底层原理 GIT 存储对像 GIT 树对像 GIT 提交对像 GIT 引用3.1GIT存储对像(hashMap) Git 是一个内容寻址文件系统，其核心部分是一个简单的键值对数据库（key-value data store），你可以向数据库中插入任意内容，它会返回一个用于取回该值的 hash 键。 git 键值库中插入数据 echo 'luban is good man' | git hash-object -w --stdin 79362d07cf264f8078b489a47132afbc73f87b9a 基于键获取指定内容 git cat-file -p 79362d07cf264f8078b489a47132afbc73f87b9a Git 基于该功能 把每个文件的版本中内容都保存在数据库中，当要进行版本回滚的时候就通过其中一个键将期取回并替换。 模拟演示 git 版写入与回滚过程 查找所有的 git 对像 find .git/objects/ -type f 写入版本 1 echo 'version1' > README.MF; git hash-object -w README.MF; 写入版本 2 echo 'version2' > README.MF; git hash-object -w README.MF; 写入版本 3 echo 'version3' > README.MF; git hash-object -w README.MF; 回滚指定版本 git cat-file -p c11e96db44f7f3bc4c608aa7d7cd9ba4ab25066e > README.MF 所以我们平常用的 git add 其实就是把修改之后的内容 插入到键值库中。当我们执行git add README.MF等同于执行了git hash-object -w README.MF把文件写到数据库中。 我们解决了存储的问题，但其只能存储内容同并没有存储文件名，如果要进行回滚 怎么知道哪个内容对应哪个文件呢？接下要讲的就是树对象，它解决了文件名存储的问题 。 3.2GIT树对像 树对像解决了文件名的问题，它的目的将多个文件名组织在一起，其内包含多个文件名称与其对应的 Key 和其它树对像的用引用，可以理解成操作系统当中的文件夹，一个文件夹包含多个文件和多个其它文件夹。 每一个分支当中都关联了一个树对像，他存储了当前分支下所有的文件名及对应的 key. 通过以下命令即可查看 查看分支树 git cat-file -p master^{tree} 3.3git提交对象 一次提交即为当前版本的一个快照，该快照就是通过提交对像保存，其存储的内容为：一个顶级树对象、上一次提交的对像啥希、提交者用户名及邮箱、提交时间戳、提交评论。 git cat-file -p b2395925b5f1c12bf8cb9602f05fc8d580311836 tree 002adb8152f7cd49f400a0480ef2d4c09b060c07 parent 8be903f5e1046b851117a21cdc3c80bdcaf97570 author tommy 1532959457 +0800 committer tommy 1532959457 +0800 通过上面的知识，我们可以推测出从修改一个文件到提交的过程总共生成了三个对像： 一个内容对象 ==> 存储了文件内容 一个树对像 ==> 存储了文件名及内容对像的 key 一个提交对像 ==> 存储了树对像的 key 及提交评论。 演示文件提交过程3.4GIT引用 当我们执行 git branch {branchName} 时创建了一个分支，其本质就是在 git 基于指定提交创建了一个引用文件，保存在 .git\\refs\\heads\\ 下。 演示分支的创建 git branch dev cat.git\\refs\\heads\\dev git 总共 有三种类型的引用： 分支引用 远程分支引用 标签引用 查询比较两个版本 git log master..experiment 版本提交历史网络 git log --pretty=format:'%h %s' --graph 查看分支树 git cat-file -p master^{tree} 4.基于 gogs 快速搭建企业私有 GIT 服务 概要： gogs 介绍与安装 gogs 基础配置 gogs 定时备份与恢复 gitlab ==> 功能多一些 4.1gogs 介绍安装 Gogs 是一款开源的轻量级 Git web 服务，其特点是简单易用完档齐全、国际化做的相当不错。其主要功能如下: 提供 Http 与 ssh 两种协议访问源码服务 提供可 WEB 界面可查看修改源码代码 提供较完善的权限管理功能、其中包括组织、团队、个人等仓库权限 提供简单的项目 viki 功能 提供工单管理与里程碑管理。 下载安装 官网：https://gogs.io 下载：https://gogs.io/docs/installation选择 linx amd64 下载安装 文档：https://gogs.io/docs/installation/install_from_binary 安装： 解压之后目录： 运行： 前台运行 ./gogs web 后台运行 $nohup ./gogs web & 默认端口：3000 初次访问 http://:3000 会进到初始化页,进行引导配置。 可选择 mysql 或 sqlite 等数据。这里选的是 sqllite 注：mysql 索引长度的问题没有安装成功,需要用 mysql5.7 以上版本 4.2gogs 基础配置 邮件配置说明： 邮件配置是用于注册时邮件确认，和找回密码时候的验证邮件发送。其配置分为两步： 第一：创建一个开通了 smtp 服务的邮箱帐号，一般用公司管理员邮箱。我这里用的是 QQ 邮箱。 第二：在{gogs_home/custom/conf/app.ini 文件中配置。 QQ 邮箱开通 smtp 服务 1、点击设置 2、开启 smtp 邮件设置 设置文件：{gogs_home/custom/conf/app.ini ENABLED = true HOST=smtp.qq.com:465 FROM=tuling2877438881@qq.com USER= PASSWD= ENABLED=true 表示启用邮件服务 host为 smtp 服务器地址，（需要对应邮箱开通 smtp 服务 且必须为 ssl 的形式访问） from发送人名称地址 user发送帐号 passwd开通 smtp 帐户时会有对应的授权码 重启后可直接测试 管理员登录==》控制面版==》应用配置管理==》邮件配置==》发送测试邮件 4.3gogs 定时备份与恢复 备份与恢复： 查看备份相关参数 ./gogs backup -h 默认备份,备份在当前目录 ./gogs backup 参数化备份 --target 输出目录 --database-only 只备份 db ./gogs backup --target=./backupes --database-only --exclude-repos 恢复。执行该命令前要先删除 custom.bak ./gogs restore --from=gogs-backup-20180411062712.zip 自动备份脚本 !/bin/sh -e gogs_home=\"/home/apps/svr/gogs/\" backup_dir=\"$gogs_home/backups\" cd dirname $0 执行备份命令 ./gogs backup --target=$backup_dir echo 'backup sucess' day=7 查找并删除 7 天前的备份 find $backup_dir -name '*.zip' -mtime +7 -type f |xargs rm -f; echo 'delete expire back data!' 添加定时任务 每天 4：00 执行备份 打开任务编辑器 crontab -e 输入如下命令 00 04 * 每天凌晨 4 点执行 do-backup.sh 并输出日志至 #backup.log 00 04 * /home/apps/svr/gogs/do-backup.sh >> /home/apps/svr/gogs/backup.log 2>&1 4、客户端公钥配置与添加 Git 配置 Git 安装完之后，需做最后一步配置。打开 git bash，分别执行以下两句命令 git config --global user.name “用户名” git config --global user.email “邮箱” git 自动记住用户和密码操作 git config --global credential.helper store SSH 公钥创建 1、打开 git bash 2、执行生成公钥和私钥的命令：ssh-keygen -t rsa 并按回车 3 下 3、执行查看公钥的命令：cat ~/.ssh/id_rsa.pub 4、拷贝 id_rsa.pub 内容至至服务~~/.ssh/authorized_keys 中 Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:51:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/maven/":{"url":"automation/maven/","title":"maven","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/maven/maven-base-use.html":{"url":"automation/maven/maven-base-use.html","title":"1.Maven基本概念与核心配置 ","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1.maven 安装与核心概念 1.1安装： 1.2maven 编译 1.3Maven打包 1.4maven 单元测试演示 创建测试目录 编写 测试类 测试类代码------------------------ 执行测试 1.5maven 依赖管理 2.maven核心配置 项目依懒 **2.1依赖传播特性: 2.2依赖优先原则 2.3可选依赖 2.4排除依赖 2.5依赖范围 手动加入本地仓库 项目聚合与继承 1、聚合 2、继承 3、依赖管理 项目构建配置 3.maven 生命周期 知识点概要： 3.1生命周期的概念与意义 执行清理 phase 执行 compile phase 也可以同时执行 清理加编译 3.2maven 三大生命周期与其对应的 phase(阶段) 执行编译 执行打包就包含了编译指令的执行 3.3生命周期与插件的关系 3.4生命周期与插件的默认绑定 mvn compile 直接执行 compile 插件目标 4.maven 自定义插件开发 知识点： 4.1maven 插件相关概念 将插件依赖拷贝到指定目录 4.2常用插件的使用 展示 pom 的依赖关系树 也可以直接简化版的命令，但前提必须是 maven 官方插件 查看 pom 文件的最终配置 原型项目生成 快速创建一个 WEB 程序 快速创建一个 java 项目 4.3开发一个自定义插件 5.nexus 私服搭建与核心功能 知识点概要: 5.1私服使用场景 5.2nexus 下载安装 解压 在环境变量当中设置启动用户 添加 profile 文件。安全起见不建议使用 root 用户，如果使用其它用户需要加相应权限 端口号 启动 停止 5.3nexus 仓库介绍 5.4本地远程仓库配置 5.5发布项目至 nexus 远程仓库 概要： maven 基本概念 maven 核心配置 maven 生命周期 Maven 自定义插件开发 基于 nexus 构建企业私服1.maven 安装与核心概念 概要： maven 安装 maven 编译(compile) 执行测试用例(test) maven 打包 maven 依懒管理 1.1安装： 官网下载 Maven （http://maven.apache.org/download.cgi） 解压指定目录 配置环境变量 检查安装是否成功 （mvn -version） maven 是什么？它的基本功能是什么？编译、打包、测试、依赖管理直观感受一下 maven 编译打包的过程。 1.2maven 编译 maven 编译过程演示 创建 maven 项目。 创建 src 文件 编写 pom 文件 执行编译命令 编写 pom 文件基础配置 4.0.0 org.codehaus.mojo my-project 1.0.SNAPSHOT mvn 编译命令 mvn compile [INFO] No sources to compile [INFO] --------------------------------------------------------------- [INFO] BUILD SUCCESS [INFO] --------------------------------------------------------------- [INFO] Total time: 0.473 s [INFO] Finished at: 2018-08-05T15:55:44+08:00 [INFO] Final Memory: 6M/153M [INFO] --------------------------------------------------------------- 请注意，在上述配置和命令当中，我们并没有指定源码文件在哪里？最后编译到哪里去？在这里 maven 采用了约定的方式从指项目结构中获取源码与资源文件进行编译打包。 1. 主源码文件：${project}/src/main/java 2. 主资源文件：${project}/src/main/resources 3. 测试源码文件：${project}/src/test/java 4. 测试资源文件：${project}/src/test/resources 将 java 文件移至 src/main/java 目录，重新执行编译. mv src/hello.java /src/main/java/hello.java mvn compile; 1.3Maven打包 maven 打包演示 mvn 打包命令 mvn package 1.4maven 单元测试演示 编写测试类 执行测试命令 编译测试类 创建测试目录 mkdir -p /src/test/java 编写 测试类 vim TestHello.java 测试类代码------------------------ package com.test.tuling; public class TestHello{ public void sayHelloTest(){ System.out.println(\"run test .....\"); } } 执行测试指令: 执行测试 mvn test 执行完指令发现没有执行我们的测试方法，这是为何？原因在于 maven 当中的测试类又做了约定，约定必须是 Test 开头的类名与 test 开头的方法才会执行。 重新修改方法名后 在执行 mvn test 即可正常执行。 package com.test.tuling; public class TestHello{ public void testsayHelloTest(){ System.out.println(\"run test .....\"); } } 通常测试我们是通过 junit 来编译测试用例，这时就就需添加 junit 的依赖。 1.5maven 依赖管理 在 pom 文件中添加 junit 依赖 修改测试类，加入 junit 代码 执行测试命令 加入依懒配置 junit junit 4.0 test 修改测试类引入 junit 类. //引入 junit 类 import org.junit.Assert; import org.junit.Test; Assert.assertEquals(\"\",\"hi\"); 注意：当我们在 classPath 当中加入 junit，原来以 test 开头的方法不会被执行，必须加入 @Test 注解才能被执行。 提问： 在刚才的演示过程当中 ，junit jar 包在哪里？是怎么加入到 classPath 当中去的？maven 是在执行 test 命令的时间 动态从本地仓库中去引入 junit jar 包，如果找不到就会去远程仓库下载，然后在引入。 默认远程仓库： 默认远程仓库 maven central 其配置在 maven-model-builder-3.2.1.jar\\org\\apache\\maven\\model\\pom-4.0.0.xml 位置 本地仓库位置： 本地仓库位置默认在 ~/.m2/respository 下 要修改${M2_HOME}/conf/settings.xml 来指定仓库目录 G:\\.m2\\repository maven 核心功能总结： maven 核心作用是编译、测试、打包。 根目录下的 pom.xml 文件设置分组 ID 与 artifactId。 maven 基于约定的方式从项目中获取源码与资源文件进行编译打包。 对于项目所依懒的组件与会本地仓库引用，如果本地仓库不存在则会从中央仓库下载。2.maven核心配置 概要： 项目依懒(内部、外部) 项目聚合与继承 项目构建配置项目依懒 项目依赖是指 maven 通过依赖传播、依赖优先原则、可选依赖、排除依赖、依赖范围等特性来管理项目ClassPath。 **2.1依赖传播特性: 我们的项目通常需要依赖第三方组件，而第三方组件又会依赖其它组件遇到这种情况 Maven 会将依赖网络中的所有节点都会加入 ClassPath 当中，这就是 Maven 的依赖传播特性。 * 举例演示 Spring MVC 的依赖网络 org.springframework spring-webmvc 4.0.4.RELEASE 在刚刚的演示当中，项目直接依赖了 spring-webmvc 叫直接依赖，而对 commons-logging 依赖是通过 webmvc 传递的所以叫间接依赖。 2.2依赖优先原则 基于依赖传播特性，导致整个依赖网络会很复杂，难免会出现相同组件不同版本的情况。Maven 此时会基于依赖优先原则选择其中一个版本。 第一原则：最短路径优先。 第二原则：相同路径下配置在前的优先。 * 第一原则演示 commons-logging commons-logging 1.2 上述例子中 commons-logging 通过 spring-webmvc 依赖了 1.1.3，而项目中直接依赖了 1.2，基于最短路径原则项目最终引入的是 1.2 版本。 * 第二原则演示： 步骤： 添加一个新工程 Project B 配置 Project B 依赖 spring-web.3.2.9.RELEASE 当前工程直接依赖 Project B 配置完之后，当前工程 project A 有两条路径可以依赖 spring-web,选择哪一条 就取决于 对 webmvc 和 Project B 的配置先后顺序。 Project A==> spring-webmvc.4.0.0.RELEASE ==>spring-web.4.0.0.RELEASE Project A==> Project B 1.0.SNAPSHOT ==>spring-web.3.2.9.RELEASE 注意：在同一 pom 文件，第二原则不在适应。如下配置，最终引用的是 1.2 版本，而不是配置在前面的 1.1.1 版本. commons-logging commons-logging 1.1.1 commons-logging commons-logging 1.2 2.3可选依赖 可选依赖表示这个依赖不是必须的。通过在 添 true 表示，默认是不可选的。可选依赖不会被传递。 演示可选依赖的效果。2.4排除依赖 即排除指定的间接依赖。通过配置 配置排除指定组件。 org.springframework spring-web 演示排除依赖2.5依赖范围 像 junit 这个组件 我们只有在运行测试用例的时候去要用到，这就没有必要在打包的时候把 junit.jar 包过构建进去，可以通过 Mave 的依赖范围配置来达到这种目的。maven 总共支持以下四种依赖范围： compile(默认): 编译范围，编译和打包都会依赖。 provided：提供范围，编译时依赖，但不会打包进去。如：servlet-api.jar runtime：运行时范围，打包时依赖，编译不会。如：mysql-connector-java.jar test：测试范围，编译运行测试用例依赖，不会打包进去。如：junit.jar system：表示由系统中 CLASSPATH 指定。编译时依赖，不会打包进去。配合 一起使用。示例：java.home 下的 tool.jar system 除了可以用于引入系统 classpath 中包，也可以用于引入系统非 maven 收录的第三方 Jar，做法是将第三方 Jar 放置在 项目的 lib 目录下，然后配置 相对路径，但因 system 不会打包进去所以需要配合 maven-dependency-plugin 插件配合使用。当然推荐大家还是通过 将第三方 Jar 手动 install 到仓库。 com.sun tools ${java.version} system true ${java.home}/../lib/tools.jar jsr jsr 3.5 system true ${basedir}/lib/jsr305.jar org.apache.maven.plugins maven-dependency-plugin 2.10 copy-dependencies compile copy-dependencies ${project.build.directory}/${project.build.finalName}/WEB-INF/lib system com.sun 手动加入本地仓库 mvn install:install-file -Dfile=abc_client_v1.20.jar -DgroupId=tuling -DartifactId=tuling-client -Dversion=1.20 -Dpackaging=jar 项目聚合与继承 1、聚合 是指将多个模块整合在一起，统一构建，避免一个一个的构建。聚合需要个父工程，然后使用 进行配置其中对应的是子工程的相对路径 tuling-client tuling-server * 演示聚合的配置 2、继承 继承是指子工程直接继承父工程 当中的属性、依赖、插件等配置，避免重复配置。 属性继承： 依赖继承： 插件继承： 上面的三个配置子工程都可以进行重写，重写之后以子工程的为准。 3、依赖管理 通过继承的特性，子工程是可以间接依赖父工程的依赖，但多个子工程依赖有时并不一至，这时就可以在父工程中加入 声明该功程需要的 JAR 包，然后在子工程中引入。 junit junit 4.12 junit junit 4、项目属性： 通过 配置 属性参数，可以简化配置。 ddd ${proName} maven 默认的属性 ${basedir} 项目根目录 ${version}表示项目版本; ${project.basedir}同${basedir}; ${project.version}表示项目版本,与${version}相同; ${project.build.directory} 构建目录，缺省为 target ${project.build.sourceEncoding}表示主源码的编码格式; ${project.build.sourceDirectory}表示主源码路径; ${project.build.finalName}表示输出文件名称; ${project.build.outputDirectory} 构建过程输出目录，缺省为 target/classes 项目构建配置 构建资源配置 编译插件 profile 指定编译环境 构建资源配置 基本配置示例： package ${basedir}/target2 ${artifactId}-${version} 说明： defaultGoal，执行构建时默认的 goal 或 phase，如 jar:jar 或者 package 等 directory，构建的结果所在的路径，默认为${basedir}/target 目录 finalName，构建的最终结果的名字，该名字可能在其他 plugin 中被改变 配置示例 src/main/java **/*.MF **/*.XML true src/main/resources **/* * true 说明： resources，build 过程中涉及的资源文件 targetPath，资源文件的目标路径 directory，资源文件的路径，默认位于${basedir}/src/main/resources/目录下 includes，一组文件名的匹配模式，被匹配的资源文件将被构建过程处理 excludes，一组文件名的匹配模式，被匹配的资源文件将被构建过程忽略。同时被 includes 和 excludes 匹配的资源文件，将被忽略。 filtering： 默认 false，true 表示 通过参数 对 资源文件中 的${key} 在编译时进行动态变更。替换源可 紧 -Dkey 和 pom 中的 值 或 中指定的 properties 文件。3.maven 生命周期 知识点概要： 生命周期的概念与意义 maven 三大生命周期与其对应的 phase(阶段) 生命周期与插件的关系 生命周期与默认插件的绑定3.1生命周期的概念与意义 在项目构建时通常会包含清理、编译、测试、打包、验证、部署，文档生成等步骤，maven 统一对其进行了整理抽像成三个生命周期 (lifecycle)及各自对应的多个阶段(phase)。这么做的意义是： 每个阶段都成为了一个扩展点，可以采用不同的方式来实现，提高了扩展性与灵活性。 规范统一了 maven 的执行路径。 在执行项目构建阶段时可以采用 jar 方式构建也可以采用 war 包方式构建提高了灵活性。我们可以通过命令 mvn ${phase name}直接触发指定阶段的执行如： 演示 phase 的执行 执行清理 phase mvn clean 执行 compile phase mvn compile 也可以同时执行 清理加编译 mvn clean comile 3.2maven 三大生命周期与其对应的 phase(阶段) maven 总共包含三大生生命周期 clean Lifecycle：清理生命周期，用于于清理项目 default Lifecycle：默认生命周期，用于编译、打包、测试、部署等 site Lifecycle站点文档生成，用于构建站点文档 |生命周期(lifecycle)|阶段(phase)|描述(describe)| |:----|:----|:----|:----|:----|:----| |clean Lifecycle|pre-clean|预清理| | |clean|清理| | |post-clean|清理之后| |default Lifecycle|validate|验证| | |initialize|初始化| | |generate-sources| | | |process-sources| | | |generate-resources| | | |process-resources| | | |compile|编译| | |process-classes| | | |generate-test-sources| | | |process-test-sources| | | |generate-test-resources| | | |process-test-resources| | | |test-compile|编译测试类| | |process-test-classes| | | |test|执行测试| | |prepare-package|构建前准备| | |package|打包构建| | |pre-integration-test| | | |integration-test| | | |post-integration-test| | | |verify|验证| | |install|上传到本地仓库| | |deploy|上传到远程仓库| |site Lifecycle|pre-site|准备构建站点| | |site|构建站点| | |post-site|构建站点之后| | |site-deploy|站点部署| 三大生命周期其相互独立执行，也可以合在一起执行。但 lifecycle 中的 phase 是有严格执行的顺序的，比如必须是先执行完 compile 才能执行 pakcage 动作，此外 phase 还有包含逻辑存在，即当你执行一个 phase 时 其前面的 phase 会自动执行。 演示 phase 执行 执行编译 mvn compile 执行打包就包含了编译指令的执行 mvn package 3.3生命周期与插件的关系 生命周期的 phase 组成了项目过建的完整过程，但这些过程具体由谁来实现呢？这就是插件，maven 的核心部分代码量其实很少，其大部分实现都是由插件来完成的。比如：test 阶段就是由maven-surefire-plugin 实现。在 pom.xml 中我们可以设置指定插件目标(gogal)与 phase 绑定，当项目构建到达指定 phase 时就会触发些插件 gogal 的执行。 一个插件有时会实现多个 phas 比如：maven-compiler-plugin插件分别实现了 compile 和 testCompile。 总结： 生命周期的 阶段 可以绑定具体的插件及目标 不同配置下同一个阶段可以对应多个插件和目标 phase==>plugin==>goal(功能) 3.4生命周期与插件的默认绑定 在我们的项目当中并没有配置 maven-compiler-plugin 插件,但当我们执行 compile 阶段时一样能够执行编译操作，原因是 maven 默认为指定阶段绑定了插件实现。列如下以下两个操作在一定程度上是等价的。 演示 # mvn compile 直接执行 compile 插件目标 mvn org.apache.maven.plugins:maven-compiler-plugin:3.1:compile lifecycle phase 的默认绑定见下表：。 clean Lifecycle 默认绑定 pre-clean clean post-clean org.apache.maven.plugins:maven-clean-plugin:2.5:clean site Lifecycle 默认绑定 pre-site site post-site site-deploy org.apache.maven.plugins:maven-site-plugin:3.3:site org.apache.maven.plugins:maven-site-plugin:3.3:deploy Default Lifecycle JAR 默认绑定 注：不同的项目类型 其默认绑定是不同的，这里只指列举了 packaging 为 jar 的默认绑定，全部的默认绑定参见：https://maven.apache.org/ref/3.5.4/maven-core/default-bindings.html#。 org.apache.maven.plugins:maven-resources-plugin:2.6:resources org.apache.maven.plugins:maven-compiler-plugin:3.1:compile org.apache.maven.plugins:maven-resources-plugin:2.6:testResources org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test org.apache.maven.plugins:maven-jar-plugin:2.4:jar org.apache.maven.plugins:maven-install-plugin:2.4:install org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy 4.maven 自定义插件开发 知识点： 插件的相关概念 常用插件的使用 开发一个自定义插件4.1maven 插件相关概念 插件坐标定位： 插件与普通 jar 包一样包含 一组件坐标定位属性即： groupId、artifactId、version，当使用该插件时会从本地仓库中搜索，如果没有即从远程仓库下载 org.apache.maven.plugins maven-dependency-plugin 2.10 插件执行 execution： execution 配置包含一组指示插件如何执行的属性： id： 执行器命名 phase：在什么阶段执行？ goals：执行一组什么目标或功能？ configuration：执行目标所需的配置文件？ 演示一个插件的配置与使用 将插件依赖拷贝到指定目录 org.apache.maven.plugins maven-dependency-plugin 3.1.1 copy-dependencies package copy-dependencies ${project.build.directory}/alternateLocation false true true 4.2常用插件的使用 除了通过配置的方式使用插件以外，Maven 也提供了通过命令直接调用插件目标其命令格式如下： mvn groupId:artifactId:version:goal -D{参数名} 演示通过命令执行插件 展示 pom 的依赖关系树 mvn org.apache.maven.plugins:maven-dependency-plugin:2.10:tree 也可以直接简化版的命令，但前提必须是 maven 官方插件 mvn dependency:tree 其它常用插件： 查看 pom 文件的最终配置 mvn help:effective-pom 原型项目生成 archetype:generate 快速创建一个 WEB 程序 mvn archetype:generate -DgroupId=tuling -DartifactId=simple-webbapp -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=false 快速创建一个 java 项目 mvn archetype:generate -DgroupId=tuling -DartifactId=simple-java -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false 4.3开发一个自定义插件 实现步骤： * 创建 maven 插件项目 * 设定 packaging 为 maven-plugin * 添加插件依赖 * 编写插件实现逻辑 * 打包构建插件 插件 pom 配置 4.0.0 tuling 1.0.SNAPSHOT tuling-maven-plugin maven-plugin org.apache.maven maven-plugin-api 3.0 org.apache.maven.plugin-tools maven-plugin-annotations 3.4 插件实现类： package com.tuling.maven; import javafx.beans.DefaultProperty; import org.apache.maven.plugin.AbstractMojo; import org.apache.maven.plugin.MojoExecutionException; import org.apache.maven.plugin.MojoFailureException; import org.apache.maven.plugins.annotations.LifecyclePhase; import org.apache.maven.plugins.annotations.Mojo; import org.apache.maven.plugins.annotations.Parameter; /** @author Tommy Created by Tommy on 2018/8/8 **/ @Mojo(name = \"luban\") public class LubanPlugin extends AbstractMojo { @Parameter String sex; @Parameter String describe; public void execute() throws MojoExecutionException, MojoFailureException { getLog().info(String.format(\"luban sex=%s describe=%s\",sex,describe)); } } 5.nexus 私服搭建与核心功能 知识点概要: 私服的使用场景 nexus 下载安装 nexus 仓库介绍 本地远程仓库配置 发布项目至 nexus 远程仓库 关于 SNAPSHOT(快照)与 RELEASE(释放) 版本说明5.1私服使用场景 私服使用场景如下： 1、公司不能连接公网，可以用一个私服务来统一连接 2、公司内部 jar 组件的共享 5.2nexus 下载安装 nexus 下载地址： https://sonatype-download.global.ssl.fastly.net/nexus/oss/nexus-2.14.5-02-bundle.tar.gz 解压并设置环境变量 解压 shell>tar -zxvf nexus-2.14.5-02-bundle.tar.gz 在环境变量当中设置启动用户 shell> vim /etc/profile 添加 profile 文件。安全起见不建议使用 root 用户，如果使用其它用户需要加相应权限 export RUN_AS_USER=root 配置启动参数： shell> vi ${nexusBase}/conf/nexus.properties 端口号 application-port=9999 启动与停止 nexus 启动 shell> ${nexusBase}/bin/nexus start 停止 shell> ${nexusBase}/bin/nexus stop 登录 nexus 界面 地址：http://{ip}:9999/nexus/ 用户名:admin 密码：admin123 5.3nexus 仓库介绍 3rd party：第三方仓库 Apache Snapshots：apache 快照仓库 Central: maven 中央仓库 Releases：私有发布版本仓库 Snapshots：私有 快照版本仓库 5.4本地远程仓库配置 在 pom 中配置远程仓库 nexus-public my nexus repository http://192.168.0.147:9999/nexus/content/groups/public/ 或者在 settings.xml 文件中配置远程仓库镜像 效果一样，但作用范围广了 nexus-aliyun * Nexus aliyun http://192.168.0.147:9999/nexus/content/groups/public/ 5.5发布项目至 nexus 远程仓库 配置仓库地址 nexus-release nexus release http://192.168.0.147:9999/nexus/content/repositories/releases/ nexus-snapshot nexus snapshot http://192.168.0.147:9999/nexus/content/repositories/snapshots/ 设置 setting.xml 中设置 server nexus-snapshot deployment deployment123 nexus-release deployment deployment123 执行 deploy 命令 mvn deploy Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:47:10 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/jenkins/":{"url":"automation/jenkins/","title":"jenkins","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/":{"url":"automation/k8s/","title":"k8s","keywords":"","body":"Introduction Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-13 09:07:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/kubeadm-install-one.html":{"url":"automation/k8s/kubeadm-install-one.html","title":"1.快速部署一个K8s集群","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 安装要求 2. 准备环境 3. 所有节点安装 Docker/kubeadm/kubelet 3.1 安装 Docker 3.2 添加阿里云 YUM 软件源 3.3 安装 kubeadm，kubelet 和 kubectl 4. 部署 Kubernetes Master 5. 加入 Kubernetes Node 6. 部署 CNI 网络插件 7. 测试 kubernetes 集群 kubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具。 这个工具能通过两条指令完成一个 kubernetes 集群的部署： # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join 1. 安装要求 在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点 禁止 swap 分区2. 准备环境 角色 IP master 192.168.62.132 node1 192.168.62.133 node2 192.168.62.134 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭selinux sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭swap swapoff -a # 临时 sed -ri 's/.*swap.*/#&/' /etc/fstab # 永久 # 根据规划设置主机名 hostnamectl set-hostname # 在master添加hosts cat >> /etc/hosts /etc/sysctl.d/k8s.conf 3. 所有节点安装 Docker/kubeadm/kubelet Kubernetes 默认 CRI（容器运行时）为 Docker，因此先安装 Docker。 3.1 安装 Docker $ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ yum -y install docker-ce-18.06.1.ce-3.el7 $ systemctl enable docker && systemctl start docker $ docker --version Docker version 18.06.1-ce, build e68fc7a $ cat > /etc/docker/daemon.json 3.2 添加阿里云 YUM 软件源 $ cat > /etc/yum.repos.d/kubernetes.repo 3.3 安装 kubeadm，kubelet 和 kubectl 由于版本更新频繁，这里指定版本号部署： $ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 $ systemctl enable kubelet 4. 部署 Kubernetes Master 在 192.168.62.132（Master）执行。 $ kubeadm init \\ --apiserver-advertise-address=192.168.62.132 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.18.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 由于默认拉取镜像地址 k8s.gcr.io 国内无法访问，这里指定阿里云镜像仓库地址。 使用 kubectl 工具： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes 5. 加入 Kubernetes Node 在 192.168.62.133/134（Node）执行。 向集群添加新节点，执行在 kubeadm init 输出的 kubeadm join 命令： $ kubeadm join 192.168.62.132:6443 --token dcxghk.5zgiiw6yk7qf5wol \\ --discovery-token-ca-cert-hash sha256:aad826e486e6728e176b14e803199a42805572ed8b266269d7581f1e244df33c 默认 token 有效期为 24 小时，当过期之后，该 token 就不可用了。这时就需要重新创建 token，操作如下： kubeadm token create --print-join-command 6. 部署 CNI 网络插件 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 默认镜像地址无法访问，sed 命令修改为 docker hub 镜像仓库。 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 或者用啊里的源 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-aliyun.yml kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE kube-flannel-ds-amd64-2pc95 1/1 Running 0 72s 7. 测试 kubernetes 集群 在 Kubernetes 集群中创建一个 pod，验证是否正常运行： $ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 --type=NodePort $ kubectl get pod,svc 访问地址：http://NodeIP:Port Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:40:07 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"automation/k8s/kubeadm-install-all.html":{"url":"automation/k8s/kubeadm-install-all.html","title":"2搭建高可用的K8s集群","keywords":"","body":"TreeviewCopyright © aleen42 all right reserved, powered by aleen42 1. 安装要求 2. 准备环境 3. 所有 master 节点部署 keepalived 3.1 安装相关包和 keepalived 3.2 配置 master 节点 3.3 启动和检查 4. 部署 haproxy 4.1 安装 4.2 配置 4.3 启动和检查 5. 所有节点安装 Docker/kubeadm/kubelet 5.1 安装 Docker 5.2 添加阿里云 YUM 软件源 5.3 安装 kubeadm，kubelet 和 kubectl 6. 部署 Kubernetes Master 6.1 创建 kubeadm 配置文件 6.2 在 master1 节点执行 7.安装集群网络 8、master2 节点加入集群 8.1 复制密钥及相关文件 8.2 master2 加入集群 5. 加入 Kubernetes Node 7. 测试 kubernetes 集群 kubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具。 这个工具能通过两条指令完成一个 kubernetes 集群的部署： # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join 1. 安装要求 在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点 禁止 swap 分区2. 准备环境 角色 IP master1 192.168.44.155 master2 192.168.44.156 node1 192.168.44.157 VIP（虚拟 ip） 192.168.44.158 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭 selinux sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭 swap swapoff -a # 临时 sed -ri 's/.*swap.*/#&/' /etc/fstab # 永久 # 根据规划设置主机名 hostnamectl set-hostname # 在 master 添加 hosts cat >> /etc/hosts /etc/sysctl.d/k8s.conf 3. 所有 master 节点部署 keepalived 3.1 安装相关包和 keepalived yum install -y conntrack-tools libseccomp libtool-ltdl yum install -y keepalived 3.2 配置 master 节点 master1 节点配置 cat > /etc/keepalived/keepalived.conf master2 节点配置 cat > /etc/keepalived/keepalived.conf 3.3 启动和检查 在两台 master 节点都执行 # 启动 keepalived $ systemctl start keepalived.service 设置开机启动 $ systemctl enable keepalived.service # 查看启动状态 $ systemctl status keepalived.service 启动后查看 master1 的网卡信息 ip a s ens33 4. 部署 haproxy 4.1 安装 yum install -y haproxy 4.2 配置 两台 master 节点的配置均相同，配置中声明了后端代理的两个 master 节点服务器，指定了 haproxy 运行的端口为 16443 等，因此 16443 端口为集群的入口 cat > /etc/haproxy/haproxy.cfg 4.3 启动和检查 两台 master 都启动 # 设置开机启动 $ systemctl enable haproxy # 开启 haproxy $ systemctl start haproxy # 查看启动状态 $ systemctl status haproxy 检查端口 netstat -lntup|grep haproxy 5. 所有节点安装 Docker/kubeadm/kubelet Kubernetes 默认 CRI（容器运行时）为 Docker，因此先安装 Docker。 5.1 安装 Docker $ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ yum -y install docker-ce-18.06.1.ce-3.el7 $ systemctl enable docker && systemctl start docker $ docker --version Docker version 18.06.1-ce, build e68fc7a $ cat > /etc/docker/daemon.json 5.2 添加阿里云 YUM 软件源 $ cat > /etc/yum.repos.d/kubernetes.repo 5.3 安装 kubeadm，kubelet 和 kubectl 由于版本更新频繁，这里指定版本号部署： $ yum install -y kubelet-1.16.3 kubeadm-1.16.3 kubectl-1.16.3 $ systemctl enable kubelet 6. 部署 Kubernetes Master 6.1 创建 kubeadm 配置文件 在具有 vip 的 master 上操作，这里为 master1 $ mkdir /usr/local/kubernetes/manifests -p $ cd /usr/local/kubernetes/manifests/ $ vi kubeadm-config.yaml apiServer: certSANs: - master1 - master2 - master.k8s.io - 192.168.44.158 - 192.168.44.155 - 192.168.44.156 - 127.0.0.1 extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta1 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: \"master.k8s.io:16443\" controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.16.3 networking: dnsDomain: cluster.local podSubnet: 10.244.0.0/16 serviceSubnet: 10.1.0.0/16 scheduler: {} 6.2 在 master1 节点执行 $ kubeadm init --config kubeadm-config.yaml 按照提示配置环境变量，使用 kubectl 工具： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get nodes $ kubectl get pods -n kube-system 按照提示保存以下内容，一会要使用： kubeadm join master.k8s.io:16443 --token jv5z7n.3y1zi95p952y9p65 \\ --discovery-token-ca-cert-hash sha256:403bca185c2f3a4791685013499e7ce58f9848e2213e27194b75a2e3293d8812 \\ --control-plane 查看集群状态 kubectl get cs kubectl get pods -n kube-system 7.安装集群网络 从官方地址获取到 flannel 的 yaml，在 master1 上执行 mkdir flannel cd flannel wget -c https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装 flannel 网络 kubectl apply -f kube-flannel.yml 检查 kubectl get pods -n kube-system 8、master2 节点加入集群 8.1 复制密钥及相关文件 从 master1 复制密钥及相关文件到 master2 # ssh root@192.168.44.156 mkdir -p /etc/kubernetes/pki/etcd # scp /etc/kubernetes/admin.conf root@192.168.44.156:/etc/kubernetes # scp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*} root@192.168.44.156:/etc/kubernetes/pki # scp /etc/kubernetes/pki/etcd/ca.* root@192.168.44.156:/etc/kubernetes/pki/etcd 8.2 master2 加入集群 执行在 master1 上 init 后输出的 join 命令,需要带上参数--control-plane表示把 master 控制节点加入集群 kubeadm join master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba --control-plane 检查状态 kubectl get node kubectl get pods --all-namespaces 5. 加入 Kubernetes Node 在 node1 上执行 向集群添加新节点，执行在 kubeadm init 输出的 kubeadm join 命令： kubeadm join master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba 集群网络重新安装，因为添加了新的 node 节点 检查状态 kubectl get node kubectl get pods --all-namespaces 7. 测试 kubernetes 集群 在 Kubernetes 集群中创建一个 pod，验证是否正常运行： $ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 --type=NodePort $ kubectl get pod,svc 访问地址：http://NodeIP:Port Copyright © ohter.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2021-04-19 13:39:43 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}